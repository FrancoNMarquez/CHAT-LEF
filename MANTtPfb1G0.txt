[Música] Esta es la segunda parte de la clase número 25 te invito a empezar con [Música] ella [Música] el siguiente tema que tenemos que ver es la tokenización y vamos rápidamente a tratar de entender Qué es la tokenización la tokenización es dividir un texto en tokens los tokens son las unidades individuales de un texto por lo general eh van a ser palabras pero en realidad la tokenización puede dividir el texto en otras unidades que no sean palabras aquí tengo un ejemplo basado en python donde tengo un texto Hola mundo coma Buenos días dentro de una variable que se llama texto y luego lo que hago justamente es aplicar el método split que ustedes ya lo conocen por python que lo que hace es dividir justamente un texto en palabras y lo coloca dentro de una variable que le he dado llamar tokens y luego hago un PR de tokens con lo cual lo que veo es que me sale la palabra que tiene perdón la frase que tiene tiene cuatro palabras con cada una de sus palabras en una Ray individualmente identificadas fíjense que hay algo muy importante aquí que en el caso de olola y en el caso de mundo hay una diferencia mundo tiene una coma y eso lo toma como parte de la palabra y Díaz que es la frase final tiene dos signos de exclamación que también los toma como parte de la palabra Bueno eso es parte de lo que vamos a tener que empezar a ver más adelante de si estas estas estos separadores o estas expresiones que no conforman una palabra del abecedario las voy a querer juntar con la palabra las voy a poder tratarlas como un token aparte o simplemente voy a querer prescindir de ellas que ya vamos a ver más adelante que en realidad la interpretación que se hace a través del nlp muchas veces puede prescindir de estos signos sin perder calidad de [Música] interpretación como dijimos recién en la evolución de la toca haciendo un análisis mucho más fino del proceso tenemos formas diferentes de tokenizar en principio basado en palabras lo más tradicional lo más común auto moto bueno el elemento que sea representativo de una palabra de un texto basada en caracteres puedo tomar letras o números o expresiones como la roba o lo que debos recién una coma o un signo de admiración o basado en su palabras Es decir por aquí tengo una palabra que está conformada por otras dos o que se puede subdividir por el sentido que tienen otras dos contraataque en contra o ataque bien consideraciones a tener presente un poco lo que repasamos recién diferentes casos de letra es decir tenemos letras que tienen caracteres complicados como puede ser un tilde una diéresis depende del lenguaje que se trate y justamente lo que tengo que tomar como criterio es si voy a utilizar eso o no en el proceso de aproximación que haga hacia interpretar el texto que viene a mí que yo transformo en un en un vector o en una estructura de números por eso también es importante tener en cuenta lo que dijimos recién la forma de tocan ia para ver si tomo los elementos o no de manera individual o como parte colectiva o parte de la expresión en cada palabra donde está eh vinculado ese ese carácter especial y justamente la puntuación ver si la voy a tener en consideración o no de acuerdo al sentido que yo busque de el tipo de inteligencia que aplique para la interpretación que haga en algunos casos ustedes ya van a ver más adelante que se puede prescindir de una coma se puede prescindir de un punto o no y en algunos casos el tomar la opción de hacerlo no nos va a hacer peligrar la la eficiencia como decía hace un rato de la interpretación del texto Obviamente que todo esto que estamos hablando no hace otra cosa que como dijimos hace un rato generar información transformada de texto en vectores pero para que la inteligencia empiece a entender el sentido de ese texto y empiece a serer distintos tipos de tarea justamente con textos que empiecen a aparecer como input Por ende este tipo de información representa un conjunto de datos que está a disponibilidad de esa inteligencia para ser entrenado Por ende ese volumen de datos para el aprendizaje obviamente es un factor muy importante y como cuando vimos el resto de las inteligencias que vimos en el curso o antes en el curso de que venimos llevando adelante justamente Cuanto más datos tengamos obviamente más preciso y efectivo va a ser el modelo sí sabemos que siempre hay un límite no quiere decir que siempre tenga que ser el número máximo Pero obviamente Cuanto más tengamos en principio de Debería ser ese algoritmo mucho más eficiente Por ende o por ello Mejor dicho el el éxito de los grandes este las grandes aplicaciones que hay hoy como GP t4 que han sido entrenadas con cantidades enormes de información y de texto y por eso el alto nivel de eferencia que tienen un tema final a tener presente en este proceso asociado a la tokenización es el manejo de las letras minúsculas y mayúsculas en realidad para la máquina un hola donde la H está en mayúscula o la misma palabra donde la H está en minúscula son dos palabras totalmente diferentes yo tenemos que tenerlo en cuenta porque justamente si nosotros ponemos esta o dejamos libre esta diferenciación entre una palabra y otra simplemente por tener la primera letra mayúscula estamos creando un vector con mayor dimensión ustedes imagínense que todas las palabras tengan esta esta situación de dos palabras diferentes por tener la primera letra en mayúscula o no estaría duplicando Prácticamente todo el vocabulario literalmente todo el vocabulario Por lo cual por ejemplo recién el vector que teníamos 18 posiciones va a pasar a ser un vector de 36 posiciones siendo que la palabra es exactamente la misma con lo cual lo que me conviene hacer como proceso de tokenización o parte del proceso de tokenización además de dividirlas con el método split que vimos hace un rato es aplicar lower para poner toda la expresión en minúscula de esa manera mi diccionario va a estar conformado por cuatro palabras de otro modo modo yo tendría que tener un diccionario donde por ejemplo Hola mundo y buenos que tiene la posibilidad de tener una expresión en minúscula y mayúscula tendría que tener contemplados los dos casos con lo cual tenía un diccionario de siete palabras en lugar de cuatro como hay aquí el siguiente concepto importante son las stop words sí o palabras de parada bien tenemos por ejemplo aquí una frase de ejemplo que la he puesto en una var de texto en algunos países los días son muy largos y las noches muy cortas bien si yo toqu eniso esta frase que tiene 13 palabras tendría una toca de 12 palabras por qué Porque está la palabra muy que se repite dos veces como vimos recién el diccionario sería de 12 palabras a partir de este texto está bien Ahora en esta frase existen lo que se llaman stop Wars sí las spw Por lo general son palabras que forman parte de las frases pero representan algo que no cambia el sentido de la frase y Por ende se puede llegar a prescindir de ellas en este caso por ejemplo las palabras en los las muy e son palabras del lenguaje español que están en todas partes Es decir no va a ser algo que voy a encontrar en esta frase y no voy a encontrar en muchas otras frases porque son justamente elementos clásicos de nuestro lenguaje y Por ende justamente no aportan mucho a la frase ni tampoco cambian mucho el sentido de la frase si esas stop Wars no están en este caso si yo me circunscribieron las work que mencionábamos recién Entonces esto puede verse que a la hora de leerlo desde la que es nuestro conocimiento del lenguaje podía representar una frase que se puede interpretar más o menos Hacia dónde va pero no es muy clara bueno Esto es muy importante en el mundo en LP porque la máquina sí lo va a poder interpretar de manera correcta y lo que estoy haciendo aquí es prescindiendo de una cantidad de dimensiones en este supuesto vector si contemplara las stop Word dentro que justamente al no tenerlos se reduce el vector yo gano como ustedes ya imaginan por supuesto en velocidad en rapidez y en eficiencia justamente teniendo menos dimensiones que teniendo una cantidad de dimensiones mayor y quizás innecesaria a los fines del nlp Bueno pero se terminaron las palabras se terminaron las definiciones y los conceptos es hora de aplicar nuestro primer código de nlp para empezar a trabajar en este caso particular con stop Wars y tokenización para ello tenemos este la pnl 1 que es un archivo de colap que es el primero con el cual vamos a empezar a trabajar y es uno de los archivos que ustedes tienen en el campo virtual vamos hacia ello aquí estamos entonces en el ámbito de Cola con el la pnl 1 bien tenemos la variable texto a la cual le ponemos la misma frase que hablamos hace un rato en algunos países los días son muy largos y las noches muy cortas lo que vamos a hacer como primera medida es importar la librería nltk nltk significa librería natural Language tool kit Entonces tenemos que importar esa librería y tenemos que bajar dos archivos que son importantes el primero para la tokenización y el segundo para trabajar con las stop World Así que vamos entonces con la primer celda que no lo habíamos ejecutado que es poner dentro de texto esa frase y luego importar estas librerías paso seguido que lo ponemos en una celda aparte pero lo podríamos Haber puesto junto con las dos las tres líneas anteriores Perdón tenemos que también incorporar eh d nltk Corpus stop Wars y d nltk tokenize Word tokenize bien importamos esas librerías también y ahora empezamos a ver el tema de las stop Wars en principio voy a crear una variable que le voy a poner stop bajo Wars en la cual voy a volcar todo el dicionario de palabras de stop Wars que existen en el ámbito del Castellano sí palabras que tienen que ver justamente con lo que voy marcando aquí arriba con esta librería stop Wars que pertenece al natural Language toolkit y de hago una impresión para que se vea justamente Cuáles son todas las stop Wars en español así que ejecuto esta instrucción y aquí tengo una lista muy larga que bueno la podrían tirar a una Ray si quisieran podrían convertir esa variable como sabemos hacer de costumbre stws taray para poder verla de una manera más más amigable est una al lado de otra y no una bajo otra pero a los fines prácticos si aquí tenemos entonces todas las palabras que forman parte de las stop words o lo que está definido dentro del natural Language tool kit en español como stop Word obviamente es una lista muy muy larga si como siempre es complicado recorrerla de este modo Pero bueno lo dejo en ustedes Poder recorrerla De punta a punta bien una vez que tenemos esto lo siguiente sería tokenizar la frase texto si la que teníamos arriba de todo que volvemos ha arriba nuevamente en algunos países los días son muy largos y las noches muy cortas bien Voy a tokenizar esa frase de De qué modo con Word tokenize de texto de la variable texto y voy a poner ese resultado dentro de la variable tokens es decir que en tokens van a estar Qué cosa todas las palabras de esa frase luego lo que voy a hacer va a ser recorrer cada una de esas palabras por eso pongo Word Forward in tokens por cada palabra de cada una de las palabras que están en la colección de token se acuérdense que la colección de token fíjense que ahí me paro y me aparece la ayuda está conformada por tres items bien y luego le pongo como condición If not Word la palabra concretamente in stop Wars con esto resumo por cada palabra que está en tokens pero que no está en la hop Word con lo cual me quedo con las palabras que están fuera de las stop Word y luego imprimo el contenido de esta variable que le he puesto texto ssw como siendo bueno stop texto sin stop Wars Perdón entonces ejecuto esto y fíjense lo que me muestra que es lo que habíamos visto hoy en el ejemplo en la parte teórica en países días largos noches cortas pero yo veo aquí si me voy a las stop Wars y subo un poquito aquí rápidamente veo que n está dentro de las stop Wars Qué pasó que me muestra quién siendo que yo le puse como condición que lo que tenía que estar dentro de texto ssb debían ser aquellas palabras que no están dentro de War Bueno lo que pasa es que como ustedes pueden apreciar aquí en está con la e en mayúscula mientras que en las stop Wars volvemos a aquí en está como todas las stop Wars en minúscula entonces lo que tenemos que hacer es reconfigurar lo que hicimos Recién con una instrucción previa texto igual a texto pun l también lo vimos en la teoría esto y también es una instrucción de python un método de python que ya lo conocen bastante con lo cual con esto voy a convertir toda la frase De punta a punta en minúscula y voy a volver a ejecutar las mismas tres líneas que ejecutamos antes y voy a ver que ahora van a aparecer la misma toen ación es decir la misma eh el mismo arreglo el mismo aray que tiene aquellas palabras tokenizadas pero que no están en el Word en este caso sin la palabra en que había salido antes bien con esto Terminamos el primer lab de este [Música] curso los conceptos que tenemos que ver a continuación son estos dos stemin y lematización en realidad tienen propósitos similares pero tienen técnicas diferentes de qué se trata esto en realidad vamos a tener en cuenta al algunas cuestiones antes de entrar en las características de cada uno de ellos las palabras similares se tratan como entidades separadas es decir corre corriendo o corre son palabras muy similares pero habitualmente se tratan por ser palabras distintas con entidades separadas las mismas en realidad van a ser representadas por vectores que van a estar muy cerca a unos de otros por qué porque las tres palabras tienen un sentido similar o un sentido que asociativamente es parecido pero van a estar representadas por vectores diferentes esto lo teníamos en la imagen que vimos hace un rato vamos a ver un poco ella para poder recuperar este concepto que está aquí recuerdan este gráfico bueno supongamos que esto de correr corriendo corre fueran estos vectores que probablemente justamente como son palabras que tienen un sentido muy similar estén cerca estos vectores eso va a ser justamente una representación vectorial que representa esa realidad de palabras similares pero concretamente yo voy a estar teniendo un espacio con tres dimensiones Y eso es lo que trato de evitar con el steaming y la lematización por eso como dice aquí lo que vimos recién genera una gran dimensionalidad la cual se podría reducir si utilizáramos una misma raíz de esas palabras para representarlas todas del mismo modo es decir correr corriendo y corre tienen una palabra raíz que puede ser asociada a las tres y de esa manera representar las tres con una palabra que sea representativa y no con las tres por separada para eso existen dos técnicas que son stemin y lematización y vamos a ver justamente cada una de ellas de qué se trata el stemin es una técnica que elimina los sufijos de una palabra por ejemplo tengo caminar y caminando ambas palabras se van a transformar en camín es decir toma lo que tienen en común ambas palabras y a la primera le quita el ar y a la segunda le quita el and con lo cual de dos palabras paso a tener una de dos dimensiones paso a una la lematización busca un objetivo similar pero con una una técnica o una modalidad diferente la lematización lo que hace es Buscar la palabra raíz u origen de esas palabras que tienen un sentido similar por ejemplo caminar caminando Caminaré camino se transforman en que su palabra raíz es caminar aquí no quita una parte de la palabra para llegar a lo que sería una palabra resumida quitando lo que decíamos recién los sufijos como hace el steaming sino que busca la palabra raíz de esas cuatro palabras esto obviamente es un proceso que es más lento porque lo que hace el St Es simplemente recortar parte de una palabra aquí lo que tiene que hacer es tomar todas esas palabras tod Ese Conjunto de palabras en este caso de cuatro y buscar cuál es la palabra raíz que representa esas cuatro Pero nuevamente el objetivo es el mismo porque paso de tener en este caso cuatro dimensiones a solamente una [Música] dimensión al igual que hace un rato esto lo vamos a ver en la práctica por eso existe otro archivo de colab que tienen en el campus virtual que se llama lab pnl 2 y justamente me da un ejemplo de streaming y de lematización vamos hacia ello bien est vamos ahora nuevamente en colap pero ahora en el la pnl 2 en este caso el tema es stemin como decíamos recién y lo primero que vamos a hacer es importar bueno natural Language toolkit y vamos a hacer un Download parecido a lo que hicimos En el lab anterior pero ahora de wordnet Por qué wordnet cuando Antes había elegido Punk y stop Wars bueno en el caso de punkt tenía que ver con la tokenización y en el caso que nos ocupaba en el primer lab yo quería sola ente poder tokenizar esta frase es decir Tomar las palabras separadas de esta frase y las stopwords eran con el propósito que dimos recién que tenía que ver con la posibilidad de quitar de esa frase las stws en este caso la el stemming y la lematización se trata de poder inducir a que la inteligencia me permita en algunos casos quitar el sufijo y en otros casos encontrar la palabra raíz y para eso necesita como referencia una librería que contenga el vocabulario luego el vocabulario yo lo puedo configurar para que esté en un determinado idioma Pero por eso en este caso particular que se diferencia claramente de los propósitos del lab anterior tengo que carrar la librería wordnet Así que lo primero que hago es eso y una vez terminado esto lo que voy a llamar d nltk stem importar snowball steamer esta librería con la cual voy a crear una instancia una variable que le voy a poner steamer instancia a través justamente snowball steamer configurando esa instancia de snowball es decir básicamente lo que va a representar la acción del steamer en español con lo cual creo esta variable Y a partir de eso y justamente con esta variable de objeto lo que voy a hacer va a ser tomar tres palabras como las que tomamos recién como ejemplo y ver el steamer Cuál es la reducción que hace al quitar los sufijos de cada una de esas palabras por eso hago tres prints Cuando pruebo esto veo que comiendo comer y comió en los tres casos el steaming lo que hizo fue quitarle los sufijos y reducirlos a las palabras que tienen en común la c la o y la m bien es el turno ahora de la lematización en el caso de la lematización no vamos a usar la librería nltk que usamos antes para el steaming y que también usamos en el lav anterior dado que si bien tiene la herramienta para hacer la lematización esa herramienta no está disponible en lengua castellana por eso vamos a recurrir a la librería spacy que sí tiene esa opción de lematización en lengua castellana y de paso me sirve esto para decirles de que estamos sumando una librería más para ese tipo de actividades u otras y que eso no implica que sean las dos únicas que existen para este tipo de tareas en el mundo del nlp pero sí es importante que tengan presente que son las dos o dos de las más conocidas el spacy no viene naturalmente instalado en python como el nl tk por lo tanto tenemos que instalarlo para eso usamos el argumento que ya conocemos el método que ya conocemos la herramienta de python que es pip pip install spacy y el menq básicamente es para que no muestre una serie de información que tiene que ver con la instalación y luego lo que tengo que instalar es el diccionario o el conjunto de palabras justamente que viene para usar el spacy en castellano con esta frase que está aquí Bueno esto tarda mucho yo lo tengo instalado para no per tiempo lo vamos a considerar como que ya está pero esto es simplemente hacerle clic al al Play de cada de cada una de estas celdas y está instalado por un lado del spacey y por el otro lado del dicionario bien Vamos al código concretamente y lo primero que hago es importar spacey luego lo que voy a hacer a continuación va a ser Perdón me qu un error aquí volvemos a la línea lo que vamos a hacer a continuación es lobar de spacy es decir cargar el dicionario de Castellano y lo voy a cargar dentro de una variable que se le voy a poner nlp la cual obviamente va a contener todo este conjunto de palabras que me ofrece spacy para la lematización en castellano paso Seguido lo que voy a hacer es usar justamente esa variable nlp para hacer una tokenización de una frase una frase que en realidad es un poco ficticia pero tiene que ver con rápidamente demostrar de qué se trata la lematización y Cómo podemos visualizar los resultados de ella Por ende esa frase dice comiendo comer comió evidentemente no tiene un sentido en la lengua castellana Pero insisto a los fines de esta mini práctica viene bien con lo cual en tokens gracias a nlp lo que voy a tener justamente es esta frase separada entre tokens comiendo comer y comendo y Por ende con este for voy a recorrer for token in tokens es decir por cada uno de esos elementos que están dentro de tokens por cada uno lo que voy a hacer es arme un String digamos para que se vea por pantalla claramente texto dos puntos token pun text es decir el token ese mismo comiendo por ejemplo en el caso y luego lema Qué sería Cuál es la lematización o cuál es la palabra resultante de aplicar la lematización sobre comiendo luego sobre comer y luego sobrecomo acuérdense que la lización no quita el sufijo como el steaming sino que busca una palabra raíz por eso cuando ejecuto esto me muestra que para comiendo para comer y para comió existe una palabra raíz única que seama se llama comer con lo cual estoy reduciendo esta posible dimensionalidad de vectores de tres palabras a una sola algunas cuestiones que hay que tener en cuenta con la lematización que recién referenciamos Es que la lematización puede ser más efectiva que el stemin pero su gasto computacional es mucho mayor por qué Porque el uso de la lematización requiere de una etiquetado previo ya que hay que encontrar una palabra que represente a un grupo tal cual veíamos en el ejemplo de reciente Por ende hay que hacer un trabajo extra que no existe en el estamiento simplemente recorto la palabra si veo aquí estos ejemplos donde este cuadro que está a la izquierda representa una lematización comiendo comer y comó se sintetizan en una palabra comer que es una etiqueta con la cual está etiquetando estas tres palabras comiendo comer y comió con una etiqueta comer en los tres casos por el contrario en el caso del stemin con este ejemplo que está aquí yo lo que estoy haciendo es simplemente sacar el sufijo es decir comiendo comer y comió tienen en común las tres primeras letras con lo cual aquí el gasto computacional al no tener que buscar esta palabra raíz como el caso de comer y etiquetar cada una de esas palabras con esa etiqueta es obviamente en el caso del stem mucho menor el gasto computacional y Por ende el algoritmo es más rápido para cerrar el tema St y lematización vamos a poner foco sobre algunas aplicaciones en donde se pueden aplicar valga la redundancia estas dos técnicas en principio hay empresas que usan asistentes virtuales o chatbox por ejemplo Amazon con Alexa o empresas que proporcionan servicios al cliente que utilizan la lematización y el stemming para comprender las consultas de los usuarios con mayor precisión a ver por ejemplo si un usuario pregunta al chatbox de una empresa que tenga atención al cliente Dónde está su paquete el chatbox Debería ser capaz de entender la pregunta así como también otra pregunta parecida a esa en el análisis de sentimiento por ejemplo si vamos a monitorear Twitter con las redes sociales y alguien twitea por ejemplo estoy enfadado con la empresa Nno o XX se podría entender que enfadado y enojado o irritado son sentimientos similares y por lo tanto deben ser tratados de la misma manera para análisis entonces una lematización en este caso podría ser que cada vez que diga enfadado enojado o irritado todas tengan la misma base raíz que sería un sentimiento de enojo también con los motores de búsqueda Ocurre algo parecido a lo que decíamos en el caso de los chatbox dado que una consulta expresada de varias formas diferentes debe dar el mismo resultado de la búsqueda zapatillas para correr en la montaña zapatillas de treking zapatillas de en senderismo etcétera otro caso son las empresas que tienen sistema de recomendación Como por ejemplo Netflix Spotify Amazon u otras en donde utilizan la lematización y el stemming para mejorar la precisión de la recomendación a ver por ejemplo si un usuario Busca películas de acción debería Netflix ser capaz de entregar películas de guerra o policiales es decir que tengan que ver con películas de acción finalmente también se usa mucho en la publicidad online por ejemplo para hacer etiquetado en redes sociales a ver si queremos hacer una publicidad y queremos vender celulares por ejemplo o algún accesorio para gente que le gusta esa tecnología o convive con esa tecnología es posible que esta empresa me ponga anuncios no solo con la palabra celulares sino también con un disminutivo popular como celus o una denominación más formal como smartphones o cualquier denominación con palabras similares que básicamente representen lo mismo Hemos llegado al final de esta clase nos vemos en la próxima [Música] clase