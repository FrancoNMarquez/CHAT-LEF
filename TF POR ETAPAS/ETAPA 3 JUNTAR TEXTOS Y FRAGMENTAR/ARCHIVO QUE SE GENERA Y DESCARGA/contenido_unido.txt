Esta es la segunda parte de la clase número 26 te invito a empezar con  ella  otro concepto del nlp es la similitud entre vectores en realidad esto de alguna manera Ya lo hablamos cuando decíamos de que cada palabra o cada documento depende el caso estaba representado por un vector y la cercanía entre esos vectores me daba la idea de que eso podría ser similar si Estaban cerca o no tan similar si no Estaban cerca por lo tanto esto que tenemos aquí este gráfico lo vamos a recordar justamente porque tengo la idea de tres vectores en este caso A B y C y justamente puedo ver si A B y C son o no similares en virtud de su cercanía la cercanía en realidad se calcula en base al coseno del ángulo entre esos vectores es decir por ejemplo en este caso entre a y b la cercanía se me diría calculando el coseno de este ángulo que está aquí dibujado con esta línea roja o de a con c con esta línea verde det trazos bien en el caso que yo calcule el coseno de 0 y me de un es cuando literalmente un vector está encima del otro es virtualmente la misma palabra con lo cual la similitud sería máxima en La Virtud que eso no pasa hae bueno Obviamente que describirá un ángulo que en la medida que sea más chico va a describirme una situación de un texto o una palabra más similar y lo contrario sucede justamente que el ángulo fuera mucho más lejano o más grande tenemos que recordar como siempre venimos diciendo que justamente el vector en nlp representa una tokenización de los textos los textos están representados o las palabras de los textos ya vamos a ver más adelante depende que tomemos como unidad de tokenización están representados por vectores Y nuevamente la cercanía o la lejanía de ellos me habla de la similitud vamos a ejemplos de la aplicación de la similitud dees una de las aplicaciones prácticas es la similitud de documentos Sí ya que se usa mucho por ejemplo para si tenemos una biblioteca de datos muy grande que queremos clasificar podemos usar la similitud de vectores para juntar los documentos que son más parecidos y clasificarlos otra aplicación es el spinning de artículos y seo seo es el search engine optimization optimización en motores de búsqueda esto es muy importante porque muchas veces por ejemplo Google hace un seo de la página web que es cómo va a posicionar la página concretamente en base a la riqueza del documento determina esa ese posicionamiento perdón y hay muchas personas que piensan que copiando un documento y pegándolo en su página web van a lograr posicionarse mejor en Google pero Google tiene sistema de similitud de vectores justamente que hacen que descubran cuando hay muchas copias de los textos para evitar eso justamente también la aa nos propone una solución a través del spinning de artículos que se trata de tomar un artículo y variarlo tal que no diga Exactamente lo mismo pero conceptualmente el artículo diga o se trate de lo mismo otra aplicación son las recomendaciones por ejemplo de Netflix esto ya lo vemos aplicado en varias de las cosas que hemos visto hasta ahora Quién trabaja también usando el tema de la vectorización para ver en base a la review de las películas y buscar coincidencias con las películas que ya hayamos visto antes finalmente otra aplicación son los chatbot que también usan mucho la similitud de vectores ya que cuando hacemos una pregunta al Bot lo que hace este es vectorizar dicha pregunta y buscar en su base de datos vectorizada a cuál vector se parece más la pregunta que estamos haciendo Y nos manda la respuesta en base a ese concepto de similitud de vectores el siguiente concepto que vamos a ver aquí es el concepto de tf idf a ver de qué se trata este concepto básicamente tf significa frecuencia del término e idf significa frecuencia inversa del documento En qué se basa este método o Esta técnica Esta técnica es utilizada para reflejar Cómo o cuán importante es una palabra en el contexto de un conjunto de documentos en este caso no se toma en cuenta la relevancia o la importancia o el peso de las palabras sino por el contrario reduce ese peso y aumenta el peso de las palabras menos frecuentes dicho de otra manera las palabras que aparecen con más frecuencia en un documento pero muy poco o nada en otros documentos son consideradas las más importantes para este método Cómo se aplica este método tfidf se aplica aplicando valga la redundancia esta fórmula que tenemos aquí tf de TD por idf de TD para entender bien esta fórmula vamos a ir viéndola paso a paso primero el primer término Qué es tf de TD es el número de veces que un término aparece en el documento d dividido el total de términos en el documento d por ejemplo supongamos que el término stemen que vimos la clase pasada aparece 10 veces en un documento que tiene 1000 palabras Entonces el tf de ese término sería 0,01 porque sería literalmente 10 di 1000 paso Seguido el idf de TD significa el logaritmo base e del total de documentos del grupo denominado d mayúscula en este caso dividido el número de documentos en donde aparece el término t volvamos al ejemplo de recién supongamos que el total de documentos que tenemos es 10 Sí donde uno de los documentos el que referenciamos recién y solamente en un documento aparece el término steaming es el primero el que vimos recién en el caso anterior Entonces digamos que si tenemos 10 documentos y en un solo documento aparece el término stemin entonces la fórmula sería logaritmo e de 10 di 1 lo cual nos daría 2.3 Finalmente y habiendo logrado el cálculo de los dos términos de esta fórmula como lo vimos recién en el ejemplo tendríamos que tdf DF serí igual a 0 01 que es la primer parte de la Fórmula por 2.3 que es la segunda parte de la Fórmula lo cual nos daría como resultado 0.023 pensemos en lo primero que vimos si stemin aparece 10 veces en un documento de 1000 quiere decir que su pie sería 0.01 Pero aplicando este concepto de tf y DF donde le da más importancia a las palabras que menos aparecen el peso de esa palabra sería 0.023 es decir mayor a que hubiera tenido de manera natural como dijimos al principio de esta clase vamos a profundizar este concepto poniéndolo en práctica justamente a través del lab pnl 4 que es uno de los lab que ustedes tienen a través del campus virtual un lab de colab y vamos a aplicarlo a un ejemplo de un sistema de recomendación de películas vamos con ello bien estamos ahora entonces en el apn l4 dentro del entorno de cola donde el Ob objetivo como ya lo dijimos antes es crear un sistema de recomendación de películas aplicando el método tfidf es importante que tengamos en claro que para hacer un sistema de este tipo lo que tenemos que ver es la similitud entre una película y otra es decir para recomendar una película tengo que ver películas parecidas a una determinada cuando hablo de esto hablo de justamente en este tipo de técnicas transformar cada una de estas películas en un vector y para saber si una película es parecida a otra o no lo que tengo que hacer es justamente ver si esos vectores están cerca o están lejos para eso justamente lo que tengo que hacer es utilizar el cálculo del coseno para que me dé justamente el ángulo que existe entre esos vectores Y si están cerca estaré hablando de películas similares y si están lejos estaré hablando de lo contrario bien teniendo en claro esto vamos a pasar a ver las librerías que vamos a incorporar para este lab que en principio es pandas tf DF vectorizer que justamente el que va a ha ser la tarea de vectorización que referenciamos recién y también la librería para el cálculo del coseno y el ángulo justamente para ver lo que también referenciamos recién bien incorporamos estas librerías y luego vamos a ir justamente a montar el Drive donde tenemos este archivo de películas que como dijimos este recién también bajamos del sitio kagel vamos a ver justamente En qué sitio cag o qué página del sitio kagel bajo esta recomendación Aquí está justamente el link donde está este archivo movi metadata.csv que es el que vamos a utilizar para este lado bien habiendo aclarado eso justamente estamos con la idea de que ese archivo yo lo voy a poner obviamente en el campus virtual ustedes lo van a bajar y lo van a colocar en un Drive Drive que al montarlo como yo he hecho aquí en este caso lo vamos a usar para crear justamente un dataframe con esa información como estoy haciendo aquí luego vamos a ver el dataframe no con un Head vamos a ver todo el dataframe para ver que justamente tengo el título de la película aquí y tengo algunas características que me van a servir para poder comparar una con otra el género y un plot kw que son algunas referencias adicionales al género que son importantes para elocidad si una película es o no similar a otra lo que vamos a hacer aquí son dos cosas en principio tratar de buscar una característica que aune a estas dos pero antes que eso tengo que limpiar los datos de ambas características porque en ambos casos justamente está separado un concepto de otro a través de una barra vertical con lo cual voy a tener que reemplazar esa barra vertical por un espacio en blanco antes de ello vamos a estudiar Si este dataframe tiene valores nulos con lo cual ejecuto esto y veo que justamente género no tiene valores nulos pero plot keyw sí es más si yo miro Aquí rápidamente el dataframe con esta visualización rápida que hicimos ya veo que aquí en el cuarto elemento tengo en plot kw un valor nulo bien lo que tengo que hacer con ello como siempre es limpiar esos valores nulos y en realidad voy a reemplazar todos los valores nulos con espacios en blanco sea de esa característica pl divos u otra con lo cual si si Ahora vuelvo a ejecutar El Comando de Recién veo que en este momento ya no tengo más ningún carácter nulo paso Seguido lo que voy a hacer es reemplazar como dijimos recién estas barras verticales que veíamos Si volvemos aquí estas barras verticales que aparecen en género y en plot keywords por espacios en blanco justamente a través de esto género y con replace voy a reemplazar la barra por espacios en blanco y lo mismo para pl ejecutamos y hacemos una verificación de Data frame por las dudas y vemos que realmente ha hecho el cambio como corresponde ahora lo que tengo que hacer como dije antes es lograr una característica juntando estas dos Bueno vamos para ello con lo cual esta línea responde a esa idea texto igual a género más pl keyw con un espacio en el medio lo ejecutamos y vamos a ver rápidamente las cinco primeras observaciones para hacer un chequeo a ver si lo hizo bien efectivamente podemos ver justamente mostrando género y pl keywords y y la resultante de la Unión de ambos que le hemos puesto texto bien luego lo que tenemos que hacer es empezar a vectorizar Pero antes de vectorizar tengo que decirle esos vectores de qué dimensión van a ser y lo hago justamente especificando Max feature en 2000 con lo cual cada película va a ser representada por un vector de 2000 posiciones ahora teniendo claro esto lo que voy a hacer justamente es la transformación con lo cual voy a tomar toda la todo el Data frame completo sobre la característica texto y justamente transformar Esa esa nueva característica texto que recuerden la logramos uniendo género y plot keywords a vectores bien estamos aquí lo ejecutamos y ahora voy a ver qué tengo en x tengo una estructura de 55043 observaciones como el Data FR original pero no como el dataframe original 29 características sino que tengo ahora 2000 características porque justamente es lo que yo le dije que era el tamaño del vector que yo quería bien teniendo claro esto lo que vamos a hacer es tomar un caso testigo para hacer este este ejercicio de la comparación de una película con otra y vamos a tomar eh el vector de Piratas del Caribe Piratas del Caribe Recuerden que es la segunda película sí tenemos la primera sabat la segunda es O sea la que está indicada como número uno porque sabemos que el número ordinal empieza siempre de cero es la segunda película es piratas del carbe bien Entonces vamos a tomar aquí la segunda película con x1 tu array es decir voy a convertir Piratas del Caribe a una estructura de tipo array y aquí veo justamente cómo está conformado el vector de 2000 posiciones perdón de Piratas del Caribe lo que voy a hacer ahora justamente es con esta librería de similitud a través del coseno poniéndole como parámetro en principio la película que quio tomar como testigo la película a comparar a buscarle su par y luego todo el dataframe completo toda la x del dataframe completo para ver bueno compararme esta película con todas y decirme a Cuáles es parecida o qué Mejor dicho no va a decir a cuál es parecida va a dar el número o el porcentaje de similitud con cada una de ellas con todas las 543 en este caso con las 5042 otras películas que están en el dataframe Así que ejecutamos eso y ya tenemos dentro de similitud todas las comparaciones con cada una de las películas de El dataframe y Si vemos el contenido de similitud vamos a observar por ejemplo que tiene una similitud de 0.10 con la primer película que era Avatar como recordarán y tiene una similitud de uno es dec del 100% con la segunda película Por qué Porque se está comparando consigo misma Sí así del mismo modo veo que con la siguiente película tiene una similitud inferior a con Avatar fíjense que aquí tengo 0.05 y con Avatar era 010 si me fijo en la segunda película voy a ver que la segunda película es espectro no conozco la película pero sí vemos aquí que en caso de Avatar y pirata del Caribe tienen la Fantasía como un elemento en común y aquí esto es más un Thriller con lo cual está claro de Por qué es más parecida a Avatar que esta otra película bien Esto es para que tengan una aproximación numérica Y en este ejemplo de cómo maneja esta cuestión de la similitud en este caso si yo quiero ver la similitud la primer similitud por separado bueno pongo similitud 0,0 Por qué Porque si bien Esto está como una Ray digamos desde lo que vemos si esto que vemos aquí es una Ray es una Ray bidimensional Más allá de que a los fines de la cantidad de números es unidimensional con lo cual si yo quiero ver solamente el primer elemento podría transformar ese vector bidimensional en un vector unidimensional Cómo a través del método flatten que es lo que vamos a hacer a continuación y ahora fíjense que yo voy a Acceder al primer elemento pero no como 0 cer sino directamente como cero lo ejecuto y ven el mismo resultado pero lo tengo aplastado acuérdate que flat es un concepto de cuando quiero una matriz que está conformada como de dos dimensiones pero tiene en realidad una sola dimensión aplastarla en una única dimensión con la cual la transforma literalmente en un vector bien lo que voy a hacer ahora con esta instrucción es justamente ver Cómo ordenar las películas de más parecida a menos parecida justamente voy a crear una variable similitud ord y con el menos similitud le voy a decir que la quiero en orden inverso y con ord justamente las voy a ordenar bien lo ejecutamos y ahora con similitud gu B voy a tomar las 10 películas más parecidas de la 1 a la 11 como sabemos que se numera esto las 10 películas más parecidas en este caso a Piratas del Caribe ejecuto y veo los números de las películas más parecidas pero lo que voy a hacer es tratar de traerme el nombre de esas películas para que sea más claro entonces lo que hago justamente Es sobre el título de la película y a través del método ilock justamente con similitud gu or que es esto que tenemos aquí traer los valores 1 a 11 para que me traiga los nombres de cada uno de estos números de películas ejecuto y veo que las 10 películas más parecidas a piratas de Caribe son estas que están tituladas aquí justamente la última es otra versión de Piratas del Caribe bueno con esto ustedes pueden justamente cambiar en lugar de tomar Piratas del Caribe pueden tomar otras y justamente ver qué resultados les da y poder bueno entretenerse con este tema de la comparación a ver cuá reales a la percepción que ustedes tienen  Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número  26 Hola a todos Bienvenidos a la clase número 26 del curso de Inteligencia artificial de ifes continuamos con el módulo de procesamiento del lenguaje natural y esta clase que vamos a dividir en dos partes como la mayoría de nuestras clases va a tener una primer parte destinada a hacer una práctica integral que refuerce todos los conceptos que vimos en la clase pasada y en la segunda parte de esta clase vamos a ver nuevos conceptos y también algunas prácticas adicionales para también reforzar esos nuevos conceptos así que sin más empecemos con la primer parte de esta clase bien es momento de avanzar a otra práctica a otro lab con lo cual vamos a ver un ejemplo completo de tokenización ese ejemplo completo lo vamos a llevar a cabo utilizando un dataset de noticias que vamos a bajar del sitio kag este lab se va a llamar la pnl 3 y como todos los que vimos anteriormente los pueden bajar desde el campo virtual de ifes bien en este lap vamos a desarrollar un ejemplo completo de tokenización como bien dice aquí el título y vamos a hacerlo tomando un dataset del sitio que ya conocemos kaggle con Eh bueno dat que tiene información de noticias y cada una de esas noticias está caratulada como un tipo de noticias Sí policiales deportivas etcétera Ya lo vamos a ver bien y todas las características como está rotuladas cada una de estas noticias cuando hablo de rotuladas ya van teniendo idea de que justamente me enfrento a un dataset que tiene etiquetas y este algoritmo que vamos a tratar de construir desde nlp tiene algunos contactos con lo que ya hemos hecho en Deep learning y en Machine learning porque se trata justamente de leyendo el contenido de una noticia tratar de inferir qué tipo de noticia es esa Está bien como dijimos recién deportiva política polical etcétera etcétera La idea es que justamente yo usé este dataset para qué Para lo que hemos hecho un montón de veces entrenar una inteligencia Y a partir ir de entrenar es inteligencia puede inteligencia empezar a deducir o a inferir qué tipo como dije recién de noticia es un determinado texto vamos a incorporar librerías que ya algunas las conocemos muy bien pandas por un lado py it learn con nuestro ya conocido trest split pero no tan conocido con vectorizer esta es una aplicación que tiene que ver con lo que vimos recién la teoría trata justamente de transformar textos en vectores a través de la contabilización de la la cantidad de veces que aparece en determinado texto en determinado documento eh una palabra y finalmente vamos a usar multinomial nb de night bice que es un tipo de algoritmo que no vimos en Machine learning pero es un algoritmo muy liviano muy efectivo y muy aplicable para casos de nlp Así que empezamos por cargar todas esas librerías y luego vamos a montar el Drive como ya lo hemos hecho muchas veces para Acceder al dataset que les mencionaba recién y habiendo hecho esto lo que voy a hacer es hacer nuestro ya muy conocido rit csb voy a tomar esta información de este dataset desde un dataset que tiene formato csb con el encoding utf8 y lo voy a poner dentro de una variable que le voy a llamar DF y luego hago un Head como lo hemos hecho infinitas veces y tengo en realidad un dataset con tres características nada más la URL donde está la noticia el texto de la noticia y el tipo de noticia que es cada uno por lo tanto tengo un eh si hago un shape pued darme cuenta que tengo un dataframe de 1217 noticias y como dije recién tres características si miramos eh el primer la primer característica de la primera observación por eso DF news porque la news es el nombre como llama esa característica el subcero indicando que es el primer valor bueno ejecuto eso y voy a ver todo Sí esto que aparece aquí muy recortado Sí todo el texto completo y así todo continúa sí bien luego lo que voy a hacer como insisto tantas veces hicimos decir bueno Cuál es mi característica x y cuál es mi característica i o Target bien la x va a ser el news y el target va a ser type que t justamente lo que vemos aquí es el tipo de noticia insisto lo que queremos aquí es a partir de un texto poder hacer una inteligencia que luego me permita predecir Qué texto puede tener que ver con cuál de los types con cuál de los tipos así que bueno creamos la x creamos la i y luego vamos a hacer una contabilización de eh los los tipos y además vamos ver cuáles son todas las variantes de tipo que existen ver dos propósitos en uno en principio puedo ver que los tipos que son macroeconomía alianzas Innovación regulaciones sostenibilidad otra y reputación y que bueno la cantidad de noticias que hay dentro de cada rubro o sea nada que ver lo que yo le había dicho política deporte Pero no importa era la esencia era transmitirle cuá es la la característica de lo que se quería pretender con este dataset Pero bueno tenemos 340 noticias de macroeconomía 247 de alanzas etcétera etcétera como siempre decimos no está muy balanceado porque ser ideal que hubiese la misma cantidad de noticias de cada uno de estos tipos Pero bueno tampoco es algo que nos vaya a complicar demasiado bien visto esto hago como Insisto fíjense que este este proceso es muy parecido a lo que hemos hecho muchas veces en Machine learning lo que hago es separar el conjunto de prueba del conjunto de Test tomando la x tomando la i y considerando un test size Es decir para el conjunto de test de El 20% quedando el 80 en el conjunto de entrenamiento bien para vericar como qued como quedaron esas cantidades hago un print de x trin y de X test viendo que a partir de ahora de Las 1217 observaciones tengo una división de 973 observaciones En en el conjunto de entrenamiento y 244 en el conjunto de Test bien lo que viene ahora es la vectorización recuerden vamos a ir justamente a la parte de la teoría donde decimos un ejempl muy sencillo Pero efectivo que era el de estos tres documentos que eran virtualmente tres líneas pero lo que vamos con el espíritu de tres documentos para hacer un ejemplo chico que se entienda bien lo que hacía era generar todo un vocabulario conformado por 18 palabras porque justamente la suma de las palabras que había entre estos tres documentos sin repetir la que aparecía más de una vez representaban 18 palabras bueno Esto lo que vamos a hacer ahora justamente con este vectorizer con lo cual hago una instancia que le pongo vectorizer de con vectorizer con vectorizer lo tenemos aquí arriba como una de las librerías de psych learn que acabamos de incorporar al principio de este lap y luego lo que voy a hacer es algo insisto parecido a lo que hicimos muchas veces con Machine learning un fit transform de los datos que van a usar para entrenamiento y un transform de los datos que van a usarse para Test en este caso por qué el transform Bueno por lo que venimos hablando porque tengo que transformar cada cada uno de los contenidos de los documentos en vectores entonces una vez que hago Esto me interesa sa ver así como en aquella oportunidad yo supe que tenía vectores de cuánto de 18 posiciones o sea documento uno tenía un vector de 18 posiciones documento dos lo mismo y documento 3 lo mismo lo que variaba es que elemento Qué valor tenía en cada una de esas posiciones bien en este caso es lo mismo y lo que hago es un shape de x30f para ve eso puedo descubrir o empiezo a poder descubrir que sigo teniendo mis 973 observaciones pero esas 973 observaciones tienen 26807 posiciones es decir que cada una de esas observaciones son vectores de 26807 posiciones Por qué Porque evidentemente a los a lo largo de todas esas 973 news noticias o descripciones de noticias se ha podido determinar o llegar a la conclusión que todo ese conjunto de palabras conformó un vocabulario no repetitivo recuérdense este este tema importante que no no repite no es la cantidad literal de palabras sino la cantidad de palabras diferentes por decirlo de alguna manera de 260807 palabras bien Es decir insisto Este ejemplo sencillo que ten tenos acá de 18 de vectores 18 posiciones ahora se transforma en este ejemplo de este lap en vectores de 2687 Cuántas 973 tan igual que la cantidad de observaciones Obviamente que esta transformación también tiene lugar en las 2 44 que representan el conjunto de bien aclarado esto vamos a crear el modelo bien el modelo lo creo con model multinal nave byes como si hubiera creado modelo de regresión logística de árbol de decisión de bosque aleatorio bueno el que fuere de soport vector Machine que fuera en este caso otro modelo diferente hago un fit igual que como lo hacía antes y mido si a través del predict la posibilidad de tener la el score vinculado al modelo de train y al modelo de Test entonces creo el modelo lo entreno y establezco la predicción y luego hago la medición de la precis lo que vamos a hacer a continuación es aplicar una técnica de stemming o de lematización para reducir la cantidad de dimensiones que hemos tenido como resultado de esta primera aproximación de cada uno de los vectores que representa cada una de estas noticias Pero antes de pasar a eso quiero rever un poquito este concepto de la precisión del modelo de train y test esto nosotros lo venimos trabajando desde Machine learning y en un primer análisis podremos pensar de que es muy bueno el score de modelo de TR y el score de modelo de Test eh No es bajo pero tiene esta cuestión de que está un poquito alejado del modelo de entrenamiento y eso consideramos en el mundo de machin que no es bueno aquí en el caso de los textos esto hay que revero muy bien y hay técnicas para poder justamente superar hasta el límite de lo posible esto por qué Porque los textos en realidad muchas veces tienen una diversidad que cuando separa un conjunto de entrenamiento un conjunto de Test puede ser que en algún caso esa separación sea muy diferente y si bien uno pretende que sea un algoritmo que generalice lo mejor posible esto depende mucho en el mundo en nlp de cómo estén distribuidos los datos entre el conjunto de Test y entrenamiento Así que a tomarlo como algo no tan malo Este score Y si vamos por supuesto que se puede mejorar desde ya por supuesto hay otros modelos que son más avanzados Queen scores más altos pero si vamos a centrar nuestra idea en cuanto a este práctico con lo que dije Recién con el propósito de reducir el tamaño de los vectores para eso tenemos el stemming la climatización que vimos en el lab anterior empezamos por el stemming en donde lo primero que vamos hacer es como hicimos en el lab anterior importar las librerías siempre usando nltk en el caso de temin Recuerden que en el caso de la matización usamos space bien cargo también la información de las librerías que sirven para tokenizar y para quitar las Wars y cargo eh o Creo mejor dicho una variable que es una instancia no steaming destacando que voy a usar un lenguaje en castellano y luego voy a definir una función que lo que va a hacer es recibir un texto en este caso el de cada noticia y tokenizar Y estimarlo sí En realidad va a hacer varias cosas en principio lo va a tokenizar lo va a estimar Y además le vamos a quitar toda aquella palabra que no presente parte del vocabulario alfabético sí eh como lo son por ejemplo los signos de puntuación H Entonces vamos a hacer todo esto en esta función que tenemos aquí abajo pero Primero lo primero que es eh crear el steamer y ahora repasamos la función que estamos creando Aquí bien el criterio de esta función que es esta que está aquí sería vamos a la línea siguiente vamos a crear una nueva columna que vamos a poner New stem como diciendo bueno es la news pero habiéndole aplicado el proceso de steaming y vamos a hacerla justamente tomando la news original y aplicándole con el método punto Apple el la función token steam la función token steam va a ser lo que dijimos recién va a hacer en principio una tokenización y luego va a serer el stemming aplicando dos acciones más en principio reducir todo el texto a minúscula y además quitar cualquier elemento que no sea de tipo alfabético todo eso lo tengo en esta función Ahora sí vamos a la función justamente la función se llama token steam tal cual yo lo puse aquí en este appli y luego lo que recibe es un text el text en realidad va a ser cada una de las news es decir yo le voy a ir pasando a través de método Apple justamente cada una de las news cada una de las news va a ser tokenizado como con Word token i pero previamente esa news va a ser transformada toda a minúscula entonces token hizo lo que previamente fue transformado a minúscula Con eso tengo en tokens todos los tokens de esa newo y lo que voy a hacer va a ser recorrer cada uno de esos token estimar losos sí que tal el sufijo se acuerdan de esto que vimos hace un rato lava anterior perdón y lo voy a hacer por cada token de tokens o sea por cada palabra de esa news diciéndole que esa palabra tiene que estar dentro de token Alpha tiene que ser un carácter alfabético finalmente voy a devolver esto Esta combinación que es típica de este tipo de funciones que es un espacio en blanco punj Sims que lo que hace es como voy a recibir un montón de palabras de tokens que están estimados voy a ponerlo uno al lado de otro justamente se hace con esta instrucción con el este espacio en blanco punto join que juntar justamente los steams que son los steam la salida de toda esta lógica que acabo de mostrar recién bien ejecutamos esta función cuando se ejecuta la función Es simplemente para que quede cargada la función dec la función no hace nada sino hasta el momento que es usada y luego la voy a usar una vez definida una vez que es reconocida para este este Notebook en esta línea que está acá que como decía recién va a crear una nueva columna que es la news pero ahora estimada tokenizado y habiéndole quitado justamente las letras en mayúscula y los caracteres que no son alfabéticos bien ahí terminó ejecutamos en Entonces esta línea y para ver cuál es el resultado voy a tomar la primer noticia la primer New habiéndole aplicado todas estas cuatro cosas que dijimos recién Está bien entonces la ejecuto y veo que el resultado es esto para ver bien ya se lee Sí cuando lo estamos leyendo a primera vista vemos que ha recortado muchas palabras justamente con este criterio de quitarle el sufijo si voy hacia arriba y la comparo con la frase original durante el foro la banca articuladora empresarial para el desarrollo sostenible etcétera etcétera se transformó en duran el for la Ban articul empresarial par el bueno Esto insisto no lo tomen como una cuestión de decir bueno Esto destruye el lenguaje transforma en algo ilegible estamos hablando de cómo maneja internamente el np con esta cuestión de los vectores y esta reducción a pesar de que a la vista humana creo que se interpreta bastante claramente per obviamente no forma parte del Castellano puro puede nuestro inteligencia interpretarlo de manera correcta bien a partir de ahora qué vamos a hacer ya no vamos a entrenar un algoritmo con news Sí y con type sino con news steam y con type por eso redefin la x y la i y bueno separo conjunto de Test y conjunto de entrenamiento y hago la vectorización O sea todo lo que hice antes en TR lo hago en una sola y lo que voy a ver ahora lo más importante fíjense ahora tengo 973 observaciones igual que antes pero el vector no tiene la misma dimensión igual que antes por qué Porque justamente esta cuestión de haber quitado el sufijo de las palabras hizo que mi vocabulario ahora sea mucho menor y Por ende la dimensión de mis vectores es mucho menor si me fijo Cómo era la dimensión anterior era 26 6807 Y gracias al stemming se reduce a 11928 menos de la mitad lo importante y lo significativo de esto bien creo el modelo y voy a medir la precisión y veo que la precisión pasó a ser 092 y 078 bueno a primera vista uno puede decir pero estamos peor que antes a ver siempre seamos medidos con esto por qué Porque justamente si es un poquito inferior antes tenía 093 y 080 ahora tengo 092 y 078 Pero tengo una pequeña diferencia de precisión con un conjunto de vectores que tiene menos de la mitad de la dimensionalidad  bien y finalmente para cerrar este lap vamos a la climatización La idea es seguir un proceso parecido al stening para verificar que justamente a través de estas técnicas se reduce considerablemente la dimensión de los vectores pero Pero obviamente tomando el texto original no el último que está estimado Sí entonces lo que tengo que hacer es instalar Eh bueno que es import spacy sí habiéndolo instalado antes y instalando también el el diccionario de lenguaje en castellano bien Digo como lo vimos antes a continuación creo una variable nlp como hicimos hoy space load y cargo justamente ese dicionario en castellano Y a partir de esto lo que tengo que hacer es trabajar como hicimos hoy con una nueva función esa función la idea avanzo Está aquí es lo mismo que hicimos hoy así como teníamos hoy el news steaming creo que lo habamos puesto a ver vamos aquí arriba el news stem Sí ahora vamos a crear un news lema que es la news con Apple aplicándole el método Apple redundancia solamente que en este caso no vamos a usar la función que usamos hoy que le habíamos puesto aquí arriba token steam sino que le vamos a poner aa función token lema esta función es parecida a lo que tuvimos hoy en este caso lo primero que hace es tokenizar no obviamente con el mismo método que antes porque ahora Estamos usando la librería spacy O sea que lo hace con nlp sí nlp acuérdense que es la instancia que hice de spacy load y lo que hace es justamente tokenizar en virtud de un texto que es pasado a lo mismo que hoy con otro método con otra librería pero lo mismo que hoy sí con lo cual ahora tengo tokens y creo un for parecido a lo que estuvimos hoy donde digo justamente token lema O sea aplico la lematización a cada token por cada token de tokens sí tokens es esta variable que insisto tiene todos los el contenido de una determinada news en un determinado momento hecho en tokes dividido en palabras sí bien entonces por cada palabra del conjunto de palabras siendo que el conjunto de palabras y toda la news siendo que ese token esté dentro de Alpha sea un elemento de is Alfa que responde a este método is Alfa que es propio de python no que también lo usamos hace un rato para ver que solamente tenga en cuenta los elementos de orden alfabético entonces una vez que tomo los tokens de la New y selecciono solamente aquellos que son y Alfa hago la ligmatizacion soy aquí arriba recuerdan un sh in the steams decía que de esta manera arma como una cadena donde vuelve a juntar todas las palabras que antes separó en la toqua S como un proceso inverso s no saco o separo las palabras le aplico el stemning o le aplico la lematización y luego las vuelvo a juntar para que me devuelva toda la news completa pero habiéndole pasado antes el stemin y ahora la lematización Bueno eso es lo que aplica esta línea lo que vamos a hacer con esto es cargar sí la la función si no no me la va a reconocer para que ahora allí yo cree esta esta nueva columna bien habiendo terminado este proceso al igual que lo que hicimos hoy vamos a imprimir la primer New en el caso de la lematización y fíjense que justamente aplica la lematización en este resultado y es diferente Pero también diferente al este me tornaba esto en una en un conjunto de palabras mucho más ilegible porque quitaba el sufijo en este caso lo que hace es tratar de reemplazar eh A esa palabra por su palabra raíz con lo cual en muchos casos una palabra puede estar más de una vez sí eh o reiteradas veces en este texto y no tener otra similar a la cual que haya que recurrir para buscar una palabra raí se acuerdan que hoy decíamos correr corriendo y quizás aquí por ejemplo la palabra durante fíjense que está está al 100% Por decirlo deguna manera porque quizás no haya otra palabra en todo este abecedario de todas estas news con la cual pueda confrontarse y poder lograr una una palabra raíz que represente a durante y otra palabra que esté muy cercana a durante sí Entonces esto obviamente transforma este texto en algo más legible pero aún así ha hecho una reducción importante de la dimensionalidad de los vectores Y eso lo vamos a comprobar ahora para lo cual lo que vamos a hacer es en principio como hicimos hoy con el caso del stemin redefinir la x y la i separar conjunto de tes y entrenamiento y vectorizar todo este conjunto ya está y ahora voy a averiguar Cuál es el tamaño del vector fíjense que el tamaño del vector es de 16736 es decir es una reducción inferior Sí en resultado digamos tiene una dimensión mayor Sí pero bueno en en inferior en en el resultado porque esta ha sido la mejor reducción porque redujo a 11,000 y esta redujo a 16,000 pero si la comparo con la dimensión original que era de 26,800 aún así es una muy buena reducción que está un poco por encima del 50% del vocabulario que se obtuvo al principio cuando no había aplicado ni el stemi ni la lematización bien en cuanto a eh crear el modelo lem matizado y eh medir la precisión veo que tengo buenos scores y que es guarda bastante relación con el caso de El el stemin tenía 92 y 78 Aquí tengo 91 y 79 sí es decir que la conclusión de esto es que la reducción de dimensiones que es muy importante no ha afectado el nivel de precisión del modelo tanto en el caso del stemin como en el caso de la climatización con lo cual tengo modelos casi una precisión prácticamente idéntica si una pequeñísima diferencia pero con reducciones de tamaños de El 50% y en el caso de la alción casi un 50% Así que esto es lo más importante tener en cuenta de los propósitos de este práctico y para todo lo que venga ahora más hasta aquí  Esta es la segunda parte de la clase número 28 te invito a empezar con   ella  bien aquí estamos entonces en la segunda parte de esta clase donde como dijimos en la primer parte de esta clase vamos a trabajar ahora sí con una red neuronal recurrente con lstm y con la opción bidireccional que vimos en la teoría que abordamos en la primera parte de esta clase lo primero que vamos a hacer es instalar el la aplicación que utilizamos para leer archivos de tipo eh PDF Así que arrancamos con eso vamos a instalar luego una serie de librerías que ya vamos a ver bien dónde las vamos a aplicar vamos a montar el Drive para Acceder al archivo que vamos a usar para esta clase y luego vamos a extraer con esta función extraer texto de PDF que la venimos usando en las clases anteriores el contenido de ese archivo y lo vamos a llevar dentro de una variable texto es es exactamente la misma función que usamos en las clases eh anteriores Así que esto ya seguramente no hace falta que lo expliquemos pero sí vamos a abordar Cuál es el contenido del archivo que vamos a usar en este caso para esta práctica el archivo se llama relatos.pdf y lo he armado en base a pedirle a ch gpt que me haga o me simule o me genere relatos de relatores de fútbol argentinos sobre lo que sería una jugada previa y el gol sí es decir que la idea sería que con varios relatos de este tipo generemos un Corpus la inteligencia pueda entrenarse con ellos y a partir de eso yo haga lo que tiene que ver con el tema de esta clase que es generar texto a partir de una frase es decir la generación de texto puede ser algo que tiene que ver con un prom como nosotros vemos muchas veces en chat gpt donde le digo que me cree una poesía que me cree un una definición o un relato que yo quiera hacer una exposición que yo quiera hacer desde la nada simplemente dándole un prom o alguna serie de características pero también existe o en lado dentro de lo que es la generación de texto en cuadrado dentro de lo que es la generación de texto está también la situación parecida a lo que nos pasa a nosotros cuando por ejemplo en Google escribimos un mail y nos aparecen palabras sugeridas a la derecha bueno eso también es generación de texto y algo de eso es lo que vamos a hacer generar un texto completamente como decía recién en ch PT que pimos por ejemplo que nos escribe una poesía eh requeriría un Corpus muy grande o una inteligencia entrenado con un Corpus muy grande para generar todo un texto y que tenga sentido y que sea lo que nosotros esperamos no genere en este caso lo que vamos a hacer va a ser Buscar una inteligencia a la cual yo le puedo dar una primer frase supongamos de tres o cuatro palabras parecido a lo que insiste lo queos hacemos cuando escribimos un mail y que nos genere las 10 o 20 palabras próximas a esa primera frase y que podamos ver que tenga conexión con el inicio de la frase Sí bueno Esto un poco lo que vimos en la teoría cuando ejemplificamos el caso de bueno que cuando estudié en Alemania me enseñaban a estudiar alemán me enseñaban hablar en alemán bueno o que es un día azul Sí bueno esa cuestión es cómo generar la próxima palabra de acuerdo al contexto que traigo aquí el contexto va a ser muy cortito un grupo de palabras tres cuat cinco no más que ellas y después lo vamos a pedir Insisto que nos escriba 10 o 20 palabras que seguirían a ese texto según lo que entiende la inteligencia Según Lo Que Fue entrenada bueno y el resultado Obviamente el resultado en sisto está muy acotado este archivo está lejos de ser un archivo un muy grande pero bueno es una primer prueba y justamente La idea es que ustedes puedan después con esto poder buscar otros archivos más grandes justamente en la finalización de esta clase Voy a hacer eh una una muestra de Cómo entrenar se acuerdan con lo que usamos la clase pasada los 10 libros de recursos humanos a ver si nos genera algo mejor dado que en ese caso se trata de un Corpus mucho más grande volviendo al punto en lo relativo a esta función exalar texto desde PDF lo que vamos a abrir va a ser este archivo que lo vamos a señalar Primero aquí y después lo vamos a ver aquí en en el código sería Drive my Drive archivos nlp y dentro de nlp está relatos PDF Sí bueno cerramos esta ventana y esa ruta es la que tenemos aquí sí bien con lo cual ya tenemos definida la función Insisto que la venimos usando en las últimas prácticas y la apertura del documento que insisto es lo mismo porque usamos la misma función Así que lo ejecutamos sin más palabras y ya tengo entonces dentro de documento justamente ese contenido de relatos si ahora yo veo el contenido de documento Bueno aquí está sí obviamente es un poquito más largo que esto pero bueno me muestra como siempre la primer parte s Buenas tardes amigos nos encontramos en estea emocionante partida bueno el rato no es 100% tal cual como relato en argentino pero se aproxima bastante ha hecho un buen trabajo de chat gpt igual obviamente la idea que yo podría eh haberle puesto más proms para pedirle como ustedes saben chat gpt que no estoy conforme con lo que me generó y me gener algo mejor en base a algunas cuestiones algunas consideraciones para que vaya generando mejores l pero como base para esto es más que interesante eh obviamente como dijimos lo vamos a hacer después con los los Corpus de recursos humanos Y ustedes lo pueden hacer con el material que ustedes les parezca mejor generado por char gpt o libros o documentos del tipo que ustedes consideran Bueno una vez aclarado esto lo que voy a hacer como tantas otras veces es eh colocar todo este texto en minúscula Y splitear en este caso lo voy a splitear por enter es decir fíjense por este carácter barra n que creo que ya saben todos ustedes que esto significa el la marca de enter sí bien o el corte de línea para decirlo de otro modo con lo cual ahora en Corpus tengo bueno piteado este documento en párrafos que están separados eh en cada caso en Que aparezca un un indicador de corte de línea o que ve da de línea o ent bien para ver esto ahora veo el Corpus y veo lo mismo que vi anteriormente aquí arriba pero obviamente ahora separado porque Corpus es una array que tiene párrafos del texto original obviamente en ese párrafo ya no existen estos caracteres barra n porque han sido considerados solamente para tomar como referencia para separar en en oraciones todo el texto y obviamente los quita del Corpus bien ya tengo el Corpus Entonces explito por lo tanto lo que vamos a hacer ahora es simplemente como hacemos siempre para que se vea más así Más allá de que Perdón aquí visualmente ya es bastante claro como se ve este la separación en oraciones bueno imprimo la oración sub uno y veo Buenas tardes amigos nos encontramos qué es lo que veo aquí al principio de este texto sí ven La cero es un es un espacio vacío sí la uno es esta frase que ven aquí y que reproduzco aquí cuando le digo que el número uno bien hecho esto como siempre ahora voy a tokenizar con lo cual creo el tokenizador y lo que hago justamente es entrenar el tokenizador con el cor acuérdense que para que pueda tokenizar bien ese texto yo tengo que to entrenar Perdón ese token con el Corpus que yo tengo Sí y luego justamente empiezo a a partir de eso este tomar la referencia de Cuántas palabras tiene ese tokenizador con el Word index Acuérdese que el Word inish lo que hace es darle a cada palabra un valor numérico sí no Como tantas palabras haya sino por tantas palabras diferentes que haya sí bien Entonces ejecutamos esto y ya tengo obviamente en el total words la cantidad de de de palabras que tiene mi vocabulario en base al Corpus y lo imprimimos y podemos ver el tokenizador donde está el número que le asignó a cada una de las palabras el uno la dos en tres bueno esto ya lo hemos visto un montón de veces y Cuántas palabras en total forman parte de mi vocabulario generado a partir del Corpus que son 442 palabras bien antes de continuar con la clase práctica de código duro Vamos a abordar algunos conceptos teóricos que son necesarios ir viéndolos este en detalle y paso a paso para entender todo lo que va a venir después en principio lo que tenemos que diferenciar Aquí Cuál es el objetivo que perseguimos en la primera parte de esta clase y el objetivo que estn buscando ahora el objetivo que perseguimos en la primera parte de esta clase fue resolver un problema de clasificación clasificación de texto como clasificación hemos hecho otras veces problemas de clasificación hemos hecho otras veces con redes neuronales convolucionales con redes neuronales y con Machine learning sí es un problema clásico solamente que en este caso está contextualizado en lo que estamos haciendo justamente que es el abordaje de los textos bien ahora eh lo que vamos a hacer es otro tipo de problema como ya lo adelantamos hace un rato que es de generación de texto en el caso de la clasificación de texto y todos los problemas de clasificación en general sabemos que necesitamos un conjunto de datos de validación Sí por qué Porque justamente para ver si ese algoritmo está aprendiendo de que ese texto por ejemplo como veíamos recién era sarcástico no era sarcástico yo tenía que tener textos etiquetados como sarcásticos y como no sarcásticos para que el algoritmo pudiese entrenar pudiese aprender y luego con el conjunto de validación Yo podría ver si el algoritmo está resolviendo bien o mal en base Al conjunto de tex Bueno estoy explicando lo que ya hemos visto un montón de veces creo que está no hace falta explicarlo mucho más que esto Pero cuál es la cuestión aquí cuando estoy llevando adelante un problema de generación de texto No necesitamos como bien dice aquí un conjunto de validación Sí por qué Porque es una cuestión predictiva sobre lo cual está creando algo y no hay algo que ya ha sido creado antes para ver si esa creación está bien o está mal esto impera el criterio nuestro que somos los que vemos el resultado de la generación que hemos pedido Sí entonces aquí hay algunas cuestiones muy particulares a través de las cuales el algoritmo aprende obviamente no tiene una etiqueta con lo cual aprender Pero va a tener un sistema que va a tener como una especie de etiquetado parcial que va a ir construyendo a partir del cual va a aprender Bueno cómo es esto vamos a verlo paso a paso Vamos a tomar como ejemplo una línea cualquiera del Corpus que dice que la semana tiene más de 7 días Sí es parte del relato donde el relator usa esta frase como ya tenemos este Word Index creado donde ls un la s2 etcétera etcétera sí lo que vamos a hacer es transformar Sí cada una de esta esta frase digamos esta frase en concreto Sería para todas las frases pero en el caso del ejemplo de esta frase en concreto pasarla a una secuencia de números sí entonces 2 8 3 10 31 13 3 3 12 3 13 sería como el número que representa esto es un ejemplo por supuesto cada una de estas palabras sí supongamos que le correspondía a eso S Bueno una vez que yo convierto esta secuencia de palabras que forman el texto en una secuencia de números a partir del Word Index lo que se hace es armar subsecuencia que tienen que ver con ir Armando parejas y que eh esas parejas vayan creciendo en tríos en cuartetos en quintetos o sea arranco de un par y después le voy sumando sí En las siguientes secuencias un número hasta llegar al final con Este ejemplo se ve claramente aquí primero tengo 2 o porque son los dos primeros valores la siguiente secuencia es 28 31 la siguiente 2 8 31 31 se ve claramente es muy fácil de ver no entonces la última secuencia Cuál va a ser la que va a coincidir con la expresión original 2 8 3 10 3 11 16 3 3 12 3 13 Es decir genera sobre una secuencia de números Sí una cantidad de secuencias n secuencias que es igual a la cantidad resultante entre conformar una primer pareja y después cada uno de los elementos que se vayan osando Sí por eso en este caso tenemos 1 2 3 4 5 6 7 cuando justamente el el la Ray inicial era 1 2 3 4 5 5 6 7 8 ese siempre va a ser uno menos porque arranco de un par bien hasta ahí vamos con la primer parte de esta historia y lo vamos a ir haciendo en código para que se vaya viendo justamente en el transcurso de lo que vamos haciendo y podamos visualizarlo por eso esto que tenemos aquí es lo que vamos a codificar aquí creo una una Ray input sequences Sí y lo que voy a hacer es forline in Corpus es decir voy a recorrer cada uno de los párrafos de mi Corpus y lo que voy a hacer es convertirlos en secuencia text to sequences de cada Line y a partir de tokenizer lo voy a poner dentro de token list es decir que lo que voy a hacer es cada línea s voy a hacer algo como esto cada línea nuevo aquí la voy a convertir en est O sea que ese for está haciendo eso que señalé recién por cada línea una vez que hago esto lo que tengo que hacer es la segunda parte de lo que vimos aquí es decir ir Armando este conjunto de secuencias a partir de una secuencia de números bien cómo hago for in Range Range que va de uno a token list Qué es token list bueno es la cantidad de elementos que tiene justamente mi token es decir en este caso dijimos se acuerdan que teníamos ocho entonces va de un a o eso que quiere decir que va a ser uno menos que la cantidad total porque acuérdense que empieza desde cero la numeración Entonces qué va a hacer en cada caso va a armar token list si token list va a ir desde el principio hasta y + 1 Sí en el primer caso el principio Cuál va a ser c y + 1 es 2 y acuérdense que cuando pone hasta dos quiere decir que siempre es la anterior O se va 0 y 1 y así en cada vuelta I va a valer un valor más y Por ende el valor hasta de lo que yo vaya a poner en N Grand sequen va a ir siendo un valor más cuando termine este for el resultado va a ser algo similar a lo que vimos aquí en este sencillo ejemplo bien lo ejecutamos y ya tenemos el resultado bien en este caso por qué armo esta estrategia porque justamente la forma de aprender que va a tener este algoritmo es la siguiente aquí justamente lo he puesto en un párrafo para que se entienda bien la idea es decir yo voy a armar parejas después tríos después cor más secuencias y para cada caso es decir cuando haya una palabra va a aprender de que existe como sugerida una próxima cuando haya dos palabras también va a existir una próxima cuando haya tres palabras Existirá una próxima Y así sucesivamente Qué quiere decir que la inteligencia tiene que entender que que después de dos y 8 hay un 310 después de dos y 8 310 hay un 31 y así sucesivamente Esa es la forma en que se entrena esta Esta generación de texto este algoritmo de generación de texto porque no hay una etiqueta más allá que la etiqueta la vamos a fabricar no hay un etiquetado previo va a haber una suerte de etiqueta que justamente se va a generar a partir de esta lógica que estamos Armando por lo tanto a continuación lo que vamos a hacer en principio es eh medir eh Cuál es el largo de la máxima oración por qué porque de aquí ya tenemos un problema que ya lo vemos justamente cuando hamos esta cadena de secuencias de que todas las cadenas no son iguales la primera es de dos y la última de ocho y eso no lo podemos dejar así tenemos que tener vectores iguales sabemos que no puede haber vectores irregulares bien Por lo tanto lo primero que vamos a averiguar es cuál es el vector más largo Sí cómo bueno con esto que hemos puesto Aquí vamos a buscar justamente Max de lende x cuando x Sí es lo que está en las input sequences Es decir de todas las input sequences que acuérdense que lo generamos aquí que tiene todas las secuencias de todos los párrafos es decir insisto todo esto de todos estos imagínense todo lo que hay Sí vamos a buscar cuál es el más largo entonces for x in imp de cada uno le vamos a medir el len y del len o sea de los largos de cada uno Dame el máximo Y eso lo voy a poner dentro de una variable sequen len que después la vamos a imprimir para conocer Cuál es el valor de eso y vemos que es 29 Cuál va a ser el trabajo que vamos a llevar a cabo Ahora va a ser un trabajo que se llama padeo es un proceso Perdón que se llama padeo que consiste en tomando esta cuestión de la dimensión máxima rellenar con ceros cada una de las eh posiciones que no tienen número entonces supongamos que para el ejemplo que venimos trayendo que no es este 29 que tenemos acá sino el ejemplo teórico Sí vamos a suponer que la cantidad máxima es 10 Entonces qué tengo que hacer se acuerdan la primer secuencia era 2 o bien a esa 28 qué hago le pongo och ceros sí formando un vector de 10 posiciones y lo que le voy a poder o la opción que tengo para hacer es si quiero que los ceros estén adelante o estén atrás obviamente como yo estoy haciendo algo que me va a buscar siempre el próximo valor me conviene que los ceros estén adelante y no atrás sí bien Entonces esto es lo que le decía la clase pasada de que si bien los en beding busca eh en general que no hayan posiciones con valores ceros sino que estén ocupadas todas las posiciones A diferencia de los problemas anteriores que había por ejemplo el sistema de conteo de palabras que generaba muchos espacios vacíos para este problema particular de generación de texto no queda otra que que todos los vectores sean iguales y como tengo que ir Armando estas secuencias y estas subsecuencia no queda otra que rellenar esos espacios con cero bien se entiende Este ejemplo gráfico se enti Este ejemplo gráfico lo que vamos a hacer ahora es justamente hacerlo en el código que venimos trayendo es dec tengo que generar ahora todos vectores de 29 posiciones para ello lo que hago es decirle que a input sequences que tiene toda esa cadena que hablamos recién quiero que su largo máximo sea Max sequen Lane que es este 29 que tenemos aquí y con padding le digo como dije recién dónde Quiero los ceros pre antes post después los quiero antes Entonces le pongo pre con np justamente transformo todo este resultado en una array y lo vuelvo a poner en el mismo input sequences que tenía antes s ejecuto esto lo puedo poner otro nombre variable aparte bueno es para reutilizar la misma nom el mismo nombre de variable Perdón podría haber sido la misma Ahora viene una cuestión muy interesante que es Cómo se arman las etiquetas de una lógica que vengo trayendo de una resolución de un objetivo que no tiene etiquetado bien muy fácil yo tengo que tratar de tomar en esta secuencia que tenía acá arriba que tomamos como ejemplo Sí el criterio de establecer de que el último elemento es la etiqueta Por qué Porque como dijimos hoy sí si yo tengo que el primer elemento es un dos tiene que haber algo que me diga sabes qué después del dos lo habitual es que siga un ocho cuando tengo un dos y un O tiene que ver algo que diga sabes qué después del dos y el 8 lo mejor o lo más habitual no lo mejor lo más habitual es que haya un 310 eso es lo que tiene que ir aprendiendo la inteligencia Y cómo lo va a ir aprendiendo bueno justamente lo que yo voy a hacer es que cada elemento final de cada vector va a ser la etiqueta va a ser el I se acuerdan este concepto de i Sí y el resto va a ser la x es decir que estos vectores si tengo vectores de 10 posiciones para este ejemplo no de los 29 decir Tratamos de separar El ejemplo teórico que vengo trayendo del ejemplo práctico de lo que estamos llevando adelante en el código no bueno si estoy parado en esta cuestión de que todos los vectores son de 10 posiciones Entonces todos los vectores van a tener nueve posiciones para definir a la x y una posición para definir a la y s si vamos al código nuestro serán 28 para una cosa y la número 29 para la otra sí bien entonces Este ejemplo que está aquí gráficamente Sí bueno lo que voy ahora a hacerlo es en la práctica con lo cual lo que voy a definir ahora es xs y y lo que digo que de input sequences me tomé todas las filas y solamente las columnas hasta la men1 Qué quiere decir tomo todas menos la última y en el caso de labes que sería la i va a ser exactamente lo contrario tomo todas las filas pero la columna solamente la última bueno ejecuto esto y finalmente vamos a generar las is yo aquí le puse labes pero todavía en labes no tengo la forma de arma de las I Por qué Porque en el caso de las labes no vamos a trabajar con el número si que corresponde a cada etiqueta 8 31 31 etcétera etcétera sino que vamos a tener que formar un vector que tenga tantas posiciones como el vocabulario completo Acuérdese que vocabulario completo tenía Tenemos aquí arriba 442 si este elementos y para representar el o que sería por ejemplo la primera etiqueta vamos a tener que hacer algo como esto sí poner 0 1 2 3 4 5 6 7 Se en la séptima posición poner Perdón en la octava posición que corresponde al subíndice número siete sí voy a tener que poner justamente un uno representando que ese elemento que está allí es el número ocho Pero por qué Porque está o tiene un uno en la octava posición bien una vez logrado esto hago lo que acabo de mostrar justamente para crear mi S A partir de lab sí diciendo que la cantidad de elementos totales es igual a la cantidad de palabras totales que tiene mi Corpus y lo que voy a hacer es pasarlo a categórico Sí el número categórico sí es en una representación de ceros y unos poniendo el uno en la posición que le corresponde al número natural bien haciendo esto Entonces ya tengo mi x tengo mi es decir tengo mi x y mi etiqueta que en realidad no vino desde el principio como siempre nos pasa cuando hacemos este o entrenamos algoritmos sino que la tuve que yo ir generando con todo este proceso que descubrimos en todos estos pasos precedentes bien insisto tengo mi x tengo mi ahora lo que tengo que hacer Ni más ni menos como dice Este título aquí es crear nuestro modelo de red neuronal en este caso red neuronal recurrente vamos a recurrir a sequential que ustedes saben que es este el método que usamos la cas que usamos de tensor flow para ir poniendo o definiendo cada una de las eh capas de la red que vamos a crear de manera secuencial bien la primer capa es una capa de eding Sí qué es lo que va a hacer Bueno lo que ya sabemos es transformar todo este conjunto de vectores de 29 posiciones en embeddings por lo tanto lo que tengo que darle como información es cuál es el tamaño de mi vocabulario las 400 y pico de palabras y con 200 le digo el tamaño de los vectores en los que yo quiero que haga los embeddings Recuerden que los vectores que tengo son de 29 posiciones pero con 29 posiciones no vamos a hacer un embedding porque eso obviamente no tiene que ver con el sistema de similitud que vimos en las clases pasadas Por ende defino que quiero eh vectores de 200 posiciones que igual fíjense recuerden siempre que 200 es mucho menor que la cantidad de palabras o sea es la mitad o sea que eso Si volvemos a a sistemas más viejo como el conteo de palabras obviamente tenemos vectores de 400 y pico de posiciones en este caso son 200 las que elijo y luego le indico de todo lo que tiene que ver lo que va a entrar De qué tamaño es o el input lens sí los textos que van a entrar estas secuencias que están numéricas que representan los textos que van a entrar De cuántos son bueno de un largo igual al máximo 29 men1 porque acuérdense que la etiqueta no va a entrar en este caso sí bien Entonces luego lo que hago es agregar una capa de tipo lstm y bidireccional dos conceptos que vimos en la primera parte de esta clase lo defino con 150 neuronas y finalmente voy a hacer una capa de salida de tipo softmax por qué porque acá no tengo que hacer un problema de clasificación de cer o uno si es es sarcástico o no sarcástico acá lo que tengo que decir es cuál de las 400 y pico de palabras voy a dejar de ser y pico y voy a decir el número que corresponde para ser lo más preciso posible 442 palabras Cuál de las 442 palabras sí es la que debería seguirle a la que bueno forma parte del texto que vengo escribiendo por ello la función de activación de la capa de salida tiene que ser sof Max porque tengo 442 posibles resultados de salida y luego hacemos el sumary para ver justamente el modelo completo como lo vemos habitualmente y bueno dicho Todo esto lo ejecutamos bien allí tenemos el sumar o el resumen de lo que es la composición de la red que acabamos de crear y lo que vamos a hacer ahora es como siempre hacer la compilación y el entrenamiento bien allí terminó el entrenamiento 200 os bastante para que a pesar de que es un texto muy cortito trate de tener un proceso de aprendizaje importante y lo que vamos a hacer es graficar vos aquí que la curac es de 097 es muy bueno vamos a graficar la curva justamente de aprendizaje que es que está aquí donde vemos justamente como ha crecido y Que obviamente en los entrenamientos ya prácticamente de el 25 en adelante si ya la evolución fue muy poquita es decir que ya la mayor parte del aprendizaje la obtenido justamente hasta epoch 25 vemos cuando lo recorremos aquí que ya es así O sea inclusive en algunos casos como en el 146 ha sido superior al final bueno Esto nos puede llegar a entender que Obviamente el número de epos ha sido más que suficiente desde antes de los 200 que yo le puse bien lo que vamos a hacer ahora finalmente es generar el texto con nuestro modelo con lo cual la frase inicial va a ser lleva la pelota Sí y le vamos a pedir que eh haga 20 palabras luego de la palabra pelota que sugiera 20 palabras luego de la palabra pelota teniendo como referencia el entrenamiento hecho con el Corpus que yo le pasé bien Lo que vamos a hacer con esto es eh for una variable cuando usamos el el ión acuérdense que no voy a crear ninguna variable esto para iterar Tantas veces como nextwork o sea va a ir agregando 20 palabras y Esto va a iterar 20 veces en cada una de esas iteraciones lo que va a hacer es tomar la frase inicial Y tokenizar sí para que esté en la misma sintonía que eh toda la lo que hemos entrenado y lo ponemos dentro de tois Y qué vamos a hacer al igual que como hicimos con el Corpus original padar sí es decir agregarle con ceros completar con ceros para que tenga el mismo régimen sí que tienen los vectores del Corpus original es decir los 29 - 1 bien con los ceros puestos adelante Bueno lo que ya vimos anteriormente que hicimos con las secuencias del Corpus original lo mismo con este pequeño texto que lleva la pelota tiene que estar en la misma sintonía una vez que hago esto lo que hago es con predict de model acuérdense que ese modelo que creamos recién darle como como input el token list Sí y decirle Cuál de todas las predicciones es la mayor Sí con eso ar Max y lo pongo dentro de predict sí es decir cuál es la palabra más eh que tiene el mejor score o la que tiene la sugerencia más alta para poder justamente este ser la adecuada para continuar en este caso a la palabra pelota pero esto Va a continuar con lo cual va a ser la palabra la mejor palabra que continúa pelota y después si esa palabra fuera eh el jugador o el él puntualmente sí Cuál es la palabra ideal que va a seguir a él puede ser jugador Cuál es la palabra ideal que va a seguir a jugador Y así sucesivamente pero acá hay un tema muy importante que tenemos que ver bien Qué tiene predicted tiene la palabra sugerida por este algoritmo como que debería ser la más conveniente para seguir la frase desde la palabra pelota no en realidad tiene un número sí Recuerden que lo que me da Aquí vamos al modelo nuevamente la capa densa de salida es una softmax que tenía tantas posiciones como el vocabulario como el tamaño de vocabulario Está bien entonces solros recordemos que el tamaño vocabulario que teníamos era de 442 palabras Entonces qué Me da me da algo como lo que teníamos aquí cuando vimos el tema de las etiquetas Es decir me va a poner un número en una determinada posición si entonces lo que yo tengo que tomar a partir de allí es cuál es ese número y luego Recuerden que tengo un índice donde cada número está asociado a una palabra Por eso voy a crear una variable output Word que va a ser la palabra definitiva que me sugiere este algoritmo y voy a recorrer sí completamente el Word Index es decir el dice de palabras y justamente voy a estar comparando el número obtenido con cada uno de esos índices hasta encontrar cuando coincida el número de índice con el valor de la predicción entonces podré acceder a la palabra al texto de la palabra Y esa palabra será la que pondré en esta variable que Acabo de crear con lo cual finalmente voy a tener que la frase inicial es igual a frase inicial en este primer caso lleva la pelota más el output Word que es como el ejemplo que decíamos recién supongamos que la próxima palabra sea lleva de la pelota é Sí con lo cual la próxima frase inicial Qué va a ser lleva la pelota él y la predicción supongamos que sea jugador luego la frase inicial será lleva la pelota el jugador Y así sucesivamente entendido esto bueno ejecutamos el código y vamos a ver qué resultado nos da y vemos que dice aquí lleva la pelota están los pies del habilidoso mediocampista del Pampa Si miran el documento en alguna parte del relato le pone a un equipo lo pone lo pone como Pampa Sí realmente Endo que la Argentina tiene que ver con eso le puso ese nombre así que de allí viene Este no es algo desubicado que lo ponga porque justamente en el relato Pampa es uno de los equipos que usa el relator el medio capita del Pampa Diego el fútbol argentino que viva el fútbol argentino que viva bueno y ahí sigue bueno es bastante Bueno sí el resultado obtenido insisto porque tiene coherencia lo que dice va siguiendo una línea coherente de lo que está Armando aquí como relato a partir insisto de tres palabras y obviamente de un entrenamiento con un Corpus muy chico para hacer un ejercicio más importante yo aquí les dejo lo mismo que hicimos en el ejercicio de la clase pasada Sí donde tomamos 20 libros de recursos humanos con la idea de poder hacer el mismo circuito donde juntábamos todos los documentos sacábamos las salvaciones limpias y toda esta cuestión y lo que vamos a hacer es entrenar sí a nuestro algoritmo con eh ese con ese Corpus sí Pero cuál es el problema aquí en realidad eh hacer Esto va a llevar muchísimo muchísimo tiempo sí muchísimo tiempo para que tenga una idea yo he he pagado a cola para que me deje usar uno de sus de sus gpu sí que es este es el llamado B1 Sí una de las de las GP más poderosas y he tardado tres horas Sí aquí les muestro el entrenamiento hecho un entrenamiento de 50 sí bastante bueno eh conveniente porque fíjense que no es exagerado como el caso recién fíjense que llego a los 95 72 y todos los scors anteriores han sido inferiores a eso o sea que no he hecho entrenamientos de más Este y bueno obviamente obtengo un resultado pero para que ustedes lo puedan usar y no tengan que hacer todo esto lo que yo es he hecho es el guardado Este modelo y le he puesto model rrh h5 h5 es la extensión con que se guardan los modelos que se generan con t Flow y con model save guardo ese modelo esto lo hemos visto en algunas clases pasadas Sí con lo cual ahora lo que yo debería hacer sería poder justamente usar un modelo pero en lugar de crearlo entrenarlo como está aquí sí lo que voy a hacer va va a hac recurrir a ese modelo por eso acá al final dice utilizar el modelo guardado Sí y Bueno les muestro cómo sería todo el ciclo que tendría que hacer mucho más cortito que toda el anterior para usar ese modelo justamente lo levanto de aquí esto yo se los voy a dejar en el campo virtual por supuesto y acá les sugiero que lo que deberíamos hacer deberíamos hacer todos los pasos desde el principio hasta la creación del modelo o sea evitar el paso que es de crear el modelo de compilar y de entrenar y de guardar sí es decir vamos un poquito hacia atrás yo debería hacer todo hasta este punto Sí o sea este punto no lo voy a hacer este punto tampoco menos que menos porque es el de entrenamiento sí Y obviamente grabar no tiene sentido Porque yo lo estoy tomando de algo está grabado ya luego de esta recomendación vamos aquí abajo y ejecutamos sí esto y nos da este resultado es las eh Perdón la frase inicial es las habilidades intelectuales y acá está la frase que dice las habilidades intelectuales se refieren al saber hacer entre otras palabras son el conocimiento en acción laboral que puede ser desarrollado a través bueno Y esto ustedes también pueden jugar un poquito en ponerle este valor de next Wars en un valor más grande para que genere una frase obviamente mucho más Este mucho más desarrollada no tan cortita un detalle no menor que les dejo también este aquí para que intenten a ver si por ahí con su computadora que tienen sin comprar gpu de de Google de colap este pueden llegar a hacerlo es la posibilidad fíjense que aquí le pongo un break Sí en en esto que hemos hecho aquí de ir tomando los 10 libros de recursos humanos para que tome solamente el primero por allí tomando solamente el primer libro bueno pueda ser más factible lograr un entrenamiento con un tiempo prudencial al que ustedes puedan recurrir Obviamente que aquí ustedes pueden dejar esto andando mucho tiempo y si logran que luego del entrenamiento ustedes Bueno se desatienden de la computadora por decir de alguna manera Este pero se ejecute esta instrucción y esto lo colocan en el Drive Sí bueno si Por ende o por un motivo u otro la conexión se cae la la sesión de cola se cae no va a haber problema porque Ed ya habrán guardado este archivo dentro de su propio Drive Y bueno ya está tranquilo que el día de mañana este cuando ven vuelvan a reiniciar la sesión podrán recurrir a este modelo porque lo tienen en su propio Drive a pesar de que cuando terminó esto y cuando se terminó y se grabó y después se desconectó porque quedó la máquina sin uso como hace Cola habitualmente con Estos espacios este no van a perder el modelo y van a poder volver a levantarlo justamente con esta esta parte que yo les he dejado aquí de utilizar el modulo guardado Sí así que ya sea este modelo que yo les doy o bien otro que generen ustedes con un Corpus propio que puede ser este libro u otro no este pueden asegurarse que pueden darle la posibilidad de dar muchos entrenamientos que dure mucho pero que ustedes no estén pendientes de Cuándo se termina para que no pierdan el modelo Una vez que se les terminó la sesión de cola Bueno hasta aquí Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número    29  Hola a todos Bienvenidos a la clase número 29 del curso de Inteligencia artificial de ies continuamos con el módulo de procesamiento del lenguaje natural y el tema en cuestión hoy es l Chain con concretamente Qué es Lan Chain Por qué deberíamos usarlo y cómo funciona vamos a empezar a dar respuestas a cada una de estas preguntas Lan Chain es un framework de tipo Open source que les permite a los desarrolladores que trabajan con ia combinar grandes modelos de lenguaje como gpt 4 con fuentes externas de computación y de datos el marco Se ofrece actualmente como un paquete de python o javascript o typescript para ser específicos en este contexto en nuestro caso nos centraremos como venimos haciendolo al principio de este curso con python para comprender Qué necesidad satisface Line Chain echemos un vistazo a un ejemplo práctico a estas alturas todos sabemos que ya gpt o gpt 4 tiene un conocimiento general impresionante podemos preguntarle sobre casi cualquier cosa y vamos a obtener una respuesta bastante buena supongamos que queremos saber algo específicamente a partir de nuestros propios datos es decir a partir de de nuestro propio documento podría ser un libro un archivo PDF un Word etcétera etcétera plan change nos permite conectar un modelo de lenguaje grande como gpt 4 a nuestra propia fuente de datos no estamos hablando aquí de pegar un fragmento de un documento de texto en el mensaje de chat gpt estamos hablando de hacer referencia a una base de datos completa llena de nuestros propios datos y no solo eso una vez que obtengamos la información que necesitamos podemos hacer que l change nos ayude a tomar la acción que que debemos realizar por ejemplo enviando un correo electrónico con alguna información específica y la forma de hacerlo es muy similar a la que hemos venido aplicando hasta aquí esto es tomar el documento al que deseamos hacer referencia en nuestro modelo de lenguaje luego lo dividimos en fragmentos más pequeños y hacemos los embedding los cuales los vamos a terminar almacenando en una base de datos vectorial esto Ahora nos permite crear aplicaciones de modelo de lenguaje que siguen un proceso general un usuario hace una pregunta inicial luego esta pregunta se envía al modelo de lenguaje y se utiliza una representación vectorial de esa pregunta para realizar una búsqueda de similitud en la base de datos vectorial esto nos permite recuperar los fragmentos de información relevantes de la base de datos vectorial y enviarlos también al modelo de lenguaje ahora el modelo de lenguaje tiene tanto la pregunta inicial como la información relevante de la base de datos vectorial y por tanto es capaz de dar una respuesta o realizar una acción l change ayuda a crear aplicaciones que sigue un proceso como este y en base a las dos capacidades mencionadas nos permite pensar en una cantidad infinita de casos de uso práctico todo lo que implic asistencia personal será enorme puede tener un gran modelo de idioma para reservar vuelos transferir dinero y hasta para pagar impuestos ahora emos las implicaciones para estudiar y aprender cosas nuevas puede tener un modelo de lenguaje grande hacer referencia a un programa de estudios complejos y ayudarnos a aprender el material lo más rápido posible la codificación el análisis de datos las ciencias de datos todo se verá afectado Por esto una de las aplicaciones que se consideran más relevantes es el poder conectar grandes modelos de lenguaje con datos existentes de la empresa como datos de clientes datos de marketing u otros es muy probable que vayamos a ver un progreso exponencial en el análisis y ciencia de datos nuestra capacidad para conectar los grandes modelos de lenguaje con apis avanzadas como la de meta o la de Google realmente hará que las cosas despeguen significativamente la principal propuesta de valor de l Chain se puede dividir en tres conceptos principales Contamos con los contenedores llm que nos permiten conectarnos a modelos de lenguajes Grandes como gpt 4 o los de hing Face las plantillas de mensajes nos permiten evitar tener que codificar el texto que es la entrada a los llm luego tenemos índices que nos permitirán extraer información relevante para los llm las cadenas que nos permiten combinar varios componentes para resolver una tarea específica y crear una aplicación llm completa y finalmente tenemos y algo muy importante que vamos a ver más adelante los agentes que permiten al llm interactuar con apis  externas expuesto todo lo que es la introducción a unchain vamos a tomar este gráfico que está aquí como referencia dado que ahora en el lab vamos a identificar cada uno de estos componentes codificándolas con python y las librerías obviamente de l Chain antes de empezar con la aplicación concretamente en el ámbito de colap tenemos que ver algunas cuestiones de environment de contexto y concretamente tenemos que usar la Api de chat gpt para todo lo que tenemos que hacer y también tenemos que usar la Api de pinec que es una una de las tantas es la que vamos a elegir nosotros base para guardar los embedding de los vectores que vayamos generando en estos prácticos vamos a ver por qué el tema de python y cuál es la No solamente la utilidad sino la importancia de guardar estos vectores allí pero vamos a centrarnos primero en la la Api de openi y vamos aquí tenemos la la página open.com en la cual vamos a incorporarnos con login eh yo voy a entrar rápido porquees ya estoy logueado y yo ya he hecho la registración Pero obviamente lo que van a tener que hacer ustedes poner un uso de contraseña o bien entrar con la contraseña de Google que es lo más usual bien una vez que estamos aquí tengo chat gpt que entraría a usar el chat gpt como un corriente o la Api nosotros vamos a entrar a Api y aquí tenemos en principio sí esta esta apik esta opción apik hago clic aquí y haciendo clic en este botón puedo crear una nueva eh clave para usar mi apq una Secret key obviamente ya la he creado eh tiene para ponerle un nombre y obviamente la la apik me la sugiere en este caso Open y bueno obviamente ya la puedo cambiar eliminar Pero obviamente la voy a ver parcialmente pero puedo entrar a ver obviamente la la la la apik que corresponda está codificada aquí obviamente para que no la pueda ver suar cualquier persona por una cuestión de seguridad pero cada uno de ustedes la podrá tomar y vamos a ver que la vamos a tener que pegar ya vamos a ver en el co paso seguido vamos a ir a settings para bueno hacer lo más doloroso que es justamente el pago Sí en Billings sí tengo todas las cuestiones que tienen que ver con el aquí yo ven que tengo 8.85 pendientes porque he hecho una compra de $ todavía tengo ese saldo a favor y aquí tengo en principio el p metods que Obviamente le ponen lo que edes ya saben Sí el número de la la tarjeta de crédito o el nombre de ustedes o como si fueran hacer cualquier compra usual común y corriente y eh si quieren ver los precios pueden entrar aquí a pricing lo cual me va a llevar a esta web que tenemos aquí donde tengo todos los precios de todos los productos que tiene eh open hay y puntualmente este vamos encabezando los ya gpt pero no es el único ya vamos a ver aquí tengo gpt 4 turbo el cual depende del modelo que vas a usar es el costo en este caso Más allá de eso el costo es coincidente en otros casos no bien en principio tengo un valor para acciones de input como sería escribir una una pregunta o pedirle algo y otro para el output que sería la respuesta siempre es más caro el valor del output que el valor del input esto se cotiza esto es 0.01 cada 1000 ST Y en este caso lo mismo 0.03 cada 1000 tokens Aquí tengo inclusive una calculadora para poder estimarlo Igual vamos a poner un código dentro del cola para poder hacer con gpt 4 lo mismo los valores no cambian Perdón son un poquito más caros Sí aquí estoy viendo 003 versus 001 006 versus 003 Sí el caso en este caso el triple en este caso el doble si aquí fíjense que entre los modelos hay diferencia dentro gpt 4 gpt 3.5 Turbo es el mucho más barato muchísimo más barato fíjense que ya 0.001 cuando al principio tenía 0.01 y en este caso 0.03 bien obviamente en la medida que el modelo es menos avanzado Obviamente el costo es menor no bien luego tengo otros productos como asistentes Sí para bueno con retriable cor interpret perdón y retriable que ya vamos a ver son acciones que vamos a ver más adelante luego para para tuneo de modelos para los embeddings a b2 que también lo vamos a usar porque es parte de toda la práctica que vamos a hacer con el L change eh los modelos base d Vinci es uno de los que vamos a usar también y después tenemos otro tipo de productos que en este caso no vamos a usar Dali que son los productos que generan imágenes no es el caso lo que vamos a ver nosotros pero si los modelos de audio whisper y tts whisper lo vamos a usar para convertir un audio en texto y tts para lo contrario todos tienen Su costo obviamente bien aclarado esto esto lo que van a tener que hacer obviamente es hacer el pago como corresponde y van a tener habilitado a partir de allí el uso a través de la piki si ustedes usan la piqui pero no han pagado evidentemente les va a dar un error porque tien una piqui para Acceder al lpi pero la Api no tiene el pago para poder generar la acción que ustedes le están pidiendo y finalmente el pinec que aquí obviamente e lo puedo hacer clic en login para poder entrar y este servicio es totalmente gratuito Sí así que no no hay ningún tipo de inconveniente en poder poder este usar este producto a diferencia del de recién porque bueno es totalmente abierto y gratuito bueno acá estoy dentro de pyon hagan caso miso esto porque ya tengo un índice creado una base de datos de índices creados Así que eso en principio no lo encontrarían si cuando ustedes ingresen Aquí van a tener aquí a apik al igual que lo que hicimos recién en chpt Pero obviamente en Open perdón pero sin mediar cuestiones de de precio en este caso lo que voy a tener que hacer simplemente es crear mi piqui con este botón de aquí yo ya la tengo creada le he puesto primera y eh obviamente la la piqui es esta que aparece aquí este con este bueno estos símbolos que ocultan el valor sí estos numeral Perdón estos asteriscos eh Y con este botón la puedo mostrar Sí con esta la voo ocultar y con este botón la puedo copiar para pegarla en lo que vamos a hacer a continuación bien aclarado todo esto vamos a ver algo muy importante un pasito más antes de ir al colat que es cómo generar un archivo para guardar toda esta información de este environment lo próximo entonces que tenemos que hacer es crear un archivo con el bloc de nota cualquier editor de textos que se llame punto m como Ven aquí arriba pun enb de envir Y sí por favor muy importante no equivocarse en la representación de estos tres valores en el nombre de cada uno de estos parámetros Open apik pyon apik y Pon m porque si no obviamente van a tener un error qué datos van a poner allí los que tienen que ver con lo que vimos recién en principio la apik que la van a recoger de aquí cuando creen la picki de Open Ai es lo que van a poner en la parte de el primer parámetro de este archivo es decir esto que está aquí eso sería la información que ustedes tienen que poner entre comillas luego vamos a pycon y acá tengo dos datos en principio la apik que dijimos que la podemos visualizar con esto y luego el nombre del environment por eso cuando ustedes van el archivo de nuevo punto m ven que los datos que piden son la apik que tienen que ponerla aquí entre comillas y el nombre del envir también aquí entre comillas este archivo lo guardan Y luego sí lo vamos a tener que poner en nuestro Drive para poder levantarlo desde el cola Así que eso es todo lo pueden hacer en cualquier parte de su computador siempre después teniendo presente de moverlo hacia el espacio de Drive que es el que vamos a usar para poner todos nuestros archivos como venimos haciendo habitualmente con nuestros cols bien habiendo abordado toda la teoría introductoria y todas las cuestiones de configuración de entorno vamos a empezar con nuestro primer lab de cola la pln 9 en este caso donde vamos a ver como dice el título aquí los componentes principales de lunch de la primer parte después en la segunda parte de esta clase Vamos a continuar con el resto en principio lo que tenemos que hacer es importar las librerías pero más que importar primero tenemos que instalar todas estas aplicaciones open lch token que nos va a ayudar a poder calcular eh digamos todo lo que sera la tokenización que nos va a cobrar obviamente eh Open e kpt antes de hacerla para tener justamente la seguridad de que vamos a tener primero que tenemos saldo para hacerlo y después si estamos dispuestos a asumir ese costo luego pycon que es lo que vimos recién y python DM que es lo que nos va a permitir levantar el entorno esc el archivo que hicimos recién justamente para poder tener configuradas las apik tanto de Open Ai como la de pyon así que instalamos todo esto nos va a llevar un tiempito muy importante antes de empezar no olvidarse en este caso es muy importante conectarse con este Bueno un gpu de colap y no Simplemente con cpu porque muchas de estas cosas no van a funcionar Así que empezamos por Instalar todo esto que está aquí bien luego vamos a mostrar con show Line change la versión de Line change esto es muy importante porque es un software de Última Generación Que obviamente eh Está sacando versiones permanentemente Así que es muy importante en este momento Esta es la versión que vamos a usar que ustedes puedan constatar con cuál de todas las versiones de l change están trabajando bien paso seguido vamos a crear las variables de entorno esto les recuerdo aquí las este bueno las URL de cada una de las plataformas tanto la de la apik de openi como la de pycon Sí y bueno Y aquí si ustedes quieren dejar nota digamos de cada uno de los datos que tienen en el archivo que reci creamos con el punto m también lo pueden hacer aquí a título de dato para que justamente lo tengan siempre a mano bien dicho Todo esto lo que tengo que hacer ahora es montar el Drive para justamente ir a buscar entre otras cosas ese archivo punem que acabamos de colocar Así que montamos el drive y lo hacemos como habitualmente nos manejamos con esto bien allí terminamos de montar el drive y ahora vamos a importar os y DM para justamente hacer esta cuestión de levantar ar ese archivo que tiene el environment del cual habíamos hablado antes así que hacemos clic aquí y luego con loa DM donde voy a buscar en content drive mydrive y en el particular yo tengo este punto m dentro de archivos nlp así que de allí lo voy a levantar y luego con os en vinom and get Open ap key lo que voy a poder ver por pantalla a ver si me responde con la Api que yo sé que tengo entonces de esa manera me doy cuenta de que justamente estoy conectado como corresponde ahí La respuesta es positiva con lo cual quiere decir que yo ya estoy debidamente conectado y puedo empezar a utilizar Esta bueno esta este servicio de Open sí lo primero que vamos a ver aquí recurriendo a este imagen que teníamos en la teoría es el tema de los modelos Sí en principio vamos a ver el modelo llm y luego el modelo chat Sí en ese caso van a ver que son dos sistemas diferentes que tienen que ver con los tipos de servicios que nos da el tipo de modelos que nos brindan Open Así que empecemos por lo primero vamos a ver el Last model que saben ustedes que significa llm y vamos a usar el gpt 3.5 Turbo que es el que veíamos hace un rato sí bien para ello voy a frine change llms voy a importar Open a y allí voy a llamar justamente a la clase Open Ai le voy a dar el model name text inch 003 que es el que vimos recién cuando entramos a el detalle de los precios y voy a trabajar con la el Max tokens en 1024 que es lo que tiene configurado por defecto eh chat gpt y temperatura 0.7 temperatura es un parámetro que va de c a 2 y tiene que ver en principio Si queremos una respuesta que sea más precisa Más estructurada o más libre o más creativa Por decirlo de algún modo sí bien aquí tenemos eh e también la posibilidad de además de trabajar con text Vinci también con gpt 3.5 Turbo bueno Estos son elementos este opcionales y por eso aparecen justamente en la lista de productos que tienen en el detalle de precios eh Open Ai así que bueno en este caso vamos a usar el primero de ellos pero en realidad ustedes pueden usar el modelo que quieran Sabiendo de que en algunos casos puede que un modelo se ajuste mejor a una cosa u otra bien entonces creamos eh esta variable gpt3 que va a ser una instancia de Open con todas estas características que hemos mencionado recién bien y ahí tengo ya la instancia creada y ahora lo que voy a hacer es darle un prompt en este caso explíqueme que ch gpt a esta variable que Acabo de crear y voy a poner la respuesta en una variable que he dado en poner la  respuesta bien a partir de esto yo puedo hacer un print de la vara de respuesta para ver qué fue lo que me dio justamente como respuesta y aquí dice claramente chat gpt Es una herramienta de chatbot basada en procesamiento el lenguaje natural nlp que permite a los usuarios bueno bla bla bla bla bla bueno todo esto que está aquí es la respuesta que yo he logrado me de chat gpt a través de Eh bueno ahora python y toda esta este uso de esta librería lunch Sí es lo que habitualmente escribo a través de un prom el mismo entorno de hept ahora lo hago desde el entorno de colap mediante python y mediante la librería lch bien en este caso podemos ver que puedo pedirle que me dé la cantidad de tokens que corresponden a explíqueme que es un chat gpt y la cantidad de tokens que me da la respuesta entonces de este modo yo puedo darme cuenta que tengo ocho para uno y 149 para el otro y empezar a hacer este tema de los cálculos Sí porque en este caso le hice una pregunta de ocho tokens y una respuesta de 149 Y eso va a tener un costo bien aquí tenemos una referencia del costo de acuerdo al modelo que yo he utilizado Sí el d Vinci o el gpt 5 Turbo Pero obviamente ustedes tien que tener claro que esta referencia que está aquí no corresponde si uso gpt 4 por ejemplo sí bien en este caso yo he utilizado eh Tex davinci 003 y no gpt 3.5 turo porque hay una cosa muy importante que yo aquí se las he dejado Como comentario que en realidad la alternativa de gpt 3.5 Turbo hubiese funcionado bien con esta pregunta ahora si yo Quiero una no una pregunta sino un conjunto de preguntas un conjunto de pedidos digamos ag gpt El problema es que ese modelo No tolera eso y Por ende el que tolera tanto una pregunta o más o un prom o más es text vinch 003 bien Así que en este caso fíjense que lo que voy a poner es gpt3 generate sí no como aquí que directamente le ponía la pregunta sino gpt3 generate y le voy a mandar tres cosas que le voy a pedir a ch alunas pueden ser preguntas a otr pueden ser que me explique algo o me un detalle en el primer caso le pregunto Cuál es el equipo de fútbol más famoso del mundo en el segundo caso le digo que me explique brevemente la ley de la gravedad y en el tercer caso que me describa Cuáles son los tres pasos que yo debería seguir para hacer un dinamis no es son tres pasos más bueno eso le pedí que me lo haga tres sí en este caso insisto uso el método generate porque le estoy mandando tres proms y no uno y recuerden estoy usando el modelo text vin que es el que soporta eso lo ejecutamos y voy a tener ahora las respuestas en la variable de respuestas y la var de respuestas va a ser justamente una Ray con tres respuestas ya que le he hecho t entonces Si miro la primera respuesta 00 me va a decir que el equipo más famoso del mundo es el Real Madrid y bueno si quiero ver las tres voy a hacer un for y voy a recorrer en lugar de la 00 la i0 cambiándole el I de cer hasta el tercer punto que sería el número dos Sí así que hago el for y voy a ver en este caso la respuesta o las tres respuestas el equipo de fútbol más famoso del mundo Real Madrid la ley de la gravedad una ley física y me da los tres pasos de cómo hacer un tiramizu bien hasta aquí Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número    28  Hola a todos Esta es la clase número 28 del curso de Inteligencia artificial de ifes vamos a empezar en esta clase a ver las redes de tipo Deep learning para el mundo del nlp y con ello la red por excelencia del mundo del nlp que son las redes neuronales recurrentes y vamos a ver ver no solamente el modelo básico de este tipo de redes sino una evolución de este tipo de redes que son las redes de tipo lsm normalmente una neurona recurrente se dibuja de este modo donde hay una función que obtiene un valor de entrada y que produce un valor de salida además de la salida también produce otro valor de retroalimentación que pasa a la siguiente de neurona si desplegamos esto que estamos viendo aquí en esta neurona recurrente para poder ver lo que sería una red de tipo neuronal recurrente el resultado sería este leyendo de izquierda a derecha podemos introducir X en la primer neurona y calcular un resultado y así como un valor que pasa a la siguiente neurona en la segunda neurona ingresa un nuevo valor x el cual junto con el que viene de la neurona anterior Calcula a su vez un nuevo valor I en la tercer neurona ingresa un nuevo valor x el cual junto con el valor que viene de la neurona anterior Calcula otro nuevo valor I Y así sucesivamente esa secuencia paso a paso que va a continuar con la misma lógica va a dar lugar a lo que llamamos redes neuronales recurrentes Pero qué son estos valores x que entran en secuencia en cada una de las etapas de una red neuronal recurrente es Ni más ni menos que las palabras que pueden formar parte de una frase como esto que tenemos aquí Hola Cómo te va hoy lo cual describe lo que ya sabemos es un texto Ni más ni menos que una secuencia de palabras la arquitectura y el funcionamiento de tipo secuencial que tienen las redes neuronales recurrentes justamente hacen a que sean tan importantes en la filosofía que tienen los textos justamente de tipo secuencial y por ello la importancia de este tipo de redes en el mundo del nlp como vimos las redes neuronales recurrentes trabajan con dos entradas por cada una de las neuronas primero con cada una de estas palabras que hacemos entrar en secuencia y luego con la inform ación que viene de las neuronas anteriores con lo cual tiene la información de la nueva palabra que entra y del contexto que ofrecen las palabras precedentes pero lamentablemente ese contexto no es indefinido no es infinito Qué pasa si esta frase que vengo trabajando en la primera parte de esta animación Hola Cómo te va hoy yo le agrego más frases como la que vemos aquí en este día de otoño voy a tener un problema que se denomina falta de memoria por qué Porque el contexto de las palabras que están más cerca de las que están entrando últimamente van a tener más peso más prioridad y más relevancia que las palabras que ya quedaron muy atrás en el tiempo porque forman parte del principio del texto para solucionar este problema de falta de memoria es que se crearon unas redes especiales de tipo redes neurales recurrentes que se llaman redes de tipo lst m Pero por qué hablamos tanto de contexto porque es tan importante este concepto en el mundo del nlp bueno porque una de las tareas más importantes del nlp es la generación de texto y la generación de texto se trata de una palabra que aparece a continuación de otras precedentes y tiene que tener sentido con esas palabras precedentes justamente porque sio el algoritmo va a poner una nueva palabra sin tener en cuenta lo anterior y lo que va a ar es una secuencia de palabras que no tengan ningún sentido ni ninguna relevancia uno de los ejemplos más claros de esto es justamente el chat gpt un generador de texto por excelencia Y ustedes tienen seguramente más que presente que si ustedes le ponen un contexto a chat gpt le dicen Bueno quiero que escribas este texto como si fueses un experto en supongamos marketing el el chat gpt va a generar justamente un texto más apropiado y más relevante con lo que ustedes buscan si ustedes no le marcan en el prom Cuál es el contexto obviamente no lo va a hacer tan bien ni tan ajustado a lo que ustedes buan bien Por eso en el mundo de las redes neuronales recurrentes Obviamente el contexto es muy importante y para ello vamos al siguiente ejemplo vamos a suponer que tenemos esta frase hoy tenemos un hermoso cielo evidentemente Esta es una frase muy corta y Por ende la posibilidad de conocer el contexto completo de todas las palabras que llevan hasta este momento esta frase que solamente tiene cinco palabras va a ser muy factible y Por ende es muy probable que elija una palabra acertada y que tenga sentido con el contexto Como por ejemplo la palabra azul y la frase termina siendo hoy tenemos un hermoso cielo azul con lo cual es una frase que tiene un sentido contextual Pero pensemos Cómo sería la situación ante un ejemplo como este yo vivía en Alemania por ello cuando iba a la escuela los maestros me enseñaron cómo hablar en obviamente la palabra sería alemán dado que la cuarta palabra de esta frase es Alemania Pero esto implicaría que a esa altura después del n tiene que estar recordando la cuarta palabra y la cuarta palabra por la característica de una red neuronal recurrente simple sabemos que por el problema de la pérdida de memoria muy probablemente Ya esa palabra Alemania tenga poco peso y pueda llegar a inferir otra palabra que no sea alemán por lo tanto existen las redes de tipo lstm que son justamente como vimos antes las que me van a dar la solución para que yo pueda tener justamente el problema de memoria solucionado y que allí aparezca la palabra que sugiere este contexto que es alemán en concreto las redes neuronales que hemos estado observando van pasando la secuencia de palabras y aprendiendo el contexto pero ese contexto a largo plazo puede ser engañoso y es posible que no podamos ver como los significados de las palabras lej ganas van perdiendo relevancia y puedo confundir el contexto las redes lstm son la solución para este problema puesto que introducen algo que se llama estado de Zelda que es un contexto que se puede mantener durante la secuencia a largas secuencias de tiempo y que puede aportar contexto desde el principio mismo de la oración lo fascinante es que también puede proceder de manera bidireccional y en ese caso podría ser que las palabras posterior de la oración también puedan aportar contexto a las anteriores para que la red pueda aprender la semántica de la oración con mayor precisión bueno para terminar esta primer parte de esta clase vamos a ir al lab pnl 7 donde vamos a usar por primera vez una red neuronal pero aún no de tipo recurrente ni mucho menos de tipo lstm lo vamos a hacer para un problema de detección de frases sarcásticas es decir vamos a tener un dataset que nos va a dar justamente una serie de frases y van a estar rotuladas para que después yo pueda ponerle una frase nueva y ver justamente con esa red entrenada si puedo detectar si esa nueva frase es o no sarcástica lo primero que hago como siempre es importar las librerías primero Jason porque justamente el dataset que voy a usar es o está hecho Perdón en formato Jason tensor Flow que es lo que ya conocemos librería que ya hemos usado antes y que nos va a permitir justamente empezar a a volver a trabajar mejor dicho con redes de tipo de learning pandas maplit wordcloud que es una herramienta que les voy a mostrar justamente para ver cómo hacemos esa famosa nube de palabras y obviamente luego la librería tokenizer y pat sequences para justamente lo que es el armado de la secuencia de tokens de cada una de las palabras de este Jason que voy a trabajar así que voy a importar esas librerías que acabo de mencionar y luego voy a montar como siempre el Drive desde el cual ustedes como siempre les digo van a tomar el archivo que yo les voy a pasar a través del Campus bien Voy a leer el Jason y voy a crear un dataframe justamente en este caso con r Jason y el nombre del archivo que es sarsan Jason bien y luego voy a hacer un Head para mirar rápidamente el contenido de este dataframe bien Aquí tengo el dataframe donde veo justamente que tengo una headline que es como un titular el la URL donde está ese artículo de ese titular y una identificación con uno o cero que es lo que rotula o etiqueta esta headline como de tipo sarcástico o no es decir en este caso y sarcastic 1 es sarcástico y cero es no paso seguido voy a hacer un shap para reconocer la estructura de este dataframe y puedo ver que tengo 28619 observaciones y tres características que son las que ya vimos antes aquí arriba bien lo que vamos a hacer ahora es hacer una nube de palabras en realidad esto no es obligatorio no es exigible para los propósitos de lo que vamos a ver con el algoritmo que queremos hacer y el objetivo de ese algoritmo pero sí es una herramienta muy importante para el análisis de datos con lo cual acá voy a crear primero a través de plt la configuración del tamaño en que quiero Que aparezca ese cuadrante que me va a mostrar nu de palabras y luego voy a crear con wc una instancia de wordcloud que es justamente la librería que me permite eso con lo cual voy a definir algunos parámetros la máxima cantidad de palabras que quiero el ancho el alto y voy a con generate tomar Qué tipo de eh características Pero qué tipo de observaciones son las que quiero para este trabajo que voy a hacer y justamente lo voy a trabajar con esta condición de que headline que es lo que voy a tomar en consideración que es esta columna sí lo voy a circunscribir a solamente aquellas eh aquellas observaciones cuyo is sarcastic sea igual a cero qué quiere decir con esto que justamente voy a elegir los casos como dice el título aquí arriba no sarcásticos y luego con plt y MS lo que hago justamente es imprimir este wc que es esta númera de puntos Así que lo ejecutamos y vamos a ver como siempre sabemos que es esta mecánica de este tipo de gráficos donde muestra en con mayor tamaño Sí este las palabras que más frecuentes salen y lo inverso para las palabras que menos frecuencia tienen en este caso no se trata de frecuencia no frecuencia sino las palabras que tienen o que aparecen más identificadas con titulares no sarcásticos y más chicas las que representan lo contrario sí bien ahí tenemos entonces Trump new se Donald TR nuevamente Woman people way bien Vamos a hacer lo mismo ahora pero para lo contrario para las frases sarcásticas bien entonces en este caso la lógica es exactamente lo mismo lo que voy a cambiar simplemente aquí ese condicionamiento donde voy a poner que el campo o en este caso Perdón la característica y sarcastic le voy a poner que sea igual a uno así como antes puse que fuese igual a cer Bueno ejecutamos este gráfico y y esperamos el resultado a ver que nos muestra ahora para el caso de los titulares de tipo sarcástico y cuáles son las palabras más relevantes entre los titulares de tipo sarcástico bien acá tenemos Man New Como las palabras más importantes y luego nation report make Bueno lo que ustedes Ven aquí y pueden con tranquilidad analizar y ver por qué Por ejemplo las palabras Money New representan algo muy típico en los titulares de sarcástico bueno habiendo podido conocer esta nueva herramienta que les va a permitir incorporar una opción más a la hora del análisis de datos vamos a como hacemos siempre crear la x y la y en este caso la x va a ser el titular el headline y la i justamente la etiqueta de si algo es sarcástico o no bien creamos nuestra x y nuestra I y voy a hacer una visualización rápida del x simplemente para ver que justamente tengo en las x los cantidad de observaciones totales Las 28618 observaciones que son headlines que corresponden o yo he colocado dentro de la x para qué Para justamente empezar por hacer la división como hacemos siempre de los conjuntos de Test y los conjunto de entrenamiento en este caso no voy a usar al TR spr sino que lo voy a hacer numéricamente yo a dedo como quien dice entonces voy a poner las training synthesis y las testing synesis o sea las dos X en el primer caso de la cer a la 20000 y el último caso Perdón hasta la anterior a la 20000 y en el último caso es decir las de testing desde la 20000 hasta la última que como dijimos antes es la 28618 lo mismo voy a hacer para las training labels y las testing labels con los elementos que voy a sacar en este caso de la variable I bien ejecutamos esto y ahora vamos a proceder a la tokenización bien como estamos acostum hacerlo ya vamos a crear un tokenizador justamente utilizando la librería tokenizer en este caso la tomamos no de nltk sino de tensor flow bien Aquí le voy a indicar que el tamaño máximo de vocabulario que quiero va a ser 10,000 Ya vimos en la clase pasada que yo puedo asumir que el vocabulario es la representación total de todas las palabras de todo lo que yo tengo como conjunto de dasos o simplemente puedo decir solamente quiero esta cantidad y obviamente va a cortar a esa ese número la lo que es el vocabulario completo con lo cual puede ser que sea justo o no es un número obviamente elegido con un criterio pero no es Exacto con lo cual puede ser que el vocabulario sea más grande o más chico que eso qué pasa si es más chico el vocabulario Bueno aquí entra en juego justamente este parámetro que estoy poniendo aquí el o token donde le indico que si como el vocabolo es más chico que la cantidad total de palabras que tienen todo este conjunto de datos cuando me toque una palabra que no esté en el vocabulario no voy a saber qué número ponerle porque justamente esa palabra no existe en el vocabulario Entonces en ese caso le voy a poner este indicador que está aquí que es out of vocabulary que justamente Esto me permite hacer en el caso de que la palabra no exista qué oob token voy a elegir se puede poner esta referencia que es la clásica que se pone en estos casos u otra que ustedes pueden elegir bien una vez que creo el tokenizador lo que hago es entrenar mi tokenizador justamente a partir de la Fuente de datos que yo tengo que son los training synthesis que son las primeras 19 20000 porque empieza de cero hasta la anterior a 200000 las primeras 20.000 los primeros 20000 elementos de training sentesis sí bien entonces una vez que entreno al tokenizador lo que hago es empezar a crear un índice para cada una de las palabras Sí entonces lo que hago es con Word Index sobre tokenizer bueno bueno colocar ese resultado en una variable que he dado llamarle al igual que el método Word Index bien Ahora convertimos los titulares los headlines del conjunto de train en secuencias de números con qué con el método Tex to sequences que ya lo habemos usado con la otra librería ahora con tokenizer pero de transfer Flow sobre las oraciones de entrenamiento y esas oraciones de entrenamiento pasadas a secuencias la voy a poner en una variable que le voy a llamar secuencias de entrenamiento sí bien una vez que tengo eso voy a padar esas secuencias con piding post Recuerden que el tema de padar es si el la cantidad de números sí que representa esa oración es inferior al tamaño del vector lo que tengo que hacer es rellenar todos es espacios vacíos con elementos que habitualmente son ceros sí lo que tengo que indicar es si lo que voy a hacer es ponerlos al principio esos ceros o al final en este caso con post lo que hago es que ponga el texto al principio y ese relleno con ceros al final bien Luego hacemos Exactamente lo mismo o sea las secuencias y el padeo así como lo hice antes para el conjunto entrenamiento ahora también para el conjunto de Test bien ejecuto toda esta cadena de instrucciones y voy a pasar ahora a ver cómo poder identificar si un una secuencia de números que representa un texto sí es o no un texto de tipo sarcástico para ello vamos a construir una red de tipo de-learning para poner en en uso a nuestros embedding Sí en principio voy a crear como ya lo hemos hecho otras veces con queras de tesor flow sequential cada una de las capas de mi eh red neuronal y la voy a llamar a este modelo model lo primero que pongo es una capa de de tipo embedding donde le voy a decir con el primer parámetro que el vocabulario que ya como ya vimos antes es de 10,000 palabras la el tamaño de los embedding es el vector los vectores de cada una de estas palabras van a ser de 16 posiciones y el tamaño máximo de input es decir los textos que vayan entrando sí deberían tener una dimensión de hasta 100 caracteres luego voy a crear una capa de pulling de una dimensión sí esto no confundir con lo que usamos en las rales convolucionales más allá que que también había una capa de pulling y luego voy a hacer dos capas densas una de 24 neuronas con la función de activación relu que ya hemos usado antes y finalmente una capa de salida de tipo densa también con eh una una este una función de activación de tipo sigmoide dado que tengo una sola neurona Por qué tengo una sola neurona porque este es un un problema de tipo binario tengo que decir si un texto es o no sarcástico con lo cual tengo dos salidas posibles es sarcástico o no es sarcástico bien y finalmente la compilación que lo voy a comparar lo voy a compilar perdón con la función de pérdida vinary Cross entropy Recuerden que esta la uso cuando es justamente una salida de tipo binario así que tengo una red que me va a dar solamente dos opciones no es de tipo categórica el Cross entropy que lo usaba cuando tenía más de dos opciones de salida voy a usar el optimizador Adam y le voy a decir que la métrica para ir midiendo la precisión de esta red sea la de acur ejecuto esta celda y luego con el sumari vamos a ver como muchas otras veces s Bueno un pantallazo de cómo quedó conformada nuestra red neuronal habiendo hecho esto voy a pasar a entrenar a mi modelo y voy a guardar la la historia digamos de cada uno de estos entrenamientos para después graficarlos dentro de esta variable history voy a crear una variable para poner la cantidad de epoch que voy a definir en TR y Bueno luego lo que tengo que hacer como siempre model fit pongo justamente las los elementos de entrenamiento que en este caso son training pad que son los vectores que representan a las headlines luego las labs que me dicen si esa cada una de esas headlines es o no Eh sarcástico bueno la cantidad de epoch y los conjuntos de validación el det test para justamente las noticias y el D test para las etiquetas bien ejecutamos esto y va a tardar un tiempito porque son 30 eh epoch los que hemos aplicado Aquí bien aquí Terminamos el entrenamiento tenemos un muy buen accuracy 99% eh para el conjunto de entrenamiento y un muy buen accuracy también para el conjunto de validación que está en el orden del 80% bien Ahora vamos a probar esta red que hemos creado esta redal que hemos creado poniendo en este caso eh dentro bueno creando una variable sentences que que le voy a poner dos oraciones donde la primera bueno está escrita en inglés la pueden Traducir si quieren yo busqué funciones este dado que el dataset está puesto en inglés este he buscado oraciones típicas de habla inglesa que representan Sarcasmo Y en este caso la primera representa Sarcasmo y la segunda simplemente anuncia cuándo va a empezar a eh la temporada final de Game of thrones es una un titular no sarcástico en este caso la las dos oraciones que están dentro esta var sentences voy a aplicarles el mismo criterio que utilicé para con los datos que usé para entrenar es decir lo que tengo que hacerlos pasar por el método text to sequences y el resultado de ello lo voy a poner en una variable que le he puesto a llamar sequences luego insisto al igual que lo que hice con los datos que entrené los voy a padar Por lo cual Bueno sigo exactamente la misma lógica que en aquel caso una vez que tengo estas dos oraciones que han sido eh inicializadas y padas voy a hacer la predicción Entonces el resultado final que está en esta variable padded voy a aplicarlo a predict de model y voy a imprimir el resultado bien ejecuto eso y veo que en el primer caso me da 9.06 al elevado a la -10 lo cual me daría en realidad un 90.66 por con lo cual quiere decir que la primer frase tiene un 90 por eh está más cerca de digamos de eh de ser de tipo sarcástica en un casi 100% que no sarcástica por el contrario la otra es 5.9 elevado a la 8 a 10 a la -8 perdón con lo cual obviamente es un 0,0005 de eh posibilidades de ser sarcástica Por ende es una frase no sarcástica y justamente como dijimos antes lo que anuncia aquí obviamente es un titular serio y totalmente alejado de ser algo sarcástico aquí teng tengo en este caso tres oraciones donde las dos primeras son sarcásticas y la última no el resto de de de lo que yo aplico aquí en este caso del código es exactamente el mismo Solamente he cambiado el contenido de la variable sentence y lo ejecuto y veo el resultado donde justamente las dos primeras eh oraciones tienen en el primer caso un 96 y en la segunda un 99% de posibilidad de ser sarcásticas Sí están más cerca del uno Por ende justamente son sarcásticas y eh la otra es 2.1 elevado a la 10 a la-4 con lo cual 0.0002 y justamente está mucho más cerca del cero Por ende es una frase caratulada o rotulada por este modelo que hemos creado de tipo no sarcástico bien hasta acá  Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número  25  Hola a todos Bienvenidos a la clase número 25 del curso de Inteligencia artificial de ifes seguimos en el módulo de Deep learning pero ahora pasamos a otro campo de Deep learning o otro tipo de aplicaciones importantes del Deep learning venimos del mundo de la visión computacional trabajada con redes neuronales convolucionales ahora desde hoy vamos vamos a empezar con otra rama importante del Deep learning como dije recién el procesamiento de lenguaje natural o  nlp qué es procesamiento de lenguaje natural o nlp bien para dar una definición vamos a usar nuestro propio canal de Inteligencia artificial donde Catalina nos va a explicar claramente Qué es nlp te has preguntado cómo las máquinas entienden y responden a nuestro idioma bienvenidos al fascinante mundo del procesamiento del lenguaje Es una rama de la Inteligencia artificial que permite a las máquinas entender interpretar y responder al lenguaje humano traducción automática análisis de sentimientos generación de texto reconocimiento de voz y chatbots inteligentes son solo algunas de las emocionantes aplicaciones que puedes explorar con nlp ahora que tenemos Clara Cuál es la definición de nlp vamos a ver algunas aplicaciones del nlp y aquí podemos tener por ejemplo estas aproximaciones a lo que hoy son los usos más importantes o más significativos del nlp en principio la detección de spam esto ya lo vimos también en algoritmo de Machine learning pero es muy importante también saber que desde nlp también se puede contribuir a poder detectar si un email tiene o no spam el texto preditivo esto es al lo que vemos todo el tiempo cuando escribimos correo Por ejemplo o cualquier mensaje en cualquier red social vemos que nos aparece si nosotros lo configuramos alguna sugerencia de texto de lo que supone la inteligencia que es lo que deberíamos escribir en virtud de lo que venimos haciendo esto tiene que ver con un conocimiento de nuestras costumbres o el tipo de texto que estamos acostumbrados a escribir y hace un texto preditivo el reconocimiento de voz también es algo muy importante recién hablaba Catalina del tema de algunas inteligencias que reconocen justamente una orden y no responden en consecuencias Como por ejemplo Alexa o Google análisis de sentimiento es una técnica muy muy utilizada porque me permite saber por ejemplo si tengo un canal en el cual puedo recoger los comentarios de mis clientes de mi empresa ver si son comentarios positivos o negativos esto se usa mucho también en la red Twitter donde justamente hay muchas personas que son famosas y que recogen a partir de los comentarios de las personas bueno ver cuántos comentarios positivos hay y cuántos no son  positivos finalmente la en llp que lo vamos a abrir en el siguiente detalle para que ustedes lo puedan ver  mejor en términos de avance reciente en esta disciplina destaca el uso de la Inteligencia artificial en la traducción de idiomas como el caso de Google Translate que emplea cada vez más esta tecnología para mejorar cada una de sus traducciones en los diferentes niveles de idioma que tiene disponibilidad luego la generación de texto como la que utilizan en los chatbot ha experimentado notables mejoras en los últimos tiempos tiempos lo que ha permitido una comunicación más fluida y natural entre los clientes y las empresas Eh bueno un ejemplo emblemático de los avances del nlp es gpt3 ahora gpt 4 Próximamente seguramente gpt 5 desarrollados por Open Ai estos modelos han demostrado una comprensión excepcional del lenguaje humano y tien la capacidad de generar texto que a menudo es difícil de distinguir de un texto escrito o no por un humano finalmente eh decir que empresas como Google Facebook y Microsoft por citar los tres grandes líderes de lo que es informática están empleando estas tecnologías para mejorar sus servicios ya sea en motores de búsqueda en redes sociales o asistentes virtuales en general el procesamiento del lenguaje natural se ha convertido en un campo esencial en la revolución de la tecnología tal cual es hoy en día bien habiendo hecho una introducción hacia Qué es el nlp y hablar de un poco sus aplicaciones más conocidas hoy en el mundo de la informática vamos a empezar a abordar todos los conceptos básicos y esenciales del mundo del nlp cada uno por separado y con una explicación pormenorizada que nos dé una idea bien completa de Qué significa cada uno de ellos y por qué son importantes o por qué son conceptos esenciales en el campo del nlp el primero de ellos es vectores este concepto es muy importante porque lo que hace es dar la posibilidad de que un texto pueda ser procesado por un sistema informático a través de la transformación de ese texto en una expresión numérica Esto no es propio del nlp Nosotros sabemos que venimos del mundo de las redes neuronales convolucionales que trabajan con imágenes en el marco de las cuales también pasaba lo mismo la red no podía recibir una imagen en su formato natural sino que hace una una conversión de los píxeles de es imagen a sistemas distintos de representación como puede ser con tonos de grises o con rgb o bgr en representaciones numéricas y de ese modo un sistema o una red inteligente podía procesar una imagen el caso del nlp es Exactamente igual desde el problema obviamente la resolución es diferente en este caso lo que hace como dije recién es transformar un texto en un vector esta imagen que está aquí es bastante representativa vemos que hay una persona que está enviándole una orden sí que puede ser en voz o en texto escrito a un sistema de computación que tiene dentro un sistema de nlp y justamente lo que muestra el gráfico es que hay una conversión de esos valores en representaciones numéricas en este caso de unos y ceros bien la forma en realidad en que traducimos esos textos en vectores es el input para el entrenamiento y aprendizaje de un algoritmo de aprendizaje automático aquí volvemos a la cuestión de que esta inteligencia que ahora trabaja con textos que provienen de un audio o de algo incorporado desde el teclado de una computadora sí o de cualquier sistema como puede ser un celular también hay que justamente pasárselo a su inteligencia para que esa inteligencia tome eso como insumo entrene en base al conocimiento que le da ese input y puede ser una red que pueda predecir o trabajar o cualquiera de las acciones que nosotros hemos descrito recién que hacen con los textos para poder llevarlo a cabo de manera eficiente Pero concretamente qué es un vector y tenemos que empezar a ver por qué es la estructura elegida para poder representar un texto en números bueno en principio un vector es lo que vemos aquí en la imagen Es algo que tiene una cantidad una magnitud y una dirección Sí es decir que el vector tiene una cantidad que se representa a través de la magnitud del vector Pero además tiene una dirección ese vector es una matriz de escalares y aquí vemos justamente que esa matriz de escalares representada por este vector puede tener una asociación con otros vectores y justamente a partir de el ángulo que se describa entre ellos tener una noción de distancia es decir cuando vemos que hay una dirección vemos aquí el vector marrón tiene una distancia respecto de el que está de color negro porque tienen una magnitud y direcciones diferentes y respecto también de Que aquí tiene este color celeste verdoso Por decirlo de alguna manera que también tienen una magnitud y una dirección diferente bueno Esto es esencial para que entendamos o vamos a empezar a ver más adelante que esto es vital para poder interpretar justamente los textos y Cómo compararlos y ver cuáles son más parecidos a otros o no Cuáles son las ventajas de trabajar con vectores Bueno evidentemente yo tengo que trabajar con una representación numérica dado que no puedo hacerlo con el texto Por ende he elegido el vector como justamente la representación más apropiada para ello con esta este tipo de de idea de trabajo de eh Mis texto en vectores yo puedo hacer tareas como estas que están signadas aquí que son dos de las tantas que puedo hacer la detección de correos para saber si es spam o no o bien el eh una de las tareas también importantes que es la Organización de documentos mediante el uso de vectores es decir Yo puedo diferenciar un grupo de documentos de otro por el contenido que tiene todo esto desde la Inteligencia artificial por supuesto es decir que lo que puedo hacer es una separación de dos grupos de vectores como los que están aquí en esta imagen representados supongamos que estos puntos verdes son algunos vectores y los cuadrados rojos son otros vectores con este plano est estoy separando unos de otros por ejemplo estoy separando los mail que tienen spam de los mail que no tienen spam yo puedo presumir a través de la guía que tienen spam y también puedo hacer algo parecido a lo que dije acá abajo que puedo suponer que los puntos verdes son un tipo de documento supongamos que son documentos que hablan de geografía sí Y los otros que son representados aquí con cuadrados de color rojo supongamos que son documentos que hablan de otra ciencia como puede ser física Está bien entonces esto amente lo que me propone a través de este gráfico es la idea de que cada uno de estos círculos verdes o puntos rojos son vectores y esos vectores son traducciones de un texto en una representación numérica es decir que cada uno de estos puntos son textos pero están convertidos a vectores y justamente mi inteligencia que estoy haciendo con nlp me da la posibilidad de separar spam de no spam documentos de un tipo de documentos de otro o lo que fuer habiendo entendido el concepto de vectores vamos a otro concepto muy importante del campo del nlp que es el concepto de bolsa de palabras Qué es una bolsa de palabra o Back of Word es una técnica muy importante en el campo del nlp que trata un documento de texto como una colección desordenada de palabras sin tener en cuenta su orden o estructura gramatical A ver vamos a desmenuzar este concepto yo aquí tengo un texto sí I love this movie etcétera etcétera ese texto lo vuelco en una bolsa de palabras fíjense que esta bolsa tiene el mismo texto que está aquí es decir el mismo conjunto de palabras todas las palabras que están en este texto están en esta bolsa Pero como dice la definición aquí están desordenadas no desordenadas de manera a propósito digamos están porque lo que importa aquí es ver que esta bolsa tenga exactamente el contenido de todas las palabras que están en este texto pero no me importa el orden es decir están desordenadas de manera aleatoria pero sí también tengo una información muy importante de Cuántas veces está cada una de esas palabras en esta bolsa concretamente en este texto por eso aquí ven una lista que va de mayor a menor de Cuántas veces está cada una de las palabras y que es la palabra que más aparece seis veces hay 5 d 4 y así con el resto vamos a analizar algunas cuestiones para entender mejor este concepto de bolsa de palabras pero para ello vamos a hablar de la característica del lenguaje y aquí básicamente vamos a hablar de un concepto que no escapa a lo que ustedes puedan entender o comprender respecto de la característica de un texto el lenguaje y el texto son secuenciales ustedes saben que hay un conjunto de palabras que puestas en secuencia le dan sentido a una frase como esta que está aquí donde dice para mañana hay pronóstico de yuvia pero si me voy al concepto de bolsa de palabras Recuerden que lo que teníamos era una bolsa que tenía un conjunto de palabras Por ende esta frase si la coloco en una bolsa de palabras estoy colocando en una bolsa para mañana hay pronóstico de lluvia es decir que tengo una bolsa con 1 2 3 4 5 se palabras Por ende aquí estoy escribiendo otra frase para lluvia de mañana hay pronóstico una frase que no tiene ningún sentido desde la interpretación nuestra del texto pero para la bolsa de palabra ambas frases son exactamente iguales Por qué Porque ambas frases tienen la misma cantidad de palabras y el mismo conjunto de palabras para seguir hablando de este concepto vamos a ver la pérdida de información que me da el uso de la bolsa de palabras porque por ejemplo aquí tenemos materiales de construcción y construcción de materiales lavado de máquina o máquina de lavado las dos frases tienen sentido a diferencia de lo que teníamos en esto que teníamos recién aquí es decir cuando hablamos de para yuda de mañana de pronóstico que es una frase que no tiene sentido Aunque para la bolsa de palabra ambas frases son iguales en este caso ambas frases materiales de construcción y construcción de materiales como lavado de máquina o máquina de lavado son frases iguales para la bolsa de palabras más allá de que en este caso sí ambas frases tienen sentido Y seguramente una de las dos frases es la que supuestamente la persona que está escribiendo es la que quiere representar para la bolsa Las dos son iguales pero para la elección del usuario no da lo mismo porque una de ellas es la opción  elegida visto todos estos problemas ustedes me van a decir por qué es importante la bolsa de palabras para el nlp si tiene todos estos problemas Bueno La idea es que la bolsa de palabras no pone su foco En aquellos procesos que necesitan del seguimiento en secuencia de un texto sino solamente para aquellos procesos que tienen que hacer foco en si un conjunto de palabras está o no en un texto por ejemplo el análisis de sentimientos yo puedo tener una bolsa de palabras en donde tengo todas las palabras que representan un sentimiento positivo y otra bolsa de palabras o otro conjunto identificado con palabras que detentan un sentimiento negativo con lo cual yo puedo tener como input un texto y ver si en ese texto hay un conjunto de palabras que coinciden con las que están en la bolsa de palabras representando un sentimiento positivo o lo contrario y Por ende a partir de eso poder calificar un texto como positivo o como negativo lo mismo con la detección de spam yo puedo tomar como input el texto de un correo y ver si sus palabras están referidas a lo que tengo en una bolsa de palabras como palabras que o frases que pueden representar una conducta negativa o algo que puede ser calificado como como spam o lo contrario un texto que puede verse como un correo normal y lo mismo también en los modelos probabilísticos y de aprendizaje profundo y el la representación más importante de este ámbito es el chat gpt donde justamente utiliza la bolsa de palabras para esta comparación similar a la que estoy diciendo recién en cuanto análisis de sentimiento o detección de spam pero aplicada Obviamente con una base enorme de información a procesos más complejos y profundos bien el siguiente tema a abordar es el de conteo de palabras y justamente a través del conteo de palabras vamos a poder empezar a entender esto de transformar un texto en vectores de qué se trata el conteo de palabras justamente es una de las técnicas que existen para llevar a cabo esta tarea que mencioné recién convertir un texto en una representación numérica concretamente en un vector Cuál sería la técnica para hacer esto bueno en principio yo voy a tomar un texto y y supongamos que tengo un texto de 100 palabras Por ende si tengo un texto de 100 palabras voy a tener representaciones de texto en vectores de Dimensión 100 y luego voy a contar Cuántas veces está esa palabra en ese texto y Por ende esa cantidad se va a tomar como referencia para cada uno de los valores de esa dimensión ahora vamos a pasar un ejemplo práctico se va a entender bien pero este sería el concepto fundacional de eso sí además de eso vamos a definir Qué es o Qué entendemos por documento el documento es una cadena de texto que vamos a usar en llp y que puede ser de cualquier longitud sí Y puede ser un paper un tweet un post de Instagram un comentario sobre un producto un Script de YouTube o cualquiera de estas cuestiones con las que cotidianamente nos enfrentamos Aquí vamos a tener por ejemplo una situación en la cual vamos a empezar a dar un ejemplo práctico para poder entender esta cuestión del método de conteo de palabras el método de conteo palabras es un proceso para determinar el tamaño de un vocabulario como dije recién si tengo un texto de 100 palabras pues entonces mi vocabulario restringido pero vocabulario al fin va a ser un vocabulario de 100 palabras y va a estar conformado por cada una de esas 100 palabras que están en ese texto el proceso de creación de un vector para cada documento basado en el cono de palabras es al lo que justamente vamos a ejemplificar Empezando por tomar como idea muy básica de documentos estas tres líneas que están aquí Tengo estas tres líneas y cada una representan un pequeño documento el primero dice me gusta hacer deportes el deporte genera vínculos humanos primer documento el segundo dice el deporte que más me gusta es el tenis creo que soy un buen jugador de tenis segundo documento y el tercero dice no me gusta el Ski porque el esqui se practica en un ambiente frío y yo odio el frío bien a partir de esto Cómo se implementa Esta técnica del método del conteo de palabras bueno basado en este ejemplo de la siguiente manera sigo manteniendo tres frases y lo que voy a hacer en principio es armar mi vocabulario en virtud de estos tres documentos sí Entonces fíjense que yo arranco por ejemplo tomando el primer documento y digo me y acá tengo la primer palabra gusta el deporte luego cómo sigue él pero qué pasa él ya lo tengo Entonces ya no lo voy a agregar al vocabulario porque justamente es una palabra que está repetida con lo cual lo que voy a poner aquí es que en el documento uno la palabra é está dos veces luego viene deporte que también ya está entonces no la voy a agregar al vocabulario sino que voy a cambiar la cantidad de veces que aparece esa palabra en es documento pasando de uno a dos y finalmente genera vínculos ahí termino la primer frase diamos el primer documento el primer texto de este mini documento y paso al segundo documento que está en esta segunda columna y empiezo mi no está Lo agrego el vocabulario deporte sí está entonces directamente pongo uno luego favorito no está Lo agrego es no está Lo agrego é sí está por de la palabra é digo Bueno ya está en el diccionario lo que tengo que decir es que esa palabra del diccionario que ya existe en el diccionario en este documento dos está una vez y luego así sigo con el resto de bueno la la frase del segundo documento y del Tercer documento completando de esta manera todo el vocabulario completo El vocabulario completo empieza en me gusta é y llega hasta frío es decir todas estas palabras están formando estos tres documentos con lo cual mi mundo de vocabularios es esta columna de palabras y luego lo que hago es poner Cuántas veces está esa palabra en el contexto de ese documento con lo cual lo que yo estoy haciendo aquí es creando vectores de qué dimensión de la dimensión que yo tengo aquí es decir 1 2 3 4 5 6 7 8 10 11 12 13 14 15 16 17 18 dimensiones tienen estos vectores porque mi vocabulario Tiene 18 palabras y cómo o qué valor le doy a cada cada una de esas dimensiones Perdón la el valor que tiene que ver en cada documento la cantidad de apariciones que existe en ese documento con lo cual justamente la palabra me va a ser un elemento que va a tener una representación de uno en el documento uno de uno en el documento dos y en total dos veces sí la palabra el es la que más aparece a lo largo de los tres documentos y así con el resto de los casos sí donde puedo ver no solamente Cuántas veces aparece esa representación en el contexto de un documento sino en el contexto de este conjunto de eh documentos que va a formar lo que se va a llamar un Corpus el Corpus es el conjunto de documentos que yo voy a tomar para analizar en este caso una técnica de control de palabras o la que fuera en el marco de El nlp para terminar de cerrar este ejemplo vamos a volver una diapositiva hacia atrás y recoger esta información respecto del método del conteo de palabras es un proceso de creación de un vector para cada documento basado en el conteo de palabras es decir el documento uno Cómo está representado por un vector de 18 posiciones y Qué valores va a tener cada uno de esos elementos cada una de esas dimensiones Bueno en ese vector va a tener en la primer posición un número uno porque tiene una vez la palabra que está identificada con esa posición un uno en la posición número dos porque está una vez la palabra identificada con esa posición y así el resto Pero obviamente en el resto de los casos que van de mí hasta llegar a la última palabra va a tener un valor cero porque cada una de estas palabras en el documento uno está cero veces en el documento dos y el documento 3 la dinámica es la misma en el documento dos no existe la palabra me con lo cual en la primera posición del vector que representa el documento do el valor es cer en la segunda lo mismo en el tercero está una vez esa palabra con lo cual en el contexto de las 18 posiciones que tiene el vector del documento dos en la tercer posición el valor es se entiende la lógica es muy sencilla y justamente es la base de cómo transforma un texto en un vector el método de conteo de palabras hasta aquí  Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número  27  Hola a todos Bienvenidos a la clase número 27 del curso de Inteligencia artificial de ifes hoy vamos a ver uno de los temas más importantes del mundo del nlp el tema  embeddings Qué es un embedding un embedding es una forma de vectorización similar a la que vimos pero que plantea una evolución respecto a esas formas para recorrer esas formas vamos a recurrir a esta imagen que graficab612 y de poner un ejemplo bastante sencillo habíamos tomado como que cada uno de estos renglones era un eh documento y cada una de estas columnas que tenos tenemos aquí dibujadas era el vector que representaba a cada uno de sus documentos y todo lo que tenemos aquí a la izquierda el abecedario me gusta el deporte hasta llegar a que odio y sí eran 18 palabras con lo cual tenía una representación de cada uno de los documentos con vectores de 18 posiciones y cada una de las posiciones tenía un número que representaba Cuántas veces estaba Esa palabra que estaba a la izquierda estaba en el contexto de ese documento Pero cuál es el problema aquí aquí tenemos vectores de 18 posiciones pero fíjense cuántas posiciones están realmente ocupadas en el primer caso tenemos ocupadas seis posiciones con lo cual tenemos 12 posiciones que no tienen ningún tipo de información en el caso del documento dos está un poquito digamos más relleno tenemos 1 2 3 4 5 6 7 8 pero tenemos igual más del 50% de las posiciones Sí están siendo ocupadas por espacios en blanco por ceros en el caso del documento 13 es lo mismo bueno Esto pensemos que es un ejemplo muy muy sencillo pero justamente si lo tratáramos de llevar a un ejemplo más complejo más real donde puedo tener un vocabulario de 2000 3000 y hasta por qué no 20,000 palabras ustedes piensen si en ese contexto yo dejara de utilizar O estaría sub utilizando de alguna manera Por decirlo este las posiciones de los vectores en esta misma proporción en una proporción del 60 por por decir algo estaríamos hablando de vectores de 20,000 posiciones de las cuales estaríamos usando solamente el 40% dec 8,000 es decir sería un despropósito y estaríamos Obviamente con una sobrecarga de información que no ayudaría nada a nuestro algoritmo y nuestro proyecto los enedis vienen a proponer una forma diferente de vectorizar un concepto diferente de vectorizar justamente tratando de proponer una solución que no eh bueno que proponga justamente una postura diferente que evite desperdiciar la cantidad de espacio que estamos desperdiciando en vectores que no tienen ningún tipo de información por eso este concepto de embeddings me lleva a una idea que sería la siguiente supongamos que en el caso de recién yo pensara en tener vectores en lugar de 18 posiciones que tengan con un vocabulario de 18 palabras supongamos ocho posiciones sí es decir un número significativamente menor con el método del contol de palabra Eso no se puede hacer pero sí lo podría hacer pensando en tener vectores que tengan valores que no tengan que ver como en el caso anterior que tenemos aquí la cantidad de veces que aparece una palabra sino otro tipo de valor que tuviera que ver con justamente ahorrar la cantidad de dimensiones pero que cada una de las posiciones del vector tenga un valor es decir que no haya posiciones que tengan valores cero o que estén subutilizadas en este caso fíjense que yo tengo por ejemplo Un ejemplo muy sencillo en el cual supongamos que yo pudiera estar representando un conjunto de palabras solamente con dos dimensiones y aquí tengo por ejemplo la palabra auto que está representada por un vector 0109 o sea de dos posiciones que tengan los valores 0.1 y 0.9 y la palabra camioneta eh representada por otro vector de dos posiciones pero que tiene valores 015 y 085 veamos que más allá de esta cuestión de que las palabras no están representadas por una gran cantidad de dimensiones sino solamente por dos los valores que tiene auto y camioneta son bastante parecidos esto es importante y lo tenemos visto ya desde la clase pasada donde hablamos justamente de la similitud de vectores Porque estos valores de camioneta y de auto representan la similitud que tienen sem iamente esas dos palabras entonces obviamente los vectores tienen que actuar en consecuencia tienen que tener valores en consecuencia del mismo modo tengo el ejemplo de casa y departamento donde veo que los valores 075 027 o 07 y 02 son muy similares y muy distintos a su vez de auto y camioneta es decir casa es similar a departamento auto es similar a camioneta pero ni auto ni camioneta son similares a casa y departamento y viceversa bien importante aquí es que el concepto de embedding me lleva a utilizar vectores que no tienen la misma cantidad de Dimensión que la cantidad de de palabras que tiene el vocabulario tiene una dimensión mucho menor y los valores de cada una de esas dimensiones representan un valor en un espacio vectorial de la cantidad de posiciones que sea pero que no tiene que ver con la ya con la cantidad de palabras que o la cantidad de veces perdón que aparece esa palabra en el documento sino con un número que en el espacio me permite Sí con un valor determinado establecer una similitud entre una palabra y otra o diferenciarla con otra palabra diferente los modelos para en bedim utilizan eh conceptos vinculados al Deep learning al mundo del Deep learning que nosotros Ya estuvimos incursionando en él justamente con lo que tenemos aquí como primer opción de tipo de de red para eh realizar o llevar a cabo el embedding que son las redes neuronales convolucionales bueno es una de las opciones no es la opción más utilizada la opción más utilizada es las redes neuronales recurrentes que es lo que aparece aquí arriba de la derecha y dentro de ese contexto Y con esa ampliación que plantea esta rnr hay un concepto que es concretamente el que se usa hoy en día que es el de Transformers que es el concepto de mayor evolución y de mayor uso en la actualidad vinculado a los procesos de embedding técnicas para eh llevar a cabo los procesos de embeddings hay muchas nosotros vamos a empezar por ver dos de las que hoy son muy importantes en el mercado Pero después vamos a ver otras que plantea justamente la evolución de las últimas tendencias esta primera que está aquí arriba es Word to Back es decir básicamente un juego de palabras que es o implica la transformación de una palabra en un vector y justamente es un algoritmo como dice aquí que utiliza un modelo ronal para aprender asociaciones de palabras a partir de un gran Corpus de texto o sea cuando hablamos de Corpus hablamos de un conjunto de palabras que puede estar representado por uno y hasta por varios documentos bueno el Corpus de texto es todo ese gran conjunto de texto a partir del cual yo voy a entrenar una red como bien dice aquí una vez que entreno el modelo puedo detectar palabras sinónimas o sugerir palabras adicionales para una frase sin terminar es tal cual lo que vimos lo que venimos hablando perdón de lo que nos pasa cuando estamos en gmail y escribimos un texto y nos aparecen palabras sugeridas a continuación de lo que estamos escribiendo Word tob representa cada palabra distinta con una lista particular de números llamada vector es decir lo que vimos recién en el ejemplo de esta relación aut camioneta o casa departamento Bueno eso es Ni más ni menos lo que hace Word o lo que hacen los enedis en general Sí y eh A partir de eso puedo justamente ver la similitud que existe entre vectores y en consecuencia ver la similitud que existe entre los textos a través de la teoría o el teorema del coseno que vimos la clase pasada que justamente midiendo el ángulo que existe entre los vectores puedo ver el nivel de similitud entre las palabras que representan esos vectores otra técnica para embedding es glow glow es un desarrollo de la Universidad de Stanford y significa vectores globales es una opción similar a Word desarrollada por otro proveedor como muchas veces pasa en el mundo de la informática y en el mundo de la ia y eh si bien tiene propósitos muy similares tiene una particularidad tiene en cuenta las estadísticas de coocurrencia del Corpus es decir se fija o va registrando estadísticamente los conjuntos de palabras o los pares de palabras Cuántas veces aparecen juntas en el Corpus para de esa manera cuando una palabra determinada esté siendo redactada por una persona por ejemplo en gmail como referenciamos recién la palabra que le sugiera a continuación sea la que más frecuentemente aparece cuando esa persona escribe al lado de la palabra anterior el próximo tema que tenemos que ver es el concepto de analogía de palabras en el ámbito de los embeddings y aquí tengo un ejemplo en un espacio bidimensional donde tengo países Argentina y Uruguay y sus respectivas capitales pero básicamente son ciudades Buenos Aires y Montevideo la idea aquí es que Argentina y Uruguay están cerca entre ellas porque en realidad identifica que se trata de dos países y del mismo modo Buenos Aires está cerca Dentro de este contexto de esta estructura bidimensional de Montevideo porque entiende que son dos ciudades pero más allá de eso también se puede plantear un concepto de analogía de qué se trata la analogía que yo pueda decir que si hago una relación Argentina Buenos Aires pueda descubrir Cuál es la potencial relación que puede tener Uruguay o Montevideo es decir que si yo digo que Buenos Aires es a Argentina como Montevideo es a Uruguay en realidad cuando yo digo estoy diciendo esto yo pretendo que la ia sea quien me lo diga por ejemplo yo puedo decir Buenos Aires es Argentina con Montevideo es a y ahí la ia me va a responder Uruguay Ese es el planteamiento de analogías y es parte de lo que vamos a ver a continuación en el lap número cinco donde vamos a ver justamente similitud de vectores con embedding y el concepto de analogías utilizando un dataset cagle de palabras en castellano nosotros vamos a hacer un primer lab en esta primera parte de la clase en el cual Vamos a abordar el tema de utilizar un embedding o un archivo que tiene los embedding ya hechos y en la segunda parte de esta clase Vamos a abordar el tema de eh hacer nuestros propios embedding con nuestro propio conjunto de palabras o nuestro propio Corpus bien aquí estamos en el apn l5 en el ámbito de colab y vamos a empezar con el propósito que aquí tenemos similitud de vectores con embedding utilizando una de kaggle con palabras en castellano antes de empezar a configurar todo este trabajo vamos a hablar un poquito de este eh dataset que vamos a bajar este Corpus hablando más en términos de nlp que bajamos de kaggle eh este dataset se llama pretrained Word vector for spanish el el título de El dataset como se lo identifica en kaggle es un dat que contiene 1,653 Word embeddings de Dimensión 300 o sea 1,653 palabras cada una de esas palabras está representada por un vector de 300 dimensiones fíjense eh lo importante de lo que hablamos hace un rato Es decir yo quizás con el conteo de palabras hubiera tenido un vocabulario de 1,653 palabras pero a su vez vectores para cada una de esas palabras de 1,653 en este caso fíjense la enorme diferencia que hay entre la dimensión del vocabulario y la dimensión de los vectores bien esto ha sido entrenado justamente con Word tob que es una de las herramientas que vimos recién en la teoría y obviamente ha sido entrenado con Corpus de billones de palabras en español esto es muy común hay muchos archivos que ya tienen los vectores armados con un vocabulario muy grande de palabras y me sirven mucho para esta fase de aprendizaje para probar cosas y no tener que formar yo mi propio Corpus pero como dijimos hace un rato en la segunda parte de la clase eso es lo que vamos a hacer obviamente no vamos a llevar a un Corpus tan grande como esto pero es importante que sepamos usar lo que ya está hecho bienvenido Que esté hecho y también empezar a usarlo nuestro dicho esto vamos entonces al principio y vamos a importar pandas y luego de ello vamos a montar el Drive como siempre hacemos dado que allí tiene que estar el archivo que hablábamos recién sbw vectors 3001 5.txt obviamente Este es un archivo que ustedes van a tener en el campus virtual Y ustedes lo van a tener que poner como siempre animos haciendo en el Drive que ustedes quieran utilizar Bueno ya tenemos montado el Drive lo que vamos a instalar ahora es una librería que se llama hensim que Eh no la tenemos seguramente instalada con lo cual vamos a hacer primero un pip install genim Recuerden que siempre que les digo que eh cada vez que no tengan alguna librería en la actividad que estén llevando adelante bueno pueden instalarla con pip install enim el signo de admiración refiere a que este Comando no se ejecuta dentro de colap sino fuera de colap y el menos q es para que no me muestre mucha de la información que muestra habitualmente un proceso de instalación es un detalle Simplemente no pasa nada si no pongo el menú q bien luego de importar el genim eh Perdón luego de instalar el genim lo importamos y como lo hicimos tantas veces si creamos un dataframe con pandas con rsb en base a este archivo que referenciamos recién y lo ponemos dentro de una refame que le ponemos como nombre DF bien Ahora vamos a ver el contenido de DF y podemos ver rápidamente queé tenemos en principio la palabra d dígito la enl O sea toda la palabra que tiene este gran voc formulario de 1,653 palabras y cada una de esas palabras está representada como dijimos antes por un vector de 300 posiciones en el cual aquí estamos viendo las primeras cuatro y un poquito de la quinta posición en algunos casos aquí solamente las cuatro Pero podemos observar las cuatro primeras posiciones de cada una de estas palabras con valores que son los que tiene justamente cada uno de esos vectores en el contexto de un espacio dimensional de 300 dimensiones sí bien eh aquí tenemos entonces toda la la información en una sola columna o sea está junto la dimensión del vector con el texto de la palabra y como dijimos antes las eh 1,653 palabras bien una vez que vemos esto vamos a empezar a crear los vectores o traernos los vectores a nuestra estructura a través de una variable que le voy a poner vectores y la voy a eh llenar justamente a través de la biblioteca gensin que acabamos de instalar recién y con lad Word tob format le voy a decir que vaya a buscar esa información de los vectores si a el archivo que recién acabamos de observar a través de este Data frame entonces a partir de esto todos estos vectores que estamos viendo aquí s todos estos vectores que están aquí al lado de cada una de las palabras van a ir o van a ubicarse dentro de esta variable de vectores en este práctico como dijimos antes vamos a ver dos cuestiones propias de los embedding la similitud y las analogías aquí por ejemplo Argentina y Uruguay son palabras similares Buenos Aires y Montevideo son palabras similares ahora Argentina y Buenos Aires son palabras análogas y Uruguay y Montevideo lo mismo eso lo vamos a ver justamente ahora en la codificación de El colap aquí estamos entonces luego de haber haber eh colocado todos los vectores en esta variable de vectores lo cual nos llevó un buen tiempo pero ya tenemos esta cantidad enorme de eh vectores dentro de esta variable con lo cual vamos a definir en principio una función analogía para ir poniendo distintos valores y no tener que estar reescribiendo el código tantas veces esta función le voy a poner ver analogía le vamos a dar como input tres valores La idea es como dijimos hoy en el ejemplo de la de la clase teórica que yo le pase por ejemplo Argentina y Buenos Aires y luego le pase Uruguay y automáticamente la inteligencia me dé la palabra Montevideo con lo cual siempre le voy a tener que dar tres valores y va a aparecer automáticamente el cuarto que va a ser la analogía de uno de esos valores para ello vamos a usar de eh la variable que creamos aquí vectores el método m similar en el cual le voy a pasar la el valor de entrada p1 y p3 que se dirían las dos palabras similares pero no análogas y luego con el argumento negativo le voy a pasar la palabra análoga de la primer palabra que puse Aquí concretamente vamos un ejemplo supongamos que en p1 yo pongo Argentina en p2 podría Buenos Aires en p3 pondría Uruguay y estaría la la representación de la palabra faltante como algo pendiente a resolver por parte de justamente m similar de vectores eso lo voy a poner dentro de esta variable similitud y luego voy a hacer un print donde voy a poner que p1 es ap2 como similitud 00 que es justamente el valor que me va a tirar la ia es ap3 bien con esta lógica que vamos a cargar porque esto acuérdense que las funciones la tenemos que declarar y ejecutarla no como que vaya a hacer una acción sino que esté identificada y dispuesta eh disponible Perdón esta función vamos a usar en principio con rey y hombre y luego la palabra mujer lo cual nos tiene que dar la posibilidad de que la palabra análoga que nos tiene que brindar esta función y Por ende la ia es la palabra reina lo ejecutamos y justamente el resultado es rey esa hombre como reina esa mujer lo probamos con Argentina argentino y español y dice que Argentina es argentino como español es a español como española Perdón es a español bien en el caso anterior la palabra que me brindó la ia es reina y en el caso este es española en este caso fíjense que toma Argentina como una mujer es decir no como la la patria como el país porque justamente la primer letra está puesta en minúscula si yo ahora pruebo poner la primer palabra en mayúscula va a tomar Argentina como país y o como una persona de sexo femenino propia de nuestro país sí Entonces ahora la respuesta va a ser Argentina es argentino como España es a español bien y así puedo probar aquí con taller mecánico y médico talleres a mecánico como consultorio ex a médico y con algunas ciudades Madrid es España como montevo es Uruguay bien con eso tenemos varios ejemplos de analogías ahora vamos con el tema de similitud con lo cual vamos a hacer otra función que le vamos a llamar ver cercanía donde aquí no entran eh un conjunto de palabras como en el caso de la función anterior sino simplemente una palabra a la cual le vamos a buscar cuál es el vector más similar o que representa una mayor similitud con esa palabra con lo cual en este caso vamos a usar el Moss similar pero ahora solamente con un argumento positivo antes teníamos el positivo los positivos y el negativo por esta cuestión de la analogía ahora simplemente Busco algo que es similar al argumento que entra con lo cual Busco algo que sea similar a lo que yo tengo aquí como argumento y luego hago un print de eh Cuáles son los elementos cercanos a p Por qué digo los porque justamente lo que vamos a ver aquí es tratar de determinar una cantidad de valores similares o sea no voy a tomar la sugerencia de solamente el vector que tiene mayor similitud sino voy a tomar una serie de candidatos eh de similitud con lo cual me va a dar un conjunto de palabras similares a la que yo le doy por eso hago un for con el cual recorro Sí justamente la variable vecinos que es la que aloja a todo el conjunto de vectores similares a la palabra ejemplo que yo le estoy dando y Forward Score In vecinos voy a mostrar justamente cada una de esas palabras y en este caso no estoy mostrando el score lo podría hacer también con lo cual puedo estar mostrando la palabra y el nivel de factibilidad que me da la la inteligencia respecto de cuán similar entiende es esa palabra a la que yo he candidatado bueno habiendo cargado esta función vamos a probarla con lo cual voy a poner la palabra fútbol y aquí tengo balonpie fútbol soccer béisbol basketbol o sea palabras similares recuerde que similares no quiere decir que sea concretamente algo que sea Exactamente igual sino palabras que tengan un alto nivel nivel de asociatividad pongo Argentina en minúscula representando como hoy sí una una persona de exceso femenino que pertenece a nuestro país no el nombre de la patria bueno y aquí me aparecen las recomendaciones de similitud ahora Argentina mayúscula y me aparecen países ven que en este caso aparecen países y en este caso aparecen otras cuestiones que tienen que ver justamente con Eh bueno las bolivianas las mendosinas las paraguayas las brasilenas tomando el femenino de una persona que está representada por esa parte bien con esto Terminamos el lab y em les dejo para ustedes a ver si quieren ponerles aquí Este esta variable score Sí este parámetro score para ver justamente Eh bueno qué nivel de score le da a cada una de estas palabras que consider las similares obviamente esto es importante porque después pu en base a eso pueden determinar con cuál de todas estas palabras ustedes se van a quedar a la hora de hacer una una inteligencia o programar un un algoritmo de nl bien con esto terminamos decía este lab con esto terminamos también esta primer parte de esta clase lo que nos queda para la segunda parte Entonces es algo similar de un proceso de embedding pero con nuestro propio Corpus hasta aquí 