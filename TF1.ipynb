{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "En esta primer parte del proyecto nos abocamos a la obtención y depuración de los datos que usaremos para que nuestro chatbot responda. Hemos desarrollado esta parte del proyecto en etapas que describimos a continuación:\n",
        "\n",
        "**ETAPA 0**: Instalación y Carga básica.\n",
        "\n",
        "**ETAPA 1**: Transcribimos (convertimos) en texto los videos de las clases que están visibles por medio de YouTube. Para ello guardamos datos de las url de cada video. Además, recopilamos información sobre los participantes del curso y algunos datos del CV del profesor.\n",
        "\n",
        "**ETAPA 2**: Limpiamos cada uno de los archivos de texto generados a partir de los videos (eliminamos corchetes y símbolos).\n",
        "\n",
        "**ETAPA 3**: Juntamos todos los archivos de textos (los generados a partir de los videos, los de los datos de participantes del curso, los datos del profesor, los datos de las url de los videos) en un único texto.\n",
        "\n",
        "**ETAPA 4**: Realizamos una segmemtación (spliteo) del archivo completo con todos los textos juntos (definiendo tramos de 200 caracteres y un solapamineto de 20). Calculamos una aproximación del costo al usar el servicio de OpenIA. Generamos un índice en la nube por medio de Pinecone para guardar los vectores que obtendremos con el embedding. Realizamos el embedding y guardamos los vectores en Pinecone.\n",
        "\n"
      ],
      "metadata": {
        "id": "-DIKntYJg2Bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ETAPA 0**"
      ],
      "metadata": {
        "id": "eqlWxF8VnHhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "!pip install pytube -q\n",
        "!pip install youtube-transcript-api\n",
        "!pip install google-colab\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "from pytube import YouTube\n",
        "from google.colab import files # se utiliza para bajar los archivos txt"
      ],
      "metadata": {
        "id": "ZqDrFVrhnFRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510894c3-d73b-4f8b-ae2f-a97f5fd8868a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.2.2)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.2\n",
            "Requirement already satisfied: google-colab in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.27.0)\n",
            "Requirement already satisfied: ipykernel==5.5.6 in /usr/local/lib/python3.10/dist-packages (from google-colab) (5.5.6)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.5.5)\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (1.5.3)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (1.5.2)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.31.0)\n",
            "Requirement already satisfied: tornado==6.3.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.3.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (4.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (6.1.12)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython==7.34.0->google-colab)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (3.1.3)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.7.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->google-colab) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->google-colab) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->google-colab) (1.25.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker==1.5.2->google-colab) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->google-colab) (2024.2.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.5->google-colab) (4.2.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (23.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (4.19.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0->google-colab) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->google-colab) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook==6.5.5->google-colab) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.18.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google-colab) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook==6.5.5->google-colab) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (2.21)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ETAPA 1**"
      ],
      "metadata": {
        "id": "fDY1d3o-ns75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1** Montamos el drive para acceder al archivo con los datos de url de los videos."
      ],
      "metadata": {
        "id": "gOUQ6Qv4n7v-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q7JeQVTgom5"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive en Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta al archivo de texto en Google Drive\n",
        "file_path = '/content/drive/MyDrive/chatlef/Videos.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2** Preparamos el acceso a los videos de YouTube en formato corto (que es como trabaja YoutubeLoader de langchain)."
      ],
      "metadata": {
        "id": "HduyqyaloYsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expresión regular para encontrar URLs en el formato corto (hay que enviarle el formato corto a YoutubeLoader.from_youtube_url)\n",
        "import re #  Libreria que  utiliza funciones de espresiones regulares para trabajar el tema de las cadenas\n",
        "urls_youtu_be = []\n",
        "# Expresión regular para encontrar URLs en el formato largo\n",
        "regex = r\"https://www\\.youtube\\.com/watch\\?v=([\\w-]+)\"\n",
        "# Leer las líneas desde el archivo de video\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Buscar coincidencias con la expresión regular\n",
        "        matches = re.findall(regex, line)\n",
        "        # Convertir las URLs en el formato corto\n",
        "        urls_short_format = [f\"https://youtu.be/{match}\" for match in matches]\n",
        "        # Agregar las URLs en el formato corto a la lista\n",
        "        urls_youtu_be.extend(urls_short_format)\n",
        "# Mostrar las URLs en el formato corto\n",
        "#for url in urls_youtu_be:\n",
        " #   print(url)\n"
      ],
      "metadata": {
        "id": "_YR6VoJWoZbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3** Transcribimos cada video de cada clase del curso, generamos un archivo con la transcripción y los datos de YouTube para cada clase, los guardamos con el nombre de su respectiva url y descargamos los archivos."
      ],
      "metadata": {
        "id": "IKDF8XriokHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcripciones = []\n",
        "for  url in urls_youtu_be:\n",
        "    # print (url)\n",
        "     loader=YoutubeLoader.from_youtube_url(url,add_video_info=True,language=[\"es\"])\n",
        "     transcripcion=loader.load()\n",
        "     transcripciones.append(transcripcion)\n",
        "     nombre=transcripcion[0].metadata['source']+\".txt\"\n",
        "     # grabar con el nombre de la url\n",
        "     with open(nombre, 'w') as file:\n",
        "         file.write(transcripcion[0].page_content)\n",
        "     # Descargar el archivo .txt\n",
        "     files.download(nombre)\n",
        "\n",
        "# Mostrar las transcripciones\n",
        "for i, transcripcion in enumerate(transcripciones):\n",
        "    print(f'Transcripción del video {i + 1}:\\n{transcripcion}\\n')"
      ],
      "metadata": {
        "id": "B7wZE1V2oknB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ETAPA 2**"
      ],
      "metadata": {
        "id": "DO38uVyW6d6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpiamos cada uno de los archivos de texto generados a partir de los videos"
      ],
      "metadata": {
        "id": "LFx1S3JC6jl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# Definir una función para eliminar palabras entre corchetes de un texto\n",
        "def eliminar_palabras_corchetes(texto):\n",
        "    # Expresión regular para encontrar palabras entre corchetes\n",
        "    patron = r'\\[(.*?)\\]'\n",
        "    # Reemplazar todas las ocurrencias de palabras entre corchetes con una cadena vacía\n",
        "    texto_sin_corchetes = re.sub(patron, '', texto)\n",
        "    # Devolver el texto sin palabras entre corchetes\n",
        "    return texto_sin_corchetes\n",
        "\n",
        "# Directorio donde se encuentran los archivos de texto\n",
        "directorio_origen = '/content/drive/MyDrive/chatlef/Txt/'\n",
        "directorio_destino = '/content/drive/MyDrive/chatlef/Txt/Modificado/'\n",
        "\n",
        "# Crear el directorio de destino si no existe\n",
        "if not os.path.exists(directorio_destino):\n",
        "    os.makedirs(directorio_destino)\n",
        "\n",
        "# Obtener la lista de archivos en el directorio\n",
        "archivos_txt = [archivo for archivo in os.listdir(directorio_origen) if archivo.endswith('.txt')]\n",
        "\n",
        "# Procesar cada archivo de texto\n",
        "for archivo in archivos_txt:\n",
        "    # Ruta completa del archivo de origen\n",
        "    ruta_archivo_origen = os.path.join(directorio_origen, archivo)\n",
        "\n",
        "    # Leer el contenido del archivo de texto\n",
        "    with open(ruta_archivo_origen, 'r') as file:\n",
        "        texto_completo = file.read()\n",
        "\n",
        "    # Eliminar todas las palabras entre corchetes del texto completo\n",
        "    texto_sin_corchetes = eliminar_palabras_corchetes(texto_completo)\n",
        "\n",
        "    # Ruta del archivo de destino\n",
        "    ruta_archivo_destino = os.path.join(directorio_destino, archivo)\n",
        "\n",
        "    # Escribir el texto modificado en el archivo de destino\n",
        "    with open(ruta_archivo_destino, 'w') as file:\n",
        "        file.write(texto_sin_corchetes)\n",
        "\n",
        "    print(\"Palabras entre corchetes eliminadas en el archivo\", archivo, \"y guardado en\", ruta_archivo_destino)\n"
      ],
      "metadata": {
        "id": "LU1N2swB7DLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ETAPA 3**"
      ],
      "metadata": {
        "id": "PIPPOb9V7fjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1** Juntamos todos los archivos de textos que usaremos como datos para ChatLEF."
      ],
      "metadata": {
        "id": "-Pu428bI8a3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Montar Google Drive en el entorno de Colab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta de la carpeta que contiene los archivos .txt en Google Drive\n",
        "carpeta = '/content/drive/My Drive/Archivos/NLP/Modificado'  # RUTA: OJO, PONER LA RUTA CORRECTA SEGUN CADA UNO\n",
        "\n",
        "# Generar la lista de nombres de archivo en el orden deseado\n",
        "orden_archivos = ['perfilProfe.txt', 'participantes.txt'] + [f'V{i}.txt' for i in range(1, 67)]\n",
        "\n",
        "# Lista para almacenar el contenido de todos los archivos\n",
        "contenido_total = []\n",
        "\n",
        "# Iterar sobre los archivos en el orden específico\n",
        "for archivo in orden_archivos:\n",
        "    ruta_archivo = os.path.join(carpeta, archivo)\n",
        "    with open(ruta_archivo, 'r') as f:\n",
        "        contenido_archivo = f.readlines()  # Leer el contenido del archivo\n",
        "        contenido_total.extend(contenido_archivo)  # Agregar el contenido a la lista total\n",
        "\n",
        "# Guardar el contenido combinado en un nuevo archivo\n",
        "ruta_salida = '/content/drive/My Drive/Archivos/NLP/Modificado/contenido_combinado.txt'  # guarda 'contenido_combinado.txt' en el drive\n",
        "with open(ruta_salida, 'w') as f:\n",
        "    f.writelines(contenido_total)\n",
        "\n",
        "# Descargar el archivo combinado\n",
        "files.download(ruta_salida)\n"
      ],
      "metadata": {
        "id": "uaaNK7JL8bas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ETAPA 4**"
      ],
      "metadata": {
        "id": "mPnl6IUw9Szl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1** Montamos desde el drive las api keys"
      ],
      "metadata": {
        "id": "y6fyAaphDIlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install tiktoken -q\n",
        "!pip install pinecone-client -q\n",
        "#!pip install pinecone-client --upgrade!\n",
        "!pip install python-dotenv -q\n",
        "!pip install langchain_pinecone ## Subir a pinecone los vectores\n"
      ],
      "metadata": {
        "id": "QBf-C5fWCi8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee055b06-b507-4755-c4b8-4f3acc88224a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.0.3-py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (0.1.26)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (1.25.2)\n",
            "Requirement already satisfied: pinecone-client<4,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (3.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (0.1.8)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (8.2.3)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4,>=3->langchain_pinecone) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4,>=3->langchain_pinecone) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4,>=3->langchain_pinecone) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<4,>=3->langchain_pinecone) (2.0.7)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_pinecone) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_pinecone) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_pinecone) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain_pinecone) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1->langchain_pinecone) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain_pinecone) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain_pinecone) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_pinecone) (3.3.2)\n",
            "Installing collected packages: langchain_pinecone\n",
            "Successfully installed langchain_pinecone-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KP6ojB7UCqMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "967169ea-7248-4668-e077-f1e2e29ac87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Carga OPENAI_API_KEY\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv('/content/drive/MyDrive/Archivos/NLP/.env')\n",
        "#os.environ.get(\"OPENAI_API_KEY\")\n"
      ],
      "metadata": {
        "id": "yZQ_8QfVC1I4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95dba267-9c59-4ec0-b4c0-9633db05e290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2** Preparamos el archivo con todos los datos para poder segmentarlos usando LangChain"
      ],
      "metadata": {
        "id": "M0djRwKlCNIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer el txt contenido_combinado.tx para  extraer el txt en el formato que reconoce\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "#loader = TextLoader(\"/content/drive/MyDrive/chatlef/Txt/Modificado/FINAL/contenido_combinado.txt\")\n",
        "#loader = TextLoader(\"/content/drive/MyDrive/chatlef/Videos.txt\")\n",
        "loader = TextLoader(\"/content/drive/MyDrive/Archivos/NLP/Modificado/contenido_combinado.txt\")\n",
        "\n",
        "documento = loader.load()"
      ],
      "metadata": {
        "id": "V6ESuei9DA68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer los fragmentos del archivo\n",
        "# No funciona cuando lo extremos como un archivo texto al parecer necesita la estructura de datos documento[0].page_content ect...\n",
        "#with open(\"/content/drive/MyDrive/chatlef/Txt/contenido_unido.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "#    documento = file.readlines()"
      ],
      "metadata": {
        "id": "B9hy98QYDihA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostramos algunos datos\n",
        "print(documento)\n",
        "print(len(documento[0].page_content))"
      ],
      "metadata": {
        "id": "CO3tD57vDulu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5729d139-dc5f-4599-b4d0-571bcdbc669e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=\"Profesor: Gustavo Cabrera.\\nTítulo: Analista Universitario de Sistemas, Universidad Tecnológica de Córdoba. Argentina.\\nCargos Actuales: Director Académico de IFES Educación Superior, Neuquén, Argentina. Director del Centro de Formación y Empleo en Informática de IFES, Neuquén, Argentina. Cofundador de grupo IFES (Instituto de Formación y Educación Superior), Neuquén, Argentina. \\nCargos Anteriores: Director General de Instituto ICONO, Neuquén, Argentina. Analista de Sistemas en el Banco Provincia de Neuquén, Neuquén, Argentina. Programador de Sitemas en XCR, Córdoba, Argentina.\\nPerfil Profesional: Especialista en desarrollo, implementación e innovación de propuestas educativas en formatos de estudios presencial, blended learning o distancia. Especialista en diseño de estrategias comerciales y de comunicación para venta de productos de oferta académica. Capacidad de liderazgo, gestión y organización de equipos y grupos de trabajo. Capacidad de desarrollo de proyectos con iniciativas propias. \\t\\t\\tListado de participantes del curso de IA\\nGonzalo Irusta\\tNunca\\tgonzaloirus@gmail.com\\nPatricio Enrique Sebastian Villalba\\tNunca\\tvillalbapes@gmail.com\\nJoaquin Gabriel Mardones\\t302 días 23 horas\\tjoaco_16_96@hotmail.com\\nCesar Augusto Salazar Molina\\t301 días 1 hora\\tcesalazar00@hotmail.com\\nEliana Soledad Escudero\\t301 días\\telianasoledadescudero@gmail.com\\nGaston Tala\\t300 días 22 horas\\tgaston.at91@gmail.com\\nGustavo Seguin\\t300 días 22 horas\\tgustavo.seguin@gmail.com\\nMarcos David Martinez\\t294 días 9 horas\\tmartinezmarcos82@gmail.com\\nCarolina Ruiz Campos\\t294 días 2 horas\\tcarolinaruizcampos@gmail.com\\nChristian Murray\\t291 días 19 horas\\tcristianmurray@hotmail.com\\nStefani Beatriz Urrea\\t289 días 9 horas\\tstefiu23@hotmail.com\\nManuel Aron Romero\\t286 días 23 horas\\tromeromanuel92@gmail.com\\nROCÍO TAMARA BENIGAR ROJAS\\t286 días 22 horas\\tROCIO.BENIGAR@GMAIL.COM\\nEnrique Gustavo Ortega\\t279 días 22 horas\\tenriquegusavoortega@hotmail.com\\nGraciela Zayas\\t276 días 23 horas\\tgracielazayas03@gmail.com\\nJulio Samir Chaar\\t273 días 19 horas\\tjchaarIII@hotmail.com.ar\\nNahuen Doffo\\t270 días 21 horas\\tnahuend@gmail.com\\nGuido Felix Romani\\t257 días 23 horas\\tromanion22@gmail.com\\nAndres Esteban Lopez Alaniz\\t257 días 21 horas\\tandreslopezalaniz@hotmail.com\\nRoberto Daniel Rojas\\t253 días 3 horas\\trdanielr@outlook.com\\nNicolas Agustin Moya\\t216 días 22 horas\\tnicomoya954@gmail.com\\nGerman Dobler\\t205 días 3 horas\\tgerman.crack@hotmail.com\\nPatricia Sosa\\t203 días 5 horas\\tss_patricia@hotmail.com\\nNicolas Churrain\\t192 días 2 horas\\tnicolasch6@gmail.com\\nFrancisco Rodriguez\\t188 días 23 horas\\t\\nJoel Ezequiel Rojas\\t177 días 2 horas\\tjoel-jp@live.com.ar\\nRodrigo Scorolli\\t174 días 23 horas\\trodrigoscorolli@gmail.com\\nGabriel Nicolas Moreno\\t169 días 20 horas\\tgabriel.moreno5054@gmail.com\\nPatricio Alejandro Olivera\\t166 días 5 horas\\t\\nJuan Agustín Carpinello\\t164 días 19 horas\\tjacarpinello@gmail.com\\nAgustina Fabiana Avilés Billiet\\t160 días 23 horas\\tagusaviles98@gmail.com\\nBARBARA FLORENCIA ROJAS DIAZ\\t157 días 13 horas\\tbarby.19.23.29@gmail.com\\nDiana Sofia Canio\\t156 días 3 horas\\tcaniosofia@gmail.com\\nGaston Emiliano Michelan\\t148 días\\tgaston_m.e@hotmail.com\\nIvan Maximiliano Pereyra\\t142 días 6 horas\\tivanmaxipereyra@gmail.com\\nPaula Rocio Lopez\\t111 días 22 horas\\tlopezpauu@gmail.com\\nGiuliano Nicolás Ruiz\\t104 días 21 horas\\tgiuliano-1998@hotmail.com\\nSILVINA GISELL BALBOA\\t100 días 23 horas\\tsilvinabalboa5@gmail.com\\nDaniel De los Santos\\t97 días 22 horas\\tddelossantos86@gmail.com\\nJose Emiliano Gutierrez\\t92 días 17 horas\\temigut2000@gmail.com\\nLeonardo Molinari\\t90 días 23 horas\\tleonardoalbertomolinari@gmail.com\\nBelen Marite Riquelme\\t85 días 6 horas\\tbm_riquelme@hotmail.com\\nJuan José Medina\\t83 días 9 horas\\tflyjuanjo@hotmail.com\\nDYLAN AXEL ROMERO\\t79 días 12 horas\\tromeroaxel71@gmail.com\\nJUAN VALENTIN SOZZI GARCIA\\t79 días 7 horas\\tvalentinsozzi143@gmail.com\\nLUCA MARTIN RIVERA\\t74 días 20 horas\\tlucamartinrivera@gmail.com\\nFernando Mario Manara\\t72 días 1 hora\\tfernandomanara@yahoo.com.ar\\nJorge Yepum Della Cella\\t64 días 2 horas\\tjorge.yepum@gmail.com\\nJulia Matiazzo\\t63 días 14 horas\\tjulimatiazzo@gmail.com\\nHORACIO ANDRES DIMARCO\\t62 días 22 horas\\tdimarco348@gmail.com\\nOrnella Paola Cevoli\\t62 días 22 horas\\tornellacevoli@gmail.com\\nERICK SAMUEL CISTERNA\\t61 días\\terick.cis123@gmail.com\\nLUCIANA ANTONELLA SCORCIONE\\t58 días 21 horas\\tluciana.scorcione@hotmail.com\\nFederico Del Señor\\t57 días 23 horas\\tfedericodelsenor@gmail.com\\nEzequiel Torres\\t56 días 1 hora\\tezequiel_torres2019@outlook.com\\nJurgen Marcano\\t56 días 1 hora\\tjurgenmarcano@gmail.com\\nEMMANUEL FAMBRINI\\t56 días\\temmafambrini@gmail.com\\nJuan de Dios Fuentes\\t40 días 20 horas\\tfuentes.jdd@gmail.com\\nMarcos Manuel Cortez\\t36 días 3 horas\\tcortezmarcos@gmail.com\\nGustavo Cabrera\\t34 días 19 horas\\tgcabrera@ifes.edu.ar\\nEdgardo Benitez Piccini\\t31 días 19 horas\\tedgardobenitez@gmail.com\\nCesar Nicolas Lopez\\t31 días 6 horas\\tnicolaslopeznqn@gmail.com\\nJavier Leonardo Benavidez\\t26 días 1 hora\\tjavier.benavidez@outlook.com.ar\\nMercedes Di Lorenzo\\t23 días 22 horas\\tdilorenzomercedes@gmail.com\\nNatalia Soledad Cisnero\\t22 días 21 horas\\tsolecisnero3@gmail.com\\nNANCY ANAHI LOPEZ\\t19 días 23 horas\\tanahilopez13@gmail.com\\nAlejo Valenzuela\\t17 días 23 horas\\talejo.valenzuela.electronica@gmail.com\\nFranco Nicolas Guido\\t17 días 19 horas\\tguidonicolas25@gmail.com\\nJosé Eduardo Gutierrez\\t17 días 9 horas\\tjg51421@gmail.com\\nRodrigo Torrealba\\t15 días 2 horas\\trtorrealba@neuquen.gov.ar\\nLuis María Ricardo Nowak\\t15 días 1 hora\\tluisencomahue@gmail.com\\nFranco Nicolas Marquez\\t14 días 6 horas\\tfrancomarquez2013@gmail.com\\nLuis Eduardo Garcia Carrillo\\t13 días 20 horas\\tedugarciacnqn@gmail.com\\nGianinna Stefani Campos\\t12 días 6 horas\\tgianicpos@gmail.com\\nHumberto Buscarini\\t11 días 6 horas\\thabuscarinit@hotmail.com\\nRicardo Alexis Lagos\\t9 días 10 horas\\tfalange04@msn.com\\nMaria Selva Cappella\\t9 días 7 horas\\tselvacappella@gmail.com\\nTadeo Soresi\\t8 días 5 horas\\ttadeosoresi23@outlook.com\\nCarlos Manuel Poggi\\t7 días 23 horas\\tcmpoggi@gmail.com\\nCarla Giorgina Pappalardo\\t7 días 21 horas\\tccpappalardo@gmail.com\\nJuan Carlos Benigar Zapata\\t7 días 1 hora\\tjuanbenigar2017@gmail.com\\nJOAQUIN LORENZO MANRESA\\t7 días\\tmanresa.joaquin@gmail.com\\nSebastián Emanuel Spuler\\t5 días\\tspuler.sebastian@gmail.com\\nLuciano Andres Macanelli\\t3 días 4 horas\\tlucianoamacanelli@gmail.com\\nDennis Nelson Fernandez\\t3 días 1 hora\\tfernandezdennis.ia@gmail.com\\nLuis Garnica\\t2 días 17 horas\\tgarnicaluis497@gmail.com\\nMario Sebastian Zampetti\\t2 días 7 horas\\tmszampetti@hotmail.com\\nLucas Ariel Barros\\t1 día 12 horas\\tlabarros75@gmail.com\\nYolanda Morales\\t1 día 6 horas\\tybmora@gmail.com\\nAndre Paola Rojas\\t1 día 2 horas\\taprojas81@gmail.com\\nCarolina Elizabeth Garces\\t3 horas 40 minutos\\tcarolaelizabeth1982@gmail.com\\nLorena Elizabeth Lepe\\t3 horas 28 minutos\\tlorena.e.lepe@gmaul.com\\nEduardo Miguel Priotti\\t49 segundos\\teduardo_priotti@hotmail.com\\n Titulo: Clase1 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/zPw_xqn_Ztk  \\n 1540 segundos de duracion \\n Hola  bienvenidos Esta es la primera parte de la clase número 1 del curso de Inteligencia artificial de ifes  Hola Bienvenidos a todos a la primera clase del curso de Inteligencia artificial vamos a hacer una primer clase de introducción teórica en algunos conceptos que son importantes para poder afrontar este curso y lo primero que tenemos aquí es una frase que de alguna manera representa lo que entiendo es el lugar que tiene hoy la Inteligencia artificial en principio aquí habla del futuro de todo es difícil hablar de futuro cuando esta este fenómeno de la Inteligencia artificial viene pasando hace un tiempo atrás y tiene un presente muy fuerte Lo que sucede es que hay que analizar esta palabra futuro en el contexto completo es decir futuro de todo y porque lo digo porque en realidad si bien ha venido utilizándose este fenómeno y se está utilizando hoy no aplica todo por eso es muy importante aquí la frase completa y creo yo y creen muchos expertos en la materia que en un futuro Esto va a aplicar un total de pensar que casi no va a haber ninguna disciplina que esté exenta de tener algún grado de participación de una ia por eso es importante rescatar esta frase y no perder la vista porque seguramente nos va a acompañar a lo largo de todo el curso y va a representar la idea de lo que yo quiero volcar en el mismo En qué se aplica la Inteligencia artificial hoy en día bien en muchos rubros vamos a empezar por desandar algunos en principio la salud Sí el análisis de imágenes ayuda a los médicos a poder tener más información para desarrollar un mejor diagnóstico también en gmail esto es muy común que nos ayude por un lado a detectar posibles correos de spam y también por otra parte y lo habrán notado ustedes que cuando escriben un mail seguramente cuando van desarrollando la frase aparecen a la derecha sugerencias de lo que entiende una Inteligencia artificial que puede ser aquello que ustedes tienen intenciones de escribir facilitándonos Y de alguna manera siendo más rápida la escritura lo que son las Apps que tenemos en los celulares eso también es muy común atrás de cada app o de la mayoría de las Apps hay una Inteligencia artificial que lo que hace esta tarde que esa app esté más vinculada a nuestra preferencias a nuestros gustos y podamos tener una mejor experiencia con el uso de la también con la domótica o lo que es las casas inteligentes es otra aplicación del Inteligencia artificial bueno obviamente ni hablar el tema de tesla el proyecto de tesla creo que todos lo conocen Que es los autos que tienen un manejo autonómico donde no hace falta que un conductor pueda desarrollar bueno la tarea normal de manejo del auto sino que lo hace automáticamente una Inteligencia artificial con conceptos de visión computacional bueno las preferencias esto ya creo que es una de las cosas que más claro tiene todo el mundo que justamente aplicaciones como la que vemos aquí como Netflix Otra como Spotify mismo YouTube Bueno lo que hacen es entender Cuáles son nuestros gustos Y de alguna manera sugerirnos la película la canción o el vídeo que entiendes inteligencia que queremos ver las aplicaciones comerciales Obviamente que también es importante para todo tipo de comercio entender los gustos de sus consumidores Y de alguna manera poder preparar los productos de la forma o en el lugar o con el tipo de experiencia que quiere tener el usuario a la hora de comprar todo lo que tiene que ver con la seguridad es Otro aspecto importante no solamente en el manejo de las cámaras como habitualmente vemos Que en todos lados tenemos ahora cámaras de seguridad sino también en el análisis que se puede hacer A través de lo que es a cámara detecta un análisis a través del cual hay una Inteligencia artificial que puede detectar algún comportamiento de alguna persona que la cámara detecta la seguridad de nuestra identidad personal es Otro aspecto importante creo que hoy más que nunca los problemas de ciberseguridad están presentes nos afectan a todos y justamente tener una ia que esté detrás de ese problema y pueda de alguna manera el comportamiento típico de un intruso es importante para nuestra protección todo lo que tiene que ver con los chat Bot esto que seguramente ustedes habrán visto como experiencia cuando entran a WhatsApp e intentan tener comunicación con cualquier tipo de comercio cualquier tipo de actividad Bueno ese chat Bot lo que trata de hacernos esta tarde orientarnos hacia lo que entiende según lo que nosotros Le vamos dando como información es el objetivo que estamos buscando bueno Y podemos hablar de muchos casos más marketing tendencias de Mercado educación agricultura videojuegos y un largo etcétera concretamente hoy se estima que para el 2030 más del 70% de las empresas estarán usando al menos una tecnología basada en iap esto no hace más que reflejar la frase que yo había hecho la apertura de esta clase donde decía que la ia es el futuro de todo bien Pero por qué ha crecido tanto la ia en los últimos años bueno Esto se debe a tres motivos dos de esos tres motivos están resumidos en esta imagen por un lado el Big Data que es el Big Data la cantidad enorme de disponibilidad de datos que existe hoy en la comunidad y justamente que lo acompañe en esta imagen el poder de almacenamiento si tuviésemos esa cantidad de datos pero no tuviésemos donde almacenarlos bueno No tendríamos hoy la disponibilidad que tenemos de estos dos recursos que está acompañado por un crecimiento terriblemente importante también en lo que es la velocidad de procesamiento o el desarrollo de los microprocesadores concretamente Aquí hay un ejemplo donde podemos ver lo que era un procesador O la velocidad o el tipo de cálculo que podía desarrollar un procesador hace unos años atrás y lo que puede hacer hoy un Core 7 que es una de las últimas tecnologías disponibles Por qué existen tantos datos hoy en día Por qué existe ese Big Data que hablábamos recién bien porque en realidad Más allá de toda la disponibilidad de datos que no puede ofrecer habitualmente un sistema de base de datos un Excel o cualquier otro medio de almacenamiento de información típico de los sistemas que usamos hasta hoy en día se suman otros elementos más que nos Proponen datos aún sin que nosotros nos podamos dar cuenta o seamos conscientes de ello en principio Más allá de la base de tocar hablamos recién un like un like Es un dato un tweet Sí aquí vemos una persona que dice que no sabe manejar bien el idioma Bueno eso es una información un chatbot como vimos recién también obviamente es una información un clic en un anuncio es una información una elección como vimos recién de una película o de un video también es información es dato todo eso es dato concretamente hoy se procesan dos coma cinco trillones de bytes pero aquí está un mensaje que está resaltado que es muy importante que según un estudio IBM este Big Data Está compuesto por un 100% de datos de los cuales el 90% de ese 100 sea generado solo en los últimos dos años fíjense el impacto del crecimiento de la información de esta manera se quiere ver informal que hablábamos recién que no es una estructura conocida como una base de datos o como una planilla de cálculo responde a otro tipo de forma de poder brindar información que cada uno de los usuarios cotidianamente bien esa cantidad de datos qué podemos hacer con esa cantidad de datos un montón de cosas pero acá lo más importante a tener presente que esa cantidad de datos será imposible que la procesara un ser humano pero gracias a Dios tenemos la Inteligencia artificial que justamente acompañar del poder de registración de como veíamos recién el poder de almacenamiento esa información y los microprocesadores que nos permiten acelerar todo lo que tenga que ver con procesar esa información podemos tener resultados que jamás podríamos haber tenido antes Sí por eso con el pasar de los datos ese crecimiento de información seguramente va a crecer si lo produjo en los últimos dos años por qué no seguirá siendo así Obviamente que va a seguir siendo así por lo tanto laia se va a afianzar y cada vez va a tener más lugar de nuevo como el mensaje que dijimos al principio en parte de nuestro futuro Qué puedo hacer con tantos datos bien por un lado tenemos el business sí Qué es este concepto Bueno lo que tiene de importante este concepto hoy es que me permite con toda la información que existe poder ver el que y el Cómo es decir qué está pasando hoy con esa información o mejor dicho que está pasando hoy con el resultado que yo tenga de procesar esa enorme cantidad de información pero el otro concepto Es sobre el cual vamos a trabajar nosotros Es sobre el business análisis que hay una diferencia no trabaja sobre el hoy sino sobre el mañana Es decir no me dice qué es lo que está sucediendo con el análisis de los datos presentes hoy en el presente sino que puede llegar a pasar mañana a través de lo que se llama una probabilidad una predicción sobre eso es lo que vamos a hablar bien ahora en este campo de la ia Cómo está compuesto o qué conceptos acompañan hoy o forman parte de este fenómeno en principio lo que podemos hacer es empezar por Definir la ia la ia es básicamente una combinación de algoritmos que lo único que tienen como objetivo tratar de emular el comportamiento de la mente humana en un sistema de computación dentro de eso existe un concepto que se llama Machine learning el Machine learning Es una rama de la ia que lo que hace es Buscar adquirir esa inteligencia a través de algoritmos matemáticos y también tiene a su vez un hermano así menor que es el Deep learning Cuando digo de menor mayor tiene que ver con la con la el tiempo digamos de maduración que tiene este concepto sí es decir el Deep learning de todos estos conceptos es el más novedoso y qué es lo que hace Busca el mismo objetivo emular la mente humana en un computador Pero esta vez hasta emulando la estructura mental de neuronas y relaciones entre neuronas que tenemos nosotros nuestro cerebro Pero hay un concepto muy importante que nosotros tenemos aquí y que venimos hablando que es el learning Qué es el Machine learning el Deep learning Por qué la palabra learning porque dentro de la guía existe un concepto de aprendizaje Qué tipo de aprendizaje un aprendizaje automático cuando yo desarrollo un programa yo le estoy dando a ese programa un conocimiento es decir que ese programa toma un aprendizaje pero depende exclusivamente de mí si yo no les doy las pautas es el programa pierde esa inteligencia lo concreto de la guía Es que la guía puede crecer en conocimiento sin que yo intervenga simplemente tomando información si yo le doy la posibilidad a esa guía de Machine learning o de learning de poder alimentarse de información ese y a va a crecer en conocimiento automáticamente de manera diferente como vimos recién Machine de un modo pero el concepto básicamente es el mismo pero no respondimos la pregunta aún Cómo aprenden esos algoritmos en realidad esos algoritmos aprenden a través de un proceso de entrenamiento que se basa en repeticiones a través del uso de esos datos es una definición muy concreta pero vamos un ejemplo más de la vida nuestra para poder tratar de entenderlo podemos suponer que nosotros queremos aprender a jugar al voley y no sabemos jugar el voley sí Entonces que lo que vamos a hacer Vamos a aprender a través de los datos que nos dé la persona que nos enseña todas las características que tiene el deporte sí como sacar Cómo dar un pase Cómo golpear Cómo rematar Cómo recibir etcétera etcétera sí Se supone que eso me va a dar a mí un nivel de expertise un nivel de conocimiento y yo voy a lograr ser un determinado nivel de jugador pero con el tiempo Yo que voy a hacer voy a ir entrenando cada vez más cada vez más y me voy a valer de la información que tengo más nueva información con lo cual todas mis técnicas de manejo del deporte van a ser mejores ergo voy a hacer un mejor jugador de voley Bueno este entrenamiento es lo que hace la guía y en la medida que más entrene mejor va a ser la ya porque más conocimiento va a tener mi conocimiento va a estar más Pulido y va a ser mejor y más efectivo y en la medida que yo tenga más datos es entrenamiento va a ser más válido ustedes piensen que si yo los pongo a ustedes a entrenar un deporte pero siempre la información que lo de la misma van a tener un determinado nivel de adquisición de lo que es la disciplina Pero si yo en la medida que ustedes van entrenando les voy nutriendo de más y mejor información ese entrenamiento va a llegar aún más lejos con la guía pasa Exactamente lo mismo vamos un ejemplo gráfico y vamos a suponer que aquí tenemos datos quítenle el nombre de viejos vamos a poner datos actuales para ser más más gráficos esos datos yo se los doy como dijimos recién a un algoritmo que está dentro de una computadora para que haga un entrenamiento con ese entrenamiento ese algoritmo va a lograr un modelo sí esto vamos a hablar de esta palabrita que nos va a acompañar a lo largo de todo el curso que es un modelo es un modelo de algoritmo que a través de ese entrenamiento logra el objetivo que yo quiero y cuál es el objetivo que yo quiero bien el objetivo que yo quiero es justamente que ese modelo me ayude a mí a hacer predicciones se acuerda que hace un rato habíamos hablado del concepto de predicciones que es mirar el futuro business análisis qué va a pasar mañana bien cómo pasa eso bueno porque lo que va a pasar a partir de ahora es que van a entrar nuevos datos y esos nuevos datos yo los voy a comparar con esas predicciones y voy a ver si esa predicción es real o no es decir como toda predicción puede tener un determinado grado de certeza entonces lo que voy a hacer es compararla Pero además de compararlo esos nuevos datos Se los voy a volver a dar junto con los datos anteriores a el training que yo tengo la computadora con lo cual Ese training como dijimos recién en el caso del voley con los datos que tenía antes Y con los nuevos datos me va a originar un nuevo modelo es decir yo voy a hacer un mejor jugador el algoritmo va a ser mejor y Por ende con ese nuevo modelo voy a establecer nuevas predicciones sí casos de ejemplos de análisis predictivos con Machine learning bien tenemos por ejemplo una previsión de la demanda de un consumidor o de los consumidores una necesidad de contratación o automáticamente detectar cuando hace falta encontrar una persona Cuál es el perfil el fraude bancario y los seguros poder estimar quién puede ser una persona que puede ser más proclive a generar un fraude el comportamiento del consumidor o venta Cruzada decir cuál puede ser el comportamiento típico de un consumidor ante un conjunto de opciones o de ofertas el mantenimiento predictivo esto puede aplicar a cualquier ámbito donde la mecánica incida el riesgo crediticio si voy a dar un préstamo Qué posibilidad hay de que esa persona a la cual le voy a dar el préstamo pueda llegar a ser un incumplido en el pago diagnóstico de enfermedades habíamos hablado hace un rato en el caso de las análisis de las imágenes en por ejemplo la detección del cáncer bueno o los tumores si son malignos o no Bueno es muy importante el lugar que tiene hoy la ia personalizar las recomendaciones de productos y servicios Netflix sí Spotify YouTube deportes mona igual bueno Esto es una película en realidad que habla de un fenómeno muy particular donde que sucedió es que un equipo de béisbol de Estados Unidos justamente pudo acceder a un algoritmo de Inteligencia artificial para poder tratar de diseminar O discriminar cuál era los jugadores ideales para poder lograr este un mejor equipo de hecho lo hicieron se hizo una película porque justamente eso fue exitoso y bueno un equipo que no tenía un gran presupuesto terminó siendo campeón gracias a ese tipo de estudios los procesos agrícolas es algo muy importante porque porque puedo analizar qué partes de de un campo por ejemplo pueden ser más o menos productivos y con eso poder estimar los espacios que son menos productivos deberían tener un tipo de fertilización por ejemplo diferente Que los otros la seguridad ante el crimen bueno esto tiene que ver con lo que hablábamos recién de las cámaras y no solamente la seguridad que tiene que ver con la visión Yo podría también predecir dónde hay más posibilidades que ocurra un crimen justamente en base a los datos bien concretamente así como dijimos de que posibilidades de implementar una guía Hay muchísimas y cada vez van a ver más por cada vez hay más datos fíjense este dato que curioso simplemente trayendo El ejemplo de un avión un avión tiene 6000 sensores esos 6000 sensores generan más de dos terabytes de datos por día hoy no se está analizando obviamente esa cantidad de información pero justamente se está tratando de evolucionar en esa realidad en esa disponibilidad enorme de información porque Cuanto más podría ser seguro el funcionamiento del avión y el ahorro en el mantenimiento para hacer el mantenimiento que sea preciso y no quedarse en un proceso exagerado respecto al rabia o que realmente no alcanza a lo que debería ser la seguridad de las cosas que pueden llegar a pasar en el funcionamiento de un avión bien con esto podemos tener una idea de Cuántas otras cosas Además del avión presenta no hay la posibilidad de disponer de tanta información y no se están analizando en el análisis predictivo existe un concepto central que es el modelo de Machine learning para la predicción que vimos recién pero está acompañado de dos componentes muy importantes a la izquierda tenemos el análisis de datos yo no puedo tener un buen Modelo Si no tengo previamente un buen análisis de datos qué es esto los datos pueden ser que estén dentro de una base de datos como dijimos antes entre una estructura formal conocida o puede ser que sean un conjunto de clics y un conjunto de clics un conjunto de likes o un conjunto de tweets que es otra cosa que también se usa no está formado para que yo pueda inmediatamente tomarlo y generar un modelo de Machine learning con eso lo que tengo que hacer es acomodar esa estructura depurarla ver que pueda estar de alguna manera estructurada para que yo Después pueda lograr un buen modelo de Machine learning si no tengo una buena Estructura de datos Jamás voy a tener un buen modelo y finalmente la visualización de datos que bueno lleva que el usuario pueda ver esas predicciones de un modo presentable y que lo pueda justamente realizar y entender cómo corresponde bien pero hablamos de análisis de datos esto es un tema muy importante y que nos va a empezar a acompañar en la primera parte de este curso Data running y análisis de datos Data Rally es algo así como una cirugía de datos Sí en realidad el concepto quiere transmitir de alguna manera eso Por lo tanto aquí hay algunas acciones que representan ese Data Ranger esa cirugía de datos sí que es por ejemplo el origen de datos el origen de datos de dónde vienen los datos de dónde pueden venir los datos aquí entre paréntesis aparece un concepto muy importante que se vuelve scrapping que también lo vamos a ver el web scrapping es como tomar datos de los sitios de internet de los sitios web que no están preparados no tienen una app para informarme esos datos yo puedo tomar ese tipo de información esto simplemente un ejemplo de que el origen de los datos muchas veces puede estar en algo que no tiene nada que ver con una estructura formal estructural que sí es la que aparece aquí en el siguiente ítem donde dice formato archivos aquí tenemos cuatro ejemplos csv xlsx Jason txt bueno los formatos pueden ser aquellos que ustedes quieran Pero lo importante es que nosotros tenemos que tenerlos en claro Cuál es el tipo de formato que voy a trabajar y justamente ese formato pueda ser reconocido de la manera que corresponde cuando yo intenté generar el modelo La Estructura de datos muchas veces hay que entender que La Estructura de datos no tiene que ver con el tipo de datos que nosotros queremos ver por eso aquí habla de datos categóricos en la mayoría de los casos de los algoritmos de Machine learning nosotros necesitamos que los datos sean numéricos y Nosotros sabemos que en muchos casos los datos nos van a ser numéricos Van a ser categóricos Qué quiere decir esto bueno por ejemplo que yo diga que una persona vive una determinada ciudad bien no me va a servir el nombre de la ciudad voy a tener que codificar esas ciudades y yo me voy a valer del número para poder hacer un algoritmo de Machine learning no del nombre de la ciudad bien y así como eso muchísimos ejemplos categóricos sí datos nulos e inválidos bien Qué tengo que hacer con ellos bueno identificarlos y reemplazarlos o eliminarlos pero yo sé que un dato que no me sirve para analizar como puede ser estas dos situaciones justamente es algo que tengo que tener presente porque de nuevo insisto sino el algoritmo no va a dar resultados buenos concatenar datos otra tarea de Data ragling que justamente es si tengo orígenes distintos de datos no solamente por el tipo de archivo sino por De dónde proviene la información la tarea de concatenar datos es una de ellas y luego una vez que tengo esos datos Y antes de generar este el algoritmo de Machine tengo que hablar de lo que es el análisis de datos ese analizar la información ya no la estructura sino la composición la importancia el contenido por eso uno de los conceptos aquí que vamos a ver tiene que ver con el peso y relevancia de los datos para lograr un algoritmo de Machine learning no todos los datos son Igualmente importantes las necesidades de escalar los datos bien dijimos que todos los datos tienen que ser numéricos bien otra cuestión que necesita el Machine learning es que no haya una gran disparidad entre el nivel de cifra que maneja un dato y el nivel de cifra que maneja otro por ejemplo si yo voy a poner que una persona está catalogada como de tipo 1 2 y 3 vamos a suponer por un tipo de cliente es un dato que me va a dar un dígito si después pongo el sueldo de esa persona me va a llevar un dato que tiene por lo menos cinco seis cifras más con lo cual Eso es malo para poder generar un buen algoritmo de magia grande Qué tengo que hacer Tengo que escalar los datos para que todos estén a pesar de Esa diferencia dentro de una misma escala la necesidad de mayor volumen de datos bueno Esto También es importante tener pocos datos no me va a llevar a tener un buen algoritmo de Machine learning tengo que tener una buena cantidad de datos no es infinito es una enorme cantidad de datos tampoco Es efectivo porque justamente a veces hay que ver hasta dónde es necesario que esos datos sean en gran cantidad para lograr un gran algoritmo de Machine ejemplo si tengo 100 datos probablemente no tengo un buen algoritmo si tengo un millón de datos quizás tenga uno muy bueno Pero quizás con 100.000 datos sea también bueno es decir el dato de 100 a 100.000 probablemente es un salto muy grande pero el de 100 mil un millón no sea tan grande y yo lo que esté haciendo allí es gastar tiempo computacional de más cuando con menos podría haber obtenido un algoritmo muy bueno también y el punto final Es la falta de etiquetas bueno Esto es un concepto que no lo vamos a abordar en profundidad ahora no es mi intención que lo entiendan perfectamente ahora ya en un ejemplo de vamos a hablar sobre Machine learning un rato lo van a entender bien pero es importante que a esta altura puedan conocer que los datos pueden tener o no etiquetas pero que en muchos casos es muy importante que lo tengan por el tanto el desarrollo del proceso para etiquetar aquellos datos que no tienen etiqueta bueno es una tarea típica del análisis de datos hasta aquí llegamos con esta primera parte en ella te hemos brindado conceptos básicos de la ia te invito a ver la segunda parte de esta primera clase  Titulo: Clase1 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/UXxeAYfkdkw  \\n 1358 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 1 del curso de Inteligencia artificial de ifes en ella vas a poder conocer las herramientas que vamos a usar en este curso empecemos  bien Vamos a continuar con esta clase número 1 con la segunda parte donde vamos a ver el tema de las herramientas que vamos a usar para este curso por eso respondiendo esta pregunta que está aquí la primera herramienta principal y que todos ustedes saben que es un requisito importante para empezar a ver este curso es python Obviamente que es importante que tengan conceptos básicos de python no profundos pero creo que con el correo del tiempo sí es algo bueno que vayan adquiriendo otro tipo de conocimientos más avanzados justamente cuando estemos en etapas de lo que tiene que ver con la implementación de lo que es un modelo en principio para poder desarrollar un modelo y poder probarlo y evaluarlo no va a hacer falta pero después seguramente para poder implementarlo sea bueno que ustedes puedan tener conocimientos un poquito más avanzados de python otra herramienta es el Google colapse el Google colap es este Bueno uno de los las aplicaciones de Google que más ha crecido en los últimos tiempos fundamentalmente en este ámbito de la guía y nos propone un trabajo una forma de trabajo un entorno de trabajo muy Dinámico donde intervienen otras aplicaciones también de Google como Google Drive y que representa un esquema de trabajo que se llama Notebook sí no es el único Notebook que existe en realidad en el contexto en la comunidad de ia hay otro muy conocido y anterior a cola que se llama Júpiter que también es muy bueno pero bueno evidentemente cola ha crecido mucho en preferencia y muchas de las cosas que se están desarrollando y se desarrollan en esta plataforma y además bueno tiene la posibilidad de articular con otros otras herramientas de Google Como por ejemplo Google Drive también ofrece ventajas en lo que tiene que ver la forma de acelerar procesos que son necesarios dentro de lo que es este trabajo así que no no vamos a adelantar mucho el tema pero hoy por hoy nosotros elegimos o vemos más conveniente el uso de cola vamos a dar otros elementos más adelante pero no para esta primer parte del curso otro de los temas es el numpai Es una herramienta que me sirve para desarrollar Bueno este cálculos matemáticos y que trabaja mucho con o de manera muy eficiente con el tema de las matrices el otro la otra herramienta que vamos a utilizar es pandas además de país vamos a usar cola otra herramienta que vamos a utilizar es Google cola Google colap es un espacio de trabajo que se llama Notebook en el cual bueno ha crecido mucho en preferencia en los últimos tiempos muchas de las cosas que vienen desarrolladas hoy vieran en esta plataforma no es la única hay otra plataforma muy conocida que se llama Júpiter pero la que vamos a usar para este curso es esta numpai es también otro aplicativo de la familia de python que va a ser utilizado en este curso fundamentalmente para el tema de manejo de matrices y operaciones Matemáticas luego también pandas pandas es un aplicativo también que se usa mucho en la familia de python en la familia de lo que tiene que ver la programación de ia con python y que tiene que ver con lo que son instrucciones o procedimientos que de alguna manera facilitan el manejo de las estructuras de datos básicamente de los datasets y finalmente Matt es una librería que sirve para desarrollar gráficos en dos dimensiones con estas cinco herramientas vamos a manejarnos en lo que tiene que ver el curso pero vamos a tratar de adentrarnos un poco a alguna característica de cada una de ellas en principio el Google cola Google cola es un producto de Google research como decíamos recién y permite a cualquier usuario ejecutar y escribir código de python y tiene una característica muy particular que es que muchas de las aplicaciones que habitualmente deben instalarse cuando se usan otro tipo de entornos de trabajo con Google cola no es necesario porque ya vienen preinstaladas bueno obviamente es muy interesante como dice aquí para cualquier espacio de aprendizaje automático análisis de datos y lo que vamos a hacer aquí Educación qué más Google colapse permite ejecutar código python de versión 3 en adelante sobre diferentes configuraciones del equipo Cuando utilice nuestro propio cpu pero acá hay un tema muy importante Google con app nos permite utilizar un concepto de gpu o tpu propio de colap es decir es un espacio de procesamiento adicional que lo que hace es potenciar el nuestro propio que tenemos con nuestro cpu a través de Google obviamente hay versiones gratuitas a versiones de pago pero es muy importante fundamentalmente Cuando alguien ya que procesar grandes volúmenes de datos finalmente Google colap es muy sencillo de utilizar hay dos formas de poder utilizarlo Obviamente que aquí lo más importante es contar con una cuenta de Gmail obviamente y se puede ingresar de dos maneras como decimos recién directamente a través del enlace que vemos aquí que estamos destacando aquí luego que accedo a esa URL me lo guía con una cuenta de Gmail y una vez allí lo que hago es ir al menú archivo y seleccionar la opción nuevo cuaderno acuérdense que lo que estamos nosotros trabajando como entorno de trabajo Se llama Notebook por eso Siempre vamos a hablar de cuaderno la otra forma es Ingresar a nuestro Google Drive una vez allí apretamos el botón nuevo luego desplegamos el menú más y finalmente accedemos a la opción colaboratorio que justamente me lleva a Google cola lo que de nuevo dará lugar a la creación de un nuevo cuaderno es decir un notebook bien Esto es una imagen de lo que es el entorno de Google colapse que aquí hago simplemente el famoso print Hola mundo fíjense que la orden la escribo aquí el resultado lo veo aquí debajo para ejecutar la orden tengo que hacer clic en Ese botón Play y a mi izquierda siempre y cuando yo quiera tengo todo lo que es la estructura de datos de almacenamiento que voy a tener gracias a Google estructura que corresponda Google no a mi unidad de disco Sí y otro Dato curioso fíjense que aquí a la derecha y arriba tengo el nivel de consumo de ram y el nivel de consumo de disco esto es importante porque bueno todos los procesos de este tipo son procesos pesados que de acuerdo a la cantidad de datos que manejen pueden tardar mucho y muchas veces puede ser que bueno llenen o tomen el 100% de la ram y del disco por eso es que les hablaba recién de la opción que tenemos de utilizar gpus o tpus que nos ofrece Google para que justamente no nos quedemos con un proceso bloqueado bueno demás está decir que lo que estamos viendo aquí una instrucción puede dar lugar a que escriba todas las instrucciones que yo quiera dentro de esa misma unidad o bien agregue unidades adicionales para ejecutar otras instrucciones por eso aquí arriba ustedes pueden ver la opción código para agregar más líneas de código o texto porque esto es otra característica muy importante del porque yo puedo ir insertando texto en el medio de las instrucciones y esto me va a permitir de alguna manera crear concretamente lo que llama un libro de trabajo donde voy poniendo los comentarios y el código que hace a ese comentario o al revés el código de programación y la explicación correspondiente Qué es numpy numpai cuyos nombres significa numérica el python es una biblioteca para el lenguaje de programación python que da soporte para crear vectores y matrices grandes de nivel n dimensión es decir multidimensionales sí con ello tenemos además no solamente una estructura disponible para poder procesar de manera eficiente y rápida sino también tengo una gran cantidad de funciones matemáticas para operar con esos datos de esas matrices aquí tengo tres imágenes de las estructuras típicas de Nampa Y sí de una estructura de una raíz de una dimensión de dos dimensiones de tres dimensiones y abajo fíjense que habla de shape es un concepto que vamos a utilizar mucho no solamente para matriz sino también para lo que son Data set que me dice justamente la forma la shape que tiene esa estructura esta estructura es una estructura potente que permite manejar matrices simples sí o unidimensionales obviamente aquí estamos dibujando matrices de hasta tres dimensiones porque sabemos que no podemos dibujar elementos que tengamos a tres dimensiones pero el concepto que ustedes tienen aquí que tener presente es que en un país me permite manejar matrices de hasta n dimensiones garantizándome cálculos rápidos y eficientes por sobre todo Qué ventajas tienen ampay bueno en principio ocupa menos espacio cualquier otra estructura y para ello vamos a hacer un ejemplo vamos a poner un código que tenemos aquí y vamos a poner dos casos en principio sin usar nampai y después usando Sí en principio fíjense que hasta la primer parte del código hasta el primer print que tenemos aquí tenemos justamente una lista de python es decir una matriz unidimensional Sí con órdenes de python Sí con estructuras de python Sí luego podemos usar lo mismo pero siempre dentro de python pero ahora usando en ampay sí lo que hacemos finalmente es poder establecer justamente el resultado de un modo y del otro fíjense cuánto espacio ocupa una lista de python sin usar numpy y una lista en este caso o una estructura matricial de una dimensión usando luego tenemos que otra característica y para eso también Vamos a recurrir a un código y vamos a usar la misma idea es decir usar una estructura de python sin un pay y otra Estructura de datos con nombre en el primer caso podemos ver que al imprimir el tiempo de recorrido de una lista fíjense que no se lleva 207.1392 etcétera etcétera segundos mientras que en el caso usa Nampa fíjense que es bastante fácil de estimar que tengo prácticamente un 25% del tiempo que obtuve sin usar bueno estas son dos de las características que hacen de que van a justificar de alguna manera que vamos a usar Nampa a lo largo de todo este curso qué expandas pandas cuyo nombre significa panel Data es una librería también de python especializada en la manipulación de análisis de datos sí concepto que ya vamos a empezar a desarrollar más adelante bajo el nombre de Data frame panda se ofrece estructuras de datos y operaciones para manipular tablas numéricas y series temporales es algo así con un ejemplo muy básico Como si fuese el Excel de python en realidad fíjense que acá hay dos cuestiones que podemos tratar de trazar un paralelismo entre Nampa y pandas nanpa y tenía matrices y funciones que servían para manejar esa matriz aquí lo mismo habla de estructuras de datos y funciones que también permiten manejar esas estructuras bien qué más pandas proporciona herramienta para analizar manipular datos de alto rendimiento si a través de los cuales yo voy a poder cargar modelar analizar manipular y preparar cualquier conjunto de datos Ese Conjunto de datos como dije recién se llama Data frame el Data frame tiene en principio la característica de una estructura de datos etiquetada de tipo bidimensional además tiene tres componentes principales datos índices y columnas y además se pueden especificar nombres para los índices y para las columnas vamos un ejemplo aquí comparando como recién hablamos Nampa y con pandas lo que tienen a la izquierda es una raíz de nampae lo que tienen a la derecha es una estructura tipo Data frame de pandas fíjense que la información es la misma pero está de desarrollada con una estructura totalmente diferente bien aquí tenemos por ejemplo una estructura de tipo Data frame donde voy a empezar a identificar cada uno de los componentes de Data frame fíjense que tengo en principio arriba marcado en rojo las los nombres de las columnas Sí luego lo que en cualquier Estructura de datos se llama habitualmente films Sí luego tengo etiquetas si yo quisiera para cada una de las filas tengo un número 101 2002 estas etiquetas por lo general son automáticas pero yo sí quiero puedo poner una etiqueta que no sea ese número Yo podría aquí fíjense que el primer dato es 101 regularmente empezaría de Cero y luego tengo los datos Sí en cualquier cruce de la columna y de la tengo el dato concretamente que puede ser de cualquier tipo A diferencia de Nampa y que tiene todo valor de un mismo tipo Qué puedo hacer con pandas bueno Muchas tareas que van a ser una de las primeras cosas que vamos a hacer en la clase número 2 pero la vamos a adelantando por insertar y eliminar filas o columnas Qué puedo hacer también reemplazar y depurar datos faltantes también Hablamos esto en la parte primera de esta clase número 1 el tema de reemplazar valores sea porque son nulos o tienen datos que no han sido bien cargados y depurar los mismos también en el caso de que estén faltantes luego puedo unir y agrupar datos que provengan de distintas fuentes de información y finalmente remodelar las estructuras de datos lo que habíamos hablado de alguna manera también hace un rato de que muchas veces los datos de un Data frame con los cuales Yo quiero generar un modelo No todos son buenos No todos son importantes no todos tienen peso o relevancia a la hora de Establecer un modelo que me dé una tendencia ustedes piensen que por ejemplo yo puedo Establecer un modelo en el cual predecir el comportamiento de un consumidor y que uno de los datos que tenga por ejemplo puede ser si hace deporte o no Sí bueno probablemente ese deporte no tenga mucho que ver con las características de si la persona consume o no consume determinado producto o por ahí sí sí esto es muy difícil de predecirlo pero a lo que quiero ir es que justamente en la remodelación de estructuras de datos tiene que ver justamente con cambiar los tipos de datos habíamos hablado de cambiar datos de tipo categóricos a numéricos o también poder eliminar información que entendemos no puede contribuir a nuestro modelo finalmente Qué es la librería de gráficos en dos dimensiones que mencionamos al principio sí es una biblioteca también de python que me ayuda a generar gráficos como esto que estamos viendo aquí y que es muy importante tener presente que la debo trabajar con estructuras matriciales como la que vemos aquí en el ejemplo aquí Un ejemplo muy sencillo que lo que hace es graficar esta línea que vemos a la derecha si yo tengo dos conjuntos o dos matrices unidimensionales la matriz a la matriz b y directamente lo que hago es pedirle que me explote que me exploté esos dos conjuntos de valores con una línea que tenga un color Blue que sea de ancho 3 y que bueno tenga una etiqueta que identifique que significa esa línea Bueno le pusimos nombre línea simplemente pero en realidad eso Debería ser un contenido un concepto que justamente voy a poner en escena luego bueno con peleate Legend digo Que aparezca la etiqueta así como aparece y con pellette directamente lo que hago es show que me lo muestra bien Aquí hay una cuestión muy particular que ya vamos a profundizar luego que es que estoy usando la librería Pipe plot de Matt bueno Esto es una característica que ya vamos a ver por qué la estoy usando en este caso tiene un conjunto de herramientas para gráficos dentro de ellas existe Qué es o Por qué es importante y por qué hablábamos recién de que en este gráfico uso Pilot fíjense que características tengo acá no tengo ninguna referencia acerca de qué tipo de dato es a Y qué tipo de datos ve no me refiero al tipo de dato de ciencias numérico alfanumérico sino concretamente a Qué significa a como concepto y Qué significa ve como concepto es decir que pilotte nos va a ayudar muchas veces cuando queremos hacer un gráfico rápido sin mucha información pero que queremos tener es la idea visual de esa fuente de datos Bueno voy a usar y finalmente muy importante en el presente algo que no lo redondeamos recién porque hablamos del concepto pero no de la herramienta los datos para hacer he trabajado con Maple tienen que responder a una estructura de la librería ahora vamos a responder un poco a esta pregunta hablamos de datos hablamos de datos Pero de dónde puedo obtener yodatos obviamente dijimos que yo podía obtenerlos de distintas Fuentes formales estructuradas menos estructuradas pero hay una realidad y es muy importante es que hay un número muy grande de fuentes de datos de Data sets para que yo pueda recurrir y poder entrenar o aprender todo lo que tenga que ver con ia qué quiero decir con esto seguramente después yo con el tiempo quiera desarrollar mi Inteligencia artificial para aplicarla en una en una empresa en una organización y tenga que contar con los datos originales de esa organización pero yo puedo en instancia de aprendizaje o hasta instancia de desarrollo del futuro modelo de esa empresa usar modelos de otra organización justamente a partir de la posibilidad de eso es que yo puedo empezar a adelantarme algunas cuestiones hasta tanto yo tenga la información que yo quiero y del modo que yo quiera bien Quiénes son los que me pueden brindar esa información en principio tengo organizaciones o sitios web que me pueden dar esa posibilidad uno de ellos y uno de los más importantes es un sitio web que lo que propone son competencias en las cuales Busca justamente que desarrolladores independientes puedan desarrollar modelos en base a una determinada Consigna y entre ellos lograr bueno la mejor propuesta posible con esto se han logrado hoy resultados maravillosos y modelos extraordinarios y justamente Lo bueno que tiene esto es que todo lo que se hace está disponible en este sitio y yo lo puedo visualizar uno de los recursos que tengo además de los modelos y las competencias por eso ven aquí en este dibujo el dibujo de la copita el icono de la copita también todos los conjuntos de Data set que yo quiera bajar y poder utilizarlos para mi modelo tensorflow bueno herramienta que no hemos nombrado hasta aquí porque va a formar parte de la segunda parte de este curso cuando veamos Deep learning en Deep learning es la herramienta o por una de las herramientas más importantes bueno además de poder darme información sobre también modelos también me da la posibilidad de obtener Data sets qué más esto es una plataforma muy reciente Comparado con obviamente atención que me da muchísimos recursos y está creciendo mucho en los últimos tiempos y es muy importante también tenerla presente porque justamente me da más variedad no solamente en tener el mismo tipo de información Flow por ahí apuntan algunos modelos y Face complementa con otros por eso es muy importante tenerla presente está creciendo mucho es justamente una herramienta que nos viene muy bien para todo lo que tenemos que desarrollar de ahora en más y otra forma es Buscar simplemente utilizando Google lo que puedo hacer es bueno escribir como yo puse Aquí Data set para análisis de datos y bueno van a aparecer un montón de sitios que no son estos tres famosos que nombre antes para poder tomar datasette y bajar porque porque puede pasar que el tipo de Data hacer que yo busco no lo voy a encontrar a ninguno de los otros tres sitios bueno puedo buscarlos por aquí y si no empezar a crearlos si no existen y esto también lo vamos a ver a lo largo del curso tengo que crearlos si yo quiero tener una supongamos una Inteligencia artificial que me permita a través de visión computacional diferenciar un caballo de un ser humano bueno tendré que agarrar fotos de caballos fotos de seres humanos y poder entrenar mi propia Inteligencia artificial porque quizás no tenga un Data el camino más largo todo lo que dije antes es aquello que está preparado para hacerme bueno tránsito un poco más más acelerado pero a veces eso no está y tengo que caer en este tipo de cuestiones esto nos va a pasar también lo vamos a ver en el curso porque tenemos que estar preparados para este tipo de circunstancias de este modo terminamos la clase número 1 Ahora sabes de qué se trata la ia y con Qué herramientas vamos a empezar a trabajar desde la próxima clase nos vemos en la segunda clase  Titulo: Clase2 (parte 1) del curso de Inteligencia Artificial \\n URL https://youtu.be/uIrEkVyrFT0  \\n 1853 segundos de duracion \\n Hola  bienvenidos Esta es la primera parte de la clase número 2 del curso de Inteligencia artificial de ifes en esta clase vamos a empezar a recorrer el camino del análisis de datos Así que empecemos    bueno Bienvenidos a la clase número 2 la primer parte de la clase número 2 de este curso de Inteligencia artificial y vamos a empezar concretamente con la parte de manejo de datos en esta primera parte de la clase vamos a ver básicamente aquello que tiene que ver con acceder a datos vimos que teníamos posibilidad acceder a todo tipo ssd txt xlsx de Excel vamos a ver todas esas clases y poder visualizarlos por primera vez desde Google colap reconocerlos y ver algunas cuestiones que tienen que ver con la estructura de esos datos Pero antes que todo eso antes de la clase en sí misma Vamos a ver algunas cuestiones que tienen que ver con cómo nos vamos a manejar con los archivos que vamos a utilizar en las clases y ya justamente viéndolo ahora vamos a conocer la mecánica para todo el resto de las clases lo primero que van a tener ustedes son archivos de este tipo están aquí de extensión IP y nb larga Estos son notebooks de Google colapse estos archivos van a estar obviamente en el campo virtual ustedes lo van a bajar y lo que sí van a tener que poner es en su propio Drive si yo estoy acá en mi unidad Drive logueado con mi usuario y tengo bueno una serie de archivos que son los que ustedes van a ir bajando de la plataforma Este es el primero que vamos a utilizar nosotros aquí y además van a tener también otros recursos que también van a estar a partir de la del campus virtual y lo van a ir guardando directamente en el disco por ejemplo los Data sets nosotros ahora vamos a usar en un rato más el Data set de la carpeta Titanic allí hay varios archivos bueno ya vamos a llegar a esto pero está la idea de que ustedes vayan quiero que vayan teniendo de cómo va a ser la dinámica del curso entonces una vez que hayan bajado este archivo de extensión está tensión larga y rara sí que tiene que ser tiene que ver con los notebooks de Google colap lo van a poner en el drive y una vez que está en el Drive hago un doble clic aquí y automáticamente voy a aparecer en Google colapse logueado con el usuario porque ya lo traigo desde Google Drive y con todo mi archivo fíjense vamos rápidamente aquí arriba Bueno ya se pasó un poquito la situación lo vamos a cerrar y vamos a abrir de nuevo y fíjense lo que sucede aquí cuando está cargando el archivo ven que dice que está conectando sí en este momento está conectando inicializando y ya está conectado Entonces ya lo puedo empezar a utilizar Sí ya estoy en Google colapse con mi archivo con mi usuario y con mi archivo el archivo es tal cual todo el que ustedes bajaron o deben bajar digamos de la plataforma virtual y aquí tengo con esta carpetita la posibilidad de ver La Estructura de datos que también yo les mencioné y les referencia en la segunda parte de la primer clase bueno esta estructura luego va a ir creciendo conforme vaya necesitando otro tipo de estructuras sí bien con un clic la dejamos para tratar de ver la mayor cantidad de pantalla posible y arrancamos con el primero de los temas que lo que dijimos recién el acceso al conjunto de datos Y a partir de ese acceso a conjunto de datos poder no solamente saber cómo accederlo sino también poder visualizar el contenido y las estructuras de esos conjuntos de datos pero lo Primero lo primero nosotros lo que tenemos que hacer antes que nada es importar las librerías que vamos a utilizar si las Ya vimos en la segunda parte de la primera clase nampai pandas matlock pero aquí agregamos la librería warnings si warnings es una librería que yo acá la estoy utilizando para tratar de evitar de que me aparezcan por pantalla mensajes de advertencia advertencias que pueden ser más o menos importantes en algunos casos me puede decir que un determinado Comando alguna método de instrucción en poco tiempo más va a ser reemplazado por otro que tenga presente ese tema me sugiere ir seguramente muchas veces un sitio a buscar información sobre eso en otros casos me dice que algo que por ahí este estoy utilizando tiene una versión mejor me sugiere Cuál es la versión bueno cosas de ese tipo habitualmente son mensajes que por ahí obstaculizan no sirven de mucho si yo estoy trabajando todos los días con esto es repetir una información que ya la sé Entonces por lo general lo que se hace es ponerle esto para que ignore esos mensajes de advertencia y luego la librería io que tiene que ver con un paso que vamos a hacer a continuación que es el de poder acceder a nuestro primer conjunto de datos para traerlo a nuestra plataforma en un formato de Data frame bien las librerías que están aquí las voy a usar ya no es necesario que estén desde entrada importadas tampoco es una cuestión de orden Lo mejor es que todo esté de entrada cosa que después ya tenga todas las librerías que necesito dentro de un archivo y tenga o pueda conformar luego más adelante una especie de archivo de requerimientos y no tenga todo diseminado a lo largo del Notebook Y eso puede hacer que haga que algo me olvide sí bien entonces todo esto está escrito pero no está ejecutado como lo ejecuto haciendo clic en el botón Play lo hacemos ahora fíjense que lo que hace es hacer un círculo de líneas de trazo luego de líneas sólida y finalmente cuando la orden se haya ejecutado obviamente va a aparecer el icono igual que al principio pero aquí al lado va a aparecer un tilde verde y el tiempo que tardó en ejecutar sus instrucciones evidentemente en este caso no tardó nada porque es muy rápido bien Esto es un bloque de instrucciones yo ahora voy a pasar al próximo bloque de instrucciones que este que está aquí con este bloque de instrucciones lo que pretendo hacer es traerme sí incorporar a este Notebook un archivo del exterior que está en el disco puede estar en una red o puede estar en cualquier otro lugar Sí pero no dentro del cola Entonces yo aquí lo que hago es from Google cola import files y upload files.ablo esto lo que va a hacer va a abrir una típica ventana digamos de selección de archivos y con eso voy a ir Buscando el archivo como busco habitualmente en cualquier aplicación a través del explorador de Windows hago clic aquí fíjense que se queda dando vuelta uno parece que dijera bueno no termina más no porque está esperando que yo haga clic aquí en este botón elegir archivos o puedo Cancelar por supuesto yo voy a venir aquí a elegir archivos y me voy a ir a la carpeta donde tengo mi Data set en este caso el que quiero abrir es el de Titanic o uno de los que tiene que ver con la carpeta Titanic concretamente Titanic 3.csb lo elijo como si quisiera linkear un archivo o adjuntar un archivo a un mail o sea como se hace cualquier aplicación normal le doy a abrir y automáticamente ven que me marca Aquí ya está incorporando este archivo al Notebook 100% hecho fíjense que ahora se detuvo este icono y en 56 segundos esta hora se llevó acá obviamente 52 segundos que tienen que ver con con el tiempo que yo tardé en elegir el archivo sí Bueno fíjense lo que yo voy a hacer aquí vengo a la carpeta archivos y fíjense que lo tengo dentro del cola Es decir me lo traje desde la carpeta donde estaba hasta el cola sí bien Cierro la carpeta simplemente le voy a mostrar eso y pasamos a la siguiente instrucción la siguiente instrucción es abrir el archivo ya dentro del cola lo que hice fue traermelo al cola lo que voy a hacer ahora es abrir el archivo cómo lo abro lo hablo con el método read csb Por qué Porque es un archivo de extensión csv de pandas sí Recuerden que pandas era la librería que me servía justamente para ejecutar acciones con un Data frame sí bien en este caso es un conjunto de datos aún sí lo que voy a hacer Voy a tomar el archivo titanicsb de carga recién sí a través del cual lo cargué recién no me lo traje alcohol y justamente con el método bites yo de la librería io librería que se acuerdan que estaba aquí arriba que referenciamos aquí arriba vuelvo Aquí bueno con eso voy a colocar todo ese conjunto de datos que está dentro de Titanic 3sb dentro de una variable de datos Y una vez que tenga eso en la variable de datos voy a hacer un dato Hell Qué es Head me muestra como dice el texto aquí arriba las cinco primeras líneas renglones sí de ese conjunto de datos eso va a ser siempre y cuando yo ponga los paréntesis vacíos en tanto en cuanto yo ponga otro número distinto me va a mostrar la cantidad de líneas que yo ponga dentro de esos paréntesis bueno ejecuto esta orden y veo que me va a aparecer bien siempre 5 empezando desde cero aquí está un poco la estructura del Data frame que referenciamos en la primera clase cuando hablábamos de las etiquetas tenemos etiquetas para cada una de las columnas Sí y etiquetas para cada uno de los renglones que le vamos a llamar o empezar a llamar observaciones y Aquí vamos a el contenido porque no hablamos atrás sabemos que se llama Titanic el archivo pero no sabemos bien de qué se trata en realidad lo que tiene este archivo son todos los registros de todas las personas que viajaban en el Titanic con información general de cada uno de ellos y al final con una referencia de si esa persona sobrevivió o no sobrevivió el accidente Por ende trayendo a colación este concepto de observación cada registro se llama una observación porque se entiende que todos los datos lo que hacen es traerme una muestra de aquello que yo quería investigar en este caso lo que vamos a investigar es esta cuestión de una persona si sobrevivió o no sobrevivió con lo cual cada persona es una observación porque es algo que yo observo justamente de una realidad que quiero analizar y eso me viene en forma de dato Por eso cada fila se va a llamar observación y cada columna También es importante que vayamos identificando a la forma de llamar a las cosas que vamos a utilizar aquí en ia se llaman características Sí entonces columna característica fila observación bien tengo entonces aquí en este archivo Titanic los cinco primeros registros porque porque justamente los paréntesis No utilice ningún número y por default justamente me muestra los primeros cinco si yo quisiera alterar eso podría utilizar como dice aquí datos G y cualquier número en este caso use el 15 entre paréntesis si ejecuto esta instrucción voy a ver que el resultado es similar al de recientemente con más filas 14 fíjense que tengo una forma de escroliar Dentro de este cuadro y a su vez una forma de explorar en el Notebook O sea que ojo porque quizás a veces yo voy a querer ir hasta abajo con esta barra y voy a tener que desplazarme a lo largo porque si no no va a bajar porque justamente esta barra se mueve dentro del cuadro donde yo voy a mostrar los datos el cuadro no va a ser tan grande como datos tenga va a tener un tamaño determinado fíjense que acá inclusive yo tengo 14 datos pero en que no me entran los 14 porque si no no existiría esta barra de navegación entran solamente los primeros 14 sí es decir hasta el 13 bien del mismo modo también se pueden visualizar las últimas observaciones y no solamente las cinco primeras con el método Tail en lugar de Head es decir que pongo datos el nombre del Data frame.tail y ejecutó y voy a ver las cinco últimas observaciones de más esta decir que si yo dentro de Tail pongo un número x me va a mostrar esa cantidad de observaciones y no las cinco que aparecen por bien con esto tenemos todo lo que tiene que ver con la incorporación al cola de un archivo que está en el disco la apertura de ese archivo y la visualización de sus observaciones sí bien Ahora vamos a ver cómo abrir un archivo de tipo txt y ya que vamos a aprender algo fíjense que no hay nada acá así para ejecutar pero acá hay un texto que me dice una celda oculta bueno esto sirve muchas veces porque yo justamente puedo tratar en algunos casos de contraer El notebook cerrar aquellas instrucciones que no me interesa ver bien Cuál es el objetivo ahora vamos a ver el mismo archivo que recién pero resulta que ese archivo no tiene una sola versión sino que tiene dos versiones en este caso la txt cómo lo hago Bueno voy a hacer fíjense acá voy a juntar las dos instrucciones que antes puse en dos cuadros en una sola esto quiere decir evaluar las claras de que yo puedo juntar tantas líneas de código o separarlas tanto como a mí se me dé la gana voy hacia atrás a ver si recordamos esa cuestión sí tengo la ploas esto no me hace falta repetirlo por un Google porque ya está abierto Sí ya está importado Perdón la librería files y luego el Ruiz csv bueno todo eso y el Head por supuesto todo eso lo tengo en un mismo cuadro de instrucciones Ahora sí aploas datos t le voy a llamar en este caso al Data frame para que no se me mezcle con el otro simplemente y el gel bien Primero lo primero ejecutó esto y voy a ir a buscar a través del botón elegir archivos el archivo concretamente Aquí está Titanic 3 txt le doy a abrir y lo abre fíjense 100% hecho me fijo acá Ven ahora me apareció Titanic 3sb Titanic test txt si en algún momento algo aquí no aparece voy aquí a actualizar y automáticamente lo hace acá no tuve problema apareció inmediatamente bien Pero además de incorporar el archivo traerme el archivo al colap automáticamente le dije que me lo abra Cómo hice para que me lo abra con el mismo ritzsb que utilice antes sí O sea aquí por más que el archivo de tipo txt uso el mismo método vamos a ver que no siempre va a ser así para otros tipos de archivos y luego el Head bueno el resultado es el mismo solamente que en lugar de obtener este resultado habiéndolo traído desde un archivo de tipo csv como pasó aquí lo hice desde un archivo de tipo txt como está aquí sí bien seguimos con otro tipo de acciones ahora vamos a ver un archivo ahora vamos a ver un archivo de tipo txt pero sin header Qué quiere decir sin header que no viene referencia a las etiquetas de las columnas es decir las características que ya dijimos que la forma de hablar de características y observaciones bien qué voy a hacer de nuevo una plus para abrir el archivo pero como ese archivo no va a tener header lo que tengo que hacer es crear un array un array es un arreglo donde se arregló lo creo con paréntesis Y corchetes sí y cada uno de los valores separados por coma Cómo creo una raíz y aquí aparece en un país a través de la librería np acuérdense que np si me voy arriba de todo es la forma o es el alias que utilice para Sí entonces estoy creando una raíz que después voy a utilizar para darle nombres a ese txt que no tiene nombres en sus columnas y ese array lo creo con NPR y lo pongo en una variable nombres o como se me dé la gana llamar sí bien qué es lo que voy a hacer ahora voy a crear otro Data frame con otro nombre esto Obviamente que lo hago para no pisar uno con el otro puedo usar el mismo nombre todas las veces que quieras sabiendo que voy a perder el anterior no vuelvo a abrir con ccb porque es un archivo de tipo txt Pero además de utilizar la orden que ya venía utilizando cuando abrí Titanic 3 punto txt me voy acá arriba lo recuerdo sí esto que está acá le voy a tener que agregar alguna referencia dado que como no tiene header yo se lo tengo que agregar entonces lo que voy a poner acá es girar no tiene header y que los nombres para exigir que no existe y que necesito Eso lo voy a sacar de la variable nombres es los names los voy a sacar de la variable de nombres variable nombre que la que está aquí y Acabo de crear a través de nampai array sí bien luego a un Head y no hay nada nuevo respecto de lo que tiene que ver con visualizar el conjunto de datos ejecuto Aquí como siempre me lleva a elegir los archivos ahora voy a ir a buscar este Titanic 3 sin header Lo abro lo traigo y fíjense que digo el mismo resultado Pero bueno el proceso fue otro porque este archivo no tenía textos en sus columnas o sea no tenía sus características con nombres y se las agregué justamente a través de una lista que cree con este raid a la vista Obviamente el resultado es el mismo bien seguimos ahora voy a intentar abrir un archivo pero en este caso de tipo Excel Sí y aquí sí va a cambiar algo importante no tanto en la forma sigo siendo a blog sigo abriendo el Data frame a través de pd y sigo mostrando ese contenido pero ya no lo hago con Rift SB sino con RIP Excel Sí entonces y voy a buscar el archivo Titanic 3xls bien ya no importa y hago y el resultado es el mismo en realidad siempre la imagen final es similar lo que estoy tratando de mostrarles justamente a través de esta clase es Cómo abrir las diferentes formas de archivo Y qué cosas tengo que hacer en algún caso u otro con alguna variante como en el caso de el archivo que no tenía header próximo tema a considerar abrir un archivo csv desde una URL esto que quiere decir ya no me voy a traer con el upload el archivo aquí a la estructura de carpetas sino que directamente lo voy a vincular al Data frame que voy a crear con una información que está en una URL Aquí vemos por ejemplo una información referida a transfusiones sanguíneas entonces lo que hago es en lugar de poner aquí el nombre del archivo como hacía antes y habiendo hecho previo a la bload no y aquí habló a directamente pongo el nombre completo de la URL con el nombre del archivo que quiero traerme y con el mismo read csv de pandas lo pongo en una variable que en este caso le he puesto datos y hacemos un Head para ver Su contenido vamos a ejecutarlo y vemos el contenido sí un Data frame parecido al que teníamos recién Pero obviamente con otra información en este caso de transfusiones de sangre ahora vamos a hacer un ciclo inverso es decir en lugar de traernos información ya sea traerlas alcohol directamente o traerla vía una URL directamente al Data frame voy a hacer el proceso inverso Es decir me voy a llevar información que tengo aquí en el colap en algún Data frame a un archivo externo en este caso voy a usar tres formatos en un caso csv en otro caso Excel tiene el tercero Jason un formato que no hemos usado hasta ahora para traer información pero justamente lo vamos a hacer ahora para usar el sentido contrario cómo lo hago lo hago con el nombre del Data set y Data frame Perdón tu csv tu Excel o tu Jason sí Y obviamente después el nombre que yo le quiero poner a este archivo que va a ser un archivo nuevo no en este caso como lo tomo del datos u que es el que usé aquí para traer la información de las transfusiones lo voy a Le voy a poner el nombre a cada uno de los archivos Transfusión convertido a y el archivo que corresponde bien ejecutamos eso ahora vamos a ir ahora a la carpeta que nos abre el explorador de cola y acá tengo Entonces los tres archivos que acabo de pasar de Data frame de nuevo a formato archivo obviamente esto no va a parar al explorador de Windows si yo lo quiero pasar al explorador de Windows hago clic aquí y voy a la opción Descargar y ahí lo descargo donde yo tenga ganas de hacerlo bien qué más también puedo acceder a un csv de una URL como en este caso que usamos acá pero de un gift Hub Si todos saben lo que es el gift Card es un software muy conocido pero el git happ tiene una opción raw que me permite visualizar el contenido de un archivo de datos sí no para hacerle un Download sino directamente para traerme lo desde la propia URL Así que cuando yo hago un raw con github automáticamente lo que hago es copiar la URL que me arroja ese proceso de raw la pego acá como hice antes con la opción de la URL como lo habíamos hecho aquí con esta URL bien lo hago aquí pero con la URL que me arroja haber hecho un raw sobre un recurso de datos de un gigante y después hago es decir que el proceso es Exactamente igual que el anterior la única diferencia que aquí ya estaba en la URL el archivo y aquí yo lo promoví de alguna manera a través de la opción rap de kitka lo concreto Es que yo ejecuto esto y busqué justamente un github que tenía ya el Titanic Sí para no cambiarnos tanto de de forma de Estructura de datos y de contenido así que bueno Obviamente el resultado es el mismo bien vamos a pasar ahora a ver otro tema que es el de ver los nombres de las columnas de un datasette Por qué digo esto porque uno dice pero a ver aquí yo veo los nombres de las columnas sí esto en realidad es posible porque aquí hay una casualidad que es que en el ancho del Data frame no se supera el ancho de la ventana de El libro de trabajo sí del Notebook Pero eso no siempre va a pasar muchas veces vamos a tener tantas características que no van a entrar en una sola vista y voy a tener que trolear hacia la izquierda y hacia la derecha con lo cual si yo quiero ver todos los nombres de cada una de las características cosa que no voy a ver en una simple visión como esta lo que voy a tener que hacer es recurrir a values de Columbus de El Data frame que queda en este caso datas datos r Perdón que es el último que hice justamente aquí a partir de haber trabajado con el raw del github del Titanic sí Entonces ejecuto esto y voy a ver lo que ya teníamos aquí array me devuelve una raíz pero con todos los nombres de cada una de las características de El Data frame Sí el primero es el ID del pasajero y el último 100 barco o no sí que es lo que tenemos aquí el primero del ID y el último de 100 barconos sí Recuerden que este en muchos casos hubo mucha gente que compró el ticket haciendo poca historia pero no subió al barco bien luego lo que tenemos que ver también como una se acuerdan que yo les había dicho en la en la clase pasada hablada del shape cuando hablábamos de nampai sí bien Le dije que lo íbamos a usar también en el Data frame bien acá está el shape sirve para ver la estructura de Cuántas observaciones Y cuántas características tiene un datas Sí entonces pongo datos r punto shape ejecuto y muestra el número de 12 columnas 12 características y 891 observaciones 891 registro bueno estas cosas son muy importantes para empezar a tratar de descubrir todo lo que tiene que ver con el conjunto de datos que me he traído Por qué Y bueno porque porque yo aquí lo puedo como dije recién verlo en un golpe de vista Pero muchas veces no alcanza con eso Por ende Por ende Perdón esta información es muy importante obtenerla de este modo y hay una forma de visualizar ya no la estructura sino las características de tipo numérico del contenido del Data set que es justamente a través del método Yo tengo un montón de valores o de parámetros que muchas veces son necesarios para muchos de los análisis que tienen que ver con un conjunto de datos concretamente con discrite fíjense que toma todos y cada uno de los campos o de las características de tipo numérica fíjense ahora tengo 6 y tiene 12 Sí porque porque hay un montón de datos por ejemplo embarcado sí o cabina nombre por supuesto que no son numéricos Entonces eso lo deja de lado solamente toma los seis que son numéricos y Calcula por cada uno de ellos En esta suerte de matriz la cantidad el promedio la desviación estándar concepto muy importante En estadística el mínimo el máximo y los percentiles Qué quiere decir los percentiles es otra medida de estadística también muy importante donde se calcula por un lado todos los valores es decir cuántos valores están por debajo del 25% de Las observaciones Cuántos están por debajo del 50 y cuántos están por debajo del 75 todos aquellos que están debajo del 50 sí son reconocidos como ese 50 En referencia como la mediana es decir el para sentirse en cuenta es la mediana Sí con lo cual en este valor me va a dar todos los elementos a la cantidad de elementos que están por debajo de la mediana sí no es lo mismo que el promedio la mediana es el valor medio que no siempre es el promedio sí bien entonces aquí tengo ven para el pasajero para los sobrevivientes para la clase para la edad bueno para todos los otros la tarifa todos los otros elementos que tiene este Data set bien tengo la cantidad el promedio la dirección estándar el mínimo el máximo etcétera etcétera sí fíjense acá un tema importante que ya lo vamos a ver en profundidad que miren Cuál es el mínimo de la ID del pasajero uno y el máximo 891 sí fíjense En el caso de la edad el mínimo es 0,42 obviamente esto este tendrá que ver con algún recién nacido y 80 esto lo que yo le comentaba hoy respecto de que muchas veces vamos a tener que trabajar con estos valores porque no puede haber un nivel de dispersión tan grande entre un valor con dos cifras y con tres cifras Sí eso es a lo que ya vamos a trabajar más adelante porque justamente puede afectar a la consecución de un buen modelo bien y lo último con el método de types podemos ver el tipo de dato que está representado en cada uno de las características sí esto es otro tema también muy muy importante porque Más allá de que sean todos numéricos no todos los numéricos son iguales fíjense que acá tengo un in 64 y un float 64 sí es decir valores enteros y valores con decimales Sí y otros que son de tipo obviamente no corresponden a valores de tipo numérico Bueno qué quiero decir con esto y esto después lo vamos a profundizar un poquitito más con el ayuda de gráficos además de descubrir a simple vista el contenido de un conjunto de datos es muy importante reconocer determinadas características específicas que después van a ser importantes para determinar el modelo hasta aquí llegamos con esta primera parte ahora ya sabes Cómo acceder a un conjunto de datos y ver sus características te invito a seguir en la parte número 2  Titulo: Clase2 (parte 2) del curso de Inteligencia Artificial \\n URL https://youtu.be/qL_8Un-C2YI  \\n 1603 segundos de duracion \\n Hola  bienvenidos Esta es la segunda parte de la clase número 2 del curso de Inteligencia artificial de ifes ahora que pudiste acceder y visualizar el contenido de un conjunto de datos vamos a ver qué cambios podemos hacer con esos datos empecemos   bien Vamos con la segunda parte de la clase número 2 para arrancar desde el principio y que no perdamos de vista cuál es el procedimiento arranco desde el Drive que cada uno de nosotros tenemos ahora se supone que habremos bajado el clase 2-2 punto ipy nb con lo cual teniéndolo en mi unidad voy a hacerle doble click y como ustedes ya saben voy a estar abriendo colap ya lo tengo abierto porque no se re nunca este otro archivo que lo podría haber cerrado tranquilamente simplemente lo que estoy haciendo ahora es abrir este nuevo archivo con un doble clic de nuevo como otro recurso de El Cola y allí vamos a empezar a trabajar con el tema de hoy que es limpieza de datos es decir ya vimos como en la primer parte de la clase de hoy vimos Cómo acceder al dato a diversas formas de tipos de datos Sí y miramos y observamos digamos con un golpe de vista elegimos Su contenido y vimos algunos indicadores que nos hablaban justamente de esa estructura ahora lo que vamos a ver es qué dato tengo adentro Mejor dicho qué calidad de datos que tengo dentro porque dije antes si no tengo una buena estructura y no tengo una buena calidad de datos jamás tendré un buen modelo bien entonces tema de la clase limpieza de datos y un grupo un conjunto de acciones que tienen que ver con esa idea lo primero que hago como antes importar las librerías que lo haya hecho en este otro archivo no quiere decir que impacten en este segundo O sea que los tengo que volver a hacer aquí este salvo Que obviamente todo esto que escribe acá lo hubiese escrito en el otro archivo pero como tengo dos notebooks separados tengo que volver a importar las mismas librerías que importe cuando arranque con el Notebook clase 2-1 bien ya está hecho lo que voy a hacer ahora voy a volver a importar el mismo archivo de la primera parte es decir el Titanic 3.csb vamos a eso aquí está Bueno ahí lo está importando otra cosa que tienen que tener presente yo tengo aquí La Estructura de datos si no tiene nada que ver con La Estructura de datos de este otro recurso es muy importante que tengan presente son dos archivos totalmente independientes Por más que esté en la misma sesión No yo acá tengo una misma sesión de trabajo porque estoy logueado con mi usuario son dos archivos totalmente independientes bien ya importó Titanic 3.cv Bueno le hago un Head lo abro perdón y le hago un Head como ya hasta ahora son los mismos pasos que hicimos en el clase 2-1 bien bajamos Ahora sí lo primero que tenemos aquí como novedad verificar datos nulos en un Data set Cómo hago para edificar si hay nulos que ya estoy viendo aquí con un simple golpe de vista así porque todos los Nan Nan significan valores nulos decir es un dato que no está bien Esto es muy importante porque obviamente ese va a servir para hacer ningún tipo de estimación ni predicción ni algoritmo bien aquí lo que hago es usar de la librería pandas el método ignore y dentro el la característica que de su analizar como vi que en Body hay varios nans ya insisto con un golpe de vista voy a apuntar primero a ese y con esto lo que hace es listarme cada una de las observaciones y al lado un valor que dice true o false dependiendo de que dependiendo de si strú verdadero que es nulo o false si es falso que es nulo fíjense subo un poquito acá Aquí entonces tengo los primeros son true es decir son nulos y después Falls este tiene valor con lo cual es falso que es nulo Es decir no es nulo bien puedo hacer lo mismo pero expresando lo del modo inverso en lugar de pregunta de pobreza por not null surrealismo campo y obviamente los valores van a ser exactamente lo mismo pero al revés es decir no tenía ahora voy a tener Falls sí lo ejecutamos para verlo Pero obviamente la realidad va a hacer esa Obviamente que aquí yo podría poner más de un campo puse una modo de prueba pero lo puedo hacer con tantos Campos como yo quiera expresándolo como buscar nulos o como buscando no nulos bien Ahora yo sé que tengo nulos pero qué pasa aquí me muestra estos tres puntos suspensivos porque porque me muestran las primeras cinco apariciones las primeras cinco observaciones perdón de trata ser y las últimas cinco pero me muestra todo el contenido con lo cual yo no puedo saber cuántos false not null o cuántos null existen o bien lo que voy a hacer es Data snool zoom me va a dar la cantidad de nulos que hay por cada observación Sí perdón por cada característica entonces aquí en el caso de Age fíjense que hay 263 observaciones que tienen valores nulos en el caso de la cabina es una de las más significativas junto con Body sí 1014 1188 boa también y el destino también sí es decir que así como hay un montón de características que tienen todos los valores así que no tienen ninguno nulo como clase nombre sexo bien esto porque es importante porque yo voy a poder ver justamente más adelante qué voy a hacer con esos elementos si los voy a eliminar o los voy a reemplazar bueno así como teníamos aquí arriba la posibilidad de analizar los nulos y los no nulos también tengo la posibilidad de contar los nulos y contar los obviamente no así que pongo Data punto zoom yo tengo justamente el valor inverso Sí si en clase había 0 Acá hay 1309 porque justamente 1.309 son la cantidad de observaciones que tiene este bien luego lo que puedo hacer también es contar Cuántos datos nulos Y no nulos hay en una sola característica sí es decir en lugar de contarlos en todos acá los cuentos solamente en Body como rata Body punto fíjense que cuando referencia un una característica del Data set lo hago con el nombre el darset dato Data perdón con las llaves y este el nombre de la característica entre comillas fíjense que aquí arriba dice lo mismo para el Lo mismo para el snool y acá de nuevo cuando referencia una característica lo hago con esta forma y tengo de nuevo ver cuántos nulos hay para Body y cuántos nódulos hay para hoy bien Qué más voy a ver ahora como opciones para seguir depurando nuestro datasette voy a borrar todas las observaciones que tengan valores na es decir en este caso lo que voy a buscar es si hay observaciones que tengan todos sus valores enea y encuentro en esta suerte de casualidad justamente una fila la fila 1309 tiene todos sus valores en edad con lo cual lo que vamos a hacer la vamos a eliminar como con el método Drop na justamente dice Drop eliminar na y lo que tengo que poner entre paréntesis es axis 0 porque puedo borrar filas o puedo borrar columnas que tengan todos con India sí en este caso axis 0 quiere decir que voy a borrar aquella fila que tenga todos sus valores con nena y How que quiere decir en este caso all que quiero decirle con esto que quiero que borre aquellas filas que tengan todos hoy todos sus observaciones con NBA bien lo ejecutamos y me tiene que quedar 13009 filas sí que tienen justamente este todos sus datos completos Perdón aquí cometí un error no es la fila 1309 la fila 1.310 solamente que al numerarse se llama 1309 por eso la última es la número 138 y son 1309 filas sí lo que estaba aquí como fila que tenía todos sus valores en Nan era la fila número 1309 bien ya quedó entonces depurado este datasette y bueno Qué vamos a hacer ahora en lugar de borrar todas las observaciones que tengan todas características con na voy a borrar Las observaciones que tengan al menos una de esas características con n a está bien cómo lo hago bien lo voy a hacer también con Drop na axis 0 Y en el House en lugar de ponerle all le pongo n es decir alguna característica fíjense que aquí estoy haciendo una cuestión que es interesante que no voy a usar el mismo Data frame sí entonces voy a pasar este Data a Data 2 y voy a usar Data 2 porque porque justamente al ejecutar esta instrucción lo que me va a dar como resultado es que me borró absolutamente Todas Las observaciones porque porque todas las asociaciones tienen por lo menos un na Entonces como le puse como colisión que al menos tenga un nna y todos tienen al menos un na Entonces me quedo totalmente vacío el Data frame Sí bueno Esto es para que lo tengamos presente hasta donde el impacto puede llegar a ser este tan terrible si no tomo la precaución de pasar justamente o hacer una copia o trabajar sobre una copia de la reserva original bien qué más podemos hacer reemplazar los valores faltantes con film a filas relleno Sí y filial lo que busca es reemplazar los n a con algo que yo le indique sí Entonces esto lo puedo hacer del siguiente modo en principio yo puedo poner Data punto film sub 0 como dice aquí esta acción no cambia la verdad del Data Sí pero puedo en una primera vista ver Buddy que tenía muchos Nan Fueron reemplazados por cero cero cero cero aquí fíjense que hay otro aquí hay otro aquí hay otro en el caso de la cabina fíjense aquí también 0 aquí también es decir lo que le puse Aquí es Perdón aquí abajo que me rellene todos los CNA con el valor que le pongo entre paréntesis es decir con cero bien Esto insisto no cambia el Data frame sí Solamente lo deja a modo visual para que pueda darle un primer golpe de vista de resultado bien aquí lo que voy a hacer es algo parecido pero en lugar de decirle que me rellene los na con un cero que lo haga con un valor cuyo texto sea desconocido Entonces le pone desconocido a todo aquello que hoy le puse cero sí fíjense ven desconocido desconocido desconocido en este caso el agregarle la palabra desconocido fíjense lo que me provocó me provocó que por primera vez este dataset sea más ancho que el espacio que le destina el Notebook por eso me aparece por primera vez aquí la barra de desplazamiento entonces Porque bueno porque yo le puse en el body por ejemplo la desconocido Y antes tenía un cero y ahora tengo una palabra justamente la palabra desconocido eso hizo que se ensanche la estructura que estoy visualizando Bueno un pequeño dato para que lo tengan presente pero lo concreto y lo más importante aquí es que así como antes le dije rellename con cero a todos los na y lo hizo ahora le dije reséname con la palabra desconocido todos los na y es lo que hizo bien seguimos vamos a hacer una opción más variada es decir no vamos a reemplazar todo sna por un valor numérico el mismo o para un texto el mismo lo vamos a hacer ahora por cada característica voy a tomar características por separado fíjense que acá lo que estoy creando nuevamente es un Data set en virtud de otro y lo que hago aquí es en el caso de Body decirle que me rellene con ceros en el caso de Home desk decirle que me rellene los Nan con desconocido y en el caso de la edad le voy a decir que me lo rellene pero le voy a agregar un cálculo matemático es decir no le voy a poner un valor fijo y esto se usa mucho cuando hay valores faltantes es decir si los valores faltantes no son tantos muchas veces lo que conviene es reemplazar esos valores faltantes por el promedio va a ser lo que va a estar más cerca de la realidad insisto si no son muchos los valores faltantes si los valores faltantes son más de la mitad y bueno allí vamos a estar en problemas Entonces en este caso lo digo es que la edad va a ser reemplazada por el promedio de todas las edades Bueno lo ejecuto Y eso justamente es lo que hace y lo puedo visualizar obteniendo en este caso una visualización de los primeros 16 registros dice dos oraciones para poder ver justamente en particular este caso bueno fíjense que acá El reemplazo de los códices los tres primeros que sabíamos que eran Nan les puso 0 en el caso de Home desk bueno tenemos desconocido un poquito más abajo aquí desconocido y en el caso faltante que es el caso de la edad justamente en el valor 15 reemplaza la edad que no está por el promedio como me di cuenta bueno porque obviamente este número es muy raro todos los otros obviamente son cifras enteras entonces este obviamente se trata de ese el valor que ha elegido para que este bueno sea el que es reemplazado el Nan por el promedio esto es muy fácil si lo quieren de alguna manera probar a ver si es real o no Si lo ha hecho bien o no Bueno lo que pueden hacer es un g16 no de Data 1 sino de Data y van a poder ver si justamente si esos valores corresponden a que sea justamente eso no es más otra forma de chequear promedio es como lo hemos puesto aquí debajo fíjense que agregó un texto donde les digo que una de las formas de poder verlo es que si yo hago Data uno ege punto min me va a dar este valor si hago solamente se calcula parte me va a dar ese valor que es el que está aquí abajo Sí entonces Bueno hay diversas formas de poder chequearlo Y ustedes pueden corroborarlo si ese es el valor o no este lo podemos probar acá si quieren ya de paso aprendemos a Cómo agregar una línea de código yo me paré acá abajo fíjense que me aparece código texto Quiero agregar un código bien me parece una línea en blanco Sí aquí lo que voy a poner lo que puse allí Data 1 entre llaves y comillas age punto min así se escribe y ejecuto y obviamente lo que me va a dar es este famoso número 2881 bueno que es este que está aquí Bueno esto insisto es otra forma de poder corroborar si ese valor que ha puesto allí en el caso promedio está bien o mal en el caso de desconocido y cero es más fácil porque es un valor no hay ningún cálculo Sí pero como en este caso había un cálculo Bueno ese cálculo que está allí lo podemos calcular aparte y ver el valor y después ver justamente que está bien lo que puso aquí en este caso porque tenía unan y lo reemplazó por el promedio bien qué más nos queda para esta clase librería Matt si se acuerdan que hablamos que es la librería de python para bueno graficar en Bueno una estructura de dos dimensiones para hacer eso vamos a importar otro archivo Sí con lo cual vamos a volver a ejecutar un nuevo apload voy a buscar el archivo customer bien allí está esta instrucción no sería necesario hacerla porque porque en el contexto de este Notebook la hice al principio cuando importe nuevamente el archivo Titanic sí es decir aquí está podría haber puesto directamente el abdomen sí lo vuelvo a importar no pasa nada no pero bueno para que vayamos teniendo en cuenta y entendiendo más en profundidad las implicancias o las necesidades de cada una de las cosas que hacemos bien ya importamos customer model que es un Data set que habla de bueno la cantidad de llamados que hace un call center y la cantidad de consumo la cantidad de duración de los llamados bueno algunas cuestiones generales pero lo que vamos a hacer hincapié es en la cantidad de llamados que se hacen de manera nocturna y de manera de diurna Sí y la duración de cada uno de ellos bien hacemos entonces una vez que lo tenemos este incorporado siempre Recuerden que lo veo a través de aquí Ya tengo la idea de que está dentro de mi sesión de cola bien y hacemos el get y vemos lo que decíamos recién tenemos la cantidad de carga la carga diaria la cantidad de llamadas la cantidad de minutos bueno acá lo mismo pero de noche y a la tarde También Perdón este día de tarde y de noche bien vamos ahora a ver qué vamos a graficar con esto bueno como es un gráfico en dos dimensiones lo que vamos a hacer aquí es elegir dos características para confrontarlas y justamente obtener este gráfico vamos a elegir la cantidad de minutos por día y la cantidad de carga por día y vamos a hacer con kine que quiere decir tipo un tipo de gráfico scatter es un gráfico de puntos y distribución de puntos sí lo que se busca es relacionar en este caso dos elementos daymins y Day charge y ver justamente esa relación que existe que figura dibuja evidentemente Aquí ya está ejecutado obviamente la instrucción como tenemos todo siempre el Notebook se ha ejecutado La idea es repasarlo volviendo a ejecutar la instrucción veo que justamente esta este gráfico de puntos me dibuja una línea casi perfecta sí bien qué más podemos hacer podemos trabajar con sub plots subplots es otro tipo de gráfico que lo que hace es permitirme no solamente hacer un gráfico sino hacer más de uno en una estructura matricial es decir puedo poner una estructura matricial con en este caso por ejemplo pongo dos gráficos y dos gráficos sí como si fuera una matriz de dos por dos pero podría poner todo lo que yo quisiese Esto me permite ver distintas comparaciones en una sola vista entonces aquí lo que voy a hacer es primero definir este subplot que estructura quiero que tenga dos por dos Sí y luego el tamaño en pulgadas de cada una de esos cuadros sí en este caso le pusimos 10 por 10 y luego de ello voy a repetir si cada uno de los gráficos que en lugar de hacerlos como los hice antes con datablock voy a hacerlo con datablock y las coordenadas le voy a decir ese gráfico En cual de las coordenadas de esta matriz dos por dos quiero que vaya el primero en la 00 el segundo la 01 luego en la 110 y luego en la 11 y obviamente en cada caso voy a usar puedo usar el mismo tipo de gráfico u otro en este caso elegí hacer scatter en los cuatro casos pero obviamente las comparaciones tienen que ser diferentes para que tenga sentido si está este ejercicio que estamos haciendo aquí entonces pongo damise contra de charge night con nite dakours contra day charge y Nike course contra nikearse Sí y bueno y veo aquí este gráfico de dispersión de puntos fíjese que en este caso están totalmente dispersos en este caso también pero en este caso representan una tendencia Sí porque una tendencia porque una línea que me demuestra una especie de patrón Entonces yo puedo ya ir viendo aquí que en estos dos gráficos hay una potencial buena relación que me permite pensar en un modelo de tendencia y Por ende un buen algoritmo de predicción en este otro caso no digo que no exista esa posibilidad pero es mucho más complejo evidentemente con el primer tipo de este algoritmo que vamos a ver que es el de regresión lineal nunca con estos gráficos vamos a tener ningún modelo puede ser que tampoco lo haya es decir no siempre la relación entre dos variables me lleva a que siempre es posible Establecer un modelo de predicción fíjense ahora lo que vamos a hacer vamos a hacer exactamente lo mismo pero con un pequeño cambio aquí donde pongo el suplots y defino que van a hacer gráficos o esta estructura van a ser distribuidas en una matriz de dos por dos voy a poner ser x y jerre y true Qué quiere decir esto vamos un poquito a ver el caso anterior miren las coordenadas que yo tengo aquí van de 0 a 300 y de 100 a 400 y aquí de 2,5 a 17,5 y 18 60 o sea nada que ver un caso con el otro obviamente tampoco tienen que ver con esto sí en este caso lo que voy a hacer es compartir las coordenadas fíjense que todos los gráficos tienen un x que va de 0 a 400 y un eje de y que va de 0 a 60 por eso estos gráficos son los mismos que estaban acá arriba pero ahora que se comparten la numeración de las coordenadas obviamente la muestra gráfica es totalmente diferente esto también permite analizar justamente los modelos cuando lo que estoy haciendo aquí no es Ni más ni menos que tener una misma escala bien esto sería todo lo que tiene que ver con los gráficos de tipo scatter pero tengo un tipo de gráfico más para cerrar la clase que es el de histogramas los histogramas son este tipo de gráficos conformado por estas estructuras que se llaman bins que decir cada una de estas columnas se llaman bins Sí y sirven más que nada para ver en este caso un solo dato Sí una sola característica si tiene una forma normal o no en la estadística cuando algo tiene una representación acampanada como en este caso se dice que representa o está representado por una distribución de tipo normal con un valor central y con valores Igualmente distribuidas a la izquierda que distribuidas hacia la derecha esto También es importante porque a la hora de establecer los modelos visualizar si los datos tienen una distribución normal o no es justamente un elemento que voy a tener tener en cuenta en la búsqueda de un buen modelo bien en este caso nos fijamos que el promedio de llamadas porque el elemento que elegimos es day calls es de 100.43 y esto lo vemos en el gráfico fíjense que el valor Más alto que sería el promedio está en el 100 concretamente en el 100.4356 y todo lo que sigue de más Sí fíjense cómo calculé de nuevo se acuerda que debemos calcular el promedio para el caso de la edad del Titanic Bueno aquí el promedio es exactamente lo mismo Data el nombre del campo el nombre de la característica para eso con propiedad inmin que es el método que me ayuda a calcular el promedio hasta aquí llegamos con la clase número 2 con ellas podido acceder a datos visualizarlos modificarlos y hasta graficarlos nos vemos en la próxima clase en donde vamos a profundizar el conocimiento sobre el análisis de datos hasta la próxima clase  Titulo: Clase3 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/XVjzqtohGKM  \\n 1569 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 3 del curso de Inteligencia artificial de ifes en esta clase Vamos a aprender a manejar todas las variantes que existen para hacer cambios en la estructura de un conjunto de datos empecemos   Hola a todos como dijimos en la presentación Esta es la tercer clase del curso de Inteligencia artificial el tema de hoy es operaciones de manejo de datos bueno algunas de estas cuestiones vimos ya la clase pasada pero tenía que ver más que nada con el contenido de los datos acá vamos a trabajar con operaciones que hacen a cambios no en el contenido de los datos sino en la estructura de los datos tema muy importante y ese tema se llama como ya lo mencionamos en la primera clase Data rumbling Qué quiere decir traducido al Castellano algo así como cirugía de datos Esta es la primer parte vamos obviamente en esta clase número 3 también tener dos partes como venimos trabajando en las clases anteriores Y empezamos con la importación de las librerías a lo que ya venimos utilizando en los dos notebooks de la clase número 2 es exactamente para este caso lo mismo que venimos escribiendo las mismas librerías Así que simplemente lo ejecutamos para trabajar con esta clase vamos a importar nuevamente el mismo archivo que habíamos importado en la segunda parte de la clase pasada customer charge Así que vamos con ello también con la forma que lo venimos haciendo habitualmente aquí lo seleccionamos bien y lo va importando Recuerden que esto me trae el archivo aquí lo puedo ver en la estructura del explorador de archivos lo cierro ahora era simplemente para repasar ese tema y lo próximo insisto lo que venimos haciendo habitualmente que es bueno abrir el archivo con red csv y hacer un Head para reconocerlo Más allá de que ya lo tengo visto desde la clase pasada Recuerden que este Data set tenía información de un call center donde hay referencias de las llamadas en diversos datos que tienen que ver con las llamadas de Día de tarde de noche y bueno Y algún otro dato que vamos a usar ahora que tiene que ver con el estado al cual pertenece el teléfono al que se ha realizado la llamada bien ahí tenemos la información y ahora sí vamos a ver el primer tema que tiene que ver con esta clase crear un subconjunto de datos es decir de esta estructura que tengo acá yo me quiero quedar solamente con algunas características sí Entonces cómo hago con esto Bueno creo un Data set lo selecciono desde el Data poniendo el nombre de cada una de esas características encerradas entre llaves y a su vez en una segunda línea de llaves con cada referencia de cada característica encerrada entre comillas y separada por comas eso lo pongo dentro de un nuevo Data set Que obviamente este le puedo poner como yo quiera Sí luego una vez que yo ya lo hago la un Head para ver Su contenido y bueno veo sí como todo Head las cinco primeras observaciones pero solamente de las tres características y no de todas como tenía antes aquí bien segundo tema crear en lugar de un subconjunto de columnas como tengo acá sí un subconjunto de filas Es decir subconjunto de observaciones cómo lo puedo hacer Bueno en este caso fíjense que no voy a poner el resultado dentro de otro Data set sino que literalmente lo que voy a hacer es Mostrar ese contenido bueno haciendo una selección y mostrándolo simplemente sin que implique esto que voy a cambiar Data frame Data sí simplemente es una extracción de datos a fin de poder visualizarlos lo que hago aquí es seleccionar un conjunto de filas que va desde la 10 a la 29 Sí desde la 10 a la 29 no hay ningún error de mi parte aquí pero recuerden que en python cuando ustedes ponen Aquí un conjunto una referencia de un conjunto de valores el último valor no representa concretamente el último valor sino el anterior a ese con lo cual cuando yo pongo 30 siempre es n menos 1 es en este caso 29 bien siempre Recuerden que en python se pone primero el valor inicial dos puntos y el valor siguiente como dijimos recién al valor final bueno hago esto lo ejecuto y el resultado es que me muestra solamente esas observaciones Sí desde la 10 hasta la número 29 son 20 Roses desde la 10 a la 29 sí mostrándome obviamente Ahora sí todas las columnas es decir todas las características lo que no me muestra todas son Las observaciones simplemente de la 10 a la 29 bien Ahora lo que vamos a hacer es también hacer una extracción de algunas observaciones sí filas solamente que en lugar de poner como puse Aquí antes una referencia acerca de un Rango que describen un conjunto de valores lo voy a hacer A través de una expresión lógica poniendo una especie de condición que oficia de filtro con lo cual Solamente voy a ver aquellas observaciones que responden positivamente a ese condicionamiento en este caso le estoy diciendo que quiero ver solamente Las observaciones cuyo gamings sea mayor a 200 sí como dice aquí llamadas diarias que utilizan más de 200 minutos eso en este caso lo voy a poner dentro de un Data set y lo voy a mostrar Entonces ejecutó esto y veo el resultado fíjense que en este caso importante no me olvidé no puse punto Head esto también para que vayamos aprendiendo cosas nuevas sí Recuerden que yo siempre venía creando no es este el caso vamos al anterior aquí por ejemplo creaba un Data set y cuando lo visualizaba lo veía con Head yo puedo no poner gel qué es lo que va a hacer lo que hace aquí Mostrarme todo el contenido obviamente no me lo muestra todo porque no puedo hacerlo sino que lo que hace es mostrarme los primeros una línea de puntos y los últimos que lo que está haciendo Aquí está haciendo un Head y un Tail porque me muestra los cinco últimos y los cinco primeros Sí bueno Esto pasa si yo no pongo aquí ni Head nitale o si no pongo un Head con un número o un Tail con un número Bueno no es el tema principal Lo principal es esto que está aquí que es que van a aparecer las filas que responden a esta expresión lógica hacemos algo similar solamente que ahora en lugar de hacerlo con una característica de tipo numérico lo hago con una característica de tipo texto y digo Aquí que solamente quiero los datos cuya característica State que es la primera que está aquí primera columna es igual a Nueva York Recuerden que en el caso de python cuando yo quiero poner la expresión igual a se ponen dos signos iguales no uno solo sí Y de nuevo tomo creo un subset y lo muestro fíjense Aquí también para seguir aprendiendo cosas sí que reutilizó la variable qué es lo que pasa subses ahora tiene un nuevo contenido y Por ende este contenido que había puesto antes Obviamente que se pierde no escapa a las generales de la ley de cómo actúa una variable y esto Exactamente lo mismo bien volviendo al punto aquí tengo un subsiste que tiene solamente Las observaciones cuyo State es Nueva York y eso me lo muestra como veo aquí Sí me parece que no lo ejecutamos ejecutamos de nuevo Recuerden que el resultado siempre están cargados en el Notebook Pero siempre es bueno ejecutarlo para comprobarlo bien luego vamos a hacer algo similar pero sumándole un poquito más de una complejidad mayor es decir teniendo no una condición sino dos condiciones es decir las dos condiciones que vimos recién pero en una sola expresión vimos antes que creaba un subset donde tenía dainese mayor a 200 y después otro con State igual a Nueva York en este caso voy a poner las dos cosas como lo hago bueno como está aquí poniendo las dos expresiones pero enlazadas con una persona que quiere decir and es decir and y esta condición y esta Otra condición Sí y ambas cosas se encierran entre corchetes este y luego se pone el resultado dentro de la variable sí bien y ejecuto y muestro el contenido de la variable donde solamente van a aparecer aquellos registros que tengan esas dos condiciones y es lo que está mostrándome aquí bien fíjense este caso que yo puse subset o sin Tail pero me lo mostró todo por qué me lo mostró todo porque el resultado de aplicar estos dos filtros arrojó una cantidad de 23 filas que entran dentro del tamaño digamos este máximo que puede Mostrarme el cuadro de este nombre Sí bueno pequeños detalles para que vayan viendo Es decir si no hubiera pasado esto se hubieran sido 50 por decir algo me hubiera vuelto a aparecer aquí en el medio una línea de puntos bien así como teníamos aquí la condición and también ustedes imaginarán que tenemos una conexión o u or como se dice en informática sí es decir voy a poner las mismas condiciones de recién pero con o Recuerden que que se cumpla esta condición o esta o entre alguna de las dos se cumpla ya por entonces estará ese dato ingresando al subset y muestro El subset Bueno lo ejecuto y el resultado obviamente ahora es mucho más amplio porque porque no es am donde le pido las dos cosas estoy pidiéndole una u otra Cómo se pone el or se pone con esta barra Sí esa barra vertical que este simboliza el or como el Anderson simbolizaba el bien en este caso como son muchos números fíjese son 1.246 filas Bueno entonces vuelve a aparecer la línea de puntos suspensivos porque justamente no entra esa cantidad dentro del cuadro que tiene el loto bueno Son detalles que no es lo principal importante aquí es esto que estamos viendo acá pero bueno Son detalles que me interesa que los tengan presente porque bueno obviamente este después pueden desorientarlos en el tipo de resultado que ven también le voy a agregar otro dato que estamos viendo aquí fíjense que aquí también aparece signos digamos puntos que me simbolizan que la cantidad de características tampoco entra en el ancho del cuadro y también excede la posibilidad de la barra de scroll Así que la barra de crol tiene pero también tiene un límite no es indefinido entonces me pone una columna que representa varias columnas que están aquí en el medio que no pueden ser mostradas bien Obviamente que esto Una vez que ustedes lo quieran exportar alguna estructura Sí una base de datos o una planilla de cálculo lo que fuere obviamente lo van a ver todo el contenido esto solamente tiene que ver con lo que muestra el Note bien Entonces resumiendo tenemos la condición or aplicada aquí seguimos con otro tema en este caso vamos a usar condicionales pero que no se relacionen con un valor fijo se acuerda que habíamos hablado de una cantidad de llamadas mayor a 200 y un estado igual a Nueva York en este caso también puedo hacer filtrados que comparen o hagan expresiones de comparación o lógicas entre los propios entre las propias características Este es el caso fíjense que voy a poner aquí que quiero las tres cosas primero uso dos ampersan con lo cual Recuerden que lo que implica esto es decir and esto y esto y esto bien lo que estoy pidiéndole aquí es que la cantidad de llamadas nocturnas sean mayores a la cantidad de llamadas de el día de la mañana y que la cantidad de minutos de la noche sea mayor a la cantidad de minutos de la mañana y que el estado sea Nueva York sí es decir que tienen que cumplirse esas tres condiciones para que esa observación vaya al subset y luego a mostrar el subset que lo ejecutó Aquí voy a ver el resultado y obviamente es el resultado debe ser bastante corto presumo no Sí nada más que 30 Rose con lo cual de nuevo aparece la totalidad del resultado en el cuadro Pero bueno los registros que aparecen aquí Las observaciones que aparecen aquí son aquellas que cumplen en este caso con tres condiciones Sí en realidad la cantidad de condiciones no hay ningún problema pueden poner toda la cantidad de condiciones que quieran lo novedoso aquí respecto al anterior es que el filtrado se ha hecho en un caso como lo veníamos haciendo contra un valor fijo y en los otros dos casos como estos dos de aquí contra hemos características propias del mismo dataset pasamos al próximo caso y lo que es otro método que también me da la posibilidad de filtrar filas y columnas características de observaciones y se escribe del siguiente modo se pone como el método que está aquí puesto sobre el Data set y obviamente se asigna a otro que es el mismo subset que venimos utilizando Cómo se expresa lo que yo quiero poner aquí como objetivo que seleccionar los registros 10 a 30 y columnas 7 a 8 bueno recuerden lo que hablábamos recién respecto de cómo utilizar la numeración de rangos python si quiero los registros de 10 a 30 pues entonces tengo que poner 10 dos puntos 31 con lo cual ahí estoy diciendo vivir un Rango que hace en este caso a Las observaciones y luego si quiero elegir las columnas o características 7 a 8 tiene que poner 7 dos puntos nueve siempre un valor superior al último que quiero con lo cual al ejecutar esto veo el resultado y efectivamente me muestra las filas de la 10 a la 30 y me muestra la columna 7 y 8 sí en este caso me voy para arriba 0 1 2 3 4 5 6 7 y 8 acá puedo comprobar que justamente damiens son las columnas 7 y 8 que hago referencia aquí arriba bien existe también otro elemento otro método que se llama lock lock tiene un propósito similar a ilock solamente que existe una diferencia Más allá de la i que justamente es allí donde está la diferencia entre una y otra cosa porque porque la i justamente representa o hace referencia al Index justamente y lock me permite seleccionar características u observaciones a través de su número de índice en el caso de lock lo hace a partir de las etiquetas en el caso de la selección de observaciones elegiría la etiqueta del acero a la 19 mucho cuidado aquí porque yo voy a ver que el resultado me va a mostrar la fila cuya etiquetas 19 con lo cual ustedes se van a confundir con lo que hablamos siempre de lo que representa el último valor en un Rango expresado en python si yo miro aquí y tengo razonamiento anterior diría que la última fila Debería ser la 18 porque siempre dijimos que este valor representa el siguiente Pero en este caso 19 no lo identifica o no lo hace referencia como un valor numérico sino como una etiqueta por lo tanto aquí dice esta expresión que quiere desde la etiqueta número 0 a la etiqueta número 19 y allí no corre esto de la expresión numérica de python o del Rango de python que esto representaría el valor anterior es el 18 pero en este caso hace referencia la etiqueta y se expresa de un modo similar a un Rango numérico pero no representa lo mismo con lo cual aquí de esta manera me va a mostrar la etiqueta 19 y luego los valores de las características damiens parecido a como paso antes lo vuelvo a ejecutar más allá que resultado ya lo observa y Bueno tengo parecido a lo que pasó recién que eran las dos características de Mini Pero bueno elegí otro Rango diferente de filas bien entonces lockeylock seguimos con este caso que sería la situación de tener que agregar una columna a un Data set es decir poder poner más características en el puede ser por cualquier este motivo en este caso lo que elijo como motivo es poder tener una característica que me represente la suma de las llamadas nocturnas más las diurnas más la de la tarde y después lo mismo con la cantidad de minutos los minutos de noche los minutos de tarde de mañana perdón el momento de tarde esto insisto es un ejemplo cualquier motivo puede dar a la necesidad de tener una nueva característica Cómo se hace bueno simplemente cuando yo pongo como referencia a través de El Data Setup un nombre que no existe Es que no está asociado a ninguna de las características existentes automáticamente entiende que estoy intentando crear una nueva característica y después el igual le pongo lo que yo quiera puede ser un valor fijo puede ser una operación matemática en este caso es una suma y en el caso de bueno total de minutos se procede del mismo modo Sí con lo cual hago esta operación hago un Data Head y voy a ver que voy a tener en el final porque obviamente me lo va a poner al final de este Data set las dos columnas total calls y total means que acabo de agregar otro tema más de esta clase representa la generación de números aleatorios esto es muy importante Machine learning porque muchas veces vamos a tener la situación en la cual queremos generar un modelo pero no tenemos datos Y tenemos una idea de Cuál es la característica de los datos si los tuviésemos pero concretamente no los tenemos entonces con Random puedo generar números aleatorios que de alguna manera representen la característica de esos datos esto es un método de nampai fíjense que empieza la expresión aquí con np Random Random esto genera números aleatorios entre 0 y 1 Cuántos números tantos como el valor que yo ponga aquí entre paréntesis con lo cual en este caso como es Este título género 100 números entre 0 y 1 bien esos valores los pongo dentro de una estructura en este caso rmd que le he llamado y al imprimirla veo que lo que generó aquí no es un Data set muy importante fíjense Cuál es la palabra que hay aquí esto genera un array no un Data lo más importante después se puede pasar un array a un taller por supuesto Son estructuras que se puedan pasar tranquilamente de una u otra pero es importante que identifiquen que en este caso Random Random no me genera un dato set sino un array Pero puede pasar también que yo no quiera valores que vayan entre 0 y 1 sino que vayan entre un Rango de valores determinados por ejemplo quiero generar 50 números entre uno y 100 en ese caso no voy a generar a través de Random Random Pay porque eso solamente como vimos Recién me genera valores entre 0 y 1 voy a usar Random Run int entonces con Randy yo le pongo el valor mínimo y el valor máximo del Rango de valores que quiero que genere y luego la cantidad de valores entonces aquí me muestra 50 valores entre 1 y siempre esos valores pueden más sea que son aleatorios puede Que tengan un determinado orden esto en realidad ustedes tienen que entender que en la medida que los valores con los cuales yo voy a generar el modelo sean lo más generales posibles en modelo va a ser más sólido es decir si bien Se sigue un patrón y el modelo está inspirado en un patrón los datos o la realidad no es siempre igual va a cambiar entonces tiene que tener un buen modelo la posibilidad de generalizar los datos hay una tendencia fantástico para la tendencia no marca que siempre va a ser tan mecánica como que los datos siempre van a ser un patrón este al 100% de esa realidad los datos van a variar Entonces esto que me da la posibilidad sufling es la posibilidad de mezclarlo quiere decir mezclar o barajar los datos sí Esto está como representa una opción muy importante porque justamente me nos permite barajar los datos probar un modelo volver a barajarlos volver a probarlo Y si ese modelo es bueno tantas veces que los probé con distintos grupos de datos que van mezclándose quiere decir que definitivamente es un modelo que generaliza bien ese que se adapta a un montón de situaciones y no solo a un patrón fijo esto ya lo vamos a ver más adelante va a ser tema de la próxima clase pero es importante que lo vayamos entendiendo de Cuál es la importancia de un modelo que generalizan concretamente para este caso lo que hago es tomar un conjunto de valores como el de la raíz de recién rnd y le hago np Random es los mezclo lo vuelvo a imprimir y fíjense Si miro el valor de recién y miro la raíz de ahora no tiene nada que ver uno con otro más ya que todavía no lo ejecute lo ejecutamos y vemos el resultado si lo comparo con el de arriba Recién veo que los valores están en un orden diferente son los mismos valores pero están ordenados de modo diferente este no va a cambiar nunca porque ya fue generado ahora si este que está aquí yo lo vuelvo a ejecutar fíjense que arrancaba con el 22 ahora con el 12 lo ejecuto nuevamente Sí o sea cada vez que lo ejecute va a alterar el orden que tenía el rnd sí Entonces esto insisto es muy importante como herramienta para mezclar los datos y poder buscar un mejor modelo semilla de generación aleatoria vamos a suponer que yo tengo un conjunto de datos en el marco del cual voy generando un modelo Sí pero ese conjunto de datos puede que vaya cambiando con situaciones como la recién veíamos con lo que ustedes quieran de alguna manera tener como datos que puedan llegar hacer nuevos para poder un programa nuevo modelo en este caso la semilla lo que hace es congelar un conjunto de datos que puede ir cambiando Sí entonces si yo genero un conjunto de datos aleatorios en algún momento puede ser de que ese conjunto de datos me sirvió para generar un mejor modelo sí Entonces es bueno cada vez que voy a hacer una generación aleatoria de datos poner un Random State es como ponerle un número de documento por decir de alguna manera o identificar con alguna ID a ese grupo de datos Entonces qué hago con esto Esto me genera en este caso un Random este 42 Yo sé que lo que voy a hacer a continuación va a estar tipificado como con una id42 yo genero un Random resulta que ese Random a mí me gustó me fue Útil para generar el modelo Entonces cuando lo vuelva a convocar o lo vuelvo a invocar puedo volver a hacerlo recuperando este ID porque si no ustedes piensan que si voy mezclando los datos voy mezclando los datos después Cómo puedo volver a recuperar ese grupo de datos que fue bueno para generar el modelo bueno con Random State lo puedo hacer en este caso lo ejecuto sí y fíjense que yo tengo aquí el resultado no fíjense que lo sigo ejecutando y los valores son los mismos ven son exactamente los mismos a pesar que es Random o sea debería cambiar no cambia genera siempre lo mismo fíjense ahora que yo voy a quitar este número sí no pongo semilla Entonces lo ejecuto fíjense Cuál es el conjunto de valores lo ejecutó nuevamente ven que cambia permanentemente va a ir cambiando porque no hay semilla Sí ahora yo vuelvo a poner 42 y vuelvo a recuperar se acuerdan el 0,49 67 que estaba al principio vuelvo a recuperar el mismo conjunto de datos que yo tenía al principio sí bien Por eso estas referencias un poco que nos nos explican Cómo proceder en este caso para poder verificar esto que estamos tratando de explicar aquí usen esta Referencia para justamente probar lo que acabamos de probar y puedan comprobar lo que acabamos de comprobar pueden establecerse también otros tipos de semillas sí es decir No es que solamente puedo guardar la 42 en este caso hago la semilla 43 y tengo otro conjunto de valores que también ya quedan congelados y por más que lo ejecute la cantidad de veces que lo ejecute el resultado va a ser el mismo o sea que tengo aquí un Random 642 que representa un conjunto de valores y aquí un Random este 43 que representa otro conjunto valores 42 43 puede ser el número que ustedes quieran eh o sea simplemente seguir este esta línea Porque eran números que estaban cercanos Pero puede ser el número que ustedes quieran siempre y cuando represente una ID que represente a un conjunto de valores hasta acá llegamos con esta primera parte Acompáñame a conocer más técnicas en la segunda parte nos vemos  Titulo: Clase3 (parte 2) Curso Inteligencia Artificial \\n URL https://youtu.be/6Npzfhjrkb4  \\n 944 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 3 del curso de Inteligencia artificial de ifes vamos a seguir aprendiendo formas de cambiar la estructura de un conjunto de datos vamos con ello   bueno recuerdan que recién hablábamos de que tenemos estructuras que son a raíz Sí desde aquí venimos hablando de cuando generamos el primer conjunto de datos aleatorios Y yo le dije cuidado es un array no es un Data frame pero que lo puedo pasar un array bien es el tema que vamos a ver ahora justamente voy a generar un Data frame pero íntegramente conformado con valores aleatorios es decir esto no tiene nada que ver con ningún ccv ni ningún archivo de tipo txt xls que provenga del disco esto es un invento que hago yo una simulación que hago yo de datos justamente por el hecho de como le explica Hoy no tengo datos para poder probar en este caso voy a generar un array con dos valores masculino y femenino otro de ingresos con tres valores bajo medio y alto perdón de la raya anterior tiene que ver con el género obviamente voy a tomar un voy a definir una variable n de 500 y voy a crear dos arrays vacíos género datos ingresos Data sí bien Por qué hago esto porque voy a hacer una estructura Ford    un ciclo dentro del cual lo que voy a hacer va a ser crear un conjunto de valores aleatorios que va a variar por su cuenta entre masculino y femenino y entre bajo medio y alto Sí qué voy a hacer aquí foreig in Race n que sn 500 es decir va a ser un ciclo 500 veces 500 veces va a generar un valor aleatorio entre masculino y femenino como con np Random Choice de género Qué quiere decir chores de género elija Cualquiera de esos valores libremente el que sea sí es decir que indistintamente por cada vez que pase por aquí para generar un valor que puede ser o masculino o femenino y se lo va a agregar al array género Data por eso genera toda appen de un Choice una elección de El array lo mismo va a pasar con ingresos en ingresos Data va a agregar un valor por cada uno de los ciclos que dé que va a ser un Choice una elección de El array ingresos con lo cual va a elegir o bajo o medio o alto con lo cual cuando termine este proceso voy a tener 500 valores dentro de género date dentro de ingresos Data que van a variar entre los valores que acabamos de mencionar luego de esto vamos a crear cuatro variables altura peso edad de ingreso que los voy a tomar como referidos a alguna tendencia de lo que entiendo puede ser los posibles valores que voy a poner fíjese como armo la altura 160 más 30 ese sería 190 pero no el 30 lo multiplicó por Random 500 veces para tener 500 valores con lo cual se acuerdan que esto genera valores entre 0 y 1 con lo cual Esto va a ser que este 30 pueda ir en un valor inferior a 30 o superior a 30 libremente se entiende tengo 160 más o 30 o 35 o 25 o 20 o 40 es decir ese Random va a multiplicar a este valor 30 y lo va a hacer variar y siempre lo va a sumar a 160 como resultado que obtengo una altura que va a tener 160 más ese 30 que puede ir hacia arriba y hacia abajo Esta es una manera de tener una base y un valor que va a fluctuar lo mismo el peso un peso base 65 y después con 25 puede ser que inclusive tenga valor menor a 65 sí o mayor entonces lo que tengo aquí es lo mismo para peso Lo mismo para edad y lo mismo para ingresos sí bien Esto es una opción que me permite justamente aplicar Random rumbled porque justamente lo que haces es hacer variar este 30 este 25 este 2 Entre 300 y luego sumárselo al valor fijo 1030 65 lo concreto que aquí yo gracias a todos estos cálculos tengo dos conjuntos de 500 valores que varían entre masculino y femenino y entre bajo medio alto y cuatro variables que tienen también 500 valores que varían en este Rango de altura en este Rango de peso en este Rango de edad y en este Rango de ingresos una vez que tengo esos 500 valores alojados en esos seis elementos estos cuatro que están aquí y estos dos que están aquí género un Data frame como luego Data igual a pd de pandas punto Data frame y luego entre paréntesis y encerrado entre llaves etiqueto cada una o sea le pongo una etiqueta a cada característica quiero quedar de este Data frame que no existe lo estoy creando desde ahora sí entonces digo Bueno voy a crear una etiqueta género qué le voy a poner a género el contenido de género Data es decir este array de aquí esto que está quieto lo mismo con nivel de ingresos le pongo ingreso a Data este rey aquí en ingresos le pongo el contenido de ingresos esta en altura es la altura está el de peso el de peso esta el de la edad con eso estoy generando un Data frame ya no es una red en base a distintos a raíz porque acá lo que yo tengo son 6 arrays que se lo pongo a seis variables cada raíz tiene 500 elementos con lo cual Cuál será la estructura del Data frame Cuál será el shape de Data frame bueno cuántas características Tengo seis uno dos tres cuatro cinco seis y cuántos observaciones tengo 500 Entonces ella es 500,6 lo ejecutamos primero esto luego esto ahora genera el Data frame y hago el y ahí tengo resultado si ven que tengo las seis características y obviamente no me muestra aquí los este 500 este las 500 observaciones Pero obviamente están como lo podemos probar Bueno agregamos aquí una línea de código se acuerdan que la otra vez habíamos visto este tema de cómo ir a insertando líneas de código o agregando también se puede hacer al final Data punto shape bien lo ejecutamos y el resultado tiene que ser lo que hablamos recién 500 observaciones y seis características de las cuales obviamente como pusimos queda aquí estoy viendo las primeras cinco concretamente todo esto que hicimos para que es para generar este Data frame este conjunto de datos que nunca tuve son todos datos simulados todos los datos inventados bueno Con qué seguimos con redondeo de valores numéricos Yo aquí con round aplicado sobre el Data set correspondiente Puedo ponerle un redondeo para todos todos los valores numéricos que tiene un Data sí en este caso ingresos altura peso y edad si lo estoy redondeando a cero sí es decir terminan en punto cero porque están redondeados sin decimales redondeo customizado de valores es decir en este caso a un redondeo viendo en cada característica la cantidad de decimales que quiero ingresos 2 altura cero peso cero edad cero Entonces fíjense como corresponde aquí el resultado ingresos tiene sus dos valores luego de la coma altura peso y edad tiene el punto cero que significa nada en realidad yo lo podría mostrar directamente sin ningún tipo de decimal porque de hecho el punto será aquí no hace a ninguna expresión que tenga validez está redondeado a las tres o dos cifras que están a la izquierda del punto agrupación de datos por categoría esto es muy común para aquellos que manejan algún lenguaje tipo sql donde se agrupan los datos de acuerdo a un campo En este caso en este caso una característica quiero agrupar por género Entonces qué hace me agrupa femenino por un lado y masculino por el otro y lo que hace es contar cuántos hay femeninos y masculino Obviamente que sí hay 247 femeninos quiere decir que hay 247 ingresos alturas pesos y edades de femenino y otro tanto de masculino ahora esta contabilidad como la veo a partir de count con esta primera instrucción lo que hace es agrupar y con esta segunda lo que hace es Mostrarme la cantidad podría mostrar el promedio por ejemplo en lugar de cont mean se acuerdan que habíamos usado eso bueno una vez que está agrupado el cálculo numérico que opero con los valores numéricos valga la redundancia puede ser que yo quiera bien puedo tomar uno solo de esos grupos una vez que están generados porque yo cuando hice el Group by de género lo puse dentro de un nuevo datase que se llama Grupo género Entonces yo digo Bueno grupo geno punto o sea dame uno de los grupos en este caso femenino me lo da y ya me muestra todo el detalle fíjense que me muestra Ni más ni menos que todos los que yo tenía aquí como femeninos que son cuantos los 247 Obviamente que me muestra aquí todos los que corresponden a femenino No acá están Bueno aquí no hay una correlatividad porque por la cooperativa puede suponer puede Bueno aquí el 0 y 1 no están evidentemente son masculinos y aquí por ejemplo el 494 evidentemente masculino y aquí también el 96 y 97 evidentemente son masculinos por eso todo lo que me está mostrando aquí más allá que los números parece que demostrar una correlatividad son como dice aquí abajo ven los 247 casos de género femenino bien qué otras operaciones tenemos bueno puedo hacer un Group by por más de una característica ya no por género lo quiero por género Pero dentro de género por nivel de ingresos entonces me agrupa femenino por un lado masculinos por el otro y dentro de femenino separa Los Altos Los bajos de los medios en el caso de ingresos por supuesto y en el caso de masculino otro tanto alto bajo y medio y de cada uno de estos casos yo le vuelvo a pedir un count Sí bueno acá genere otro Data set grupo género Ink a diferencia del anterior que llamaba grupo género porque era solamente género acá le puse otro nombre Grupo género Inge porque hago referencia a que tiene género y dentro de género ingresos igual como siempre digo el nombre de El datazo puede ser cualquiera sí no sé que no se compliquen con eso si lo importante es que puedan diferenciar lo que yo usé antes aquí Group byte con un solo campo una sola característica y acá use Group by con género y nivel ingresos luego aplicó con como siempre bien qué más tenemos en este caso de nuevo parecido lo que hicimos antes se acuerdan que nosotros primero hicimos buscamos la agrupación por género después le pedí que me diera uno de los grupos femenino a cada grupo por dos características ahora lo mismo le pido que de ese Data frame me dé solamente el que tiene la característica femenino y alto get Group Dame esos grupos femenino y dentro femenino alto es decir esto sería del todo que muestro acá y bueno eso es lo que me da pero ahora abierto en detalle sí No consolidado acá lo que me decía es que hay 80 personas con esa característica bueno acá me muestra el detalle de las 80 sí 80 como dice aquí ven me muestra el detalle de las 80 bien y también puedo hacer algún tipo de operación matemática además de con como decía recién puede ser Mine o puede ser justamente suma vuelvo acá atrás se acuerdan acá mostramos las cantidades hay 80 de ingresos altos y femenino hay 86 de ingreso bajo y femenino bueno acá voy a hacer algo similar pero en lugar de calcular una cuenta lo que voy a calcular es una suma entonces en cada matriz en cada valor de esta matriz lo que me va a dar es la suma de todos los ingresos de todas las personas de ingresos altos y de género femenino en este caso te va a sumar Todos Los sueldos de todas las personas de nivel ingreso bajo y que son de género femenino lo mismo puedo hacer como decía Recién con min es decir calcular en lugar de la suma el promedio entonces aquí este 1028.43 64 11 simboliza el promedio de ingresos de todas las personas del sexo femenino cuyo nivel de ingresos es alto y bueno con la misma lógica aquí la edad aquí el peso bueno el resto de los datos correspondientes  hasta aquí llegamos con esta clase número 3 hemos aprendido una gran cantidad de técnicas para manejar y modificar las estructuras de un conjunto de datos pero aún quedan muchas más por conocer y eso lo vamos a hacer en la siguiente clase nos vemos en la clase número 4  Titulo: Clase4 del Curso de Inteligencia Artificial \\n URL https://youtu.be/O_VUT1zJRyQ  \\n 2092 segundos de duracion \\n Hola  bienvenidos Esta es la clase número 4 del curso de Inteligencia artificial de ifes te invito a seguir descubriendo formas de seguir modificando la estructura de un conjunto de datos empecemos    Hola a todos como se dijo en la presentación estamos en la clase número 4 continuidad en desde el punto de vista temático de la clase número 3 continúa directa porque en la clase número 3 habíamos hablado de operaciones de manejo de datos primera parte y aquí estamos en la segunda parte Data running es el tema que sería como dijimos en su momento algo así como cirugía de datos lo primero que tenemos aquí es bueno la la incorporación la importación de las librerías que son las mismas que usamos en las dos clases anteriores o sea esto por ahora bueno va a continuar igual seguramente en algún momento vamos a agregar alguna librería seguramente No la vamos a quitar alguna de estas porque se nos va a acompañar hasta hasta el final del curso bien Así que lo lo que hacemos aquí es ejecutar esta instrucción para justamente hacer las importaciones luego vamos a empezar a trabajar con el mismo Data set que con qué terminamos la clase anterior Sí sí recuerdan en la clase anterior nosotros habíamos armado Data set con formato de Data frame Sí con datos aleatorios habíamos hablado de datos aleatorios y habíamos creado una radio aleatorio de géneros de tipos de ingresos y después Hicimos lo mismo para altura peso de edad y valor que representa el ingreso con ello y con este último código creamos el Data frame con 6 características aquí están las etiquetas y con los seis contenidos que generamos en los dos bloques anteriores referenciar esta cuestión que es importante lo que quiero hacer hincapié un detalle nosotros aquí nos fuimos haciendo en tres bloques Sí en tres cuadros de código del Notebook porque tiene que ver con como nosotros queremos ir avanzando en el curso o sea haciendo explicaciones parciales Sí este para mejor entendimiento pero como ya les dije en algún momento Esto no es necesario que sea así es más yo podría escribir casi todo el Notebook en una sola instrucción sí no digo en algunos casos porque quizás este haya algún corte que hacer porque el tema cambio porque no es lo mismo pero en este caso son tres bloques que sí tienen que ver con un mismo objetivo lo cual que pasaba en el Notebook número 4 voy a necesitar este Data frame pero lo perdí porque no tenían el Notebook 3 tampoco lo puedo importar porque esto no es un archivo que me traje del disco como vimos en los casos anteriores es algo que genera netamente con lo cual qué hago Voy a copiar estos tres bloques aquí para volver a tener ese conjunto de datos pero en un solo bloque ven esto Estoy esto aquí está en un solo bloque y lo ejecuto todo junto y no hay ningún problema porque de hecho es un poco lo que estaba mencionando recién Así que lo copie lo pegué y lo ejecutó Y tenemos como resultado el mismo Data set que tenía en la clase anterior bien Esto insisto más que nada para que vayamos todas las clases aprendiendo un poquito más de pequeñas cosas que son detalles pero que es bueno ir explicando lo en su momento y no explicarlo todo desde entrada y que bueno aporte más confusión que conocimiento bien seguimos el primer tema de esta clase es filtrado de datos así que ya veníamos trabajando en esto por eso decimos que esta clase es continuidad de la otra pero de una forma diferente la idea de esta clase y la anterior es proporcionar no solamente una variedad de elementos que hacen a cambiar la estructura del Data set sino también poder ver variantes que hacen lo mismo pero bueno Proponen diferentes formas de hacer lo mismo para que esto sea más rico más variado y ustedes después elijan la forma que más les convenga o mejor consideren bien en este caso que quiero hacer filtrar a las personas que tienen ingresos mayores a mil pesos con lo cual Cómo puedo redactar esto bien Data Dentro de este conjunto de corchetes pongo el nombre nuevamente del dataset punto y el nombre de la característica en este caso ingresos y después bueno la expresión lógica que yo quiera en este caso mayor a mí bien lo ejecuto yo tengo un Data filtrado donde solamente están las personas como Ven aquí tienen ingresos superiores a mí sí bien otra forma de filtrar ya no por datos numéricos sino por datos categóricos acuérdense la palabra categórico viene de categoría sí un número puede representar una categoría por supuesto tranquilamente pero habitualmente la palabra categórico en lo que es el ambiente del análisis de datos se referencia a algo que está descripto de una manera textual sí y que representa una categoría un nombre no representa una categoría nombre es un texto pero no representa una categoría masculina y femenino sí bajo medio de alto sí es decir son formas de categorizar darle categoría a algo pero en lugar de usar un número se usa un texto bien en este caso el dato categórico es el género y lo que yo quiero indicarle con esta expresión de aquí es que me quiero quedar con Data punto género es decir la característica género el Data solamente aquellos que son o que tienen valor igual a masculino Acuérdese que el igual en python cuando es una comparación sí no una asignación se usan dos símbolos iguales consecutivos entonces aquí con esto filtro para que muestre solamente lo que corresponden al género masculino lo ejecuto y veo el resultado ven solamente en la columna género estará aquellos que corresponden a el tipo de género masculino bien otra forma de filtrar es utilizar una lista con Easy is in quiere decir si está dentro Entonces en este caso voy a poner nivel de ingresos voy a tomar en este caso la característica a nivel de ingresos Y en lugar de decirle igual a tal cosa que no me va a servir en este caso salvo que pusiera nivel de ingresos igual a alto o igual a medio uso esta otra forma que es práctica sí es decir pongo nivel de ingresos punto y entre paréntesis toda la variedad de posibles valores que quiero filtrar concretamente en este caso quiero solamente las personas que tienen ingreso alto y medio no quiero las que tienen ingreso bajo entonces en el Easy pongo los dos valores que me interesan y no agrego el que no me interesa está bien Obviamente que esto en el contexto de que sea mucho más variada la la cantidad de de categorías en este caso son solamente tres supongamos que hubiese 10 se ve mucho más la diferencia de usar esta metodología en lugar de usar el or porque yo pusiera que quiero los que tienen alto o medio o o y por cada una de las otras nueve características o 10 características como el ejemplo sería una expresión mucho más larga con lo cual es mucho más conveniente que en este caso yo pusiese los 10 valores me haría una expresión mucho más corta y hasta mucho más clara y fácil de leer bien entonces aquí nivel de ingresos y sin que el nivel de ingreso esté dentro de esta lista dentro de la lista que tiene los valores alto y medio ejecuto y me parecen solamente los Las observaciones Perdón que tienen los valores alta y medio bien más formas de filtrar con en este caso para valores de tipo texto por supuesto Star with Start with quiere decir que empiece con Es decir me va a buscar todos aquellos elementos O valores de una característica determinada que empiecen con una letra o puede ser más una letra sí es decir puede ser m en este caso puede ser mma bueno o una palabra completa Sí el tema es que lo que va a buscar es que todos esos valores empiecen con esa letra o con ese conjunto de letra en este caso los circunscribí a una sola letra lo cual lo que tomo es Data punto género punto String punto Star with m ahora sé que String se usa porque el dato es de tipo texto y le digo que selecciones solamente aquellos que empiezan con m Sí bueno obviamente me trae más cool y no hay muchas variantes porque es masculino y femenino con lo cual se empieza con m no queda otra que sea masculino pero podría ser que fuese una categoría donde hubiese más de un valor que empiece con m entonces bueno sería mucho más rico el ejemplo simplemente Esto es para que ustedes lo puedan probar y después lo aplique seguramente a un caso más concreto más rígido O quizás un caso simple como este sí lo concreto Es que aquí en preguntando o filtrando de esta manera como filtrando de esta otra que teníamos aquí arriba directamente diciéndole Quiero los masculinos es exactamente el mismo resultado no pero bueno como la el ejemplo que tenemos no es tan variado usamos digamos esta otra cuestión Obviamente que si quisiera otro ejemplo llega cambiar género por otro otra característica por ejemplo a nivel de ingresos y poner el Starbucks con la letra o la letra que yo tenga necesidad bien otro caso es el contains contéis guarda una cierta relación con lo que vimos recién solamente que en el caso de recién veíamos todos los que empezaban con m acá con quiere decir que contenga ese texto en alguna parte del texto completo es decir le estoy diciendo que quiero que busque todos aquellos valores que están dentro de la característica género que tengan las letras i n o juntas pero en cualquier parte del texto al principio del medio al final sí Bueno justamente por la característica masculino y femenino tienen el ino al final si lo estuvieran a medio funcionará igual si lo tuviera al principio funcionaría igual lo concreto es que en este caso ese filtro aplica cualquiera de los dos géneros y por eso me selecciona literalmente Todas Las observaciones bien qué más vamos con algunas opciones que hacen hincapié a la negación de algo hasta ahora son todas cuestiones que van por la afirmativo quiero aquel Que responda a tal patrón a tal valor numérico a tal característica del texto que está el principio que está el medio que esté al final bueno en este caso volvemos al Start es decir Quiero quiero elementos que empiecen con f dentro o valores que empiecen con f dentro de la característica género pero este simbolito de acá adelante que como lo que va arriba de la letra ñ Sí sugiere una negación es decir no dice quiero esto ese símbolo dice no quiero esto O mejor dicho quiero a todos los que no representan este filtro todos los que no son de alguna manera referenciados por este filtro entonces si yo le saco este este carácter olvidémonos que eso existe Es como que estuviera diciendo aquí Quiero todos los que empiecen con f Por ende que todos los de género femenino pero al decirle con este símbolo la negativa o al implicar con este símbolo de la negativa lo que estoy queriendo decir es No quiero todos los que empiecen con f Quiero todos los contrarios a esa lógica entonces me trae como Ven aquí todos los masculinos bien ahora también se puede filtrary obviamente es una palabra muy conocida del ámbito la informática Porque todo lo que de alguna manera generan a través de estructuras como sql estructuras de base de datos de lenguaje de gestión de base de datos en la palabra la tienen más que incorporar para los que no conocen query quiere decir consulta qué es lo que estoy haciendo aquí le estoy consultando al Data una consulta que está expresada aquí quiero que me traiga todas aquellas observaciones cuyos ingresos son mayores a 1000 y cuya edad menor que 35 acuérdense que este símbolo representa un ant o uní esta condición y esta condición bien entonces ejecuto y me trae todas aquellas observaciones cuyo ingreso mayor a 1000 fíjense que aquí aparecen todos los que son mayores a 1000 y cuya edad es menor que 35 ven que en Las edades son todas menores a 35 bien otro tema más transformación de variables bien el tema puede resultar un poco complejo en la forma que está expresada porque aparece el uso de una función lambda esta lambda se usa mucho cuando yo quiero hacer un cálculo sobre la base de en este caso la misma característica Sí vamos a suponer que yo lo que quiero aquí es proponer que el valor de la característica ingreso se incremente en un 20% Sí entonces lo que quiero es que ese valor sea reemplazado por una cuenta que se produce con la misma característica y actualice el valor de esa característica sí Entonces lo primero que hago es redactar una fórmula le pongo como nombre incremento y con lambda lo que hago es justamente es operar sobre la misma característica sobre el mismo valor diciéndole que quiero que x el valor propiamente dicho sea igual a ese valor multiplicado por 120 con esa expresión yo tomo el campo que me interesa en este caso ingresos Data ingreso usted ya esto lo tienen claro como referenciamos cada característica de cada datasette punto transforma si voy a transformar el valor de esa variable cómo lo voy a transformar aplicando la fórmula que está dentro de la variable incremento Cuál es la fórmula que está dentro de la variable del incremento Esta que está aquí Sí bueno Esto es un cálculo dentro de todo relativamente sencillo pero gracias al lambda se puede poner un conjunto de expresiones o una expresión mucho más compleja O hasta una expresión lógica dentro de una misma fórmula Esa es la potencia que tiene esta función lambda que uno diría Bueno aquí directamente lo puedo multiplicar por 1.2 y ya está Sí para este caso particular se podría operar de esa manera más sencilla pero pero en realidad con lambda se pueden hacer cálculos mucho más complejos que eso Y eso no va a ser tan sencillo como este caso que está aquí por eso me corro un poquito corrámonos un poquito de la simpleza de este cálculo que es incrementarlo solamente O multiplicarlo solamente por uno como 20 sino miremos un poquito más allá y veamos que quizás con lambda yo puedo tener un cálculo mucho más complejo Que este y bueno operarlo de este mismo modo bien lo ejecutó y obtengo bueno los valores incrementados por un 20% sí bien qué más short sorte datos que es sordo Qué significa la palabra para quien está en informática obviamente la conocen Quienes no no Por supuesto que es ordenar short quiere decir ordenar Eso es habitualmente en los lenguajes de gestión de de datos para referenciar al ordenamiento que quiero que tenga un conjunto de datos en este caso lo que yo voy a hacer es crear un Data set datazortet nuevo sobre la base de Data y aplicándole el método short- bajo valores qué es lo que le voy a poner entre paréntesis y entre corchetes los campos las características por las cuales quiero ordenar y en el orden en que quiero esta se aplique es decir hacia la izquierda es el primer elemento que va a tomar para el orden o sea lo primero que va a hacer es ordenar todos los elementos por género y luego dentro de género los va a ordenar por edad sí y fíjense que tengo obviamente primero femenino la f antes que la m y dentro de femenino me lo va a ordenar por edad fíjense 1.4 309 367 y etcétera etcétera así con el resto yo le pongo un Head 15 acá Obviamente le va a mostrar sí lo voy mejor como lo va ordenando por edad primero por femenino y después por edad sí bien ahora concatenación y aprendizaje de datos bien para esto vamos a abandonar el Data set que veníamos virtual sí aleatorio que vayamos utilizando y vamos a volver a traer otro archivo en este caso un nuevo archivo que tiene que ver con bueno cata de vinos sí un listado de vinos vamos a elegir aquí vamos a elegir vino tinto Bueno ahí lo trajo recuerden como detalle que esto viene siempre a esta parte y fíjense que estuvimos trabajando todo el resto de la clase con un Data set generado aleatoriamente por nosotros y eso no está aquí porque justamente como se genera aleatoriamente eso trabaja en memoria porque lo fabrique no lo traje de ningún lado por lo tanto ese valor Ese Conjunto de datos nunca va a estar dentro de esta estructura de explorador de archivos como si en este caso donde vuelvo a traer un archivo de disco de esta máquina bien era simplemente un detalle como siempre vamos agregando detalles que es importante que ustedes conozcan y manejen bien bueno y aquí tenemos como siempre la lectura del archivo con risa se ve que en este caso es bueno sobrevinos rojos sí sobre vinos tintos por eso el Data le puse Data rw por Red Wine lo importante en este caso o lo diferente en este caso es que yo estoy usando aquí el complemento SEP que SEP es el tipo de separador en donde viene naturalmente este diferenciados una característica de la otra en el archivo csv regularmente ese elemento es una coma Por eso siempre ASUME el Rich csv que ese archivo tiene como elemento separador de cada uno de los valores de cada una de las características una coma cuando no es es el valor obviamente me da un error entonces cuando no es el valor tengo que especificar a través de ser Cuál es el elemento separador en este caso es punto y coma por lo tanto lo específico de este modo bien y luego hago el 7 avanzamos y aquí aparece bueno la características de grupos de vinos tintos sí bien la acidez la volatilidad los ácidos cítricos los residuos de azúcar bueno De acuerdo a estas características el vino puede hacer calificado con una valoración de calidad que va a ser diferente tenemos acá algunos con cinco otros con seis bueno con el valor la gama de valores que se use para calificar un bip eso es el concepto de valor objetivo o etiqueta bien no nos adelantemos quedémonos entonces que en lo que hicimos acá de diferente es abrir un archivo pero especificándole un separador y ese archivo csv tiene información de una serie de vinos que es lo que estamos observando Aquí bien con lo cual me entero que este datase tiene 12 columnas y 1599 observaciones 12 características y 1599 observaciones luego Bueno lo ejecuto no lo ejecute lo tengo ejecutado de antes lo ejecutamos ahora para siempre poder ver in situ este la ejecución de código con columns puedo ver la los nombres de cada una de las características esto ya lo habíamos usado en otro Notebook Sí seguramente lo deben recordar bien de nuevo Lo ejecutamos y vemos el resultado que es un array donde cerrar esta conformado por todos y cada uno de los nombres de cada una de las características y estos nombres que están aquí bien aparecen acá bien lo que vamos a hacer ahora es importar pero otro archivo que tiene en este caso la información de otro tipo de vinos que son los vinos blancos No acá está mal escrito no son vinos tintos vinos tintos son los que vimos recién son vinos blancos sí entonces voy a hacer clic acá elegir archivos y voy a buscar los whites los vinos blancos bien ahí empiezo importarlo creo aquí en base ahora a wine Quality White no como antes que era el otro en red y vuelvo a repetir el tema del separador porque en este otro archivo más allá que no es el mismo que el otro tiene la misma característica tiene separadores con punto y coma bien lo ejecutamos y hacemos un Head con lo cual ahora yo tengo dos Data sets tengo el Data rw y el Data www al cual le hago un shape para ver cuál es su figura Cuál es su forma y vemos que tenemos igual que el anterior 12 características Aunque menos observaciones y también me fijo los nombres de esa características y puedo comprobar que los nombres a características son exactamente iguales en el orden y en la denominación que en el caso de los datos de los vinos tintos con lo cual tengo dos Data set que son estructuralmente iguales pero obviamente no contienen la misma información porque uno hace avinos tintos y otro abinos blancos Esta es una situación que se puede presentar y me da lugar todo este escenario que ha armado a lo que titulamos recién como tema que íbamos a ver que es el de concatenar con lo cual voy a crear un nuevo ataque que se llama datavinos que va a ser producto de concatenar p de concad es un método de pandas p de concad de El Data rw y el dato aww coma axi 0 por taxi 0 esto es importante yo puedo concatenar Data sets 1 arriba del otro o sea por filas eso lo va a hacer siempre y cuando yo ponga como dice aquí axis 0 también podría concatenar por columnas si en ese caso Serían dos datos que tuvieran la misma cantidad de observaciones pero no la misma cantidad de o en la misma el mismo tipo de características Sí entonces ahí sería como que estuviera ampliando entonces con el otro en ese caso axis 1 porque voy a estar concatenando sí de manera vertical Por decirlo de alguna manera A diferencia de esto que lo estoy haciendo de manera horizontal por filas con lo cual concad sirve para concatenar de cualquier los dos modos como este caso Yo quiero el modo por filas le voy a poner axis igual a cero con lo cual le estoy diciendo primero poneme los Data rw e inmediatamente Cuando se termine hago sale en Data www donde va a quedar conformado un solo Data set que se va a llamar datavinos que va a tener tantas filas como la suma de los dos fíjense que acá pusimos un comentario para que se vea en el primer caso tengo 1599 observaciones en el segundo 4.898 me debería dar un total de 6.497 lo probamos para ver que eso es así y bueno el resultado es exactamente 6497 filas bien concatenamos y mostramos la forma las dos cosas sí bien hacemos un Heat Si no vamos a ver mucha diferencia respecto a cuando mostramos el Data rw Por qué Porque aparecen primero los vinos tintos Entonces si abajo hay vinos blancos o no nunca me voy a enterar a través de un Head sí como voy a entrar a través de un Tail por eso ejecuto un Tail aquí Y ahí sí voy a poder comprobar que los últimos vinos sí son este los blancos y no los tintos bien entonces lo que repasando lo que hemos visto aquí que tenemos dos Data set con una estructura similar y yo he decidido hacer una operación para que para lo que hemos titulado aquí concatenación y aprendizaje de datos concatenar y agregar un conjunto de datos a otro conjunto de datos generando un tercer conjunto de datos con la suma de los dos ahora tengo este sataset datavinos que tiene una característica muy particular tiene un grupo de observaciones de un mismo tipo de dato arriba los vinos tintos y otro de los vinos blancos debajo eso no es lo ideal para poder generar luego con estos datos un buen algoritmo porque me muestra una realidad que no se pueda corresponder justamente con lo que va a pasar Es decir yo no voy a tener siempre los datos donde van a venir todo un grupo de datos de una misma característica y después los otros sí o se deja una casualidad o respondería muy pocas situaciones esto en realidad el mejor algoritmo como ya lo dijimos la clase pasada es aquel que responde a la variedad más rara más amplia más atípica porque después en base a eso va a poder a responder a las que no son tan atípicas sí es decir el modelo que mejor generaliza opciones o alternativas o situaciones es el que mejor va a funcionar para todo en este caso lo que me conviene es mezclar los datos se acuerdan que esto también lo vimos en la clase pasada donde lo que habíamos hecho es utilizar un método que lo que hacía era mezclar los datos de una matriz en este caso lo vamos a hacer Igual también el método de shuffle pero no para los datos de la matriz sino para los datos de un Data set por eso vamos a usar la librería en este caso cycles donde vamos a importar el método shuffled ya les voy adelantando que es una librería que vamos a empezar a trabajar mucho desde la clase que viene cuando empecemos con los modelos de Machine learning Empezando por el de regresión lineal bien simplemente título de adelanto le comento esto pero ahora vamos a lo concreto yo quiero que en Data vinos 1 exista una versión de Data vinos mezclada y vamos a hacer una práctica que nos va a permitir visualizar esto mejor donde vamos a ver obviamente los seis primeros registros pero vamos a repetir esta instrucción muchas veces para que ustedes vean que lo que hace es justamente por cada vez que lo ejecuto mezclarlos y obviamente al verlos lo vamos a ver de un modo diferente ejecutamos la orden vemos aquí el resultado de esta mezcla donde vemos que tenemos el 807 4383 el registro 101 etcétera etcétera están totalmente mezclados porque evidentemente este último es uno de los vinos blancos y este uno de los vinos tintos bien lo vuelvo a ejecutar y fíjense que el orden vuelve a ser totalmente diferente Es más arranca con uno de los tintos y sigue con uno de los blancos bien y cada vez que ejecute esto shuffle me va a proponer va a proporcionar una forma distinta de presentar los datos Y de este modo yo poder tener como dije antes un modelo basado en un conjunto desordenado para que nuestro algoritmo de Machine learning generalice mejor bien Vamos a ver a continuación otro tema que es el de concatenar un gran número de archivos recién vimos que habíamos concatenado dos archivos vinos tintos vinos blancos esto se puede extrapolar a cualquier cantidad de archivos y es muy común que pase esto cuando por ejemplo como en este caso tenemos un csv que proviene de un conjunto de mediciones que se realizan por día entonces voy a tener un ccv por cada uno de los días voy a necesitar en algún momento juntarlos para tener toda la información de un mes o más toda la información de un año sí entonces este tema de concatenar archivos es muy común en el análisis de datos por lo tanto vamos a aplicar ese escenario y vamos a buscar un grupo de archivos csv bien acá tenemos vamos a elegir tan solo 5 para no hacer tan largo El ejemplo Los abrimos ahí mitad de los cinco no se deben de ir mirando aquí para chequear que interesante ir viendo de que van entrando estos archivos recuerdan refrescar Ven para que lo puedan ver concretamente aquí tengo esta situación También es importante verlo es un archivo que subí Dos veces sin querer por supuesto No puedo eliminar bien volvemos al ejemplo ya tengo entonces cinco archivos y mi idea es poder de alguna manera juntarlos en un solo archivo Sí si Yo abro el primero de ellos le puse datavis Lo abro como sale siempre estos archivos Bueno tengo un Head de estos primeros registros le hacemos un shape para ver qué tiene ese archivo tiene cuatro columnas y 1461 observaciones voy a hacer un código a partir del cual voy a tomar en principio ver la cantidad de observaciones que tiene actualmente el primer archivo que tome y luego voy a hacer un Ford para recorrer el resto de los archivos e ir aprendiendo los a este primero con lo cual voy a hacer un Ford y llegamos con esta clase número cuatro del número de la próxima clase vamos a terminar con el módulo de análisis en el caso de python un valor anterior al que se expresa Aquí voy a formar como el nombre del archivo dado que fíjense que los nombres de los archivos son 001 csv00 o sea un número y dos ceros adelante Sí el 1 ya lo tengo abierto Tengo que abrir Perdón del 2 al 5 bien entonces lo que voy a hacer Voy a conformar un nombre que va a estar formado por dos ceros por un y es el que va a arrancar de 2 lo convierto a String porque si no no me va a dejar mal esta cadena más punto csv Entonces cuando iba alga 2 noma va a valer 0 0 es decir este nombre que está acá quiero hacer una expresión que me permita lograr esa denominación una vez que tengo eso qué hago hago un Rich csv de noma Es decir de 002.6b y luego lo que voy a hacer es voy a tomar la cantidad de observaciones la voy a sumar a la cantidad de observaciones anteriores Y así iré sumando lo que corresponda sí y voy a hacer un Comcast de datadis que es el que traía de antes se acuerdan este o sea el primero de ellos El 001 y le voy a sumar por cada ciclo uno nuevo Data dice va a ser el 02 en el principio luego el 03 luego 0 el 4 luego al 05 y siempre con axis 0 para indicarle que lo que quiero hacer es acoplar el nuevo conjunto de datos al anterior pero de una manera horizontal es decir debajo del otro bien y luego vamos a hacer un print donde le voy diciendo bueno por cada vuelta Cuántas filas voy sumando y bueno Cuál es el número de vuelta que estoy dando y al final la cantidad total de filas ejecutó y bueno me va mostrando por cada vuelta ven la cantidad de filas del archivo 2 es 3652 en el caso 3 21 91 en el caso 4 esta cantidad en el caso 5 esta cantidad el total de el archivo número uno más el dos más extremas de cuatro más el 5 me tiene que dar la cantidad total de observaciones de 13.878 por eso hago un shape de Data disk y verifico que eso Efectivamente es así hasta acá llegamos con esta clase número 4 en la próxima clase vamos a terminar con el módulo de análisis de datos nos vemos en ella  Titulo: Clase5 (parte 1) del Curso de Inteligencia Artificial \\n URL https://youtu.be/RJUwGLygrOQ  \\n 1028 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 5 del curso de Inteligencia artificial de ifes esta será la última clase de análisis de datos y en esta primera parte el tema será join de datos empecemos ya con ella  Hola a todos como dijimos en la presentación Esta es la quinta clase del curso de Inteligencia artificial ahora tenemos join de datos que Joyce es vinculación Sí el poder juntar reunir distintos grupos de datos intentando lograr un solo dataset que contenga datos que provengan de distintas Fuentes pero que tengan un vínculo si se acuerdan que en la parte anterior de la clase 4 habíamos tenido la posibilidad de concatenar de manera horizontal grupos de datos donde teníamos los vinos tintos y los vinos blancos aquí en la concatenación es en cuestiones de columnas es decir yo voy a tratar de reunir un conjunto de datos que tiene determinadas características o columnas con otro conjunto de datos que tiene otras Pero tiene que haber un vínculo no es juntar por juntar ni siquiera los Data se tiene que tener la misma cantidad de observaciones pueden tener distintas cantidades de observaciones simplemente lo importante aquí es que tengan un elemento en común un link Sí así que bueno vamos a empezar por abrir como siempre las librerías ponen en este primer recuadro las librerías que necesitamos utilizar que son las de siempre pero le he agregado dos en este caso una que tiene que ver con image con poder colocar qué es lo que vamos a empezar a hacer aquí en este Notebook imágenes en realidad esto es bueno porque por ahí algunos procesos que queremos tener si vemos al Notebook como una suerte de registro de documentación y también de aprendizaje poder tener no solamente textos y código de programación también podemos tener gráficos y los gráficos como sabemos muchas veces que no son los de mapflow quiero hablar gráficos digamos que de alguna nos representen alguna idea que nos ayude a entender un determinado concepto sí gráficos de tipo jpg por eso los corro de lo que tiene que ver con Maple porque ellos Ejecutan a partir de un conjunto de datos es simplemente una ilustración que yo quiera tenerla aquí como algo que me va a ayudar pedagógicamente a tener un concepto bien entonces lo que hago es importar esa imagen y la van a ver ahora en pocos en pocos momentos más como se ve como cualquier imagen en cualquier medio habitual Como se muestra un archivo de tipo jpg png bmp o similar y luego voy a empezar a poner aquí el from Google files qué es lo que uso y repito cada vez que importo un archivo bueno para ponerlo la primera vez y después no tener que repetirlo más bueno sin más palabras ya importa las librerías y vamos a empezar por importar una serie de archivos vamos a ir a la carpeta Universidad y vamos a importar alumnos y cursos pero a la vez vamos a importar también estas cuatro imágenes que es lo que les hablaba recién de los archivos de tipo png y ya vamos a usarlo más adelante por ahora solamente los importamos si vamos a aprovechar este tiempo para hablar de Cuál es la idea de este conjunto de alumnos csv y cursos ssd concretamente hacemos referencia a alumnos que están tomando determinados cursos y también un csv donde están el detalle de esos cursos bien Aquí vamos a empezar a ver el primero de ellos Sí así que vamos a crear un datase dadal a partir de abrir el contenido del archivo alumno csb ejecutamos y vemos el detalle de si tengo un número de ID de cada alumno Juan Pérez Juana Pérez Juan Perea bueno no soy muy reiterativo los nombres Pero bueno es para hacer una taza rápido y aquí está el número de curso que está haciendo cada uno de estos 15 alumnos con diferencia de los últimos tres que no están haciendo ningún curso bien luego vamos a abrir el curso ccv y lo vamos a poner dentro ejecutamos y vemos el contenido son solamente cinco registros de los cinco cursos de dataseum bien con un número de docente y el número del ID de cada curso que es Ni más ni menos que el dato que está en esta columna Aquí vemos que Juan Pérez está haciendo el curso 10 con lo cual esto quiere decir que está haciendo el curso de python sí del mismo modo Juana Pérez está haciendo el curso número 12 con lo cual quiere decir que está haciendo el curso de yang bien Y así sucesivamente obviamente esto es un ejemplo muy sencillo no intentamos hacer algo aquí que esté súper normalizado como implica el diseño de una estructura de base de datos simplemente un ejemplo con un link para poder abordar el tema en cuestión de hoy que es el de join de datos bien aquí entonces vemos por primera vez esto que les hablaba hoy de la importación de imágenes a través de emails que es lo que yo importe aquí arriba esta línea de acá voy a poner Fight name igual y entre comillas el nombre del primer archivo que vamos o que nos va a ayudar a develar el primer caso de Joe Inner joint que es Inner join es lograr un Data set que sea la resultante de todos aquellos registros u observaciones que tienen algo en común aquellos que no tengan un elemento que los vincule no deberían salir en este caso el a tiene 15 filas que es lo que vemos aquí arriba en dataset B tiene cinco filas pero ambos comparten nada más que 12 filas porque porque aquí hay tres alumnos que no tienen ningún curso por lo tanto no hay vínculo en el caso de estos tres alumnos entre un Data y el otro lo que tenemos entonces aquí es que en Data se resultante va a ser aquel que muestre el detalle de todas las observaciones que de un Data set y de otro tienen algo en común bien ese gráfico fíjense que me muestra si cruce de dos Círculos que responden o que representan cada atasette y lo que me muestra que es la intersección de ambos es decir la resultante de aquellos que tienen algo en común bien Cómo se escribe un Data set que busca ese resultado con merge método de pandas acuérdense que siempre la libera Panda es la que me propone todo tipo de que tiene que ver con el manejo de datos bien tengo que poner left y write que es una figura de alguna manera lo que representa el dataset que está a la izquierda y el que está a la derecha esto es indistinto puede ser cualquiera de los dos sí en este caso el que está a la izquierda o sea sería este el de alumnos y el que está a la derecha sería este y luego poner como quiero que se produzca ese merge ese vínculo esa mezcla Sí esa relación con la forma Inner Qué quiere decir Inner Bueno lo que describimos recién aquellos que tengan algo en común luego lo que tengo que especificar es cuál es el elemento de Unión es decir Qué elemento tienen en común y cómo se llama en cada caso bueno en el dataset left es decir el a es decir el de alumnos el elemento vinculante se llama curso por eso aquí abajo Pon left on y en el caso right elemento vinculante es esta columna que como se llama novio Entonces pongo nula y luego muestro el Data resultado lo ejecuto y lo que resultado me arroja primero las 12 columnas Sí en acero a la 11 son 12 y lo que me muestra es el dato de el número de la lupa el nombre de la luz el número del curso lo muestra dos veces podría Eso después obviamente filtrarlo para Que aparezca una sola vez En nombre del curso y los datos o todas las características de un dataset junto con todas las características de las dudas pero siempre y cuando haya un vínculo fíjense que el Mostrar estas dos columnas justamente me Proponen la idea de mostrarme que todo lo que se relaciona se relaciona porque tienen ese valor en común Sí y lo que aparece de un lado y del otro es lo que se relaciona a partir de ese valor en común es decir al lado Luis Soto no aparece cualquier curso aparece el curso que tiene registro él lo busca y me trae el resto de los datos de ese curso bien Esto es Inner joint vamos al caso ahora del raid join Qué quiere decir el ride joint es mers pero en donde se le da prioridad al Data hacer que está a la derecha por eso lo de raid por sobre el de la izquierda Entonces tengo el Data set a que tiene 15 filas tengo que tiene cinco filas no cambia porque es la cantidad de fila que tiene originalmente ambos comparten 12 filas es decir tiene un elemento de Unión que son 12 filas pero el Data es el resultado son 14 filas por qué bueno fíjese vamos a los datos originales en este caso Tengo 12 alumnos que hacen tres de estos cursos con lo cual está claro que la unión propone como vimos en el inercial recién 12 registros resultantes pero pero hay dos cursos el 4 y el 5 que no los está haciendo ninguna luz pero justamente como esta idea de hacer que está a la derecha y es el prioritario la idea Cuál es del natación de la derecha como muestra acá muy claramente el gráfico tienen que aparecer todos los registros y luego los que están de alguna manera vinculados a los datos como los que están vinculados adentro de hacer están dentro del conjunto B lo que me está mostrando aquí es la resultante de todos los que tienen en común los 12 más los dos que están en el B pero no están dela bien Obviamente que para hacer esto llamé a otro gráfico Perdón omití ese tema ese detalle ejecutamos y vemos que nos muestra ese gráfico que habíamos recién y hacemos el merge y nos muestra la resultante donde vemos los 14 registros fíjense cómo me los muestro en el caso de los primeros 12 Vamos hasta Sí hasta aquí menos muestra igual que como los veía en el caso anterior sí del 0 al 11 el 0 Pero los dos últimos me muestran como tiene prioridad el Data de la derecha todos los datos completos de cada observación del tratante de la derecha Es decir de este del de los cursos pero como no hay alumnos que usen esos cursos o que estén cursando esos cursos no tengo datos de alumnos que estén o sea soporte Entonces todos los datos de los alumnos los muestra como Nan acá aparece el famoso Nan que vimos en la clase pasada del valor nulo es decir cuál es el número de la luz nulo el nombre nulo el número de curso nulo porque no hay ningún alumno asociado al curso reac ni tampoco el curso sql bien la opción que sigue es igual a esta última pero al revés en lugar de tener prioridad en Data set de la derecha tiene prioridad de la izquierda por lo tanto en este caso tengo las 15 filas de uno las cinco filas del otro las 12 que comparten pero el tercer resultante es de 15 porque porque como prioriza el de la izquierda y el de la izquierda tiene 15 Bueno me muestra los 15 dentro de los cuales están los 12 que tienen en común ejecutamos esto para ver el gráfico ya está ejecutado Pero bueno insisto Está bueno que lo vuelvan a ejecutar por cualquier problema que puedan haber y ejecutó el merge y aquí está resultado los 15 Recuerden que en este caso tienen que ponerle como antes pusieron right y como antes pusieron Inner Sí o sea siempre con el House le digo esto de dónde está la prioridad Inner la primera lo tienen los dos Sí o sea el elemento en común de alguna manera cuando pongo raíz la primera la tiene el derecho y cuando pongo el resto es igual left right left en los tres casos es Exactamente igual aquí es igual a que es igual lo que cambia es el que es el que me dice How como como se relaciona Cómo cómo se promueve ese merge sí Y perdón y aquí en left y obviamente me muestra el resultado en cada caso obviamente en este caso es al revés al anterior en cuanto a los Nan los Nan de qué lado aparecían en el caso anterior de la izquierda porque la pelea lateral de la derecha en este caso en la prioridad la tiene el de la izquierda los nada parecen del lado a la derecha sí lo importante aquí es entender que los Nan aparecen a partir de que hay una relación o hay relaciones que no se dan o hay alumnos que no están cursando determinados cursos o hay cursos que no tienen ningún alumno si se diese la circunstancia de que todos los alumnos cursaran algún curso de manera tal que no hubiese ningún curso que no tuviese ningún alumno bueno esta relación se daría de un modo donde no existirían los nanos porque no habría ninguna relación imposible todas las relaciones serían posibles acá sí de relaciones imposibles porque hay alumnos que no tienen ningún curso y hay cursos que no tienen ninguna luz sí obviamente Esto está hecho de manera adrede para que ustedes puedan ver esta cuestión de que los Nan a partir de que esta relación puede no llegar a darse youther joint es lo contrario es aquello y fíjense que desde el gráfico que lo voy a ejecutar como siempre Para probarlo y diferente el primero el líder era estrictamente la conjunción de ambos grupos en el outer esto lo contrario es todo de la izquierda y todo a la derecha sí es la combinación total por eso el de la tercer resultante es el numéricamente más significativo porque tiene todos los alumnos y todos los cursos en el caso anterior tenía Qué cosa todos los alumnos más no todos los cursos en el anterior todos los cursos más no todos los alumnos en el ante anterior tenía Solamente los que tenían en común ahora me tienen que aparecer todos de la izquierda y todos de la derecha por eso lo que voy a usar en el jabón en este caso es out el resto idéntico a los tres casos anteriores lo ejecuto y veo resultado donde me aparecen absolutamente todos los casos los casos los 12 que tienen relación los tres alumnos que no tienen curso y los dos cursos que no tienen alumnos hasta aquí llegamos con esta primera parte en ella vimos Cómo crear un conjunto de datos a partir de otros conjuntos que aprendimos a relacionar nos vemos en la segunda parte  Titulo: Clase5 (parte2) Curso de Inteligencia Artificial \\n URL https://youtu.be/SOvyzc_pwoI  \\n 1681 segundos de duracion \\n Hola  bienvenidos Esta es la segunda parte de la clase número 5 del curso de Inteligencia artificial de ifes en ella veremos el último tema de análisis de datos el web scrapping empecemos  bueno y ahora vamos con el último tema de toda esta serie de clases vinculadas análisis de datos que es web scrapping que es web scrapping bueno todo lo que hemos visto hasta ahora tiene que ver con Data sets que bueno venían de algún origen que estaba a nuestro alcance algo que tiene que ver con datos que provienen de Nuestra Empresa de nuestro proyecto datos que provienen de sitios públicos que nos dan la posibilidad Como cada árbol por ejemplo de brindarnos Data hechos que nos permiten entrenar algunos modelos y empezar a ver también cómo adaptarlos a la necesidad nuestra vimos todo tipo de operaciones que podíamos hacer con ellos para que tengan la forma que nosotros queremos que tengan Pero hay otra situación que es la posibilidad de aprovechar la gran cantidad de sitios web que manejan un cúmulo de información muy importante para nosotros es fundamentalmente que tienen un alto impacto en la comunidad por ejemplo los sitios de venta de cualquier tipo de artículos Como por ejemplo MercadoLibre bueno tipo de sitios tienen una información muy importante que nosotros no podemos tenerla en un formato de un archivo csv salvo que alguien lo haya hecho por nosotros pero digamos en primera intención ese tipo de páginas no nos dan un Data hacer porque es información Privada de ellas por supuesto Pero hay técnicas hay metodologías y hay librerías de python que justamente nos permiten Esta técnica de web scrapping tomar información de su sitio web y poder bueno lograr tener un formato de información que sea más amigable sí o más adaptable a nuestras necesidades en virtud de lograr un modelo bien Esta técnica la vamos a empezar a ver tomando como ejemplo justamente el sitio de MercadoLibre vamos a él y voy a poner aquí por ejemplo la búsqueda de zapatillas Sí bueno como siempre como cualquiera nosotros hace habitualmente pongo la palabra hago buscar y vamos a mirar aquí arriba Cuál es la url que escribe para esto sí Más allá de que aparece obviamente la búsqueda de las zapatillas veo que lo que escribe acá arriba es la denominación oficial del sitio más la palabra de búsqueda que yo puse zapatillas un numeral una letra de mayúscula y luego entre corchetes y luego de una y dos puntos vuelve a repetir la palabra de búsqueda que yo puse vamos a complicar un poquito El ejemplo Y en lugar de poner una palabra vamos a poner dos palabras Te voy a poner zapatillas deportivas y vamos a ver qué pasa con esa URL la URL ahora lo que hace es poner la expresión que yo puse Donde antes ponía solamente zapatillas pone la expresión pero como está conformado por dos palabras no por una pone un guión en el medio es decir que reemplaza este espacio en blanco por un guión y luego aquí en la otra parte donde repetía la palabra de búsqueda hace lo mismo pero lo hace con digamos deja el elemento natural que es el separador pero lo reemplazan por un porcentaje 20 en realidad esto si yo me tomase el trabajo de pararme aquí y Borrar esos elementos y poner la descripción en el formato natural que yo puse en la llave de búsqueda ven que está el espacio aquí en el medio le doy enter y fíjense que el porcentaje de 20 me lo pone solo automáticamente Bueno ya tenemos un primer este elemento importante para empezar a ahora desde el código poder ver cómo puedo acceder esa información por lo tanto me vuelvo al Cola y lo primero que voy a ver Es que voy a usar aquí la librería request que es una librería de python que me permite tomar información de un sitio web y cuando me refiero a tomar información me refiero a tomar la información que tiene que ver con el diseño del sitio web con diseño de la página con el código con el cual yo veo esto es decir qué código es lo que me muestra todo esto bueno eso lo puedo acceder con request y luego la librería beautiful supp que es una de las tantas librerías que existen una de las más famosas de hecho para poder hacer web scrapping Entonces lo primero que hago aquí una vez que bueno escribo el código vinculado a estas importaciones ejecutarlas como siempre hacemos entonces empiezo por eso ya es tan importadas rico es y beautiful bien Ahora vamos a acceder a todos los elementos de la primer página lo vamos a hacer aquí poniendo un input para que nosotros podamos escribir en el contexto de este Notebook el texto que deseamos Buscar como recién escribimos zapatillas zapatillas deportivas por eso creo una variable Buscar a la que le pongo un input que quiere buscar bueno Esto es básico no bien luego lo que voy a hacer es a través de request get que es request como decíamos recién la librería request que acabamos de importar aquí arriba me permite a través del método que tomar todo el contenido el texto con el cual fue escrita la página de cualquier página poniéndole simplemente en la URL ahora cómo voy a escribir la URL la URL voy a escribir igual que como la tenía aquí solamente que voy a usar un pequeño truco que es donde yo debería poner los elementos de queda si se acuerdan después de la barra Sí aquí voy a poner dos llaves y lo mismo voy a hacer después del a dos puntos donde volvía a repetir el texto de búsqueda y lo que se hace en este caso son como comodines que luego con el método format se pone la expresión que quiero que vaya allí la expresión que quiero que vaya así es la variable Buscar es decir este Buscar va a ocupar este lugar y lo mismo va a ocupar este otro lugar eso lo hago con punto fortnite solamente que en el primer caso yo tengo recuerden la situación de que el espacio lo rellena con un guión por lo tanto lo que debería hacer sería poner una expresión en este caso la variable Buscar punto replace que es un método de python que lo que hace o lo que me permite hacer es reemplazar todos los elementos de un tipo en este caso este elemento vacío por un guión no un símbolo igual como puse aquí es un guión por un guión y luego Obviamente el otro Buscar lo dejamos como está porque como ya dijimos recién Por más que yo lo ponga con espacio en blanco lo va a reemplazar automáticamente al escribir la URL por un porcentaje y el 20 bien con todo esto tengo como rearmada la expresión de la página web pero no ya con una expresión fija aquí como dice zapatillas deportivas y está expresada de este modo sino con dos elementos variables que van a depender pero que yo escriba aquí en este input si escribo zapatillas deportivo va a ser esa búsqueda y si no la búsqueda que corresponde luego lo que hago es acceder a todo el contenido de la página a través de qué a través de r.content porque r.conted porque justamente el request este que hice acá lo puse dentro de una variable que bien Llamar R con r.content traigo todo el contenido y lo como se dice en informática lo parseo Sí con html parcer que es poder acceder a la estructura en cómo está Armada esta página web Que obviamente está Armada con elementos de html y javascript a mí lo que me interesa en este caso es Ver todas las estructuras html que me van a permitir acceder a los datos que estoy visualizando aquí entre ellos por ejemplo la denominación del producto y el precio para ello vamos a recurrir a un truco que probablemente los que están acostumbrados al desarrollo web manejando código html estén acostumbrados y bueno el que no lo sepa quizás ahora puede aprender algo nuevo más que es darle clic aquí con el botón derecho y acceder a la opción inspeccionar con inspeccionar lo que me muestra a mí es el código con el que está hecho esta página web Sí para probar esto vamos a hacer lo siguiente fíjense Que aquí hay un icono que si me paro encima me dice que con esto Una vez que opciono por esta herramienta puedo seleccionar un elemento de la página para inspeccionar que es esto que me muestra aquí la derecha Cuál es el código con el cual está construido fíjese que me voy a parar por ejemplo sobre Esta zapatilla y me muestra ven acá a la derecha el link al que me lleva cuando clic en esa imagen más allá de que me muestra referencias respecto a la imagen como el tamaño si me voy aquí abajo de que se que me muestra viene a la derecha el nombre que estoy viendo aquí en la página web topper coreo hombre adultos lo mismo con la unidad de al lado clic sobre la imagen no es clic es para nosotros la imagen Sí y veo la información relativa cómo está construida esa esa estructura que estoy viendo allí con la imagen y con el link que tiene asociado y lo mismo aquí abajo lo que tengo que ver ahora a través del código es donde está la estructura de referencia que me va a permitir seleccionar cada uno de estos elementos en el contexto de esta página para ello vamos a hacer lo siguiente vamos a recorrer esta estructura y vamos a descubrir que hay una sección con un dip que es una de las estructuras clásicas html cuya clase que es lo que identifica a la estructura justamente de ese dip se llama andes Card Esta es la primera referencia que voy a tener a partir del cual todo lo que esté dentro de esta estructura van a ser justamente cada uno de los artículos sobre los cuales Yo quiero extraer información estos datos son fundamentales Deep y and the scart porque justamente van a ser la referencia que yo voy a tomar para que desde el código pueda acceder a esa información me voy al colap y justamente veo que ahora solo el objeto sup que es el objeto suv el que tiene como vimos en esta línea anterior todo el contenido de la página voy a hacer un find all Buscar todo Buscar todo que buscar todos los VIP cuya clase sea and descare con esto lo que voy a estar haciendo es Buscar cada una de estas divisiones estos dips se refiere a División Sí y dentro de ello justamente Buscar todos los que tienen el estilo and desgard porque porque hay divisiones que no responden a esto estos iconos que están aquí arriba esta información que está al costado Yo solamente quiero la información de los productos sí Entonces le digo que me find all busque todos los elementos de que de suv que suv el contenido de la página completo y allí que quiero que busque todos los dips cuya clase Se han descargado una vez que tenga eso voy a pasar al siguiente paso el siguiente paso va a ser crear un array vacío como está aquí donde voy a depositar todos los productos y voy a hacer un foro de nuevo la estructura Ford para que para recorrer a través de la variable ítems nombre de variable puede ser cualquiera y yt el que ustedes se le ocurra in deeps Esto sí es importante que es dips deeps contiene aquí arriba lo que dijimos recién toda la cantidad de dips que hay dentro de la página de nuevo esto es un nombre variable le puse dips por una cuestión lógica le pueden poner como quiera for item in deeps O sea recorro sí cada uno de los dips en cada caso lo que voy a hacer va a servir a un diccionario aquí estoy creando Data donde voy a poner dentro de ese diccionario dos elementos un nombre de artículo y un precio Sí ese nombre de artículo a su vez está en una parte de El div y ese precio también está en una parte del dip Cómo sabemos Qué información ponerle para acceder esa información de nuevo tengo que usar la opción de inspeccionar la página web para ver en qué parte Guarda esa información dentro del código html para ello la página y vuelvo a la parte de inspección recuerdan que nosotros teníamos si veníamos de esta estructura de andes Card y voy a volver a hacer lo mismo marco este icono de aquí y voy a marcar la parte que me interesa ahora mirar que está de aquí abajo cuando está el nombre del producto y el precio Sí bueno al hacer clic acá veo que aparece h2 h2 es un header de nivel 2 que es otra estructura clásica html en sh2 tengo el nombre del producto topper Drive hombre adultos que es lo que está aquí y si yo sigo con la misma lógica buscando es información y bajo puedo ver aquí bajo veo que ya en la medida que voy bajando me voy parando sobre estas partes me va ubicando Ya ven el precio Sí pero está dentro de estructuras hasta llegar a la que concretamente tiene el precio que es esta última mil Perdón 14.816 es una estructura de tipo spam insisto otra estructura clásica de html pero yo tengo una información que me hacía falta tengo la información de que el precio está dentro de una estructura de tipo spam y tengo la información de que el nombre está dentro de una estructura de tipo h2 Por ende me voy ahora si al código y justamente lo que hago ahora es hacer un ítem punto fine porque ítem recuerden ítem De dónde viene es cada uno de los dears O sea yo con ítem Estoy parado en uno de los dips dentro de uno de los dips por ejemplo el primero el de la zapatillas que estamos mirando recién En esta topper si le voy a pedir que me busque que me fine si me busque un h2 de clase we Search item que es justamente lo que demuestra aquí ven y que allí me tome el texto lo mismo con el precio donde voy a poner sobre ítem find Búscame una estructura de tipo spam de clase price tag Mount que es esta estructura que veíamos recién aquí debajo Price price sí que es el spam que tiene o que contiene el valor entonces una vez que y siempre tomo el texto Perdón una vez que tengo el título y el precio lo pongo dentro de uno de los elementos de este diccionario que Acabo de crear qué diccionario va a tener el nombre del artículo y el precio de que de cada uno de los ítems Es decir de cada una de las divisiones de la página que acabo de visualizar una vez que tengo eso hago una pen de ese diccionario Dentro de este array que Acabo de crear Sí y hago un print para esto bueno de alguna manera ir mostrando El Avance y el recorrido de todos los elementos Sí ya tengo todo lo que necesitaba con lo cual lo que voy a hacer ahora es ejecutarlo y lo que me va a pedir es que objeto Quiero buscar y le pongo zapatillas deportivas vean y me va a mostrar cada uno de los productos de la página de MercadoLibre sí bien no siempre estos elementos van a aparecer en el mismo orden que ustedes lo van a visualizar Aquí vamos a cerrar esta parte porque bueno tampoco ustedes Cuando entren muchas veces a hacer búsquedas van a encontrar los elementos del mismo modo esto tiene que ver con una cuestión de competencia Obviamente si estuviesen los productos algo que esté destacado primero obviamente ese producto sería favorecido en la hora de elegir por parte del usuario Bueno pero lo concreto que aquí tenemos lo que queremos que es el dato de el nombre del producto y su precio lo tenemos en un array que tenemos que hacerlo pasarlo a un Data frame eso ya lo hemos visto en clases pasadas Así que lo podemos hacer perfectamente Y a partir de tener el Data frame puedo empezar a trabajarlo bueno haciendo o buscando un modelo de lo que yo necesite hacer justamente porque ya tengo La Estructura de datos similar a como la tenía antes Obviamente con un trabajo mucho más llevadero pero finalmente lograr el mismo objetivo pero resulta que me falta una parte todavía de la historia porque porque yo tengo como dije al principio aquí la información de los elementos de la primera página no de todas las páginas ustedes ven que esta búsqueda arrojó y lo ven aquí debajo de la página un número de 42 páginas está simplemente la primera las cuales voy a visualizando haciendo clic en siguiente pero fíjense que cuando yo voy haciendo clic en siguiente la URL cambia ya no es como la que teníamos antes y fíjese que tiene una estructura que dice que va desde un número en adelante desde un número no Index true donde Esto va a ir cambiando conforme yo vaya avanzando de página fíjese que ahora dice desde 97 voy a la siguiente ahora dice de 145 es decir va haciendo un ciclo un ciclo que desde 97 145 me da la Pauta que tengo 48 elementos por cada una de las 42 páginas eso es lo que voy a trabajar ahora Y por eso voy a crear otro código tomando este como base haciéndole algunas modificaciones que tienen que ver con el formato de este la página web de la URL de cada una de las páginas que va mostrando conforme yo vaya pasando a las siguientes páginas de los productos y después ver cómo voy tomando eso para que justamente vaya a parar también a un array como fue la primera página Pero ahora con todas bien con esto lo que voy a hacer es arrancar la primera parte con el mismo código que tenía antes sí eso no cambia demasiado y luego lo que voy a hacer va a ser escribir un código para obtener un número importante que cuál es este dato que está aquí por qué Porque yo ahora voy a tener que no hacer un Ford para recorrer todos los elementos de una página sino antes que se hacer otro fuerte recorra todas las páginas y dentro de cada página todos los elementos de cada una de ellas por eso acá en el código Yo ya voy a ir viendo que tengo dos foros el que tenía antes y el que tengo ahora que me va a permitir recorrer como dije recién todas las páginas Entonces lo primero que hago es Con este código tratar de averiguar justamente Cuál es el dato de la página tengo que buscarlo a través con fine de Sub como lo dice antes sí se acuerdan lo que habíamos usado acá find en este caso de ítem Porque recorría todos los ítem este caso es find porque estoy todavía parado en todo el contenido de la primer página Buscar ese número pero ubicándolo siempre la estructura de html por lo tanto tengo que buscar ese elemento En qué tipo de estructura está voy a si lo lo dejo como tarea de ustedes si hago una inspección voy a ver que es una estructura de tipo lee como fue antes que detecte una estructura de tipo h2 una estructura de tipo spam ahora es una lectura de tipo Los invito a poder chequearlo y justamente hay una clase que es andes pagination pass code donde voy a tomar ese texto ese texto no es Ni más ni menos que esto que está aquí de 42 Qué pasa a mí me interesaba el 42 entonces lo que voy a hacer es hacer un Ink de las page que es el valor que estoy recogiendo este text que estoy recogiendo que dice de 42 las page replace de espacio con nada Es decir reemplazar de e y el espacio pero igual 42 por nada con lo cual me queda nada más que el 42 sí Y entonces ya en la variable tengo el elemento 42 esta estructura trae SEP que es obviamente una estructura para validar errores la pongo justamente para eso por qué Porque puede que una búsqueda que no arroje varias páginas se haga una búsqueda muy restrictiva me va a dar una sola página y en el caso que haya una sola página este valor obviamente no va a estar bien entonces por eso hago un Trade Excel para que si esto no es exitoso si no hagas búsqueda porque no va a tener sentido bien luego también vuelvo a crear pros como antes el array se acuerda que tenía aquí arriba antes eray plots y lo que hago es como dije antes sí antes de hacer el Ford que teníamos para recorrer todos los dips de una página hacerlo concerniente para ir tomando cada una de las páginas dado que son más de una entonces hago un Ford variable cualquiera le llamo page in rage que va de 0 a las page Es decir desde la página 0 hasta la 41 acuérdense que el Range cuando yo pongo 42 que en este caso es el valor que tiene las Face pasaría 41 y a partir de allí lo que hago es bueno una referencia simplemente que ponemos para que nos aparezca el valor lo que hago es trabajar ahora con esa URL también dinámica pero con este cambio ya no es la url que tenía antes esta que estaba aquí sino está con el nuevo Index tal cual lo vimos recién acá entonces luego lo que tengo que hacer es volver a poner aquí así como puse antes los textos de búsqueda lo que tengo que poner aquí es el número este que va a ir variando conforme vayan avanzando las partes cómo logro ese valor Bueno lo logró multiplicando a page que va a arrancar de cero por 48 + 1 es decir que page por 48 + 1 en el primer caso que me va a dar Sí entonces de allí en más va a ir escroleando todo el resto para lograr el siguiente número el siguiente número page va a ser uno por 48 va a ser 48 más 149 que es si vuelvo aquí es este valor que está acá bien entonces una vez que tengo eso lo que hago bueno estos print insisto son más que nada para que ustedes vayan viendo El Avance dentro del código no son necesarios de que de que sí o sí los tengan que poner pero bueno está bueno cuando lo vayan probando que van a verlos aquí que justamente el efecto que se busca es ese bueno y ahora tengo el request como habíamos hecho antes enriquez pues de la página la primera página voy a hacer lo mismo ahora para lo que son cada una de las urls que voy a ir formando conforme vaya avanzando cada una de la selección de las páginas luego lo que hago es parcial el contenido le digo que me dé todos los dips esto ya es parecido a lo que veníamos haciendo en el caso anterior se acuerdan de estas dos líneas Perdón esta línea de acá lo mismo aquí ahora lo hago para todas y cada una de las páginas que voy ciclando y luego el resto del texto es Exactamente igual a lo que veníamos trabajando insisto cómo lo hacía antes yo antes tenía una sola página esta y bueno tomaba todos los elementos parciaba tomaba todos los elementos de allí pedía que me identificara todos los deeps y los recorría y cada vez que los recorría iba tomando cada uno de los elementos aparte ahora es exactamente lo mismo estas dos líneas son las mismas lo que cambia es que en este caso estas dos líneas digo dos porque los primos obviamente no cuentan lo que hacen es ir dándome las páginas una por una dado que antes lo hice con la primera y ahora lo quiero hacer con cada una de ellas el resto es Exactamente igual con lo cual lo vamos a ejecutar pero pero vamos a hacer un pequeño cambio aquí porque los invito a ustedes se lo quieren probar Pero obviamente Esto va a ser extremadamente pesado porque va a tener que mostrarme 42 páginas por 48 elementos y va a ser muy largo vamos a poner una búsqueda mucho más restrictiva Sí vamos a poner a ver si hacemos esta búsqueda que no sea tan restrictiva como que no nos arroje ninguna página más que una bueno evidentemente acá voy a tener tres páginas una una buena búsqueda y es posible que me salga un resultado interesante Sí bien entonces lo que hice aquí es poner camisetas de chacarita para que se copia y pego sea más rápido para ejecutar esto así que ejecutamos el código no voy a pegar acá Así también trato de no equivocarme no en la escritura Obviamente si el filtro que pongo diferente resultado no va a ser el mismo así que eso trato de evitarlo de esta manera bueno acá fíjense que me pone 0 y ya me pone el primer la primer URL y después de allí me da ven uno la página 1 la página 1 y la URL la segunda URL que toma y todos los valores la página 2 la URL de la página 2 y hasta donde llega son 103 productos sí los que me muestran en tres páginas sí evidentemente yo recorro acá voy a ver que la tercera página obviamente no debe estar completa segunda página tercera página ven tiene menos productos sí tiene siete por eso que el número final me da 103 bien lo que tenemos entonces con esos print es la posibilidad de ver esto En qué número de página voy y cómo me va formando la url para el primer caso para el segundo caso y para el tercer caso luego de esto insisto tengo toda esta información dentro de un array y la puedo convertir tranquilamente un csv y empezar a operar Bueno hasta aquí llegamos con esta explicación de lo que es el web scrapping es algo interesante es bueno para que puedan probar Obviamente el que no salga html puede que le cueste un poquito más pero bueno tampoco es algo extremadamente complejo quien quiera empezar a tener conocimiento html poder este hacerlo con cualquier este recurso público y el que está al alcance de la mano en redes sociales para poder verlo por lo menos los conceptos básicos que son los que hemos tratado aquí atentos a poder identificar las partes de una página y poder tomar allí la información Bueno hasta aquí como dijimos recién estaba en clase y Bueno nos vemos en la próxima hasta aquí llegamos con la clase número 5 y con la misma también terminamos con el módulo de análisis de datos de este curso ahora estamos capacitados para darle forma a nuestro datasette y con ello poder desde el próximo módulo comenzar a desarrollar los mejores modelos de Machine learning nos vemos en la clase 6 para empezar con ello Titulo: Clase6 (parte 1) del Curso de Inteligencia Artificial \\n URL https://youtu.be/wHbg-O1P-Nc  \\n 1356 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 6 del curso de Inteligencia artificial de ifes con ella iniciaremos el módulo de Machine learning y sus algoritmos más conocidos en esta clase veremos el primero de ellos la regresión lineal empecemos  Hola Bienvenidos a la clase número 6 del curso de Inteligencia artificial de icfes Y empezamos a partir de esta clase con el módulo de Machine learning tal cual lo dijimos en la clase pasada cuando terminamos justamente el módulo de análisis de datos y rápidamente vamos a recuperar información de la primer clase de este curso donde nos preguntábamos si hablábamos Qué es este concepto de Machine learning bien Es una rama de la Inteligencia artificial que estudia como dotar a las máquinas de capacidad de aprendizaje Cómo logra esto lo logra a través de un conjunto de datos que yo le doy en la máquina y la máquina con ello aprende y detecta patrones a partir de los cuales puede elaborar predicciones es decir llevar a cabo lo que llamamos también en aquella primer clase análisis predictivo en esta cuestión de comparar elementos y no confundirnos en los términos podemos darle una respuesta a esta pregunta que ponemos Aquí Cuál es la diferencia entre la Inteligencia artificial y el Machine learning bien la inteligencia arti ficial es como dijimos al principio la capacidad de que las máquinas puedan pensar como seres humanos ahora el Machine learning lo que hace es darle una técnica que mejora esa capacidad para establecer como dijimos antes un análisis predictivo es decir predecir acciones que aún no han ocurrido y que van a tener lugar en el futuro Qué tipos de algoritmos se usan para Machine learning Existen tres tipos en principio el aprendizaje supervisado en el cual la máquina aprende por los datos de entrenamientos que están etiquetados concepto de etiquetado que abordamos en la primer clase que de alguna manera también hicimos referencia en el módulo de análisis de datos y que vamos a terminar de redondear justamente aquí en este módulo de Machine learning luego el aprendizaje No supervisado que es justamente lo contrario al anterior desde su denominación se entiende porque justamente trabaja con datos que no están etiquetados y finalmente el aprendizaje reforzado en donde la máquina tiene un nivel de autonomía en el aprendizaje superador a los dos modelos anteriores ese módulo en este curso lamentablemente no lo vamos a alcanzar a ver Cómo funcionan los algoritmos supervisados bien Aquí vamos a dar un ejemplo donde vamos a empezar a darle más fuerza al concepto de etiqueta a la máquina yo le voy a dar una imagen como la que está viendo aquí y le voy a dar una etiqueta Qué quiere decir le digo a la máquina esa imagen corresponde a un elemento que se llama átomo a partir de eso en la máquina prende y ya tiene un patrón de referencia en el cual sabe que esa imagen se etiqueta bajo el nombre de un gato más esa máquina que aprendió va a recibir nuevos datos Y tiene que tratar de identificar de qué se trata sin la etiqueta fíjense que yo ahora lo que le estoy mandando es la imagen del gato más no le estoy diciendo que es un gato pero la imagen con la información del entrenamiento anterior es decir está ya tiene esa referencia por lo tanto de ahora en más la imagen sabe que corresponde a una etiqueta y lo que me responde justamente es que esa imagen corresponde a la de un gato ahora bien Qué tipo de problemas se pueden resolver con un algoritmo de tipo supervisado en principio problemas de regresión que son aquellos cuyos resultados son valores continuos ejemplo Por ejemplo yo Quiero establecer el valor de una vivienda tomando como parámetro Bueno una serie de datos que tienen que ver con la economía y la situación social por ejemplo de nuestro país bien en ese caso el valor que puede tener la vivienda puede ser un Rango muy muy grande e incuantificable Sí porque porque puede valer un valor que vaya de 10.000 20.000 30.000 40.000 y en el medio todo tipo de posibles valores que estén dentro de ese Rango pero también tenemos problemas de clasificación un problema de clasificación trabaja sobre resultados que son valores discretos que son valores discretos bueno por ejemplo algo que sea de tipo binario hacer un análisis para ver si en una imagen puede detectarse que una persona tiene o no una enfermedad yendo algo un poco más deportivo Si se quiere yo puedo tratar de tener un algoritmo para predecir por ejemplo en el caso del mundial Cuál de las 32 selecciones iba a salir campeón Bueno en ese caso son 32 opciones no hay más que 32 opciones ni menos que 32 opciones como en el caso anterior de una enfermedad hay solamente dos opciones que esté enfermo o que no eso es valores discretos es decir no hay una gama muy grande son dos casos en el primer ejemplo o 32 casos en el segundo alguno de los tipos de algoritmos supervisados más conocidos son los que aquí aparecen regresión lineal revisión polinomial revisión logística árboles decisión bosques aleatorios vectores de soportes y cabesinos más cercanos en realidad en este curso vamos a ver la mayoría de ellos hay otros Pero estos son claramente los más importantes ahora el tema es algoritmos no supervisados que es la otra clase que describimos recién dentro de Machine learning este tipo de algoritmo claramente sirven solamente para problemas de clasificación no para problemas de revisión como vimos en el caso de los algoritmos supervisados vamos a volver al ejemplo de los animales para tratar de entender cuál es la idea del algoritmo no supervisado que Qué característica dijimos que tenía que no tiene etiquetas por eso aquí en la entrada Yo le doy a la máquina una serie de imágenes pero no le digo Qué es cada imagen concretamente no le doy una etiqueta para cada caso Aquí le mando la imagen de un gato de un pájaro de un oso panda de un siervo etcétera etcétera Pero en ningún caso le mando más que la imagen las imágenes sin la etiqueta Entonces qué hace la máquina con esto bien logra un algoritmo inteligente que lo que permite es establecer grupos con esas imágenes que entiende tienen un laminador común por eso fíjense en la imagen de la derecha ha encontrado cuatro imágenes similares luego tres luego dos luego dos y finalmente una sola en el caso del pájaro en ningún caso entiende o le pone nombre a cada grupo que no tiene elementos para hacerlo si yo no le dije de entrada que que entraban teniendo una etiqueta tampoco la máquina va a deducir por su cuenta Cuál es la etiqueta que le corresponde a cada caso Pero sí tiene la inteligencia suficiente para que para que cuando entre un nuevo caso como el que se puede presentar ahora por ejemplo con esta imagen la máquina sepa no que esto es un gato pero sabe que ese elemento debe incorporarlo en el grupo número 2 que estableció sobre imágenes que son similares a la que acaba de entrar Esa es la forma en que trabaja sin etiquetas un algoritmo no supervisado existen dos tipos de algoritmos no supervisados dentro de los que son más conocidos Como siempre digo están el camming clustering y el clustering jerárquico de estos dos en este curso vamos a ver el primero de ellos para descender el camino del Machine learning vamos a empezar a tomar como referencia el primer modelo supervisado del cual hablamos recientemente Qué es el de regresión lineal vamos a tomar la situación en la cual yo voy a presumir que existe una relación entre el precio medio de una vivienda de un barrio determinado y el número de habitaciones que tiene cada una de ellas voy a recopilar datos haciendo un análisis de datos como vimos en el módulo anterior y voy a llevar esos datos a un gráfico el primer dato me dice que una casa que tiene seis habitaciones en promedio está valiendo 20 mil dólares por lo tanto yo lo que hago es establecer sobre el eje de las x el primero de los valores y el segundo de los valores sobre el eje de la cita y eso me da como fin cruce estos datos este punto que está aquí luego tomo otros datos más y veo que para una casa de ocho habitaciones en promedio el valor está Aproximadamente en 35 mil dólares hago el mismo esquema que recién obtengo otro punto y busco otro dato más en el cual siete habitaciones en promedio están valiendo aproximadamente 28 mil dólares y así hago con el resto de los datos obviamente son pocos datos esto simplemente un ejemplo pero vamos a suponer que avanzan la misma mecánica y tomo como resultante todos estos puntos que están dibujados aquí en el gráfico Mi idea o mi necesidad ahora es entender que justamente lo que presumíamos ha pasado es decir que parece que hay una relación directa entre el número de habitaciones y el valor de la vivienda porque porque veo que aquí hay una distribución que tiene como una tendencia que a mí me permite pensar quizás en una figura como puede ser una recta que pueda de alguna manera intentar tocar o aunar toda la relación de esos puntos y es lo que he logrado aquí fíjense que justamente he podido dibujar una recta que pasa de la manera más cerca posible en promedio por todos los puntos que teníamos recién en el gráfico que hemos obtenido aquí más que una recta hemos obtenido un modelo Por qué Porque a partir de ahora justamente basándome en ese modelo en esa recta yo puedo predecir otros valores que no están en este gráfico yo puedo predecir ahora que una casa que va a tener 6,5 habitaciones en promedio va a tener estimado de 25 mil dólares Cómo basándome en esa recta basándome en ese modelo de predicción concretamente lo que hemos hecho es un análisis predictivo concepto que hemos abordado largamente en la primer clase y que justamente puede tener una definición como la que está aquí es decir estamos hablando de una técnica que me permite deducir o predecir el valor de una variable dependiente en base un valor de una variable independiente en este caso lo que tenemos también que tener en cuenta es que ese análisis predictivo y esa obtención de ese modelo en base a un patrón se va a obtener siempre y cuando haya variables que tengamos la idea como vimos también recién a través del gráfico que tengan una relación que demuestre una relación vamos a suponer que yo hubiese establecido que en lugar de poder presumir que puede deducir el valor de una vivienda en base a la cantidad de habitaciones hubiese supuesto por ejemplo en base a la cantidad de árboles tuviese la casa por decir un ejemplo Por más disparatado que pueda sonar seguramente esa relación no existe por lo tanto no va a existir un modelo y Por ende novaks tiene una predicción por eso para obtener un modelo necesitamos tener una buena relación entre las variables dependientes y las variables independientes esto lo vemos claramente en el primer tipo de modelo que estamos estudiando en este caso que es el modelo de regresión lineal simple cuya fórmula es esta que está aquí en este caso vamos a desandar cada uno de estos componentes para entender esa idea de bayone dependiente y variable independiente a través de el siguiente gráfico en principio el primer componente que tenemos aquí es la i la i y la variable dependiente es decir el objetivo que estábamos buscando que es el valor de la vivienda y en este caso de este ejemplo es el 28.000 que está aquí pero concretamente son todos los valores que están sobre el eje del aire luego tenemos el siguiente componente que la letra a la letra a describe la pendiente qué pendiente la pendiente de esta recta roja que tenemos aquí que es Ni más ni menos que en la descripción gráfica de nuestro modelo de predicción luego también tenemos la x la x es la variable independiente en nuestro caso en nuestro ejemplo la cantidad promedio de habitaciones es decir el 6,5 que está aquí o cualquier valor que esté sobre el eje de la x y finalmente la B la B es la intersección la intersección de la recta con el eje de la es decir esto que está aquí cada uno de estos componentes describe el formato de lo que es una expresión de un modelo de revisión línea simple lo siguiente que tengo que hacer una vez que termino de definir el modelo es su evaluación concretamente Quiero saber cuál es el mejor modelo el mejor modelo es aquel que minimiza la distancia entre la recta y los puntos fíjense esto que estoy marcando aquí con círculos de color celeste describe puntos que no están sobre la recta qué quiere decir que en ese caso puntos tienen un error respecto de la realidad fijémonos aquí el valor que me describe el modelo es este sin embargo en el caso real el valor existe otro que está aquí es decir que esta distancia describe que hay un error y en este caso lo mismo hay un error más grande en este caso un error más chico en este caso qué quiero decir con esto los modelos no son perfectos no tiene una precisión del 100% sí es predicción no una exactitud con lo cual una manera de evaluar cuán bueno su modelo es justamente la mínima distancia que tenga el modelo respecto de cada uno de los puntos de la recta ahora Cómo podemos obtener ese mejor modelo bueno justamente existen algoritmos y existen librerías que me ofrecen métodos para poder establecer el mejor modelo y una de ellas es justamente es la que vamos a usar cuando empecemos con la codificación y los ejemplos prácticos de un modelo de regresión concretamente dijimos que la es esta pero hablamos que hay variables tenemos la a y la b la dijimos que la pendiente y la vela intersección con lo cual una expresión por ejemplo podría ser Esta es decir un valor para y un valor para e con lo cual a partir de ahora cualquier valor de X que yo tenga supongamos el 6,5 que tengo acá multiplicado por 5 sumado 1 debe darme un valor que debería ser en este caso el valor establecido como precio de la vivienda esto es simplemente un ejemplo pero es para que ustedes entiendan justamente Cómo funciona la determinación a través de una librería como Side Killer de la Fórmula ideal para el caso que se presenta así como esa puede ser esta otra es decir 3 paradas y menos uno para b o esta otra 15 para y siete para b o lo que sea justamente eso es lo que va a establecer scicleta como mejor modelo para cada caso Cómo aprenden los modelos y cómo van mejorando en base al entrenamiento esto también ya anteriormente y justamente lo que yo hago es tomar datos para que ese modelo pueda entrenar esos datos de que provienen proviene justamente como vimos en el caso de el módulo de análisis de datos a partir de la clase 1 a 5 provienen de lo que yo tengo como realidad de hoy en el caso El ejemplo recién tomé datos que tenían que ver con la cantidad de habitaciones promedio y los valores de esas viviendas también en promedio en este caso con esos datos justamente este algoritmo va a aprender y justamente va a tener un desarrollo mejor para hacerlo más preciso posible ahora luego de esto van a aparecer nuevos datos que son los datos que van a aparecer de ahora más en la realidad y justamente esa predicción que es este aquí lo va a armar justamente en base a dos cosas primero en base a los datos con el cual yo creé y entren en algoritmo y luego por los datos nuevos lo cual va a permitir que ese algoritmo siga entrenando y sea mejor para sacar un mejor opus existe un concepto muy importante a la hora de entrenar un modelo que es cómo voy a manejar sus datos tenemos muchas veces un problema que puede presentarse que si yo hago un modelo en base a un conjunto de datos sí puede ser que ese modelo sea muy bueno parece conjunto de datos pero no sea tan bueno para nuevos datos que aparezcan se dice que un modelo es mejor cuando más generaliza Qué quiere decir este concepto Mientras más se adapta a cualquier diversidad en situaciones entonces una manera de probar esto ya que yo tengo datos recopilados hasta hoy y no tengo los datos que van a venir de ahora más si no luego pero hoy cuando tengo que hacer el modelo de su dato no lo tengo lo que ya es habitualmente es separar el conjunto de datos que tengo yo hoy en dos ámbitos o en dos sets por un lado los datos para entrenar y otro datos para evaluar si yo ahora voy a evaluar el modelo lo voy a evaluar con este conjunto que he denominado de entrenamiento me va a dar un determinado score un determinado valor luego voy a validar ese mismo modelo ahora con estos datos del conjunto de Test sin evaluación de este modelo y la evaluación de este modelo son buenas en ambos casos quiere decir que el modelo generaliza bien por el contrario si el modelo Tiene un buen score con este conjunto pero no con este quiere decir que ese modelo no generaliza bien y Por ende no es un buen modelo Esta es la idea que se utiliza cuando tengo un solo conjunto de datos y no tengo otra alternativa que separarlo en dos conjuntos en un conjunto de entrenamiento y en un conjunto de Test luego el test lo voy a ir haciendo poco a poco con los datos nuevos que vayan apareciendo Pero esto lo que yo necesito hacer al momento de generar un modelo y para terminar esta clase vamos a ver otro modelo de regresión Existen varios nosotros aquí vamos a mostrar dos además de El de remisión lineal simple que es el que vimos hasta ahora Uno de ellos es regresión lineal múltiple y la otra es progresión polinómica el primer caso de ellos fíjense que tiene una relación muy marcada con la regresión lineal simple de los semántico me refiero la diferencia está justamente en que la lineal simple tiene una sola variable x para determinar la variable y la relación lineal múltiple tiene más de una variable x para determinar la variable Y qué quiero decir con esto vamos al caso de El ejemplo que usamos para la regresión lineal simple donde teníamos la cantidad de habitaciones promedio y con ello Tratamos de determinar el valor promedio de la vivienda ahora puede ser de que eso sea insuficiente para determinar el mejor valor promedio y puede ser capaz de determinar y se y ese valor objetivo tengamos que dar participación a más de una variable por ejemplo la cantidad de habitaciones y por ejemplo el nivel de peligrosidad que puede tener el barrio el nivel de cercanía a centros comerciales o centros importantes de la región o cualquier otra cuestión que sea importante o que tenga peso a la hora de determinar el precio de una vivienda Bueno Este es el caso de la regresión lineal múltiple y justamente a partir de aquí la fórmula es un poco más compleja porque tiene múltiples coeficientes y no uno solo la revisión polinómica añade predictores obtenidos al Elevar los proyectores originales a una potencia que son los predictores las x si esta combinación de AX fíjese miremos la fórmula ven que yo tengo una similitud a 1x1 a 1 x 1 pero ya el segundo componente predictor lo elevó al cuadrado y así haría con cada uno de los otros componentes que formen parte de esta expresión de la relación polinómica para poder terminar de verlo vamos a ver en gráficos y lo vamos a entender mucho mejor bien aquí tenemos el de la reelección lineal múltiple si ustedes recuerdan el gráfico de la regresión lineal simple teníamos una recta en un gráfico de dos coordenadas aquí tenemos un plano en un gráfico de tres coordenadas obviamente esto habla de una regresión lineal múltiple que tiene solamente dos variables si yo quisiera tener una relación lineal múltiple con tres o más variables que lo puedo tener evidentemente eso no se podría graficar porque todos sabemos muy gráfico de más de tres dimensiones y sin posibilidad de hacerlo pero ya con esto podemos ver la diferencia entre lo que sería una revisión lineal simple que tiene una sola variable x y una reacción lineal múltiple que tiene dos variables x bien obviamente aquí la diferencia está en lugar de buscar una recta que se acerque a todos los puntos aquí yo buscaría un plano que se acerca a todos los puntos ahora tengo la regresión polinómica Y el gráfico es este porque puede darse que los puntos estén distribuidos con una forma curva eso no lo va a solucionar ni la religión lineal simple ni la relación lineal múltiple porque tiene una forma recta Sí ya sea en plano o ser una recta propiamente dicha en este caso si yo intentara hacer un buen modelo con estos puntos que están distribuidos en forma de curva no podría lograrlo con ninguna de esas dos expresiones entonces recurro a la regresión polinómica que gracias a esa idea de que puede ser una expresión x que esté elevada al cuadrado al cubo a la cuarto a quinta potencia o la que fuere va teniendo la resultante no una forma de recta sino una forma de curva de nuevo el concepto Cuál es en este caso no la recta sino la curva que más se acerque a todos los puntos que están aquí en el plano va a ser la que represente el mejor modelo evidentemente estos puntos no están puestos azarosamente aquí de esta manera sino que tienen que ver justamente con lo que yo he obtenido como recolección de datos donde tengo una x y una y que me llevan a que cada punto esté en el lugar donde está aquí en el plano solamente que en este caso lo ha hecho en forma de cubo bien hasta aquí llegamos con la introducción teórica de lo que es regresión regresión lineal simple revisión lineal múltiplenómica nos va a quedar otro tipo de regresión para la otra clase pero antes de eso vamos a ver la segunda parte de esta clase para empezar a codificar nuestros propios modelos de relacionía Hasta aquí llegamos con esta primera parte en donde abordamos la teoría del algoritmo de regresión lineal ahora pasaremos a la segunda parte para poner en práctica esos nuevos conocimientos allí nos vemos Titulo: Clase6 (parte 2) Curso Inteligencia Artificial \\n URL https://youtu.be/PL0Mc-ULZeI  \\n 1959 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 6 del curso de Inteligencia artificial de ifes en ella vamos a programar nuestro primer algoritmo de Machine learning empecemos  bien y ahora vamos con la segunda parte de la clase número 6 para lo cual abrimos en clase 6 Notebook clase 6 como siempre desde nuestro Google Drive y lo primero que hacemos como siempre es importar las librerías con las cuales venimos trabajando habitualmente en este caso vamos a trabajar con otras librerías que no hemos trabajado hasta ahora nanpay Panda sin mapu Sí pero vamos a sumar ya lo adelantamos en la teoría sidekitland es la librería que vamos a usar de ahora más para todo el trabajo con Machine learning en este caso es lo primero que importo es justamente una librería que me permite o me va a permitir crear el modelo de regresión lineal que parte de la práctica de esta clase y luego voy a importar mover selection train test split que justamente es la librería que me va a permitir a mí hacer la separación de los datos para test y para entrenamiento luego el resto de los datos obviamente es exactamente lo mismo import warness que sería el sistema operativo Así que ejecuto y ya estoy importando estas librerías a continuación lo que tengo que hacer es importar el archivo A este colap con lo cual en el siguiente paso ejecuto lo que ya sabemos Google cola import files y en up para traerme el archivo Así que empiezo por hacer eso y me voy a ir a la carpeta Boston y así voy a tomar Boston csv y le hago clic y me lo traigo a mi cola siempre chequeo a través del explorador que esté bien aquí está y sigo ahora lo que voy a hacer va a ser crear un Data set a partir de la variable de datos como siempre con p de Rich ssd y auge para ver el contenido de este Boston csb bueno Y así aparece este Boston csv de alguna manera Ya lo hemos manejado sin querer en la teoría porque justamente representa un ejemplo parecido al que usamos allí nosotros así usamos un ejemplo basado en la cantidad de habitaciones promedio que tenía una casa y en base a ello determinamos el precio promedio bien el resto de los datos yo le he dejado aquí un comentario para que bueno si les interesa ver qué significa cada una de las columnas para que quieran hacer cualquier tipo de práctica con ella lo pueden hacer bien una vez que tengo ya abierto mi Data set como siempre voy a tratar de reconocer su formato bien lo que voy a tratar de hacer a continuación va a ser tratar de ver a través de un gráfico si hay una variable independiente y una variable dependiente que puedan llegar a darme la Pauta de que existe un patrón y a partir de eso crear un conjunto de Machine learning en este caso de regresión lineal y vamos a utilizar lo mismo valores que usamos en el ejemplo de la teoría aquí habíamos citado la cantidad de habitaciones en promedio y aquí el valor de la vivienda Por lo cual voy a tratar de hacer un gráfico de puntos como vimos en la teoría obviamente va a tener más puntos ahora porque tengo como vimos aquí 506 observaciones y para ello voy a tratar de llegar a este gráfico este gráfico necesita obviamente por ser un gráfico de dos dimensiones justamente dos variables una variable independiente cantidad de habitaciones y una variable dependiente precio promedio para esto lo que voy a hacer Voy a tener que lograr un x y un Y entonces el X lo voy a tomar del dataset tomando justamente los valores de la columna RM y elli de la columna me debe corta como lo hago con este sexo que está aquí justamente con np array voy a crear un array con todos los datos de la columna RM en este caso datos lo digo por las dos cosas Sí porque es un dato además el Data se llama datos sí bien y luego con la variable y hago otro tanto pero con me debe corta en este caso verán que hay una diferencia entre la forma que uso para darle el valor ahí como el que le doy para x porque por la presencia de np array recuerdan que np tiene que ver con nanpage Sí con la librería bien esto tiene que ver porque no olviden que x puede tener una variable o puede tener más de una variable si tiene una variable estamos hablando de un modelo de relación lineal simple si tiene más de una variable va a ser un modelo de relación Ya por tengo que trabajar en el caso de X con el perra y en el caso de que siempre va a tener solamente un valor siempre va a tener solamente una columna sin NPR bien lo ejecuto y ya voy a tener mi x y mi y ahora que tengo esos elementos voy con plt punto scatter a pedirle que me dé o forme un gráfico de dispersión de punto sí Recuerden que con esto determinó el tipo de gráfico y los datos con que va a manejarse ese gráfico y con plt lo que hago es show Mostrar el gráfico con plt x Label le pongo títulos a la barra de la x y la barra de nai Entonces ejecuto esto y obviamente el dato que me va a dar el resultado que me va a dar es el gráfico que ya vi ya lo tengo hecho por supuesto como siempre en la ejecución de cada instrucción de este Notebook bien este gráfico me da la Pauta Más allá de que existen muchos puntos que están fuera de un patrón que se observa aquí como si fuese una diagonal de izquierda a derecha arriba hacia abajo existen muchos puntos que están muy alejados pero se puede decir que más o menos el 80% de los puntos siguen ese patrón o esa tendencia con lo cual me da la Pauta que puedo pensar en un modelo de regresión lineal que permita dar predicciones para este caso entonces Antes que nada lo que tengo que hacer es poder empezar a pensar en entrenar un modelo con estos datos Pero eso para hacerlo primero tengo que pasar a dividir los conjuntos con los datos que tengo en entrenamiento y en validación o test que es lo que vimos en la teoría Cómo hago esto con la siguiente instrucción fíjense que en el caso de python es muy común que yo pueda poner no una variable igual a un determinado elemento sino que cuando el elemento de la derecha brinda más de una salida o un más de un valor yo puedo poner todos los valores juntos y separados por comas a la izquierda en este caso voy a obtener x-train x test como la variable que va a tener todos los datos x de entrenamiento todos los datos x de Test o de validación y luego estas dos variables para todos los datos y de entrenamiento y todos los datos test de validación esto lo igualo a la función train test split que le pongo a 30 split o qué parámetros le pongo bueno en principio la x y la y los mismos datos que están acá sí los mismos datos que están acá cantidad de habitaciones valor promedio de la vivienda y luego le pongo con test Qué cosa Qué porcentaje en 20 es el 20% va a ir a parar Al conjunto de Test y Por ende el resto va a quedar en el conjunto de entrenamiento con lo cual con esto que es un valor que yo lo puedo por lo general siempre es 20 15 o 25 No mucho más allá de eso pero esto es un valor que lo puedo configurar yo con el nombre que quiera es decir que esto puede ser este valor u otro y finalmente Random State 2 esto es importante porque cada vez que yo ejecute esta instrucción traen text split no va a ser la misma separación es decir va a separar los datos hacia un conjunto y hacia otro Pero cuando lo vuelve a ejecutar la distribución va a ser distinta y cada vez que lo ejecute va a volver a ser distinta por lo tanto con Random lo que me garantizo es que si hay una distribución que por algún hecho fue buena para lograr mi modelo pueda volver a recuperarla justamente al invocar este número bien a continuación lo que voy a hacer va a ser crear una variable que le voy a poner nombre modelo Que obviamente va a tener el modelo que estoy generando y la voy a hacer distanciando creando una instancia de línea recreation de línea model o sea con esto le digo que esta variable va a ser una variable de tipo modelo de regresión línea y luego lo que hago es fit que es fit el famoso entrenamiento el que hemos hablado tantas veces es decir que el modelo va a entrenar Con qué datos con los datos de entrenamiento justamente con el X3 y con el itrain y una vez que hagas entrenamiento me va a dar el mejor modelo de la versión lineal que pueda existir para ese conjunto de entrenamiento lo ejecutamos sí Y esto me tiene que dar a mí una expresión como la que vimos en la teoría es decir y igual a x + B Entonces esto y lo que puedo ver aquí a través de este código voy a hacer un print con algún String una combinación de elementos pero que me permitan visualizar Cuál es el formato de esa expresión primero voy a poner un título ecuación del modelo igual las x más B que es lo que apreciamos recién y voy a conformar en base a valores que me arroja este fit si en el modelo la expresión concreta con el número de a y el número de B Cómo es esto voy a poner print igual modelo punto modelo punto guión bajo va a ser el valor de a que determinó el entrenamiento del modelo de organización lineal para ese modelo a crear y luego x para que se forme la expresión más lo que sería el B que sería el B modelo punto interceptor guión bajo intercept se acuerdan que el B era la intersección de la recta con el eje de la y bueno ese número lo tiene mal entonces con lo cual resumiendo el valor de a va a estar el modelo.coes y el valor de B Es modelo gracias a eso yo voy a ejecutar esto y voy a poder ver cuál es la expresión concreta de este modelo la expresión concreta es 8.74 y todo lo demás x o sea que este es el valor de a más menos 3255 y todo el resto para el B se acuerdan que en la teoría yo le puse algún valor así al azar de tipo ejemplo que puse un 5 a un lado un 7 del otro pero justamente lo que me refiero es esto lo que hace el fit es lograr esta expresión que está aquí en donde obviamente y x son las variables en este caso la variable independiente x y la variable dependiente y los valores que voy a ir obteniendo por cada valor de X va a haber un valor de y Pero por qué Porque x lo va a multiplicar por este valor y le va a sumar este otro valor esos valores salieron insisto del entrenamiento del modelo para lograr esta expresión que es la presión ideal para esta distribución de puntos qué es lo que viene a continuación las predicciones yo hago clic aquí y justamente voy a hacer predicciones con el método predic del modelo que Acabo de crear Este modelo que está aquí arriba si esta ecuación del modelo por lo tanto este x test le va a dar todo un conjunto de variables de entrada todo un conjunto de X a esta expresión y multiplicando cada una de esas x por este coeficiente y sumándole este intercepto va a obtener un y seguir va a ser un y producto de una predicción Está bien que no es necesariamente igual a el y test que se corresponde con este x test Sí si yo me voy más arriba ven donde hacía el tren test split tenía el X test y el itest que es un conjunto que representa un 20% del xei natural con lo cual cada una de estas x va a tener su y como a su vez cada x-30 tenía su itrain Sí cómo lo podemos ver a esto bueno en principio vamos a ver los ipred yo le voy a agregar aquí un código con lo cual voy a poner y guión bajo pez y me va a mostrar todo una raíz ven todo un array de predicciones es decir cada uno de estos que están aquí todos esos valores son ipred producto de un x test al cual se le aplicó la fórmula del modelo y se obtuvo uníprete son 120 elementos y estas son cada una de las 120 predicciones que se obtuvieron en base al x test el X test es un conjunto Insisto que es ese 20% del que hablábamos recién que también lo puedo si quiero ahora escribir ejecutando acá lo veo Estos son todos los valores de entrada todos estos valores de entrada Fueron multiplicados por el coeficiente y sumados a la intercepto y se obtuvo por ejemplo vamos un primer caso este 6,549 se llevó a esta fórmula se la multiplicó y se la sumó y se obtuvo ni como justamente con esta instrucción predict y se obtuvo un ipred Cuál es el ipred este que está acá 38 101 Sí ahora ese ipred será o habrá sido igual a listes real que tenía este X3 Bueno lo que podemos hacer es justamente imprimirlo también con lo cual un sombrero aquí abajo de los para que se pueda ver mejor y más rápido Entonces lo único tema que tenemos aquí es que vamos a tener que hacer un artilugio porque hice itest No es un array con lo cual vamos a tener que aplicar el np array para que transforme ese no array en un array meterlo en una variable perdón y poder Mostrar el contenido de acá lo tengo ven fíjense que en el primer caso el valor natural era 37 6 y la predicción fue 38101 el segundo valor de it fue 27,9 la predicción 2566 un caso más Sí el tercer caso 22,6 la predicción fue 23,78 porque hay tanta diferencia bueno porque evidentemente este algoritmo no es bueno sí Y tiene un bajo nivel de score un bajo nivel de precisión Cómo puedo ver eso como siempre en un gráfico yo voy a hacer en principio un plt scatter con x test eitest es un gráfico de puntos que me va a hacer Ya les adelanto porque tengo el modelo terminado obviamente estos puntos que están acá es decir la cantidad de elementos que son números de habitaciones las x del conjunto de Test y los y del conjunto de Test bien pero cómo puedo ver el modelo cuán cerca está de esa realidad a partir de sus predicciones justamente con esta otra instrucción que es un pl plot que lo que va a hacer es dibujarme una recta donde tomo el mismo x test tenía antes pero en lugar de élites voy a tomar el ipred Por qué Porque yo ahora que quiero ver es ese valor de entrada con el valor que predijo el modelo no con el valor Real Como aquí Bueno luego le pongo simplemente algunos datos cosméticos que tienen que ver con el color red y el ancho de línea que va a ser 3 y finalmente lo que hago es poner Bueno un título general para el gráfico título para elegir las x título para elegirla así y show y lo ejecuto qué puedo ver aquí a partir de esto que obviamente esta recta es muy buena si para acercarse a muchos puntos pero no lo es tanto para los otros puntos Sí por ejemplo estos puntos que están aquí arriba están muy lejos de esta recta con lo cual hay un error muy grande y en este caso también y en estos casos también es decir Tengo demasiada cantidad de puntos por fuera de esta recta sí Y obviamente muy pocos por encima o cerca de esta regla ahora acá no podemos hablar de lo que yo veo simplemente decir bueno son pocos son muchos me parece que están lejos o están cerca hay justamente métodos concretos que me permiten hacer que cosas lo que hablamos en la teoría evaluar la eficiencia de este modelo que es lo que vamos a hacer ahora Bueno cómo lo hacemos con score de modelo score de modelo lo voy a utilizar en principio para medir la eficiencia del conjunto de entrenamiento con x-train e-train y luego voy a hacer lo propio pero con el conjunto de Test con x test eitest lo ejecuto y me dice que tengo un 44% de eficiencia el modelo de entrenamiento es muy malo y ya vamos adelantando que un buen modelo Tiene que estar en el orden de los posibles 90% más de precisión para que sea considerado como bueno por supuesto en el conjunto de Test es mucho mayor la precisión que en el caso del conjunto de entrenamiento Cómo leemos Esto bueno a ver si la situación fuera al revés creo que sería un ya modelo pésimo no Pero de alguna manera porque encima que los score son bajos que haya un mayor nivel de score para el modelo de entrenamiento que para el modelo test no es bueno porque dice que no generaliza bien Y encima el score es muy bajo pero en este caso bueno el que justamente haya obrado también el modelo a pesar de que están muy lejos a nivel de scorde lo que se pretende se puede decir que es una buena lectura de todos modos el conjunto de Test y entrenamiento se puede volver a generar Sí yo vuelvo a ejecutar esto obviamente cambien en Random State y lo puedo volver a probar y ver si en esas condiciones estos números pueden cambiar igual ya son muy bajos y por más que yo haga eso evidentemente Este modelo para este conjunto de puntos o para este en general para lo que es el Data set de Boston no es un buen modelo vamos a probar suerte o mejor suerte con otra alternativa que sería la regresión lineal múltiple vamos entonces a la regresión lineal múltiple donde en lugar de tomar como x datos solamente RM voy a tomar age y Dish también voy a tomar dos variables más fíjense que ya no uso NPR y aquí porque ya obviamente esto es un array porque tiene más de un valor y en el caso de uso justamente me debe Como usaba antes Bueno ejecutó esto ahora como tengo más datos más variables independientes voy a tener que volver a hacer la separación de conjunto de Test y conjunto de entrenamiento vuelvo a ejecutar y vuelvo a crear el modelo y vuelvo a entrenarlo en este caso vamos a hacer una variable que va a llamar modelo m para diferenciarlo del anterior que llamaba modelo a secas y lo que vamos a hacer a continuación es al igual que en el caso anterior intentar descifrar Cuál es la fórmula definitiva que me arroja este nuevo modelo se acuerdan que en el caso anterior habíamos utilizado un String para ver el cohete ir intercept y poder expresar de alguna manera la ecuación del modelo que en ese momento era y igual a a x + B en este caso al ser una regresión lineal múltiple la fórmula es igual a varios a x tantos como elementos yo he tenido aquí en cuenta a la hora de formar la x se acuerdan que es la cantidad de habitaciones la antigüedad de la vivienda y la distancia de los grandes centros urbanos entonces aquí yo tengo tres cofres entonces tengo que sub cero uno y sub 2 y luego finalmente el intercept tal cual como lo tenía antes por eso al redactar esta fórmula y ejecutarla voy a poder comprobar una expresión similar al caso anterior también pero obviamente con más coeficientes que es esta que está aquí más el final que sería la B bien luego de ello que esto simplemente para que veamos la fórmula definitiva que genera Este modelo vamos a medir la precisión que vamos a obtener ahora con este modelo Gaming vemos que la precisión ha mejorado en el caso del modelo de train pero no así en el modelo de Test es el que teníamos los valores aquí arriba 4463 y ahora tenemos 51 y 60 no obstante es bueno Insisto que el modelo de Test tenga un score superior al del modelo de entrenamiento pero los dos indicadores siguen siendo bajos vamos a pasar ahora a ver el último algoritmo de esta clase que es la regresión polinomial para ello vamos a incorporar un nuevo archivo que no va a ser mucho más Útil para demostrar lo que queremos demostrar con este algoritmo vamos a elegir en este caso dentro de la carpeta auto el archivo Auto mpg csb Aquí van a poder ver que estoy incorporando nuevamente librerías que ya había incorporado antes bueno Esto es un ejemplo para que ustedes vean que va a depender muchas veces de si ustedes quieren en algún momento de tener el Notebook y volver a activarlo desde una parte y no del principio pueden incorporar nuevamente las importaciones desde el lugar que ustedes quieran si ya están importadas no va a haber ningún problema lo va a volver a hacer y van a continuar tranquilamente como si no hubiese estado hecho esto antes lo que voy a hacer a continuación es crear una variable de datos con el contenido del archivo que acabo de importar Y luego hacer un gel para poder reconocer el formato de este archivo vamos a eso y vemos el contenido de este archivo el mismo describe características de autos antiguos y aquí tenemos Si es original o no el año de modelo aceleración peso caballo de potencia la cantidad de cilindros y metros por galón metros que recorre por galón bien justamente Esta última característica metros por galón y horsepower son dos elementos que vamos a tomar justamente para probar nuestro algoritmo de regresión polinomial paso seguido Vamos como siempre a reconocer la forma de este este conjunto de datos y justamente activamos datos punto shape Y tenemos 406 observaciones y 9 características luego vamos a ver si algunas de esas observaciones tienen valores nulos se acuerdan que esto lo habíamos visto en el módulo de análisis de datos donde justamente con datos punto y Snoop y zoom sumaba la cantidad de elementos nulos que pueden haber en cada característica y justamente los dos elementos que voy a tomar mpg y Horse tienen elementos nulos en el caso mpg tiene 8 y en el caso es por Power tiene 6 Entonces qué tenemos que hacer aquí depurar este conjunto de datos cómo lo hacíamos con Drop na se acuerdan que los datos no lo llamamos na o Nan Sí entonces lo que hacíamos en ese caso era con axis 0 indicarle que lo que voy a eliminar son registros observaciones y con House le decía Cuál era la condición para eliminar si con que hubiese alguno n y como en este caso que sean ya es condición para que esa observación se eliminada la Otra condición recuerdan que era all es decir que todos fueran Nan Bueno Este no es el caso con lo cual al ejecutar esta instrucción yo lo que voy a hacer es eliminar las 8 observaciones de mpg malas no la mejor dicho y las seis observaciones de hard Power que también son nulas para verificarlo vuelvo a ejecutar la misma instrucción que arriba pero aquí debajo una vez que justamente hizo el dropea y veo que ahora sí está depurado este conjunto de datos con lo cual no tengo ningún npg ni ningún horsepower nulo vuelvo a hacer ahora un shape y con qué me encuentro con que baje de 400 observaciones a 392 observaciones sí es decir la diferencia es justamente las 14 observaciones 8 + 614 que tenían elementos bien ahora que tengo eso paso siguiente voy a poder plotear todos los valores cuya relación existe entre caballo de potencia y millas por galón es decir entre score Y como siempre a un plot pongo los dos elementos en este caso Hot Power mpg de datos por supuesto y este error es para indicarle el color porque quiero que salga el punto en este caso sería rojo y finalmente caballos de potencia como título del eje de las x y millas por galón como título de eje de ejecuto y veo este conjunto de puntos que tiene una forma curva es decir ideal ideal para poder aplicar no algoritmo de depresión lineal simple ni de revisión múltiple sino de regresión polinomial como lo vimos en la teoría pero vamos a probar con un algoritmo de recreación lineal simplemente para ver justamente la diferencia que puede haber entre un algoritmo de este tipo y un algoritmo de revisión polinomial que justamente por la forma como le damos recién sería lo ideal vamos a intentar aplicar un algoritmo de revisión lineal simple dado que tengo dos variables mpg por lo tanto voy a crear una x con el primero de los valores y una y con el segundo y voy a crear un modelo en este caso no le llamo model sino lm de transgresión lineal creo el modelo lo entreno y al obtener su precisión veo ahora cuando lo ejecute que del 60% obviamente muy baja para terminar de visualizar esto bien Vamos a graficarlo Sí así como el mismo gráfico que teníamos antes aquí arriba solamente que ahora le voy a agregar esta línea que lo que hace justamente es dibujar me la línea que representa el modelo que acabo de obtener y la ejecuto y veo que bueno es una línea que evidentemente está muy lejos de poder abarcar la mayor parte de los puntos que tengo aquí obviamente tiene una precisión que está muy baja tal cual lo describe este número que aquí me demuestra ahora vamos a probar con un modelo de organización lineal cuadrático es decir ya como vimos en la teoría elevando una de sus características a una potencia para eso voy a importar esta librería que tengo aquí y justamente lo primero que voy a hacer es utilizarla para empezar a crear justamente una característica featus polinomial en este caso de grado 2 con thegree igual a un número en específico el grado en este caso era un grado 2 es decir elevado a la segunda potencia por lo tanto voy a a través de esta variable poder hacer una transformación sí de horsepower por un valor que no es el propio el original sino el propio elevado a que a la segunda potencia por lo tanto x no va a tener supongamos si fues por dos por ciento en valor sino que va a tener cuatro no va a tener tres sino va a tener 9 es decir siempre el valor original elevado a la segunda potencia eso me va a permitir pensar en un modelo de organización lineal con un formato curvo propio de una descripción de Fórmula elevado a la potencia creo ese modelo lo entreno y me fijo su score y veo que eso score aumenta considerablemente respecto de El anterior teníamos 060 y aquí tenemos 068 pero vamos a hacer una prueba que va más allá de esto es decir no vamos a quedarnos solamente con el caso de un polinomio de grado 2 Si no vamos a tratar de justamente pensar en polinomios de otros grados y probar justamente con cada uno de ellos cuál sería el score la precisión en cada caso por ello voy a crear esta este Ford que tengo aquí y voy a utilizar una variable de que va a ciclar entre valores que van a ir de 2 a 5 Recuerden que siempre decimos el último valor en python no es tenido en cuenta O sea que voy a hacer un polinomio de grado 2 3 4 o 5 lo que voy a hacer dentro del Ford es repetir lo mismo que hice aquí solamente que qué voy a hacer voy a ir variando este valor Por ende fíjense que acá yo no pongo sino igual a d y d va a pasar de ser dos tres cuatro y cinco y en cada ciclo que de voy a imprimir sí Cuál es el score Cuál es la presión de cada caso ejecuto y fíjese los estado que tengo en el caso de esta prueba que he hecho aquí el polinomio de grado 5 es el que me daría la mejor precisión sí bien en este caso yo les dejo un mensajito aquí abajo que es muy importante que lo tengan presente en realidad yo puedo quedarme con que el polinomio de grado 5 es el mejor porque tiene la mejor precisión Pero como este texto lo resaque en realidad la diferencia entre el grado 2 y el grado 3 el grado 4 el grado 5 o si quieren del grado 2 al 5 que es alto más grande no está significativa y esto muchas veces puede llegar a pasar de que amerite que yo elija el grado 2 porque porque si la diferencia no es tan grande y el polinomio grado 5 me va a exigir un coste computacional esto es una palabra que ya vamos a repetir mucho decir cuánto tiene que trabajar la máquina para poder lograr directamente llevar a cabo ese modelo bueno la diferencia No amerita sí hacer trabajar a esa máquina mucho más lo que implicaría de un grado de asombrado 5 si la diferencia de precisión no va a ser tan grande no es un tema de ahora seguramente es un tema que vamos a ver mucho más adelante pero es bueno ya instalando la idea de algunas cosas que tienen que ver con la experiencia que vamos a tener que abordar cuando decidamos Cuál es el modelo más adecuado para la situación que nos toca bien con esto terminamos la clase número 6 nos vamos a ver en el caso número 7 con otro modelo de regresión que se llama regresión logística hemos terminado la clase número 6 del curso en ella aprendimos los conceptos y la puesta en práctica de los algoritmos de regresión lineal simple múltiple Y polinomial como dijo el profesor en la siguiente clase veremos el algoritmo de regresión logística nos vemos en la próxima clase Titulo: Clase7 (parte 1) del Curso de Inteligencia Artificial \\n URL https://youtu.be/F0NqID55iJk  \\n 1279 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 7 del curso de Inteligencia artificial de ifes en ella veremos los conceptos básicos del algoritmo de regresión logística empecemos   Hola a todos como se dijo en la introducción estamos en la clase número 7 del curso de Inteligencia artificial concretamente estamos en el segundo módulo que es el de Machine learning como es el título aquí y dentro de ese segundo módulo estamos en el segundo algoritmo regresión logística Recuerden que la clase pasada vimos en de regresión lineal múltiple tradición lineal simple y reacción polinomial por eso vamos a recordar un poquito lo que vimos en la primer clase donde escribíamos justamente todos los algoritmos supervisados y aquí están los dos primeros que son justamente lo que referenciamos recién abordados en la clase número 6 y luego está este que estamos viendo aquí que es el de regresión logística que es el que nos va a ocupar en esta clase habíamos hablado de que los algoritmos supervisados sirven para atacar dos tipos de problema por un lado los programas de regresión con valores continuos y luego los problemas de clasificación con valores discretos concretamente regresión logística tiene su fortaleza en este segundo tipo de problemas que son justamente los de clasificación más de 75% de los problemas abordar con learning son problemas de tipo de clasificación en ese contexto la regresión logística es uno de los algoritmos para resolver problemas de clasificación binario es decir problemas que dan resultado sí o no positivo o negativo ejemplos más amplios de esto que se puede escribir de una manera tan sencilla o tan compleja como los ejemplos que vamos a ver a continuación Por ejemplo si un cliente va a ser o no candidato a hacer una compra en un comercio o bien Por ejemplo si un correo electrónico puede o no contener spam o algo más complejo pero también muy útil y muy utilizado en algoritmos de tipo binario justamente es para ver si el diagnóstico de la enfermedad de una persona puede arrojar un resultado u otro dependiendo del tipo de estudio que se está haciendo para comprender un poco mejor el concepto vamos a usar Este ejemplo de datos de gente que ha hecho o no compras en un comercio tengo datos de esas personas que son el género la edad y su salario Y luego tengo un valor cero si esa persona no ha realizado la compra o un uno si sí lo ha hecho Qué me interesaría a mí poder saber como dueño de este comercio bueno poder predecir en base a estas características si esa persona va a comprar o no es decir concretamente encontrar un algoritmo que teniendo en cuenta el género la da o el salario pueda predecir esa información y yo saber si esa persona con esas características puede ser un potencial comprador o no voy a tratar ahora de volcar estos datos a un gráfico concretamente tengo aquí tres variables independientes y una dependiente pero me voy a valer solamente de una de las variables independientes que la del salario porque porque si miro rápidamente voy a poder ver una relación entre el valor de salario y la acción de compra no observemos que en el caso de este cliente que cobra 5000 este que cobra 3000 y este que cobra 2000 han comprado y los cuatro casos en donde no han sido clientes compradores son justamente clientes que tienen salarios menores al de los tres citados anteriormente con lo cual voy a volcar eso a este gráfico y acá tengo los cuatro casos no compradores y los tres casos compradores como el gráfico bien obviamente aquí pongo los usuarios o clientes en virtud de su salario obviamente hacia la izquierda que y hacia la derecha que cobra más por eso los tres que cobran más están en el valor 1 y los cuatro que cobran menos están en el valor cero yo debería tratar de resolver esto a través de un algoritmo obviamente estamos hablando de logística y es esa la solución pero vamos a ir siempre por el camino de lo que no es y sabemos que no es pero para tratar de demostrarlo Entonces vamos a suponer que quiero resolver esto a través de un algoritmo de revisión lineal es decir que en lo que voy a buscar es una fórmula de este tipo que me arroje gráficamente como modelo esa línea como ya lo hemos visto en la clase pasada evidentemente se nota que esta expresión gráfica que representa a este modelo desarrollado con esta fórmula de regresión lineal simple no toca cerca estos puntos y Por ende hay un margen de error muy grande concretamente Este modelo no sirve para este caso con lo cual vemos que simplemente tiene un margen de error chico este punto de aquí arriba con este punto de aquí abajo pero con los otros tres casos de acá y con los otros dos casos de aquí arriba está muy lejos y de nuevo Este modelo no es para este problema Cuál es la solución obviamente un algoritmo de relación logística que se visualiza gráficamente con esta función que se llama función sigmoide que es esta línea Celeste que está moviendo aquí obviamente esta función tiene una característica que se recuesta mucho Sobre uno de los lados después tomo una curva y termina recostándose muy largamente sobre el otro de los costados evidentemente está representación es mucho más apropiada para esta distribución de puntos que es la que vimos anteriormente con la recta propia de la regresión lineal simple entonces la regresión logística se representa gráficamente con una de este tipo que se llama función sigmoide Pero cuál es el problema aquí en realidad la función sigmoide tampoco arroja resultados ceros y unos eso no existe en la realidad ustedes piensen que si bien Yo estoy tomando aquí un valor como es simplemente el salario podría tomar otros como el género la edad o cualquier otro es muy difícil que algo sea tan tajante como para que una persona con determinadas características definitivamente sea un comprador o definitivamente no lo sea la situación no se maneja de esa manera la situación te maneja de manera probabilística es decir yo tengo una idea de la probabilidad de que alguien compre o la probabilidad de que alguien no compre es decir tengo que pensar en un Rango de posibilidades que lo hagan potencialmente comprador o lo contrario pero no es algo como dije antes concretamente definitivamente comprador o definitivamente no comprador porque en realidad esto con no se maneja de esta manera Cómo se maneja de esta manera como lo estoy ahora mostrando es decir cada uno de estos puntos no va a estar recostado sobre el 0 o sobre el uno sino que va a estar sobre la función sigmoide y Por ende va a tener una probabilidad más cercana a ser cero o una probabilidad más cercana a ser uno Cómo es esto fíjense que yo aquí este punto este y este están sobre el cero pero lo que se establece en el caso de la reelección logística es un valor que se llama umbral es un valor que en este caso se establece como 05 se puede cambiar habitualmente de manera estándar es 05 ese valor me va a decir que si un elemento está por encima de ese valor es decir entre 05 y 1 va a ser un valor que probablemente sea un comprador Y si está por debajo del 05 lo contrario potencialmente un no comprador bien pero qué pasa este elemento que está aquí arriba este cliente que está aquí arriba es probabilísticamente mucho más cercano a comprar que esta que está aquí abajo si bien los tres son potenciales compradores los tres no tienen la misma probabilidad de serlo es decir este es más candidato este un poco menos y esté un poco menos en el caso contrario lo mismo este está muy cerca de ser potencialmente un comprador y no es tan claramente un no comprador en sentido inverso este es menos candidato a no comprar este menos y este es el mejor candidato o el que se tiene las referencias más fuertes de que es una persona que probablemente no compra concretamente entonces todo lo que se maneja con la función sigmoide no es Definitivamente sí o no blanco o negro o cero y uno es una probabilidad a partir de esa probabilidad yo asumo en base al umbral que yo determino si va a ser alguien que va a ser calificado como una cosa o como otra es decir en este caso si va a ser calificado como futuro comprador o futuro no comprador pero en base a una probabilidad y tomando como referencia este umbral para ver si está de un lado o del otro resumiendo todo lo expuesto aquí en cuanto a la regresión logística la forma del modelo de relación logística que describimos aquí como función sigmoide podemos decir que pasamos de tener un problema de unos y ceros cuando estaban todos estos elementos recostados sobre los uno sobre los ceros a tener un problema de probabilidades con lo cual es muy importante definir el umbral este número de aquí este 05 que determina si un elemento en base a su probabilidad está sobre el 1 o está sobre el 0 recordemos que este es un valor estándar se puede correr habitualmente a valores que son mayores a 0.5 y No todo lo contrario una cosa más nosotros teníamos una expresión matemática que nos identificaba perfectamente la revisión lineal simple y la regresión lineal múltiple como también la dirección polinomial ahora cuál es la expresión matemática de la regresión logística Ya vimos que esta que está aquí no lo es Ya que esto Define una recta la expresión es esta que está aquí p por probabilidad es 1 sobre 1 + y a la menos y igual a x + B Qué es eso es la fórmula que teníamos antes ven que toda esta expresión ahora si está dentro de la potencia negativa a la cual está elevado a la e Recuerden que como en la clase pasada también vimos justamente la expresión polinomial todo aquello que ya describa una fórmula en cuanto esté elevada a una potencia va a dejar de tener una forma recta o plana a tener una forma curva como en este caso el caso en la regresión logística Y cómo fue en el caso de la clase anterior la regresión polinomial bien vamos ahora a las ventajas de este modelo de revisión logística bueno obviamente es un modelo muy sencillo muy fácil de aplicar y que justamente permite predecir con una referencia binaria tomando un conjunto de datos de entrada si alguien va a tener un estado u otro los coeficientes del modelo se pueden interpretar fácilmente ya que Recuerden que en la clase pasada cuando definimos el modelo justamente en la parte práctica veíamos el valor del coeficiente y el intercepto en este caso va a ser exactamente lo mismo y a través de los coeficientes si tenemos justamente más de una variable de entrada vamos a poder observar el piso de cada uno de ellos por qué porque el que tenga el coeficiente más grande va a ser la variable de entrada que más peso tenga y Por ende más relevancia a la hora de eliminar la salida en este caso que vimos en el ejemplo recién si alguien podía hacer o no un comprado es computacionalmente eficiente que quiere decir esto ya lo venimos Hablando muchas veces el tema de el gasto computacional es decir puede trabajarse con pocos recursos de Hardware y obtener buenos resultados a pesar de ellos con grandes conjuntos de datos y justamente respecto de los conjuntos de datos tenemos que ver un tema muy importante evidentemente Este modelo va a funcionar muy bien en tanto y en cuanto exista una muy buena relación entre la variable de entrada y la variable de salida para esto vamos a tomar el caso justamente que tomamos como ejemplo de los clientes del comercio que compraban o no compraban aquí tenemos el conjunto para graficar habíamos tomado el salario porque veíamos rápidamente que hay una relación muy fuerte entre el monto del salario Y la factibilidad o no de compra si recuerdan que 5.000 3.000 y 2000 eran los salarios de las personas que habían comprado y en el caso de las que no habían comprado tenían saleros inferiores al más bajo de los tres referenciados anteriormente pero si miro edad y género rápidamente voy a observar que no existe esa relación entonces en la medida que sea una relación como el caso de salario y compra una relación bien directa entre la variable entra en la variable salida evidentemente el modelo de reacción logística va a ser un modelo muy fácil de implementar y de altísima eficiencia vamos ahora en las desventajas del modelo de regresión logística y la principal desventaja es que es muy propensa al sobreajuste es decir cuando se trata de datos complejos es muy probable que pueda funcionar bien para los datos de entrenamiento pero no así para los datos de Test o los datos nuevos con lo cual es un modelo que a veces justamente por problemas de sobre ajuste no generaliza bien Vamos a dejar este concepto de sobre ajuste un poquito standby y lo vamos a abordar al finalizar esta clase bien qué más lo que deseamos recién en el sentido contrario en el caso de las ventajas si no hay una relación fuerte entre la variable de entrada de la breve salida bueno todo lo bueno que es este modelo pasa a ser todo lo contrario en el caso que veamos antes en este conjunto de datos que usamos como prueba el caso de la edad o del género si no hay una relación directa con la factibilidad de que alguien compre o no Entonces para ese caso un modelo de relación logística va a estar lejos de ser eficiente bien qué más no tiene que haber correlación entre Las observaciones Recuerda que Las observaciones son cada vez que tomamos datos de algún evento en concreto por ejemplo que vimos recién el caso de los datos de un comprador no tiene que haber una relación una secuencia entre esas observaciones tienen que ser como dice aquí totalmente independiente porque si no obviamente eso va a tener problema y este modelo no nos va a servir y este modelo también tiene dos problemas en dos situaciones puntuales una de ellas es con los datos faltantes no podemos tener un modelo con Nan se acuerdan los datos que eran nulos es decir Tenemos que tener todo el conjunto de datos completo y también con los datos atípicos recordarán ustedes que cuando en la clase pasada dibujamos por primera vez un modelo a través de esa línea en ese gráfico de dos dimensiones veíamos que había muchos puntos que estaban muy alejados de la recta y Por ende eran lo que habíamos dado en Llamar valores atípicos o sea no son muchos no su ser significativos pero a veces aparecen Y eso lo que hace es distorsionar el modelo en realidad eso afecta a cualquier modelo en general pero lo afecta mucho a este modelo de regresión logística en particular bien todo lo que hemos hablado hasta aquí han sido temas que se relacionan con regresión logística binaria es decir aquella situación en la cual la variable objetivo tiene Solo dos resultados posibles pero no es el único caso como dijimos cuando introducíamos a esta clase de regresión logística la regresión logística también puede ser tipo multinomial y cuando es multinomial bueno justamente cuando la variable objetivo tiene tres o más categorías nominales por ejemplo podemos predecir el comportamiento de una acción en ese caso tenemos tres estados posibles puede ser que la acción baje suba o se estabilice bien de este modo nosotros podemos pensar en resultados que no solamente binarios no es una religión logística binario 01 positivo negativo en sermo o no enfermo sino en algo que arroje tres o más resultados posibles a lo largo de este curso Vamos a abordar muchos conceptos que en el contexto de la práctica van quedando claros pero que ameritan una explicación un poco más profunda Este es el caso de El sobreajuste y la idea es respondernos esta pregunta qué es el sobreajuste para ello vamos a usar un ejemplo muy cotidiano y que nos va a ayudar a entender mejor esta idea todos nosotros identificamos rápidamente distintos tipos de estudiantes en el colegio algunos de ellos estudian muy en profundidad cada una de las cosas y le gustan analizar esos conceptos en profundidad otros utilizan una herramienta muy común que es la memoria es decir rápidamente Cómo se dice habitualmente en la jerga estudiar de memoria justamente nos lleva a grabar en nuestra cabeza conceptos como si fuesen una grabadora o un cassette para después justamente a partir de tener los presentes en la memoria rápidamente definirlos esos dos tipos de alumnos pueden tomarse como ejemplo para explicar el sobreajuste Por qué en realidad yo tengo un alumno que seguramente va a tener buenos resultados con una postura y otro alumno que va a tener buenos resultados con la otra postura pero miremos un poco más allá qué pasará el día de mañana si tenemos un problema que se nos presenta que se corre un poco de lo habitual o de lo que quizás el alumno de la derecha está memorizando Cuál de los dos alumnos piensan ustedes que serán exitosos cuando se presenta un problema diferente a todo lo visto hasta el momento Obviamente el de la izquierda y obviamente no el de la derecha bien Eso es lo que hace el sobreajuste es decir toma tanta referencia de los datos de entrada y se basa tan fuertemente en ellos que es como el caso de la derecha es como que los registra como un estudiante que estudia de memoria de modo que cuando aparecen datos nuevos ese modelo fracasa por eso es que hablamos muchas veces que nosotros hacemos una medición de score en el conjunto de entrenamiento que sería esta memorización de los datos Y en el conjunto de Test que sería una representación de lo que a futuro serían nuevos datos si el score es bueno en el conjunto de entrenamiento y en el conjunto de Test quiere decir que como dijimos antes es algoritmo es bueno porque porque está preparado para lo que estudió supuestamente ya sea de memoria o no y está preparado para lo que viene de alguien más es decir el modelo que generaliza bien va a ser aquel modelo que no estudia de memoria sino que toma un patrón de comportamiento muy general para que después dentro de esa generalidad cuando aparezcan datos nuevos ese modelo sea exitoso si Por ende copia mucho o muy fuertemente en los datos de entrenamiento a punto tal es excelente para ese dato de entrenamiento pero no lo es para el conjunto de Test y mucho menos para los datos nuevos pues entonces ese modelo no es bueno porque porque lo que hizo fue estudiar de memoria Bueno espero que con este ejemplo sencillo se haya podido entender el concepto sobre ajuste que es algo que tenemos que tener muy presente y debemos combatir y gracias a Dios tenemos herramientas como skyler que nos permite a través de su método score analizar si estamos en presencia de un modelo sobre ajustado o no bien hasta aquí esta clase número 7 de regresión logística primera parte ahora vamos a la segunda parte donde vamos a poner en práctica todos estos conceptos que acabamos de ver los espero en la parte número 2 hasta aquí llegamos con esta primera parte ahora ya tenemos los conocimientos de regresión logística y también del concepto de sobreajuste que nos permitirán empezar a programar algoritmos en la segunda parte nos vemos allí Titulo: Clase7 (parte2) del Curso de Inteligencia Artificial \\n URL https://youtu.be/oVYt4UtkvPI  \\n 1969 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 7 del curso de Inteligencia artificial de ifes en ella vamos a poner en práctica el algoritmo de regresión logística que recién aprendimos vamos por ello  Bueno Hola a todos de nuevo estamos en la segunda parte de la clase número 7 ahora con la práctica que le sigue a la introducción conceptual de la regresión logística que es el título que aquí tenemos en nuestro Notebook clase 7 que vamos a tener que abrir para poder seguir justamente la práctica de este concepto vamos a ver dos ejemplos de regresión logística que tienen que ver con justamente una de las últimas cosas que tratábamos en la introducción teórica por un lado una regresión logística con salida binaria es decir con un Target que es 0 o 1 dentro del Data set que vamos a utilizar y luego vamos a ver un ejemplo con salida discreta no binaria es decir con más de una opción de salida bien pero empecemos con lo primero que es justamente en la opción con salida binaria Y empezamos por la descripción como siempre de las librerías que vamos a utilizar bien vamos a empezar por lo que ya venimos usando habitualmente que es pandas y hasta ahí estamos porque en este caso no vamos a usar ni Nampa ni maplogy si vamos a usar muchas librerías nuevas de sideking learn pueden empezar vamos a usar de Side Killer linear model que habíamos usado en la clase anterior pero en este caso con logística integration porque obviamente es el tipo de algoritmo que vamos a ver ahora pero también vamos a usar además de 30 splits que también lo veníamos usando anteriormente Data sets que es de donde vamos a sacar para el primer ejemplo el conjunto de datos estándar que sirve para escalar datos concepto que hablamos también en la parte teórica de la importancia de escalar los datos y finalmente con fusión Matrix que es justamente una de las formas nuevas que vamos a aprender de cómo evaluar bueno el nivel de eficiencia de un algoritmo Por ende Entonces lo primero que hacemos Es ejecutar esta celda para poder empezar a importar las librerías bueno tenemos un tiempo acá como siempre de delay porque no estábamos conectados Ahí estamos conectados por lo tanto ahora sí le va a dar curso a la ejecución de esta parte importando todas las librerías bien Ahí está tardó dos segundos obviamente porque incluye el tiempo de la conexión con el Google luego vamos a trabajar con el Data set fíjense que en este caso no vamos a tomar un Data set de la forma que lo veníamos haciendo si lo vamos a hacer para el segundo ejemplo Pero de esta manera Vamos a aprender Bueno una forma nueva de incorporar un dataset fíjese que aquí en el título yo lo pongo para que le sirva como referencia que justamente lo que vamos a importar no es un archivo de tipo csv es un Data set preparado sobre casos de cáncer de seno en este caso tenemos Bueno un montón de detalles que tienen que ver con ese tema de salud y finalmente un indicador que una característica Obviamente que nos indica si ese paciente tuvo o no cáncer bien cómo se importa un Data set de este tipo bueno se hace justamente es de la librería que hemos importado datasettes la tenemos aquí arriba Esta de Side y dataset punto loadbrest Cáncer ya es un datas incorporado dentro de todos los que tiene esta librería de datos Sí todos secarán con load y la referencia hacia el nombre del obviamente esta información está dentro de la página de cycle Y ustedes pueden consultar bueno por otros datase que no son estos bien eso lo ponemos dentro de la variable Data y una vez que hacemos eso vamos a hacer un print de Data Kiss y Qué es Data Kiss es un diccionario como Ven aquí con un conjunto de objetos que describen distintas características del conjunto de datos por un lado tengo Data y Target que son los más importantes estos son los datos Y este es el dato objetivo y otros que vamos a usar más adelante como este que describe características de detalle de la composición de ese conjunto de datos Pero lo primero que vamos a hacer es tratar de lograr una forma parecida a la que veníamos trabajando cuando importamos la información de un archivo de tipo ccb o txt o xls bien en este caso la información la tenemos separada como dijimos recién dato por un lado o datos por un lado y dato objetivo por el otro por lo tanto para buscar esa forma que referenciamos hace un rato vamos a tener que usar el método Data frame de pandas y por un lado vamos a tener que poner dentro de DF que es nuestro futuro Data frame justamente los datos y el nombre de esas características porque Data punto Data si acuérdense que esto es Data de el objeto Data me traen los datos puros pero sin los nombres de las columnas para ello voy a tener que especificar con columns igual a Data fitur names fíjense Que aquí tiene filtro names de que cada uno de estos datos los tomas de este diccionario de X es decir estoy aquí con ambas cosas yo logro tener ahora los datos de cada una de las características menos el dato objetivo y los nombres de esa característica finalmente voy a tener que agregarle a ese Data frame el target es decir el dato objetivo con lo cual voy a tener que crear un nuevo una nueva columna aparece otra frame justamente de esta forma como se hace habitualmente y luego el dato concretamente es decir que con esta instrucción lo que estoy haciendo es crear una nueva columna y ponerle el dato para crear la nueva columna implícitamente le estoy poniendo el nombre a esa columna justamente con lo que pongo entre los corchetes y entre comillas una vez que logro con estas dos instrucciones la conformación del Data frame auged para ver resultado bueno todo eso lo ejecutamos ahora y veo resultado Ven aquí tengo los nombres de cada una de las columnas los datos y finalmente el dato objetivo el target que me dice con un cero si esa persona no tuvo cáncer o con uno que esa persona tuvo cáncer bien Ahora vamos a ver lo que referenciamos recién como otra de las claves de este diccionario de claves de el conjunto de datos brest cáncer vamos a ver la descripción de características generales esto es una cuestión que ustedes pueden usar o no no es exigible pero les va a permitir conocer mejor justamente las características del Data lo que vamos a hacer es recurrir a Data punto desk fíjense que recurro nuevamente al nombre Cuando cargue el conjunto de datos Y una de las claves del diccionario es en este caso descom bien y hago un print para ver justamente la composición de la información de ese elemento fíjense que me muestra toda la información del conjunto de datos bien el nombre la cantidad de instancias ese tiene 569 observaciones los atributos bien la información de Qué significa cada uno de los atributos los valores mínimos y máximos de cada uno de esos atributos los atributos a las características no obviamente bien y alguna información más general de bueno Quién fue el que hizo este del origen digamos de este conjunto de datos no bien Por eso digo esa información general no siempre es necesaria toda sí es importante Por ejemplo esto que está arriba que tiene que ver con la cantidad de distancias y el deuto que también lo podemos ver con un shape y bueno de Qué significa cada uno de los de las características para poder entender justamente mejor la característica de los datos y poder justamente operar mejor con ellos bien una de las cosas nuevas que vamos a visualizar ahora es Cross el método de pandas Para qué sirve el método bueno sirve para poder reconocer la característica de los datos Por qué Porque yo aquí tengo obviamente un conjunto de datos que dijimos tienen un dato objetivo que es este que está aquí final que me dice si la persona tiene o tuvo cáncer lo que yo voy a hacer ahora es saber cuántas personas de este conjunto de datos tuvieron o no cancelan Qué hago una tabla Cruzada quien maneja Excel este concepto seguramente lo debe tener la tabla Cruzada se alimenta de Data punto Target es decir los datos objetivos el cero y el uno y lo que le digo es que como operación me cuente los datos de cada una de las columnas es decir tengo que contar Cuántos casos targets hay cero y Cuántos casos targets hay uno lo ejecuto y el resultado se muestra de esta manera en esta primera columna me muestra cero y uno que son los dos datos de tipo binario porque lo que estamos viendo justamente es un tipo binario que tiene solamente dos tipos de resultados y me muestra a continuación con el count sí que lo que yo le pedí aquí tiene cuente Cuántos casos hay 0 y Cuántos casos hay uno bueno con esto ya puedo tener una primera aproximación para entender características de los datos luego de esto lo que voy a hacer es empezar por separar nuevamente los datos por un lado y el target por otro parece que fuera contradictorio que recién nos uní y ahora los separo pero recuerden que justamente lo próximo que tengo que hacer empezar a desarrollar el algoritmo es entrenarlo y para entrenarlo Sí o sí tengo que tener separado por un lado los elementos x los elementos de entrada y por otro elemento de salida que es el así que bueno simplemente los datos que ya tengo los pongo en variables X e Y por una cuestión de nomenclatura tradicional puede ser el nombre que quieran Así que pongo en x los datos de entrada y en los datos de salida y luego hago lo que ya sabemos la separación en xtrain y x test los datos de entrada separados en conjunto de entrenamiento y conjunto de Test y lo vimos los datos de salida separado en itras es decir la salida para entrenamiento y la salida para test y Recuerden que siempre le pongo X e Y que son los datos que yo le doy para que se pare y el t6 que es el 20% que ya lo explicamos la clase pasada puede ser otro valor diferente Pero habitualmente se estipula en el 20% Recuerden que esto lo que hace es poner el 80% en el conjunto de entrenamiento y el 20% en el conjunto de Bueno ahora tenemos que hablar del escalamiento de datos pero la primer pregunta responder aquí es Por qué es importante escalar los datos o por qué es necesario escalar datos vamos a ir al Data set para tratar de entenderlo mejor fíjense que aquí tengo cada una de las características del Data frame del Data set Pero estas no son similares más bien que son todas numéricas que es lo que corresponde pero no son similares y en algunos casos la distancia desde lo numérico de cada una de ellas es muy importante fíjense Aquí tengo por ejemplo como perímetro la cifra 122.80 pero luego en el concepto la característica suavidad tengo un valor que está cuatro o cinco dígitos Perdón después de la coma es decir entre este valor o por ejemplo este del área que es mucho más significativo aún hay una distancia muy grande esto incide a la hora de generar el algoritmo Y si bien coincidir las características las características deben incidir en cuanto a la importancia o lo determinantes que son para el dato objetivo es decir concretamente en este caso ver si esta suavidad o esta área o este perímetro son relevantes para determinar si la persona tiene o no cáncer Pero eso no tiene que depender de la magnitud de las cifras sino de la importancia del dato Espero que se entienda a qué me refiero es decir yo necesito saber si es una persona puede tener cáncer o no en base a su textura su perímetro o su suavidad pero como dato y no en virtud de la magnitud de la cifra comparada con otra para que esto pueda ser más justo y tenga el peso cada característica que corresponde y no el peso dependa del nivel de cifra que manejan es que hay que escalar los datos es decir todos los datos tienen que estar encuadrados entre 0 y 1 eso va a ser de que la diferencia entre el valor de estos reduzca considerablemente aún así va a representar dentro de cada dato justamente un valor escalarmente igual es decir este valor que está aquí dentro del perímetro va a ser siempre menor a este y este va a ser siempre mayor a este y este va a ser siempre menor a este Pero dentro de una escala por eso volviendo al punto yo tengo que ahora como próximo paso escalar los datos después de haberlos separado qué datos tengo que escalar tengo que aclarar siempre los datos x los datos de entrada no los datos de salida los datos de salida siempre va a ser cero y uno Eso no lo voy a escalar si son los datos tal cual está son binarios 0 o 1 pero si lo que tengo escalar son el resto de los datos que son cada una de las características que vimos recién por eso lo primero que tengo que hacer es cargar el estándar escáner si creando un objeto escalar que le puedo poner obviamente como varia Cualquier nombre de variable una vez que tengo ese elemento yo lo que voy a hacer es decirle que el xtrain que ya tenía de antes cuando separé los datos entre entrenamiento y test es igual a esa variable punto fit transform del mismo XT es decir transformó el mismo conjunto de datos que traía escalando todos sus datos creando nuevo x-traime Pero ahora con datos escalados y lo mismo hago con el X test fíjense que en el caso de xtrain yo uso fit guión Bajos transform en el caso de XT uso Solamente porque porque justamente este fit tiene implicancia que en que justamente lo que voy a hacer a continuación que es entrenar el modelo lo hago con xtraing no con x test Por eso uso dos métodos de escalamiento diferente fit transform para los datos que van a ser utilizados para entrenar al modelo y transform para aquellos que simplemente me van a servir como ya sabemos para poder verificar deficiencia del modelo una vez entendido esto vamos entonces a crear nuestro modelo Crea una variable logic model el nombre que quieran como siempre lo digo igualando a logística integration que justamente es el tipo de algoritmo que vamos a hacer ahora una vez que le digo que voy a crear un la variable que es justamente para crear un modelo de regresión logística lo que hago es entrenar el modelo con fit como ya lo hicimos antes con la regresión lineal dándole como parámetros el x-3 y el itrain que son justamente los valores de entrenamiento así que me olvidé de ejecutar esto Perdón ejecutó primero el escalamiento y ahora generar modelo ya tengo el modelo con lo cual hacemos lo que sabemos hacer siempre verificar justamente la precisión de el modelo lo hago y tengo una muy buena precisión fíjense que tengo 0 98 en el modelo de tren y 097 el modelo de Test como ya dijimos no es lo ideal que la precisión del modelo de Test se inferior al modelo de tren por una posible posible problema de sobre ajuste que es un concepto que vimos en la parte teórica de esta misma clase la diferencia no es significativa es decir lo grave aquí es que justamente la presión del modelo de tren sea un número y el modelo test sea mucho menor a ese número no es el caso con lo cual si bien no es lo ideal la diferencia puede representar que el modelo que tenemos para este caso es un buen modelo Qué vamos a hacer a continuación vamos a hacer las predicciones para llegar a ver el concepto de matriz de confusión que Recuerda es una librería que importamos al principio de la clase con lo cual lo que voy a hacer es armar predicciones en a todo el conjunto de Test Sí ese 20% de datos lo voy a tomar los datos de ese 20% del conjunto los voy a tomar para ahora que tengo que el modelo ya generado establecer predicciones y una vez que tengan las predicciones hechas qué voy a hacer lo que venimos haciendo siempre comparar las predicciones con el valor y test que es el valor Real del conjunto de datos que determina el dato objetivo es decir yo voy a tener ahora un dato objetivo real que es el y test sí es este valor que está aquí y un dato objetivo que es producto de una predicción que puede ser igual o no al itest Bueno ahí está justamente la eficiencia del modelo por lo tanto lo primero que hago son estas dos líneas que justamente lo que me permiten es hacer la predicción del conjunto de Test y poner una variable ipred y finalmente lo que hago es la matriz de confusión que insisto es la librería que incorporamos al principio de la clase volvemos arriba por si no lo recordamos sí es esto que importe de cycle volvemos y luego voy a mostrar la matriz la matriz de confusión justamente me permite la comparación que hablábamos recién delites e iliped lo ejecutamos y seguimos hablando de ello Acá tengo la matriz de confusión tiene cuatro componentes y aquí yo he puesto una descripción para que traten de entender Qué significa cada uno de esos cuatro números en principio vértice superior izquierdo es decir el 41 que está aquí me de me Define me detalla la cantidad de valores que debían dar 0 y se predijeron como ceros en la cantidad de aciertos de casos de no cáncer es decir hubo 41 casos que en la realidad no fueron cáncer y en la predicción me dijo que eso no debía ser cáncer pero luego tengo a la derecha verde superior derecho es el 2 de aquí cantidad de valores que deben dar uno y se predijeron como 0 es decir cantidad de valores que fueron cáncer y la predicción falló dijo que fueron casos de no cáncer con lo cual tengo dos errores y luego tengo en la parte inferior izquierdo la cantidad de valor que debían dar cero y se predijeron como lo contrario al 2 de recién o sea un dato que demuestra una falla del algoritmo porque predijo uno es decir un caso de cáncer cuando en la realidad era un caso de no cáncer y finalmente el 70 son la cantidad de valores que debían dar uno y se predijeron como uno es decir otro acierto del algoritmo con lo cual la lectura aquí que voy a ser genérica de esto es que hay 41 + 70 es decir 111 casos que debía predecir algo y justamente Esa predicción fue correcta y tres casos el 2 y el 1 donde la predicción falló esto lo que me permite justamente con una visualización muy rápida reconocer la matriz de confusión que se suma a lo que yo veo aquí como presión de modelo veo con la presión es muy alta con lo cual el error es bastante bajo porque ustedes consideran la suma total de valores del conjunto es decir 70 más 41 111 más 3 114 sobre 114 datos sí o casos Mejor dicho hay solamente tres fallas habría que sacarlos justamente del mismo porcentuales y me va a dar un valor cercano a esto es decir me tiene que dar un valor de un error del 0,1 perdón cercano al 97 al 98 porque estamos hablando del conjunto de 0,03% de error sobre el total de datos bien Esto es a partir de ahora y más que nada para los modelos de regresión logística una buena herramienta para poder medir la precisión del modelo Bueno ahora vamos con el segundo ejemplo de la práctica en donde justamente como dice que el título tenemos que ver el algoritmo de regresión logística pero con salida discreta no binaria si ya no un caso de cero o unos de una persona que tiene cáncer o no como el ejemplo recién sino en este caso una situación que nos va a dar tres resultados posibles bien antes de describir los datos que vamos a usar vamos a empezar como siempre con las librerías que vamos a importar en este caso tenemos pandas warnings y yo y además tenemos por un lado logística recreation estándar Scanner 30 splits y confusión Matrix que son las cuatro mismas que usamos en el ejemplo por eso este ejercicio va a ser bastante rápido porque muchos pasos se van a repetir y va a haber otra cuestión que también va a acelerar este ejercicio que es que vamos a volver a trabajar con un set de datos proveniente de un archivo de tipo csv tal cual lo veníamos haciendo en los casos anteriores de las clases anteriores Así que empecemos con eso importamos las librerías vamos ahora a importar el archivo y así tenemos el archivo usuarios Win Mac link ssd bien ahí ya está importado de qué se trata este Data set bueno lo vamos a averiguar cuando lo carguemos como siempre con ricsb de pandas a la variable de datos y hacemos un dato Head aquí lo ejecutamos y vemos un conjunto de datos tal cual lo que yo puse aquí arriba es un archivo csv con datos de los sistemas operativos utilizados por los usuarios donde la variable objetivo o Target es justamente la característica clase clase describe el tipo de sistema operativo que usa el usuario teniendo la codificación puesta de modo de que 0 representa Windows 1 Mac y dos Linux en este caso tenemos otras cuatro características con las cuales justamente vamos a ir buscando el mejor algoritmo posible bien visto esto podemos hacer un shape para reconocer que ese conjunto de datos tiene observaciones y como estamos viendo aquí justamente cinco características dentro de la que describimos recién como objetivo es la clase bien Vamos a hacer una tabla Cruzada como hicimos en el ejemplo de recién también repetimos la experiencia solamente que ahora tenemos tres resultados posibles antes Teníamos dos que era 01 en este caso tenemos 0 1 o 2 bien entonces con la tabla Cruzada y volviendo a hacer la operación de conteo nos dice que este Data se tiene 86 casos de sistema operativo Windows 40 Mac y 44 Linux a continuación voy a separar los datos en x y en y tal cual lo hice en el ejemplo anterior por lo tanto voy a tomar para el caso de X los datos con las características duración páginas y valor y para la y la variable objetivo clase bien una vez que tengo separados los datos lo ejecutamos voy a separar los datos en entrenamiento y test Aquí no hay nada diferente a lo que hicimos antes tampoco hay nada diferente respecto del escalamiento de datos Exactamente lo mismo que en el caso recién y ahora aparece la primera diferencia por un lado Cuando creo el modelo de agresión logística a través de logística generation y esta variable que estoy creando Aquí estos paréntesis antes estaban vacíos ahora va a tener dos parámetros por un lado múltiplas igual a multinomial es para decirle que este modelo no es de tipo binario que es de tipo multiplace es decir que tiene tres o más posibles salidas y con Max iter está escrito aquí arriba es el número máximo de iteraciones necesarias para que los solucionadores converjan una definición muy compleja para ver nuestra altura Vamos a darle una explicación muy sencilla y después si la clase que viene le vamos a dar una explicación más profunda en la introducción teórica de la clase que viene todos los modelos que trabajan buscando el mejor algoritmo lo hacen reiterando a través de procesos de entrenamiento un proceso que hace que justamente vaya mejorando y la solución se logre de la manera más rápidamente posible la solución óptima se logre de la manera más rápidamente posible esto puede darse en una determinada cantidad de ciclos que justamente puede ser que sea un valor muy alto o muy bajo pero que justamente con Max híper le decimos quiero que hagas hasta mil intentos para lograr la mejor solución si al intento número 1000 no hay una mejor solución me tomo Esa sí antes de eso existe una mejor solución No voy a llegar a esa cantidad de intentos sí de iteración de reiteraciones Entonces esto es para qué Para que no vayamos buscando a través de una cantidad ilimitada de iteración y esto nos lleve a un gasto computacional muy alto en Pos de lograr un buen algoritmo entonces este parámetro indica eso por ahora nos vamos a crear con esta aplicación sencilla la clase que viene lo vamos a ver en profundidad y vamos a ver el concepto de optimizador que tiene que ver justamente con este argumento luego justamente hacemos el entrenamiento que sí es igual a lo que hicimos antes así que bueno avancemos creado el modelo medimos su presión y vemos la presión del modelo es bastante pobre 69% en el modelo de entrenamiento 64 en el modelo de Test no es lo ideal ya la diferencia es más grande que en el caso anterior Por lo cual no es un buen modelo vamos a hacer las predicciones como lo hicimos antes las podemos ver en este caso justamente poniendo ipred me muestra la raid fíjense todo lo que predijo tomando como conjunto de entrada el conjunto de Test se hizo todas predicciones No hay ninguna predicción 1 fíjense que son todas es decir Windows o Linux No hay ninguna opción de Mac sí bien hacemos ahora la matriz de confusión que se acuerdan que era aquello que nos permitía también ver la precisión del modelo al igual que score de manera complementaria corporación de alguna manera y vemos la materia de confusión que en este caso no es una matriz dos por dos sino tres por tres obviamente porque tengo tres tipos de salida diferentes con lo cual como Leo yo aquí Cuáles son las precisiones correctas y las incorrectas bueno mirando la diagonal principal en este caso 15 predicciones correctas respecto de en la situación cero o la salida cero cero ese ninguna fue correcta en el caso de el tipo de salida Mac lo cual no me parece lógico porque justamente estamos viendo acá que no hay ninguna que sea Mac son todas Windows o Linux y 7 correctas en el caso de Linux fíjese que en el caso de Linux las 7 son las siete que había es decir no se equivocó en ningún caso en el caso de Windows 15 fueron correctas pero 7 fueron incorrectas y en el caso de Mac evidentemente es muy mala la precisión por eso no es de extrañarse el resultado visto los números de score que vimos aquí arriba son muy bajos con lo cual tengo una posibilidad en el conjunto de test del 36% de equivocarme y justamente se haga un cálculo matemático respecto de la cantidad total de elemento de test y la cantidad de errores me tiene que dar ese indicado vamos a hacer una prueba ahora sin escalar los datos como dice el título aquí y agregando un solver Qué es un solver Bueno un poco es esta cuestión que referenciamos recién cuando hablábamos de los optimizadores que me permiten a mí lograr un mejor algoritmo bien es optimizadores hay varios y se pueden ir cambiando esto Ya es empezar a lo que se conoce habitualmente en la guerra como tuneo de El algoritmo cuando voy justamente buscando el mejor o la mejor versión en cada uno de los entrenamientos en cada uno de los intentos de la mejor algo bien acá tenemos un primer ejemplo en el caso anterior también con el Max iter también es un tuneo digamos de este los algoritmos y aquí en este caso voy a cambiar no solamente el Max Inter pasándolo de mil a 5.000 es ir ampliando la la espera digamos la tolerancia de cantidad de intentos para lograr el mejor sino también le voy a cambiar el solver ya tiene un solver por defecto la regresión logística que es lo que está escrito aquí arriba lbfs pero eso también se puede cambiar buscando que a través de otros solucionador o a través de otro optimizador podamos lograr un mejor algoritmo con un mejor score en este caso hay varios tenemos Newton saga y sag además de este que es el más conocido o el que está por defecto que no hace falta ponerlo por supuesto ya está implícito y en este caso vamos a usar el Newton porque es el que me va a dar el mejor score pero ustedes si quieren pueden escribiendo ese aga o ese ag cambiar este parámetro acá y probar por sus propios medios justamente Cuál es la precisión que le va a dar bien Luego de eso hago el del Fit como siempre y mido la precisión obviamente anterior a eso hice justamente la separación de los valores y la separación de los conjuntos también de entrenamiento y test ejecuto todo tengo todo hecho en una sola en un solo conjunto de instrucciones no hay ningún problema con eso lo puedo hacer separado o todo junto bien en este caso veo que el nivel de score es muy superior al anterior tenemos 76 y 85 y algo muy bueno en score de Test es mejor que el entrenamiento lo cual es justamente una de las cosas que más deseables son a la hora de buscar un mejor un buen algoritmo no bien y luego vamos a hacer las predicciones en la misma expresión el mismo conjunto de instrucciones hacemos también la matriz de confusión fíjense que la matriz de confusión es mejor que la anterior en el caso de Linux sigue siendo muy bueno en el caso de Windows creo si no me equivoco Sí fue superior era 15 y 7 ahora es 16 y 3 y fíjense que en el caso de Mac que no había aparecido nada hoy justamente ahora aparece un 6 cuando Antes había un cero con lo cual la diagonal principal me muestra 16 + 6 son 22 y 729 casos de predicción exitosa mientras aquí tenía 22 obviamente todo responde al nivel de presión si estoy a 64 antes y ahora tengo 85 bueno evidentemente se tiene que ver también reflejado en la matriz de confusión bien hasta aquí esta clase quedó pendiente seguramente en la cabeza de todos ustedes este concepto del solver y del Max iter no se hagan problema en la clase vamos a hacer una introducción teórica donde vamos a hablar de este tema y lo vamos a hacer con gráficos Sí con animaciones que seguramente nos van a permitir entender esto mucho más fácilmente que aquí en el código por eso es que les pedí la licencia de darle una explicación muy breve muy superficial para después profundizar en la clase que viene así que nos vemos en la clase que viene hemos terminado la clase número 7 ahora ya sabemos manejar dos algoritmos la regresión lineal y la regresión logística nos vemos en la próxima clase para aprender un nuevo algoritmo de Machine learning hasta entonces  Titulo: Clase8 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/77arfIsarXI  \\n 1242 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 8 del curso de Inteligencia artificial de ifes en ella veremos los conceptos del algoritmo de árboles de decisión y también hablaremos del tuneo de algoritmos empezamos  Hola a todos Bienvenidos a la clase número 8 del curso de Inteligencia artificial seguimos en el módulo de Machine learning para ver nuestro tercer algoritmo de Machine learning en este caso árboles de decisión ya vimos en las dos clases anteriores regresión lineal regresión lineal simple revisión lineal múltiple y en la clase anterior progresión logística Hoy le toca el turno a este nuevo algoritmo árboles de decisión no obstante nos quedó el tema pendiente en la clase pasada cuando vimos el algoritmo de regresión logística habíamos tocado un parámetro que se llamaba solver al cual habíamos cambiado las opciones para poder lograr un mejor score de algoritmo bueno eso lo tratamos y lo explicamos de una manera muy sencilla recordarán y dijimos que en esta clase lo íbamos a trabajar en profundidad por eso vamos a verlo ahora incorporando un concepto que es el de tuneo del modelo cuando mencionamos la palabra tuneo seguramente esa palabra la podemos identificar con muchos ámbitos pero descarto que la mayoría de los casos esto no va a al tuneo vinculado a los autos Qué es el tuneo de autos el tuneado de autos es un proceso de personalización y ajuste de un automóvil para mejorar su rendimiento su apariencia y o su funcionalidad esto puede incluir la modificación de partes del motor la suspensión los neumáticos la aerodinámica y otros componentes para mejorar el manejo la velocidad y la eficiencia del automóvil el objetivo final del tuneo de autos y lograr un rendimiento optimizado y adaptado a las necesidades y preferencias del conductor a partir de ahora este concepto no va a estar solamente identificado con los autos sino también lo vamos a incorporar dentro del mundo de la Inteligencia artificial y las redes artificiales para poder explicarlo de la manera más clara posible vamos a iniciar Desde los primeros conceptos básicos del Machine learning para ello vamos a identificar que en realidad nuestro algoritmo de Machine learning puede verse como una caja negra caja negra que recibe valores de entrada y emite valores de salida o predicciones la esas predicciones sean lo más certera posible por eso trabajamos habitualmente tratando de mejorar los score y Tratando de buscar el mejor score concretamente para ello la pregunta que nos vamos a hacer es justamente Cómo puedo mejorar el modelo cuando vemos el modelo de relación lineal simple la fórmula era la siguiente en modo tal de que cuando yo tenía un valor de entrada x para obtener un valor de salida y lo que tenía que hacer era multiplicar este valor de entrada x por un coeficiente a y sumarle el intercepto B de modo de que esa cuenta me iba a dar mi valor de salida y esto lo veíamos cuando en la parte práctica escribiríamos lo siguiente primero definimos nuestro conjunto de entrenamiento y nuestro conjunto de Test luego generábamos el modelo en este caso de la dirección lineal y luego entre lavamos el modelo cuando eso pasaba Ya teníamos nuestra ecuación que es la que estaba aquí es decir y igual a este valor que es el a que aquí tenemos en la expresión de aquí arriba por x + B que es este valor que está aquí que es justamente lo que veíamos aquí en esta expresión eso sucedía toda y cada vez que nosotros justamente generamos un nuevo modelo a partir de un entrenamiento una vez logrado ese modelo mediamos el score que justamente lo hacíamos Con este código que tenemos aquí abajo y teníamos la precisión para el modelo de entrenamiento y la precisión para el modelo de Test pero allí terminaba la búsqueda de nuestro mejor modelo Instagram nuestro mejor modelo y sabemos que justamente aquí teníamos una precisión que dejaba mucho que desear por lo tanto es normal que Busquemos un nuevo modelo y este proceso no logró el mejor modelo entonces lo que yo puedo hacer es En búsqueda un mejor modelo volver a insistir con otro conjunto de entrenamiento y test volver a entrenar el modelo y ver Si obtengo un mejor Score por eso genera un nuevo proceso cambio el Random Style 3 antes de ir al Rango 32 justamente para generar un nuevo proceso y un nuevo modelo y al generar ese proceso obviamente los coeficientes son diferentes a los que tuve en el caso anterior y la precisión obviamente también Lamentablemente la precisión es peor que en el caso anterior Esto me lleva a buscar un nuevo modelo Por eso uso un Random Stay 5 y al reproducir un nuevo proceso veo que en este caso Además de que cambia el coeficiente y el intercepto el a y el B obtengo una mejor precisión que en los dos casos anteriores recordemos si vamos hacia atrás que tenemos en el primer caso Random State 2 tenía una precisión de 0,44 y 063 luego en el 3 tenía 0,46 047 perdón y 057 y en el Random Stay 5 tenemos 0 43 y 069 que esto obviamente pudo seguir intentando otros procesos y ver si es score aún pudo mejorarlo más O quizás este sea el mejor de todos concretamente no he hecho otra cosa más que repetir esta experiencia de cambiar el conjunto de Test y de entrenamiento generar un nuevo modelo Y a partir de ello tener un nuevo coeficiente un nuevo intercepto y Por ende un nuevo score que puede ser mejor o peor que el anterior esto no solamente aplica a la radiación lineal simple sino también a la relación lineal múltiple la diferencia aquí es que obviamente no tengo una sola variable de entrada sino que tengo más de una variable de entrada con lo cual voy a tener obviamente más de un coeficiente es momento de incorporar un nuevo concepto que es el de pesos que es un sinónimo de coeficientes pero que en este ámbito del Machine learning van a escuchar muchas veces a hablar del coeficiente como peso y es bueno que empecemos a incorporar Nosotros también en un algoritmo de Machine learning los pesos son valores numéricos es decir todas estas a que están aquí que se asignan a cada una de las características de entrada es decir por eso están justamente multiplicadas por cada una de las x esto son ajustados durante el proceso de entrenamiento del modelo como vimos el caso anterior y se utiliza para calcular la salida o está ahí que tenemos aquí en la expresión los pesos indican la importancia relativa de cada característica Qué quiere decir esto que si A3 por ejemplo Es mayor que a dos pues entonces la variable de entrada X3 va a tener más importancia para el modelo que la variable de entrada x2 el proceso de ajuste de los pesos es obviamente algo fundamental en el entrenamiento del modelo massignarle cómo lo vimos justamente en los casos de recién durante el entrenamiento el modelo ajustan los pesos para minimizar el error entre la salida real y la salida predicha es decir entre el valor que Calcula como predicción y el valor histórico que yo tengo en el conjunto de test el objetivo como siempre es Buscar el mejor score y para ello es imprescindible encontrar los pesos óptimos que produzcan las predicciones más precisas para datos nuevos y no vistos anteriormente la pregunta que puede caber ahora es es la única forma de mejorar un modelo de obtener un mejor score repitiendo insistentemente su entrenamiento cambiando el conjunto de tesis el conjunto de entrenamiento La respuesta es No y justamente esto lo vimos la clase pasada cuando abordamos el modelo de regresión logística donde trabajamos con un parámetro multiclass que no tenía que ver con mejorar u optimizar el modelo sino con explicitar que teníamos una salida no binaria y luego si Max híper que es un parámetro que podía utilizar en poder mejorar el modelo lo hicimos solamente con un valor fijo no lo cambiamos y obtuvimos una precisión que la que está aquí debajo de 069 para el modelo de tren y 064 para el modelo de entrenamiento En búsqueda de un mejor score pasamos a ver otro modelo en este caso le agregamos un nuevo parámetro que es solver bueno ese solver y yo puse Aquí en la descripción tiene tres opciones posibles Newton y ocg que la que estamos usando aquí saga y s.ag y los invite a cambiar los valores de ese parámetro en poder que scorre me daba yo tomé Newton cg y conserve el uso de los parámetros múltiplas y maxter solamente que Max iter lo pasé de 1000 a 5.000 Al haber hecho esta prueba y volver a entrenar el modelo vi que la precisión mejoró considerablemente pasando de 69 a 76 y 64 a 85 en resumen lo que hemos hecho es en principio poder cambiar el conjunto de Test y de entrenamiento y volver a entrenar el modelo En búsqueda de un mejor score pero también y en la última oportunidad lo que hemos hecho es cambiar los valores de los parámetros para justamente Buscar también otra forma de lograr el mejor score esos parámetros en realidad de ahora es más le vamos a llamar hiper parámetros y es allí donde radica la explicación del título de esta placa y de todas las anteriores que tiene que ver con el tuneo del modelo Qué es tunear un modelo de Machine learning es trabajar cambiando los valores de sus hiper parámetros En búsqueda de mejores cortes esta frase que está aquí refuerza esa idea los modelos tienen la posibilidad de tunearse a través de su simple parámetros y los valores que pongamos en ellos en la búsqueda de un modelo optimizado ahora bien yo puedo hacer ese trabajo obviamente manualmente como lo hemos hecho hasta ahora cambiando los valores de los IP parámetros y probando sus score ahora nada no garantiza que por más cantidad de prueba que hagamos no haya una combinación que no hayamos probado y que nos hubiera dado un score superior por eso además de la prueba manual existen técnicas que hacen ese trabajo que nosotros hacíamos manualmente de manera automática garantizándonos una mayor posibilidad de que obtengamos ese mejor score esta o alguna de estas técnicas la vamos a ver en la parte práctica es decir en la segunda parte de esta clase bueno terminado de desarrollar el concepto de tuneo y optimización de modelos que nos quedó pendiente de la clase pasada es momento de empezar justamente con el algoritmo propio de esta clase árboles de decisión bueno árboles de decisión es un algoritmo que lo primero que tenemos que destacar es importante tanto para problemas de regresión como problemas de clasificación tanto para valores continuos como para valores discretos este algoritmo se llama justamente árbol de decisión porque tiene la misma forma que un árbol pero a diferencia de un árbol en su forma natural tiene la diferencia que en este caso es un árbol de tipo invertido justamente como está invertido lo primero que hay que hacer es empezar por una decisión a partir de una variable de entrada lo que hay que identificar aquí prioritariamente es cuál es la variable de entrada más representativa que puede empezar a dividir la estructura de ramas invertidas hacia un lado o hacia el otro Bueno así radica una de las cuestiones principales tiene que resolver nuestro algoritmo este algoritmo A diferencia de los algoritmos que hemos visto hasta ahora maneja muy bien las relaciones no lineales Esto justamente es un aspecto débil de los algoritmos que vimos anteriormente Y lo vemos en los gráficos aquí tenemos los gráficos que habíamos logrado justamente en el conjunto de days y dail se acuerdan las llamadas telefónicas del call center y vemos que en este gráfico que está a la izquierda tenemos una relación fuertemente lineal con lo cual cualquiera de los modelos que hemos visto hasta ahora Funciona muy bien pero en el cuadro que está a la derecha esa realidad no se da bien en este tipo de circunstancias es cuando justamente un árbol de decisión es ideal bien Vamos a explicar a través de este gráfico ahora cuál es la forma en que se crea un modelo de tipo de árbol de decisión vamos a hacer un ejemplo muy gráfico para que se pueda entender lo mejor posible yo tengo aquí un gráfico de dos coordenadas ojo a no confundir con el tipo de gráfico que veníamos trabajando donde tenía en este eje la variable de entrada y en este eje de ahí o variable de salida Aquí tengo dos variables de entrada la x1 y la x2 la combinación de ambas me describe todo este conjunto de puntos que pueden dar lugar a visualmente identificar 5 subgrupos de puntos que identificados a través de colores azul rojo amarillo marrón y negro y también lo vamos a acompañar con algunas líneas que también nos pueden ayudar a identificar mejor a cada uno de esos grupos lo siguiente que tenemos que hacer y muy importante es definir Cuál es la variable más significativa que me va a dar la decisión fuerte de este árbol de decisión y yo voy a elegir aquí entre estas divisiones al valor este que está aquí que justamente representa a la x1 y el valor número 3 al elegir este valor Yo tengo un conjunto de puntos que están a la izquierda que son todos estos que están aquí los azules y los rojos y otro conjunto de puntos que están a la derecha que son los que están aquí amarillo marrón y negro a partir de esta decisión yo pongo una expresión que es la primera expresión del árbol de decisión x1 menor a 3 y eso como es el primer elemento del árbol de decisión se llama no raíz este nudo raíz me va a dar lugar a como dije recién a un conjunto de puntos a la izquierda y otro a la derecha con lo cual lo siguiente que tengo que ver es cómo empiezo a subdividir el grupo que está a la izquierda bueno están los azules y los rojos Cómo se dividen a partir de el valor 6 de la variable de entrada x Por ende el siguiente nodo lo pongo con la expresión si x2 es menor que 6 y a partir de eso reconocer si son menores serán los rojos y son mayores en Los Azules que es lo que voy a plasmar aquí es decir el sí me lleva al rojo y el no me llevara azul Qué pasa después de esto Bueno pues los rojos ya no tienen más ninguna idea de división como vemos aquí y tampoco Los Azules con lo cual justamente ese nodo rojo y ese nodo azul que responden al sí y no del x no menor a 6 se llaman no de decisión de este modo ya he agotado toda la lógica que tiene que ver con la parte de la izquierda del árbol y tengo que ir ahora en lugar de la opción Sí a ver la opción no del nodo raíz para ello tengo que pensar en todo este conjunto de puntos amarillos marrones y negros que está a la derecha de esta decisión x13 con lo cual ahora voy a el no y me lleva a que este gran conjunto de puntos tiene como elemento divisor a esta línea que representa el valor 5 de x2 con lo cual pongo esta expresión aquí y me va a dar lugar también aún sigue un no tomamos el Sí y el sí me lleva al conjunto de abajo o sea los valores x2 menores a 5 son todos estos que están abajo que a su vez se subdividen en dos grupos como se subdividen a partir del elemento 7 del eje x1 con lo cual Esa es la expresión que pongo aquí y eso me va a llevar a identificar a que si es sí identifica el grupo marrón y si es no identifica al grupo negro tanto el nodo marrón como el nodo negro al igual que el nodo rojo era azul todos son no de decisión y del mismo modo podemos decir que nodo x2 menor a 5 y x1 menor a 7 al igual que el x2 -6 son también no de prueba complementamos lo que falta del árbol justamente con la parte superior a x2 menor a 5 o sea que son mayores a 5 que son los puntos amarillos con lo cual tengo un nuevo nodo de decisión ahora el amarillo con esto terminamos de describir la estructura del árbol de decisión en relación Al conjunto de puntos que se nos presenta todo esto que yo armado visualmente es decir toda esta identificación de estos puntos y toda esta estructura de árbol lo tengo que hacer de este modo visual Obviamente que no esto es un ejemplo lo que tenemos que hacer es justamente Buscar que el entrenamiento del modelo nos dé esa respuesta Lo mismo tengo que pensar en cuál es la estructura de este árbol puede tener más nodos puede tener más brazos más ramas por supuesto y justamente también el entrenamiento del modelo es quien tiene que determinar la estructura ideal de árbol para la situación que se nos presenta con lo cual lo que tenemos que hacer es buscar como dijimos desde el principio de esta clase el modelo óptimo y el modelo óptimo como lo podemos hacer justamente ponen tuneo de hiper parámetros bien Cuáles son las ventajas ahora de este modelo en principio es un modelo de muy fácil interpretación y es muy fácil visualizar justamente a través de herramientas o librerías como la que usamos nosotros Matt es ideal para distribuciones de puntos que tienen una forma no lineal ya hablamos de esto y mostramos dos gráficos justamente que habíamos tenido en la práctica de la clase anterior es decir aquella destrucción de puntos que no tiene una forma lineal justamente da un contexto ideal para aplicar este algoritmo de árboles de decisión se requiere mucha menos limpieza de datos respecto de los otros modelos es decir este modelo no se ve afectado por valores Nan nulos ni tampoco por valores atípicos o óleos otra ventaja es que a diferencia de otros modelos como los que vimos hasta ahora este algoritmo no necesita del proceso de normalización de características de entrada y finalmente la desventaja que en realidad es una sola y muy importante que tiene que ver con un concepto que abordamos en la parte teórica de la clase anterior él sobre ajuste si la división de nuevas del árbol puede llevarse al extremo de encontrar un camino para cada situación a punto de cubrir todas las opciones posibles es decir que la estructura de nodos y de ramas llegue al máximo nivel de detalle para lograr el objetivo Pero esto no va a llevar a crear un modelo perfecto para el conjunto de entrenamiento que estamos tomando para crear el modelo Pero evidentemente va a fracasar para cualquier otro conjunto de datos Que aparezca a futuro es decir va a ser un modelo que no va a generalizar y Por ende un modelo malo este concepto de sobreajuste es mucho más grave Aún se tiene en cuenta que los cambios mínimos en la composición del conjunto de datos pueden afectar severamente la precisión del modelo y Por ende combinado este problema con el problema del sobre ajuste bueno es alguna que tenemos que tener muy en cuenta a la hora de buscar un buen modelo para la situación que se nos presente o para las situaciones futuras bien para evitar esto es fundamental el trabajo del tuneu de hiper parámetros que es lo que abordamos a lo largo de toda esta clase tanto para el problema del sobre ajuste como para el problema de la composición de los datos por lo tanto es imprescindible trabajar en este caso en particular con dos parámetros o dos hiper parámetros Mejor dicho Uno es la profundidad del árbol A qué se refiere esto a cuán largo puede llegar a ser el árbol y la otra es la cantidad mínima de datos que justifican la creación de una nueva rama en el caso nuestro del ejemplo que usamos recién usamos o identificamos cinco grupos Podrían haber sido más podría haber sido menos bueno esa es una de las cosas que tenemos que configurar en el hiper parámetro o en los hiper parámetros para evitar este problema de sobre ajuste y bien hasta aquí la primera parte de esta clase número 8 donde abordamos todos los conceptos teóricos de este nuevo algoritmo de Machine learning árbol de decisión ahora vamos a la segunda parte a poder implementarlo en la práctica nos vemos allí hasta aquí llegamos con esta primera parte los esperamos para poner en práctica todo lo aprendido en la segunda parte Titulo: Clase8 (parte 2) Curso Inteligencia Artificial \\n URL https://youtu.be/jaIg4s6iIiI  \\n 2046 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 8 del curso de Inteligencia artificial de ifes es hora de poner en práctica lo que aprendimos en la primera parte vamos por ello  Hola Bienvenidos a todos a la segunda parte de esta clase número 8 donde vamos a continuar la teoría de árboles de decisión ahora en el marco de la práctica en esta clase vamos a ver tres temas lo vamos a poner en claro desde ahora y después vamos a verlo ordenadamente en el Notebook en principio lo que vamos a hacer es implementar un árbol de decisión pero ustedes como vimos en la teoría tienen Claro que este árbol de decisión sirve tanto para problemas de clasificación como de regresión vamos a ver primero un ejemplo de un árbol de clasificación y al final yo le voy a dejar una práctica que no la vamos a desarrollar en el contexto de la clase que la vamos a dejar para que ustedes después la hagan en su casa de un árbol de decisión de tipo de regresión luego vamos a ver una nueva forma de medición de la precisión del algoritmo Ya vimos en score vimos la matriz de confusión y vamos a agregar una nueva forma de validación que se llama método de validación Cruzada y finalmente uno de los temas principales que desarrollamos en la clase teórica es el tuneo de hiper parámetros sí que lo vimos claramente con el ejemplo de comparación con el caso del auto bien Esto lo vamos a seguir viendo sí de manera manual como de alguna manera lo hicimos con los algoritmos de regresión logística pero vamos a ver un método a partir del cual podemos llegar a hacerlo de manera automática que también lo comentamos en la clase teórica es decir que yo pueda tunear algoritmo a partir de un sistema automático con lo cual yo voy a obtener los mejores valores de los hiper parámetros pero no de manera manual sino a partir de la deducción que hace ese algoritmo por nosotros bien ya estamos aquí en el Notebook y dentro del tema obviamente de la clase de árboles de decisión vamos a ver el primero de los temas es decir el algoritmo de clasificación como siempre lo primero que vamos a hacer es importar las librerías hay muchas librerías en este caso y hay muchas que son nuevas con lo cual vamos a irlas comentando aquí para que ustedes tengan una idea de qué se trata cada una de esas entradas Pero la idea es verlas bien bien claramente y hacer referencias a ellas cuando lo usemos en el contexto del código y así las van a poder entender mejor luego vamos a importar a la sesión de collage el Data set que vamos a usar para esta práctica que se llama Iris csv y lo vamos a buscar ejecutando como siempre este código vamos a buscarlo dentro de la carpeta Data sets y a su vez dentro de la carpeta Iris el archivo Iris csv luego de ello y como siempre creamos una variable Data que va a contener todo ese conjunto de datos y hacemos un Head para ver justamente ese contenido ya tenemos una primera precisión aquí pero les voy a contar un poquito más de este conjunto de datos el mismo contiene 50 muestras de cada una de tres especies de Iris que son tipos de flores hay Iris de tipo cetosa y de tipo virginica y de tipo versicolor en este caso se me dieron cuatro rasgos de cada una de estas muestras midiendo el largo y el ancho del sépalo que es una característica de la flor y el largo y el ancho del pétalo que es otra característica en todos los casos en la medición se hizo en centímetros lo comentado lo vemos en detalle aquí tenemos el largo del sépalo el ancho del sépalo el largo del Pétalo y el ancho del Pétalo y finalmente la especie que es la variable Target u objetivo al menos las mismas tenemos una Clara situación a partir la cual podemos Buscar un algoritmo que en base a estos cuatro rasgos puede determinar A cuál de las tres especies pertenece cada una de estas observaciones para indagar un poquito más sobre este conjunto de datos vamos a hacer como siempre un Data shape y con ello podemos constatar que tenemos las 150 observaciones que mencionamos al principio cuando les conté de qué se trataba este Data set y las cinco columnas que referenciamos recién dijimos que hay 50 especies o 50 observaciones de cada especie Perdón eso lo podemos constatar justamente con esto que escribimos aquí agrupar por especies y contar Cuántas hay de cada especie lo ejecutamos y podemos constatar que tenemos efectivamente 50 observaciones de cada una de esas especies ahora que ya estamos seguros de los datos que tenemos vamos a armar los conjuntos de entrenamiento y de test para crear el modelo luego entrenarlo y finalmente medir su score o precisión para ello vamos a separar primero las variables de entrada y la variable objetivo o de salida por eso que estoy poniendo un código a partir del cual generó la x con todos los datos de entrada y la i con el dato objetivo o dato de salida lo ejecutamos como dato complementario les pongo una alternativa a este código en el cual puedo lograr el mismo objetivo pero en este caso buscando referenciar las columnas es decir los nombres de cada una de las características en lugar de por su nombre por su número de índice esto es interesante porque a veces cuando tenemos muchas características este tipo de código que he puesto como ejemplo aquí abajo evita que Cometa el error de escribir mal el nombre de una característica y Por ende que el procedimiento salga mal Así que les dejo ese código para que lo puedan usar también en este caso y reemplazar esto que he puesto aquí por estas cinco líneas que están aquí debajo y finalmente hago lo que venimos haciendo anteriormente que es separar el conjunto de entrenamiento y el conjunto de Test Así que lo ejecuto a modo de verificación puedo ver cuántos quedaron Cuántas observaciones quedaron en un conjunto Y cuántas oraciones quedaron en el otro Así que ejecutó este código y a través del end puedo medir en largo del conjunto de entrenamiento que aquí veo que son 112 observaciones y el largo del conjunto de Test que tiene 38 con lo cual la suma de ambas llega a las 150 de el conjunto de datos original estos pasos que hemos hecho guardan alguna diferencia respecto a lo que hicimos en las dos prácticas anteriores evidentemente No eso es importante que lo tengan presente porque si bien vamos a ir viendo distintos algoritmos es Claro que hay muchos procedimientos que van a ser los mismos Más allá del algoritmo que se trate la diferencia empieza Ahora cuando voy a crear el modelo porque voy a usar un algoritmo diferente a los que usamos en la clase anteriores por eso usamos ahora el decision Trick clasifit al mismo modo que en su momento usamos un método para la reelección lineal y luego un método para la región logística Esta es una de las librerías que importamos que como dijimos en el principio de la clase la vamos a ir destacando en la medida que lo vamos usando ver aquí de sidekit import decision que es justamente esto que ahora voy a empezar a usar para crear justamente mi primer algoritmo de árbol de decisión pero como dijimos en la teoría de esta clase vamos a hacerlo también teniendo en cuenta el tuneo de sus hiper parámetros Y en este caso particular vamos a utilizar dos simples parámetros criterium y 1000 sapes split el primero tiene dos opciones entropy o shini como dice el texto que aquí yo le he puesto para que tenga más claro este concepto determina el criterio para la división de los nodos de un árbol como vimos en la teoría los nodos hay que dividirlos con algunos criterios bien Los criterios pueden funcionar de un modo de otro con estas dos alternativas de El tuneo del hiper parámetro epitelio el criterio de shini es mucho más rápido porque es menos costoso computacionalmente pero los resultados obtenidos en el criterio de entropía entropía son ligeramente mejores datos a tener en cuenta para probar con una opción a otra y mil SAP split es la cantidad mínima de valores para seguir con ramas desde un nodo Qué quiere decir esto es decir yo voy a llegar un momento que voy a tener supongamos 50 observaciones y lo que voy a determinar es si esas 50 observaciones ameritan que siga abriendo ramas o ese es el nodo último que voy a ir a considerar Cómo se estipula esa cantidad mínima de valores para seguir con ramas de un nuevo justamente con el hiper parámetro 1000 samples que en este caso que voy a crear Este modelo de árbol de decisión lo voy a estipular en el valor 20 bien en el caso particular insisto de esto que voy a hacer aquí entonces elegí en tropit para el hiper criterium y 20 para el hiper parámetro mini samples Speed lo ejecutamos y ya tenemos ahora nuestro modelo a partir de lo cual vamos a hacer las predicciones como hacemos Siempre sobre el conjunto de Test y vamos a ver ahora a través de la matriz de confusión empezar a medir la precisión del modelo veo que la presión es bastante buena por lo que veo aquí dado que no tengo ningún error en el caso de la primera especie tengo un solo error en el caso de la segunda especie y no tengo ningún error en el caso de la tercera especie lo podemos ver más fácilmente a través del score con lo cual voy a medir como siempre el conjunto de entrenamiento del conjunto de Test y veo que son muy buenos indicadores tanto para un caso como para el otro más allá de que en el caso de Test siempre es deseable que sea un poco superior al caso de el modelo de entrenamiento bueno la diferencia como en algunos casos hemos visto no es tan grande con lo cual pude considerarse que este es un buen modelo para este caso bien Ahora vamos a ver otro recurso nuevo que nos permite ver el modelo de árbol de decisión creado de una forma muy detallada muy clara con sus nodos sus ramas y los elementos que forman el criterio para cada uno de los nodos para ello vamos a usar la librería plot Tree tal cual lo vemos aquí que es una de las librerías que seleccionamos al principio dijimos que vamos a ir comentando en la medida que vamos avanzando justamente depende de sideking lane.3 que es la misma librería que nos permitió Acceder al precisión triple líneas precedentes al gráfico que estamos viendo ahora volvemos al gráfico y vamos a hacer así vamos a ejecutar Primero este código vamos a ver el gráfico y después vamos a aplicar cada uno de estos códigos que están aquí Qué significa o qué impacto o incidencia han tenido sobre el gráfico que vamos a ver a continuación Así que ejecutamos este código y ahora vemos el árbol bueno fíjense que esto es parecido a lo que vimos en la teoría donde cada nodo en este caso es esta cajita que está acá obviamente diferencia en colores de acuerdo a la especie la especie la clasificó con letras psy no es exactamente el nombre de la especie original sino que lo clasifica con una letra pero importante la información que tienen aquí fíjense lo primero que tiene de ese criterio en este caso el primer criterio el primer rasgo que tomó es la longitud del Pétalo y empezó a tener como criterio primero importante si es menor igual que 2.45 Qué información más tenemos aquí la cantidad de samples la cantidad de observaciones acuérdense que este es el conjunto de entrenamiento que tenía 112 y lo que me muestra aquí es que de la 112 hay 35 que pertenecen a primera especie 39 de la segunda y 38 a la tercera de allí empieza toda la lógica similar a lo que vimos en la teoría hasta llegar a este nodo último que es el nodo de decisión este no sigue para este lado sigue para este lado termino un nuevo decisión y así hasta llegar al último nivel este árbol tiene como dice el texto aquí una profundidad de Qué quiere decir profundidad la cantidad de niveles que tiene el árbol sacando el nodo raíz fíjense que tenemos uno dos tres Y cuatro Y el otro dato importante es la cantidad de nodos de decisión nodos finales Tenemos uno acá tenemos dos acá tenemos tres cuatro y cinco vamos al código bien lo primero que tengo que tener en cuenta aquí es que he usado una estrategia de graficación de subplots se acuerdan que sub plots lo usamos para cuando quiero usar gráficos que estén como si fuese una estructura matricial de filas y columnas con lo cual acá yo le indico que voy a hacer un gráfico de tipo suplos y que cada una de esas Cuadrilla que tiene que ver o esa celda eso supuesta matriz tiene un site de 12 por 10 luego hago una impresión por eso antes del gráfico aparecen estos dos textos de referencia justamente a través de dos prints donde lo que recurro esa parámetros que se desprenden del objeto creado es decir en Tri que crea aquí arriba como modelo Cuáles son esos elementos getdef que es el que Dime la profundidad del árbol y con get en el ists Deme la cantidad de no de decisión por eso esos dos parámetros que los invoco de este modo los imprimo y me dan estos dos textos que están aquí arriba bueno ahora sí vamos al plot Tree lo primero que tengo que ponerle es cuál es el árbol que quiero dibujar obviamente El Tri Cuáles son las características de entrada las que ya hemos manejado Cuál es la característica de salida la especie que también hemos manejado con filder que quiere decir rellenar tú le digo que quiero que rellene con colores Sí cada uno de los tránsitos ven que va siguiendo justamente rellenando con colores me permite diferenciarlos más claramente false es para ver si quiero poner la impureza como dato aquí no lo voy a hacer en este caso phone 6 10 es el tamaño de la letra digamos de aquí dentro precisión 2 es la cantidad de decimales que quiero que aparezcan en el caso de cada uno de estos elementos de decisión fíjense que todos los casos salen dos decimales y finalmente a x le indicó justamente que de qué cuadrícula va a tomar o que qué estrategia de cuadrícula justamente de esta que he definido acá arriba como fic AX con todos esos elementos yo lo que obtengo es este gráfico que es muy importante para poder tener una visualización Clara del árbol que Se generó justamente a partir de la creación de este modelo de seacesion ahora vamos a ver el segundo punto de esta clase tal cual lo anunciamos al principio de la misma que es el tema de el método de validación Cruzada como una forma más de la medición de la precisión de nuestro algoritmo vamos a ver el siguiente gráfico para explicarlo bien y vamos a detallar los puntos que ustedes tienen aquí de alguna manera explicados dentro del texto que continúa a Este título que acabamos de mencionar recién es decir en este caso lo primero que tenemos que hacer es elegir un número de pliegues es decir En cuántas partes voy a subdividir este conjunto de datos a ese número lo llamamos K por eso el nombre de este método de evaluación Cruzada se llama key folk o kfall bien luego lo que tenemos que hacer es dividir el conjunto de datos en esas partes supongamos en 10 partes eso se llaman pliegues por eso el nombre de fold sí es doble ceses en este caso lo que voy a hacer en principio es elegir K menos un pliegues como conjunto de entrenamiento Así que si hice 10 subdivisiones elegiría 9 pliegues para entrenar al modelo luego entreno el modelo justamente con ese conjunto de entrenamiento y calculo el score esas tres últimas acciones las vuelvo a repetir cuantas veces cada vez Es decir si elegí 10 10 veces y por cada una de ellas vuelvo a repetir lo mismo redistribuyo tomo un nuevo conjunto de entrenamiento el score y pasa el siguiente cuando llego al final voy a tener cuantos voy a tener 10k score Por decirlo de alguna manera se toma como ejemplo que tome el número 10 tendría 10 scores con lo cual el score final va a ser el promedio de todos ellos a continuación vamos a crear un nuevo modelo para empezar a probar este nuevo método de validación Cruzada donde voy a usar los mismos hiper parámetros los mismos valores de parámetros para criterium y para minsamble Speed pero voy a agregar un nuevo hiper parámetro que se llama Max de qué es lo que vimos recién en el gráfico la profundidad Max Def lo que le indica no es la profundidad que tiene que tener el árbol sino la máxima profundidad a la cual tiene que llegar en árbol bien Vamos a hacer entonces aquí Perdón la ejecución del código para crear el modelo entrenarlo y crearlo como siempre y ahora sí vamos a implementar el método de validación Cruzada de kyfall justamente con un cap igual a 10 como lo decíamos recién en el Cómo se hace primero tener una variable a través del método kyfall método Que vuelvo a las librerías aquí arriba habíamos importado al principio sí aquí lo tenemos de cycle Modern selection import Y también vamos a usar Cross Ball score que va a ser justamente la forma de medir el score de ese método de validación así que ya nos adelantamos a ese hecho que lo vamos a utilizar justamente en esta parte que estamos viendo ahora Bueno estamos aquí y volvemos keyfall lo primero que le digo En cuántos splits En cuántas partes quiero dividir ese Ese Conjunto este de datos yo filtro Recuerden que era remover los datos si este rey de alguna manera redistribuir los datos permanentemente y randomstein 1 acuérdense que también es un argumento que usamos justamente para cuando hay algún modelo o algún conjunto o alguna acción que nos gusta bueno poder volver a invocarla justamente a través de ese número luego lo que voy a hacer es obtener los scores ahora ya tengo sí el método de validación cruzado implementada lo que tengo que hacer ahora es obtener los scores de ese método de variación Cruzada Por eso ahora sí uso el crosscore que recién referenciamos dentro de la librería que estamos importando Y de paso el árbol los valores de entrada los valores de salida Le digo qué tipo de scoring Quiero el que usamos siempre el de presión aquí ahora sí y le digo justamente de dónde quiero que saquen información de la cantidad pliegues que tiene el conjunto que acabo de subdividir y justamente de cb que se ve esto que está aquí arriba y esto que está acá arriba salió de la implementación del método de evaluación usada una vez que hago eso lo vamos a ejecutar voy a obtener a través de sendos Prince Sí en principio scorcean no es un valor es una matriz que tiene Cuántos valores 10 porque elegir justamente un keyfall 10 y finalmente lo que voy a hacer con todos esos scores es hacer un minimin Recuerden que era promedio con lo cual veo que tengo 1093 1 093 tengo una variedad de scores que en promedio todos me dan un 0 96 que es un muy buen score ahora bien como siempre decimos que un modelo nos dé un score bueno no quiere decir que no pueda ser mejorable y Aquí vamos a ver algo de el tema de tuneo de parámetros nuevamente pero desde un punto de vista diferente ya que antes lo de manera manual es decir pusimos un valor nosotros que estimamos que podía ser 5 ahora vamos a hacer un algoritmo sin llegar todavía a la automaticidad que hablamos al principio de la clase simplemente un algoritmo a partir del cual lo que yo voy a hacer va a ser cambiar este valor de aquí de la profundidad del árbol de 1 a 7 a través de una estructura Ford es decir foreing rage 1 8 quiere decir de 1 a 7 va a ser que este algoritmo se ejecute siete veces partiendo de un Max 1 2 3 hasta llegar a 7 con esto vamos a crear un modelo tal cual lo hicimos antes y luego le aplicamos un método de validación Cruzada y observamos cuál es el score que tenemos e imprimimos a continuación el score Esto no es muy diferente lo que hicimos aquí arriba donde creamos el modelo y aplicamos el método de validación Cruzada solamente que está en el contexto de un Ford y Por ende va a pasar siete veces sin que yo lo haga siete veces manualmente cambiándole manualmente insisto el valor de la máxima profundidad de el árbol luego tenemos otro Ford Porque otro Ford también vamos a incorporar un tema nuevo por eso es interesante este tema Dentro de este Ford yo voy a poner otro Ford que vaya de 0 a 4 es decir que va a ejecutarse cuatro veces en el cual yo voy a recorrer cada una de las variables de entrada esas variables de entrada que las identificó con data.colums subj entendiendo que J va de 0 a 3 me va a permitir ver aquí el nombre y al lado a través de el método feature importars que es filtro de la característica es el nombre de la variable de entrada importas en la importancia de esa característica de entrada subj porque porque justamente me va a mostrar la importancia de cada una de las cuatro características con esto también vamos a poder ver luego de ejecutar este código la importancia que tiene en ese momento digamos para esa profundidad del árbol cada una de las cuatro variables de entrada lo ejecutamos como vemos aquí los resultados Sale primero el título que dice score paradef 1 es de 0,667 es decir este texto que está codificado aquí arriba Así que está programado aquí arriba es el resultado lo que se ve aquí luego está este Ford que hablábamos recién y fíjese lo que me muestra el nombre de cada una de las variables de entrada y el nivel de importancia que tiene en el contexto de ese algoritmo para la profundidad 1 justamente 0 0 1 y 0 es decir que para este algoritmo para la profundidad 1 el 1 la única variable de entrada Perdón importante es el largo del pétalo Yo podría en este algoritmo reprogramarlo prescindiendo de las otras tres variables de entrada miremos un poquito hacia abajo vemos que en el caso de la profundidad 2 ha mejorado y mucho el score ahora fíjense que el nivel de importancia se distribuye entre el largo del Pétalo y el ancho del pétalo todavía sigue siendo un poco más importante el largo pero el ancho del pétalo también va teniendo su importancia si mira un poquito más abajo el score mejora ya no tanto como en el caso de 2 a 1 pero Mejora y también el grado de importancia cambia vuelve a ser un poquito más importante el largo del pétalo que el ancho del pétalo si sigo mirando hacia abajo voy a poder comprobar que ya de 4 hasta 7 profundidades me refiero al índice de profundidad el score no va a cambiar siempre va a ser 096 bien en el resto de los casos y del mismo modo la importancia de largo del Pétalo y del ancho del pétalo va a ser exactamente la misma a pesar de que cambia de 3 a 4 ya entre 4 5 6 7 es exactamente lo mismo con lo cual podemos concluir que la profundidad sería tres o cuatro podríamos llegar hasta 5 y justamente con la importancia de las características podríamos concluir que para este algoritmo podríamos prescindir de las variables de entrada del largo del sépalo y de ancho del sépalo y ahora vamos a ver el tercer y último punto de esta clase grid ser cb que es Ni más ni menos que el tuneo automático de panel recién variamos los valores de la profundidad del árbol de manera dinámica pero los valores los fijamos nosotros Nosotros dijimos a través del Ford de Qué valor queríamos que fuera hasta Qué valor y solamente uno de los hiper parámetros fue el que fue objeto de este trabajo para a través del Ford en realidad existe una opción automática que es el uso justamente del método glitch ser se ve que vamos arriba es una de las librerías que seleccionamos al principio de esta práctica vamos a buscarla sí ven que aquí tenemos justamente en model selection donde habíamos seleccionado keyfall y validación Cruzada también está grid se ve bien volvemos al código y vamos a explicar un poquito de qué se trata este método para que se utiliza se ve en realidad es una técnica para encontrar los valores de parámetros óptimos de un conjunto dado de parámetros en formato de cuadrícula por eso lo de grid concretamente es una técnica de validación Cruzada similar a la que hemos recién para la validación del algoritmo pero ahora para determinar no solamente eso sino el mejor o las mejores combinaciones de parámetros de valores de parámetros para lograr el mejor escort en concreto lo que hay que hacer es ingresar en ese modelo un conjunto de opciones de parámetros y el algoritmo se va a encargar de determinar para cada uno de esos parámetros Cuál es el mejor valor o valor o estimador óptimo en Post como decimos siempre de lograr el mejor score bien en la práctica de grids ser cb lo que vamos a hacer es dar dos ejemplos uno a través del hiper parámetro ccp- bajo Alfa y otro tuneando 4 hiper parámetros en conjunto Por qué usamos ccp guión bajo Alfa bueno nosotros vimos en la teoría de esta clase que una de los problemas que tenía este modelo o este algoritmo de árboles de decisión es el sobre ajuste y una de las maneras de caer en ese problema es dejar que el árbol crezca indefinidamente este hiper parámetro ejecuta una tarea que se llama poda del árbol que justamente evita ese problema Por lo tanto este hiper parámetro yo lo voy a configurar con valores 10 valores que vayan entre 0 y 5 cómo lo voy a hacer A través de el método Line space de nampai y voy a poner el valor mínimo el valor máximo y cuántos valores quiero que aleatoriamente genere este método entre 0 y 5 eso lo pongo dentro de una variable que se llama parangred pero tengo que etiquetar esto que está aquí porque tengo que hacer referencia después de que esta distribución de valores tiene que ver con el hiper parámetro ccp guion bajo Alfa paso seguido uso el método y pongo el contenido de lo que va a generar dentro una variable que se llama el grid como insisto le puedo poner Cualquier nombre la primera información que tengo que poner aquí es relativa al algoritmo sí es decir un árbol de decisión para un problema de clasificación pongo en principio la profundidad máxima no nada la cantidad de samples split 2 y samples League 1 y Random State lo voy a dejar en un número como siempre yo lo pongo para que después me quede como referencia que son estos 12 parámetros Ya lo habíamos explicado dijimos que es la cantidad mínima de muestras de observaciones que tiene que haber para que sea meritorio poder generar dos ramas evidentemente si no no tengo más de dos elementos no voy a poder generar dos ramas y Miss uppers Live es la cantidad de elementos que como Mínimo debería quedar en alguna de las dos ramas si esto se configura por defecto en uno Obviamente que si yo de nuevo tengo dos ramas no puedo tener todas las muestras En una rama y ninguna muestra en otra rama entonces digo como mínimo en una de las dos ramas tiene que haber una muestra obviamente esto se puede configurar Y variando estos valores obviamente resultado va a ser diferente luego lo que pongo es justamente en el argumento parangred la variable o el contenido de la variable que puse acá arriba que son justamente los valores de El hiper parámetro ccp guión bajo Alfa luego defino que quiero un scoring deacura así como ven usando siempre una cantidad de dobleces sí de el método de validación Cruzada de 10 y le digo que quiero un refrito un reentrenamiento y que también quiero que justamente me muestre o me regrese me retorne el score de ese entrenamiento y luego finalmente hago el fit que justamente lo hago con grid bien grid poniéndole lo que venimos usando siempre el conjunto de entrenamiento x y el conjunto de entrenamiento ahí con esto vamos a tener un nuevo modelo a partir del cual podemos tener los resultados de los parámetros de los valores de los parámetros de los siete parámetros perdón de una manera automática lo ejecutamos ahí tenemos el resultado y ahora vamos a poder obtener justamente Cuáles son los best params Cuáles son los mejores parámetros Sí para el ccp- bajo Alfa Recuerden que habíamos generado valores que iban entre 0 y 5 Cuál será el mejor lo veremos aquí luego voy a crear el árbol que le voy a poner como antes llamado que la tengo la mejor solución por eso le puse ese nombre a través de el método pese estimator de encript y que es el grid el grid es la resultante de aplicar el método sí bien y finalmente bueno Esto que estoy poniendo lo importante Obviamente que es esto todos los primos son simplemente formas de conocer información que es importante de todo lo que haya generado por eso también voy a mostrar como antes la profundidad del árbol y la cantidad de nodos terminales lo ejecutó y voy a poder ver que el mejor ccp- bajo Alfa es cero sí es decir que de entre todos los valores que elegimos acá eligió el mínimo bien y luego tenemos una profundidad de árbol de 5 y la cantidad de nuevos terminales de nuevo como antes podemos ver esto de una manera mucho más clara a través de un gráfico con lo cual hacemos lo mismo que hicimos hoy bien solamente que ahora en lugar de usarlo con Tri lo hago con Tri final porque es este árbol Perdón que he generado aquí sería Acuérdese que es en algo la anterior ahora generamos este Tri final que es resultante de haber aplicado el grises así que sin más palabras vemos la estructura del árbol creado y obviamente va a responder a una profundidad de 5 1 2 3 4 5 y tengo obviamente más nodos terminales 9 podemos ver aquí uno 2 3 4 5 6 7 8 fíjense que acá como son más nodos me ha quedado más juntos y en algún caso hasta hace un lapado bueno eso tiene que ver con que con esta configuración si yo acá cambio el size si lo pongo más chiquito o más grande obviamente va a cambiar esa situación y ustedes pueden jugar con estos valores para ver justamente el estado lo que van a hacer es que van a ver estos cuadrados más chicos si estos rectángulos Perdón más chicos o más o menos distanciados bien con esto podemos comparar con el gráfico que obtuvimos anteriormente Sí sí recuerdan si lo rescatamos Aquí está ve que tenía una profundidad de 3 y 5 nodos de decisión Este es el que habíamos generado como Tri al principio bien Ahora el que generamos con el tuneo automático de hiper parámetros como dice el título aquí tiene más profundidad y obviamente si tiene más profundidad tiene más nodos terminales que esto se ve comparando los gráficos de uno con el otro y bien con esto Hemos llegado al final de esta segunda parte de la clase 8 y de la clase 8 común todo hemos podido poner en práctica el desarrollo de un algoritmo de árbol de decisión para un problema de tipo de clasificación y luego hemos ido trabajando otras opciones con el tuneo de hiper parámetros de manera manual de manera automática y también incorporamos el método de validación Cruzada con una forma de edición nueva del score con cada uno de estos ejemplos hemos obtenido un score la idea justamente es Buscar de todas estas variantes que utilizamos Cuál es el mejor score y en base hecho determinar el mejor modelo yo lo voy a dejar aquí después de esta línea como viene aquí abajo dos ejemplos ampliatorios de todo lo que vimos que van a poder interpretar fácilmente porque yo le voy a poner Aquí también Bueno una explicación y independientemente esto Quien tenga alguna duda siempre tiene el campo virtual para consultar una es la posibilidad de trabajar con los hiper parámetros pero en lugar de hacerlo solamente y ponen ccp guión bajo Alfa lo vamos a hacer como dijimos antes de manera particular con Max Death minsapple Speed y minsamble leaf y finalmente vamos a les voy a dejar también un código para que ustedes Ven aquí de lo que es un algoritmo de árbol de decisión pero de tipo de regresión o sea para un problema de revisión y no para un problema de clasificación como Vimos a lo largo de toda esta clase bueno con estos dos códigos que le dejo también en este Notebook que lo vamos a guardar en la clase pero que quedan para que ustedes lo vean terminamos entonces la clase número 8 y los espero en la próxima clase hasta aquí llegamos con la clase número 8 ahora ya tenemos los conceptos y las habilidades prácticas para un nuevo algoritmo de Machine learning los árboles de decisión nos vemos en la próxima clase Titulo: Clase 9 (parte 1) del Curso de Inteligencia Artificial \\n URL https://youtu.be/b_3_B-Ej_9I  \\n 1227 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 9 del curso de Inteligencia artificial de ifes en ella veremos los conceptos básicos del algoritmo de bosques aleatorios y de sus hiper parámetros empecemos con ello  Hola a todos Bienvenidos a la clase número 9 del curso de Inteligencia artificial de ifes seguimos en el módulo de Machine learning y en ese módulo vamos a ver un nuevo algoritmo que es el de voz que es aleatorios esta es la primera parte de la clase número 9 en la cual como hemos visto en las clases anteriores vamos a hacer una introducción a la teoría y después vamos a pasar en la parte número 2 a la práctica bueno el algoritmo de bosques aleatorios es uno de los algoritmos que habíamos identificado como algoritmo de tipo supervisado y aquí tenemos una lista que vimos en la primera clase introductoria de este curso regresión lineal revisión polinomial revisión logística árbol de decisión y bosques aleatorios bien este nuevo algoritmo Antes que nada tenemos que decir que además de ser de tipo supervisado sirve para problemas de regresión tanto como para problemas de clasificación el ver el nombre de estos dos últimos algoritmos bosques aleatorios y árboles de decisión nos permite presumir que debe haber una buena relación entre ellos y eso es lo que vamos a ver a continuación tal como sucede a la naturaleza que nos rodea es decir el concepto que tenemos de bosques y árboles en el ámbito de Machine learning también un bosque en este caso aleatorio es un conjunto de árboles que en el caso de Machine le llaman árboles de decisión aquí el gráfico de alguna manera nos demuestra ese concepto nosotros podemos ver aquí una estructura de un árbol tal cual vimos en la clase pasada ese algoritmo luego otro árbol que identificamos como un r2 y así n árboles que todos en conjunto con forman un bosque los datos para la conformación de cada uno de estos árboles que forman parte de este bosque nacen de un único datasette quizás el mismo Data set que aplicamos en la casa anterior enteramente a un árbol solamente que en este caso este Data set se distribuye en los n árboles de qué manera se distribuye bueno como el nombre del algoritmo lo indica de manera aleatoria Por eso aquí ustedes pueden ver que los árboles tienen colores diferentes en sus nodos y lo que hemos querido representar aquí es que justamente la distribución no es uniforme en todos los casos sino que es absolutamente aleatoria vamos a ver más detalles de este algoritmo este algoritmo como dijimos recién crea un bosque con un conjunto de árboles de decisión de manera aleatoria ahora siempre todo esto lo hacemos en Pos de que de como siempre Buscar el mejor nivel de score posible Qué característica también podemos Resaltar de este algoritmo la aleatoriedad se aplica a dos conceptos a dos elementos no solamente a la cantidad de observaciones sino también a la cantidad de características es decir que la distribución del conjunto de datos en distintos árboles distribuye aleatoriamente cantidades de observaciones y cantidades de características por ello se divide en 100% de las características y observaciones y se crean en el subconjuntos con cada uno de ellos bien con cada uno de esos n subconjuntos se aplica a cada uno de los n árboles que forman parte de ese Bosque pero ahora bien si tengo n árboles como sé cuál es el valor de la predicción del Bosque en su conjunto bien de dos maneras diferentes depende el tipo algoritmo de que se trate si es de clasificación o de regresión Obviamente que si tengo n árboles tengo n resultados en el caso de un algoritmo de tipo de regresión lo que se hace es promediar los n resultados para obtener la predicción final y en el caso de que sea un árbol de tipo de clasificación lo que se hace se hace un sistema de votación eligiendo de todos los resultados Cuál fue el que más veces Se repitió y ese será el valor final del Bosque en su conjunto por si acaso hayan quedado dudas respecto de este último punto vamos a ejemplificarlo a través de dos gráficos en principio aquí tenemos un gráfico de un algoritmo de tipo de regresión tenemos nuestro bosque con árbol 1 árbol 2 árbol n cada uno de estos árboles tiene resultado 1 resultado 2 resultado n Qué tipo de resultados son estos son resultados propios de un algoritmo que como pusimos aquí arriba está vinculado a un problema de regresión por lo tanto para obtener un valor final que represente a todo este conjunto de árboles que represente al Bosque lo que voy a hacer es promediar estos n resultados y obtener un resultado final por el contrario si ahora vamos a un algoritmo que aborda un problema de clasificación la lógica es similar solamente que en este caso no es un tema de regresión con lo cual no se pueden promediar los resultados sigo teniendo los n árboles con sus n resultados pero en este caso es un voto por mayoría cómo sería bueno obviamente no va nadar todos los árboles resultados diferentes muy probablemente como es un problema de clasificación en muchos de ellos los resultados sean similares Sí vamos a suponer que en este caso el resultado 1 y el resultado 2 atiende a la alternativa o la opción a de un sistema de clasificación de un problema de clasificación y en el caso resultado n arroja la opción B si Supongo que puede ser un problema binario a o b Entonces cuál sería el resultado final a por qué Porque hay dos árboles que me dan como resultante el valor a y uno solo que me da el valor B obviamente Esto no puede ponderar a una mayor cantidad de opciones de salida de un problema de tipo de clasificación y la lógica sería la misma siempre el voto por mayoría para obtener el resultado final bien una vez que entendimos Cuáles son las características de este nuevo algoritmo de bosques aleatorios vamos a meternos en el capítulo hiper parámetros tema que profundizamos mucho en la clase anterior ya que vimos que esto es muy importante para poder configurar en algoritmo Y obtener el mejor score también vimos que hay formas de tunear estos hiper parámetros de manera manual de manera semiautomática o de manera automática pero al fin y al cabo siempre La idea es estar cambiando los valores y parametros insisto para tratar de obtener en mejores cuerpos posible el primero de estos elementos en este caso los bosques aleatorios es nestimate o sea cantidad de estimadores que es o qué se trata en estimator Ni más ni menos que la cantidad de árboles que yo quiero para el bosque eso lo puedo fijar el valor estándar es 100 eso ya le da una idea de que cuando hablamos de un bosque es un bosque muy grande y se sugiere Que obviamente este 100 sea el valor mínimo si yo voy a configurar este hiper parámetro en instrumentos Debería ser un valor de 100 sería por defecto no tiene sentido o mayor a 100 nunca inferior eso para que realmente sea un bosque que tenga una cantidad de árboles que pueda ser o representar un buen modelo también en otra clase hablamos de este hiper parámetro criterium sí se acuerdan que teníamos dos opciones gini o entropía con las ventajas que describimos o desventajas que describimos en la clase anterior repasemos un poco de qué se trataba este hiper parámetro básicamente de lo que se trata es de ver cuál es el criterio para establecer la operación de decisión que está en cada nodo de cada árbol por ejemplo en la casa anterior tomamos como ejemplo el tema de las flores de Iris recordarán entonces en el nodo yo preguntaba por qué Por el largo del pétalo por el ancho del sépalo por el largo de el sépalo por el ancho del pétalo Sí bueno eso es parte de la edición y una vez que elijo la característica por qué voy a preguntar Mayor A qué valor menor a Qué valor quién decide eso bueno justamente esta este Perdón hiper parámetro criterio en el cual nosotros podemos elegir como vimos la clase pasada al igual que en el caso de los árboles también en los bosques las opciones o entropía Max filtros que quiere decir esto quiere ser cantidad máxima de características vamos un poquito hacia atrás a la teoría que vimos hace un rato dijimos que el bosque aleatorio distribuía aleatoriamente la cantidad de observaciones y la cantidad de características en este caso con este hipermetro yo le digo como máximo Cuántas características del 100% de ellas quiero que entren a cada uno de los árboles hay una norma que se puede generalizar Aunque no es obligatoria de que habitualmente se toma como criterio establecer la raíz cuadrada de la cantidad de observaciones si yo tengo por ejemplo como en el caso de Iris cuatro observaciones puedo sacar la raíz cuadrada de 4 y con eso fijar como hiper como valores tipo de parámetro Perdón que en cada árbol Tienen que entrar como máximo dos características Cuidado Que este como máximo no quiere decir que pueda ser menor es decir puedo tener un árbol que tenga una sola característica de las 4 Pero como máximo sería la raíz cuadrada de en ese caso 4 y sería el valor 2 esto insisto es un criterio universal no es fijo yo puedo poner que sea tres cuartos Sí para el caso de El ejemplo que estamos tomando recién como referencia Max samples relacionado directamente con lo que hablamos recién porque nace del mismo concepto dijimos que los bosques aleatorios distribuyen aleatoriamente en cada árbol una cantidad de características que lo que tratamos con Max feature pero también una cantidad de observaciones que es lo que vamos a tratar justamente con Max samples es decir así como en el hiper parámetro maxito yo le digo Cuántas características como máximo quiera que entra en un árbol con Max sample le dio cuantas observaciones como máximo quiero que entren a cada árbol y finalmente over score quiere decir fuera de la bolsa en inglés obviamente no porque se habla de la bolsa porque lo que se dice es que como en Max features y en Max samples sí yo tomo un conjunto y no el 100% de las características u observaciones van a quedar valores dentro de la bolsa como concepto y otros fuera de la bolsa Cuáles van a ser lo que van a estar dentro los que entren justamente a ese árbol Cuáles van a ser el resto los que están fuera de la vaca fuera de la bolsa Bueno lo que se busca en este caso si se quiere es que esos valores si sean son los cuales se va a testear para obtener el score bueno la eficiencia del modelo esto es muy importante sería como una suerte de paralelismo con lo que vimos antes con el conjunto entrenamiento del conjunto de Test Nosotros entrenábamos con un conjunto y testeamos con el otro con el que quedaba afuera se puede decir que el conjunto de Test era un ov del total sí de de datos cuando lo separamos entre entrenamiento y test bueno en este caso puede tomarse con paralelismo Eso sí los datos que quedan fuera para conformar el árbol yo Los dejé fuera para eso pero los pongo para testear Esto hace que justamente me garantice una mayor capacidad de generalización que lo que siempre deseamos de este modelo así como recurrimos a los gráficos para que ustedes pudieran entender mejor Cuál es la lógica que impera a la hora de que un algoritmo de bosque aleatorio Determine el resultado final para un problema de regresión y para un problema de clasificación también vamos a recurrir al gráfico para entender estos últimos conceptos de la distribución de las características en diferentes árboles si para ello me voy a adelantar un poquitito a la clase que viene ya que le voy a mostrar lo que sería un gráfico resultante de un bosque así como pudimos graficar un árbol también se puede graficar un bosque solamente que no los van a ver como vimos en este gráfico tentativo que aparece un árbol al lado del otro si no aparece un árbol debajo del otro obviamente para visualizar Esta esta este gráfico que hemos logrado Aquí sí este plot que hemos logrado Aquí he hecho un algoritmo muy sencillo con solamente 10 árboles lo cual contradice lo que dije recién de que como mínimo debe ser recién Pero bueno no estoy buscando aquí un buen score sino que estoy buscando aquí es una para que ustedes puedan visualizar rápidamente esto que estamos explicando obviamente tengo los 10 árboles uno debajo del otro vamos al primero de ellos fíjense que en el primero de ellos tengo en cada no decisión largo el para este nuevo raíz Perdón además un árbol tiene una decisión petand el ancho del pétalo el ancho del sépalo pero faltan una característica es decir que en este árbol hay tres de las cuatro características sigo con el próximo tengo el ancho del pétalo el largo del pétalo de nuevo el ancho del pétalo de nuevo largo del pétalo fíjense que en este caso no hay ninguna referencia a dos de las cuatro características justamente las dos que tienen que ver con Sí vamos a pasar en largo este que es un poquito grande para poder visualizarlo bien acá tenemos otro ancho del pétalo largo del pétalo ancho del pétalo largo el sépalo no hay ancho del cepal es decir tres de cuatro a este último de no este último en total sino este último ejemplo que quiero mostrarles que es el caso muy particular es un árbol muy cortito y fíjense que tiene ancho del pétalo ancho del Pétalo y nada más es decir una de las cuatro características con esto le quiero dar como ejemplo que no en todos los casos van a existir la misma cantidad de características ni la misma cantidad obviamente de observaciones Bueno espero que con este ejemplo que lo vamos a repetir lo vamos a ver nuevamente en la parte 2 de esta clase se puede haber entendido mejor este concepto de redistribución de una determinada cantidad de características por cada uno de los árboles de nuestro bosque finalmente ventajas y desventajas de los bosques aleatorios en el caso de las ventajas se puede decir que es un algoritmo de muy fácil implementación Y que es muy muy eficiente mucho más eficiente que los árboles Sí esa alta eficiencia se puede lograr aún sin hacer un gran trabajo sobre los hiper parámetros Más allá de que recién hablamos Vimos a cinco de ellos y dijimos que como siempre el trabajo ahí parametros nos va a llevar un mejor score en este caso de los bosques a veces no es necesario ajustar mucho no sirve parámetros para obtener un buen score Obviamente si se puede hacer y se puede hacer con las herramientas que vimos en la clase anterior de grid se ve qué más tenemos que con una gran cantidad de árboles obviamente 100 o más obviamente esta cuestión del sobre ajuste super ampliamente nosotros teníamos que en el caso de los árboles El problema del sobre ajuste era muy importante ahora ustedes entiendan que tengo ya no un árbol y que cada uno de ellos representa una variedad muy diferente al otro con lo cual es difícil que esta suerte de copia que hacía de la realidad el árbol cuando era un solo árbol puede darse en el caso de los bosques aleatorios se pueden manejar también en este modelo valores faltantes o nulos si lo que se hace justamente en este Universo de valores si se dan situaciones de ese tipo se pueden tomar valores promedios de una misma característica para reemplazar ese valor nulo o faltante curiosamente lo que vamos a destacar como desventaja tiene relación directa a lo que recién destacamos como ventaja bueno muchas veces esto pasa en realidad dijimos que una gran cantidad de árboles sí 100 o más me da la posibilidad de que justamente Este modelo como un todo del Bosque no esté afectado por el sobreajuste pero también con riesgo de que si en Pos de su objetivo tengo una enorme cantidad una cifra muy grande de árboles termine en detrimento modelo en términos de velocidad es decir que sea un modelo muy lento no va a estar afectado por sobre ajuste Pero puede llegar a ser muy lento hay que buscar un equilibrio entre esa ventaja y esta posible desventaja bien y obviamente que recién lo vimos en el gráfico esta ventaja de poder mirar en un gráfico el árbol y rápidamente entenderlo no se da en el caso del Bosque fíjense que recién vimos un ejemplo solamente con 10 árboles ustedes imaginen si tuviese que recorrer de arriba hacia abajo que he hecho cuando ustedes impriman en la práctica de la parte que viene van a ver que justamente aparecen los 100 árboles y tarda muchísimo viste por graficar si en árboles no no no no lo hace muy rápidamente ustedes se imaginarán bueno Obviamente que si eso es así van a poder comprobar que recordarlo visualmente es aún más lento y obviamente no va a ser tan fácil de visualizarlo Pero bueno Al fin y al cabo Esto va a pasar en la mayoría de los modelos simplemente lo tenemos que medir con determinados elementos Como por ejemplo con el indicador del nivel de precisión del modelo o sea el Score y bien hasta aquí esta primer parte de esta clase número 9 la introducción teórica de bosques aleatorios vamos a ver después la segunda parte con todo lo que tiene que ver con la implementación práctica de este nuevo algoritmo Les recomiendo repasar todo aquello que haya tenido que ver con la práctica de árboles si bien hablamos de Cuáles son las diferencias y que de alguna manera este algoritmo nuevo es superador del anterior es importante que entiendan algunos conceptos o repasen mejor dicho algunos conceptos que tienen que ver con la validación Cruzada y con el tuneo automático de parámetros recuerdan el grid se ve porque lo vamos a aplicar en la práctica de esta segunda parte también para los bosques y vamos a hacer en el caso de esta práctica una un ejemplo para un problema de clasificación y un ejemplo para un problema de regresión en el caso de la clasificación vamos a repetir el conjunto de datos de las flores de Iris y en el caso de la regresión vamos a repetir el Data set se acuerdan del valor de las casas de Boston csv la idea de repetir los conjuntos de datos tiene que ver comparar justamente un modelo con otro es decir esto nos permite poder evaluar el rendimiento de diferentes modelos en el mismo conjunto de datos y ver justamente esta suerte de competencia a la hora de determinar Cuál es el algoritmo mejor para el problema que yo tengo seguramente más adelante vamos a hacer una práctica donde justamente vamos a hacer en un solo conjunto de un solo Notebook si tomar un conjunto y aplicarle automáticamente todos secuencialmente los algoritmos que hemos visto y medir todos los scores y en una tabla poder tener justamente una idea de Cuáles pueden ser los mejores para cada caso también vamos a poder ver a través de una estructura de este tipo en algún momento Cuáles pueden ser los hiper parámetros o los valores de los parámetros y los score que se obtienen con ellos de modo que también podamos visualizar más allá de que tenemos una forma de que nos diga directamente la detección automática del tuneo de parámetros bueno Cuáles son los parámetros para cada score y poder ver no solamente los valores que fueron los más exitosos porque tuvieron el score más alto sino ver lo que los presentan también como una suerte de visualización conjunta de cómo van variando los hiper parámetros y cuáles son los resultados que se obtienen bueno en concreto repasen todo lo que vimos la clase pasada porque lo vamos a aplicar también en la práctica es decir en la segunda parte de esta clase hasta aquí llegamos con esta primera parte los esperamos para poner en práctica todo lo aprendido en la segunda parte Titulo: Clase 9 (parte 2) del Curso de Inteligencia Artificial \\n URL https://youtu.be/TLUMH0UdZ2Q  \\n 1931 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 9 del curso de Inteligencia artificial de ifes en ella vamos a poner en práctica todo lo que aprendimos en la primera parte vamos por ello  Hola a todos nuevamente vamos a empezar la segunda parte de esta clase número 9 tratando de poner en práctica todos los conceptos teóricos que vimos en la primer parte de esta clase antes de empezar con la clase propiamente dicha vamos a ir a ver lo que hicimos en la clase pasada justamente la parte número 2 en la implementación práctica de un algoritmo de árbol de decisión por qué Porque justamente habíamos tomado en principio la idea de ejemplificar el uso de este algoritmo a partir de un ejemplo basado en un problema de clasificación y habíamos tomado Para ello el dataset de Iris se acuerdan de las flores y del mismo modo habíamos hecho al final sí que lo había dejado como un ejercicio adicional un ejemplo de un problema de regresión donde habíamos tomado como ejemplo el mejor dicho como como medio para poder ejemplificar ese algoritmo en ese caso un Data set de Boston ccv de las casas de Boston del valor de las casas de Boston que ya lo hemos usado también en la parte de análisis de datos bueno por qué referencia esto porque vamos a hacer lo mismo pero ahora justamente con el Random Forest es decir con los bosques aleatorios Vamos a abordar primero el problema de clasificación con el mismo conjunto de datos es decir de Iris que en el caso de los árboles y vamos a ir luego a un problema de regresión y vamos a tomar también el mismo caso el mismo dataset perdón de Boston csv que usamos en el caso de los árboles en la clase número 8 esto nos va a permitir ver qué cosas tienen en común ambos algoritmos Qué cosas son diferentes y también poder comparar lo más importante de todo esto que es el score que logró en un caso y en otro vamos a permitirnos hacer un pequeño paréntesis porque seguramente en esta aplicación introductoria de esta clase habrán visto un detalle que quizás le habrá llamado la atención y bueno puede ser una buena oportunidad abordarlo habrán visto que en este Notebook yo lo que he hecho en el anterior es hacer clic en este símbolo y lo que hace es ocultar o Mostrar todas las celdas que están debajo de Este título simplemente haciendo guía acá fíjense que como está contraído me parece un mensaje que dice que hay 15 celdas ocultas que la veo cuando clic en este botón sí Y ahí aparecen todas las celdas y luego veo en el siguiente título lo mismo bueno Esto es una forma de poder ver rápidamente el contenido si yo soy ordenado y pongo cada una de las secciones con un título que encabece que viene después si a nivel de código de qué se trata es el código que viene después y poder tener una lectura más comprensiva del texto de todo el Notebook y después abrir cada una de las secciones que a mí me interesan para ver la cuestión del código que está de alguna manera referenciado por ese título que está allá arriba y bueno un paréntesis para aprender algo nuevo no en este caso de Machine learning no en este caso ni de bosque ni de árboles sino de lo que es Cómo se maneja el Notebook de cola vamos ahora entonces con el primer código o la primera parte de código de este ejemplo vamos a compararlo con la clase anterior y vemos que las librerías son prácticamente las mismas y o pandas nampas split aquí aparece lo diferente obviamente porque este algoritmo no es igual que el que usé antes que es un decisión triple sino en este caso es un Random Forest classippier y sale de otra librería antes era de sidekitland Tree ahora es de essamble luego viene plot Tree que es lo mismo que usamos en la clase pasada y finalmente grid set se ve que a diferencia de la clase pasada incorporamos keyfall y crossbal score que en este caso no vamos a usar y finalmente warnis es decir que la librería son virtualmente las mismas Así que las ejecutamos y va a llevar como siempre un tiempito más porque no está conectado Así que Esperamos que justamente No solamente ejecute la primera celda sino también que establezca la conexión de este cono ya está ejecutada la conexión Ahora sí ejecuta la importación de las librerías y paso seguido pasamos a incorporar el dataset que vamos a usar para esto que como ya dijimos antes es el de Iris Aquí está lo incorporamos y una vez que está hecho Hacemos como siempre un gel vamos a agrandar un poquito más esta pantalla como solemos hacer para verlo mejor bien ahí Nada nuevo ustedes ya lo han visto muchas veces y hacemos un save para asegurarnos de que realmente hemos recibido todo el conjunto de datos tal cual está prescrito y conocido por nosotros es decir 150 observaciones y cinco características a continuación vamos a preparar los datos para el entrenamiento hecho similar a lo que habíamos hecho la clase pasada donde habíamos logrado el x y el y referenciando al Data con los nombres de cada una de las características para las x y con el nombre de la única característica que es el elemento Target parali pero recuerdan que yo no había puesto aquí abajo este comentario de otra forma de hacer Esto justamente no usando el nombre de las características sino su referencia por su número de índice y eso es lo que vamos a aplicar aquí es decir este código que está aquí es lo que yo le puse como ejemplo alternativo en el cola número 8 y que los invite a que lo proba su cuenta dado que el resultado Debería ser el mismo pero como otra forma de lograrlo bien Esto lo que vamos a hacer aquí con lo cual lo ejecutamos y paso seguido vamos a separar los conjuntos de entrenamiento de Test como lo hacemos regularmente con lo cual al ejecutar este código obtenemos la misma información que habíamos tenido en el caso de los árboles de que el conjunto de entrenamiento está conformado por 112 observaciones y el conjunto de Test por 38 a continuación vamos a crear nuestro modelo con Random Forest clasifire así como en la clase anterior decisión bien Ahora usamos el algoritmo de bosques aleatorios y luego hago el entrenamiento como habitualmente lo hacemos en este caso vamos a usar hiper parámetros la explicación que está aquí arriba ya la vimos en la parte teórica de esta clase con lo cual no vamos a ahondar en ello si simplemente rescatar que vamos a utilizar 10 estimadores a pesar que en la teoría estamos diciendo que es como se debe aplicar que la cantidad de estimadores deben ser 100 o más de 100 pero bueno a título de que quiero mostrarles el gráfico de un bosque de una manera muy sencilla vamos a hacer una cantidad de estimadores que pueda facilitarnos esa visualización Pero obviamente está claro que eso no es lo que debería hacer bien lo ejecutamos y obtenemos este Random Forest clasifire y medimos como hacemos habitualmente su precisión para el modelo de tren y para el modelo de Test obteniendo 098 y 094 bien Vamos a graficar ahora el modelo y vamos a tener en cuenta lo siguiente como hemos repetido desde la teoría y hasta ahora el bosque Está compuesto por árboles si yo me remito a lo que hice en el colar número 8 tengo aquí justamente el código que me llevaba a mí a poder graficar el árbol Pero qué pasa ahora yo no tengo un árbol tengo muchos árboles porque tengo un bosque Cuántos árboles tengo bueno tantos como estimadores y yo haya indicado a la hora de crear el modelo en este caso 10 por lo tanto yo tengo ya una vez creado el modelo tengo un bosque con 10 árboles y cada uno ya está diseñado por lo tanto este código que yo creaba antes para poder graficar un árbol Ahora yo tengo que ejecutarlo 10 veces por lo tanto necesito de un Ford que me recorra y me dé información de cada uno de los árboles del Bosque y por cada uno de ellos haga un gráfico Tenemos aquí Ford árbol que es el nombre que he decidido darle a esta variable como puede ser siempre como decimos cualquiera y Ford class.estimators guión bajo Ford Class es el nombre del modelo que yo creé de Random Ford clasifit y punto estimados guión bajo es cada uno de los árboles que forman parte de ese bosque por lo tanto yo hago este Ford y por cada Ford escribo exactamente el mismo código porque lo que va a hacer es lo mismo que hacía antes solamente que 10 veces lo ejecutamos y bueno va a tardar un tiempo prudencial fíjense que ya se va visualizando abajo ven que van apareciendo los árboles o los arcos obviamente los va poniendo como ya vimos anotaría uno debajo del otro ya terminó con lo cual yo ahora puedo recorrerlos cada uno de los 10 árboles que como mostramos en la teoría no todos están conformados ni por la misma cantidad de características ni por la misma cantidad de observaciones Lo importante es que son 10 y cada uno de ellos podemos observar aquí la forma y justamente la decisión que tiene cada nodo y cuando termina cada uno de ellos obviamente Este ejemplo que he puesto aquí lo he hecho basado en 10 estimadores porque lo que va a pasar a continuación justamente que lo vamos a hacer con 100 y obviamente va a tardar mucho más y la visualización de 100 árboles obviamente va a ser mucho más compleja si lo queremos tomar a título de ejemplo obviamente lo podemos ver con mucha paciencia Pero obviamente se puede ver también así que es lo que vamos a hacer a continuación por eso tal cual dice el título Aquí vamos a repetir El ejemplo pero en este caso con 100 estimadores que como venimos diciendo reiteradamente es el mínimo recomendado por lo tanto creamos el modelo especificamos la cantidad de estimadores aquí lo entrenamos y obtenemos su precisión viendo que en el caso entrenamiento es una presión ideal del 100% y en el caso del modelo de test del 97%, no son malos scores hay que tener cuidado con este 100% del modelo de entrenamiento porque puede ser un problema de sobreajuste bien Más allá de eso lo que vamos a hacer ahora es graficar este árbol para lo cual he escrito el mismo código que había escrito antes solamente que lo he hecho de una manera más condensada fíjense que en este caso lo había hecho en varias líneas esto obviamente permite una visualización del código mucho mejor en este caso lo he hecho de una manera más condensada para poder empezar a ver ahora sí aquí abajo Cómo se van a ir creando cada uno de esos 100 árboles Así que lo ejecutamos ya y evidentemente este proceso va a durar mucho más tiempo que la anterior fíjense que aquí bajo me da referencias del tiempo de ejecución y todo el proceso digamos que va haciendo para crear este árbol bien ahí terminó vamos a corregir una pequeña F de ratas que acá dice gráfico el árbol creado y en realidad es el bosque vamos a ponerlo como corresponde para no cometer un error de concepto y vamos a guardarlo para que se haya registrado en esto que después va a llegar a sus manos bien han visto que ha tardado mucho tiempo obviamente mucho más que en el caso anterior 58 segundos está actualiza aquí abajo con respecto al caso anterior Pero bueno lo que tengo ahora es justamente dentro de esta estructura cada uno de los 100 árboles que ustedes lo han visto porque en esta visualización acelerada que le he puesto en este vídeo han podido ver como iba apareciendo cada uno de estos árboles de este bosque vamos a pasar ahora al grid para bueno ver si con los manejos de hiper parámetros podría obtener un mejor es difícil tener un mejor score porque en el conjunto de Test Debería ser Superior entrenamiento superior al 100% no va a poder ser pero bueno insisto no nos quedemos con esa idea de ese 100% como que ya logramos un objetivo sigamos probando otras alternativas y justamente glitch ser cb nos permite eso esto lo usamos recordarán en el caso de los árboles de decisión y lo vamos a reiterar también en bosques aleatorios vamos a utilizar justamente en el parangred todas las opciones con que quiero que ese grid vaya combinando en puedo tener un mejor score por lo tanto en el caso de la cantidad de estimadores voy a poner una etiqueta en este momento acuérdense que la etiqueta tiene que llamarse igual que cómo se llama o se denomina al hiper parámetro sí es decir lo que tenemos pero no en el árbol aquí en el modelo en este momento bien esos nombres tienen que ser idénticos a las etiquetas que yo pongo obviamente no tienen que ser todos tienen que ser los que yo quiera nutrir a este glitcher se ve elijo en estimados y le ponemos entre corchetes los valores opciones con que quiero que ese ser vaya trabajando en el caso en este mérito 100 200 y 300 en el caso de Max features 23 en el caso de Max sampling dos tercios y tres cuartos acuérdense que este obviamente tendrá que ver con la característica del Data set en este caso tenemos con cuatro características de entrada con lo cual obviamente las opciones no pueden ser muy amplias si fuesen más características esos valores podrían tener mucha más opciones luego más que nada 35 y 7 la profundidad de los árboles de este bosque Y criterium ahí no tengo más que esas dos características luego obviamente hago el cb le digo que quiero un método de validación Cruzada de 10 folls de se acuerdan de los 10 pliegues que esto también lo vemos en la clase pasada y bueno después el resto es similar a lo que habíamos armado obviamente una vez que haga gris lo voy a entrenar para obtener un score lo ejecuto ahora y finalmente el proceso determinado ha tomado 5 minutos esto le da la Pauta De que al igual que en el caso anterior del tiempo que nos tomó graficar el árbol ahora que tenemos 100 estimadores esto tarda mucho más en este caso no son 100 sino que tenemos 100 200 y 300 como opción Por eso tardó mucho tiempo el desarrollo de este proceso Comparado con el que nos tocó cuando hicimos en la clase 8 el mismo proceso pero para un solo árbol obviamente en este caso un grid se ve donde le pongo como opciones 7 200 o 300 árboles para un bosque obviamente era lógico que pudiese tardar el tiempo que tardó lo que vamos a ver a continuación es una forma diferente de poder visualizar los resultados de un proceso de glitch ser se ve ya sabemos porque no hablamos la clase pasada que con el gris ser Severo que se busca es un modelo que tenga el mejor score posible y para ello yo lo que hago es darle valores optativos u opcionales de cada uno de los hiper parámetros y que griser se ve los vaya combinando en Pos de obtener el mejor modelo posible con el mejor escort posible es importante destacar que probablemente haya alguna opción que yo no haya elegido supongamos un estigma de 400 o 250 Sí y quiso podría haberme dado mejor score es decir todavía no es la solución definitiva Es una herramienta más Pero sigue dependiendo de mi elección de mi intervención sin mi intervención hace que las variantes que yo le doy quizás las mejores bueno obviamente va a ser lo posible con ellas Así que es importante que tengamos en cuenta eso para que bueno cuando hayamos corrido un proceso de este tipo podamos considerar volver a hacerlo cambiando algunas opciones si es que los scores que hemos obtenido no volviendo al tema nosotros siempre tenemos a partir de lo que vimos la clase pasada que con se obtienen los mejores estimadores pero es importante visualizar otros que no son los mejores sino que quedaron relegados detrás del mejor para tener una visión conjunta de cómo va variando las opciones de los cifras parámetros y el score que se obtiene por eso vamos a armar un pequeño código acá donde vamos a hacer Que vamos a tomar todos los estimadores que me arroja el cb y los voy a poner dentro de una estructura tipo Data frame para que para poder después recorrerla y presentarla tal cual cuando tomamos un conjunto de datos como hicimos en este caso el Iris o con Boston donde lo pasamos después con un Head sus primeros registros bien acabamos de hacer algo similar con lo cual lo primero que voy a hacer es crear un Data frame como Ven aquí lo voy a poner como no Ver resultados y dónde de dónde voy a tomar la información perdón de grid que es el nombre que le he puesto al glitcher se ve y el valor cv- bajo resolución bajo eso me da el detalle completo de todos los elementos que componen ese modelo con el valor digamos dedicada y pre parámetro elegido para ese caso el score y otro tipo de información más que ya vamos a ver a continuación Por ende si yo voy a hacer esto aquí transitoriamente para que puedan ver a qué me refiero con toda la información o con el concepto de toda la información yo voy a poner un Head de el conjunto de datos resultados en Data frame resultados lo ejecuto y van a poder ver que tengo muchísima información relativa en 35 columnas y tengo 35 datos relativos a lo que me arrojó mi proceso de fíjense que también me da los score por cada uno de los splits se acuerdan que dijimos que íbamos a hacer un volvemos aquí arriba un grid se ve con un validation de 10 así con 10 pliegues bien fíjense que me da a mí aquí el score por cada uno de los pliegues o sea es una información muy amplia muy valiosa y que me va a permitir a mí poder hacer un análisis con todo el nivel de detalle que yo quiera bien lo que vamos a hacer aquí justamente es para no tener tanta información que quizás no nos hace falta sino para el caso digamos que estamos visualizando vamos a tomar solamente algunos parámetros y lo que yo voy a poner aquí con filter luego voy a borrar con Drop todas las columnas params y parciales y finalmente voy a sortear o sea voy ordenar toda estas cinco primeras apariciones porque lo que voy a hacer es un Head y finalmente como dije recién hago estoy haciendo la forma de escribir esto aquí esto sólo podría poner como resultados punto filter igual punto drop.org punto G en toda una sola expresión en toda una sola línea como lo quiebro en líneas voy a tener que bueno esa fractura especificarla para que entienda justamente el código que esto es solamente una sola expresión con esta barra inclinada sí bien luego este detalle lo ejecuto y van a ver ahora que no tengo las 35 características que ha visto recién sino que tengo una dos tres cuatro cinco seis siete características que son las que estoy visualizando pero lo importante aquí es lo siguiente fíjense que tengo los score Aquí sí para el conjunto de Test y para el conjunto de entrenamiento la cantidad de estimadores la cantidad de macsamble la cantidad de Max feets la cantidad de profundidades y las opciones de criterio para cada uno de los cinco primeros casos Si fíjense por el número que yo tengo aquí Que obviamente las variantes fueron muchas sino que lo que yo he hecho aquí es pedirle solamente las mejores porque porque las órdenes estos resultados según mi según este esta característica que está aquí fíjense que aparece justamente ordenada primero la más grande y luego a continuación la siguiente obviamente en este caso no hay muchas variantes porque todo dieron Exactamente lo mismo con lo cual es muy rico Este ejemplo porque literalmente Yo podría tomar cualquiera de ellas sí porque todas tienen el mismo score y lo interesante aquí es ver que ese score lo obtuvo fíjense los cinco casos iguales con distintos valores de hiper parámetros y bueno Esto es una información muy importante no es la única vez que la voy a poder obtener puedo repetir a pesar que sea muy largo el grises se ve para obtener otra combinación de valores en Pos de obtener el mejor score Pero bueno creo que valía la pena tomarse un tiempito para mostrarles esta forma de analizar los resultados porque justamente me va a permitir mirar no solamente el primero ignorando que hay otros cuatro en este caso y capaz que hay más porque solamente toman los primeros aquí Que bueno me han dado también muy buenos resultados con otras combinaciones de parámetros para cerrar esta práctica relativa a un ejemplo basado en un problema de clasificación vamos a comparar como dijimos al principio de la clase la situación del score en el caso de un Random Forest respecto de un 3 decision entonces fíjense que en el caso de el árbol tengo un score de 99 y 92 para el conjunto de entrenamiento y para el conjunto de Test respectivamente en el caso de El Random Forest el bosque aleatorio tenemos 99 y 97 que es mejorado luego del grid se ve llegando a 1 y 097 bueno con esto concluimos el caso de un algoritmo relativo perdón a un problema de clasificación y vamos a pasar ahora al caso de un problema de regresión al igual que como tuvimos Y de alguna manera manifestamos hoy el principio de la clase de El ejemplo que yo les puse en el último código que les dejé para que ustedes lo vieran por su cuenta de El árbol de decisión donde también abordamos un problema de algoritmo de regresión y vamos a utilizar como dijimos al principio también al igual que en aquella oportunidad el Boston ccd como conjunto de datos para realizar este modelo o para practicar Este modelo también Para este caso del Random Forest las librerías son las mismas fíjense que bueno Esto es lo mismo que hicimos en el caso del árbol reiteramos la librería básicamente para que quede un registro esto pero estrictamente esto ustedes saben que no es necesario en tanto en cuanto esas librerías tan importadas y yo no haya caducado digamos la conexión con el color ejecutamos este código para importar las librerías en este caso obviamente tenemos un Random Forest regresor es lo único que cambia respecto de las librerías que tenemos al principio que teníamos un Random classippier y importamos el archivo Boston csv vamos a la carpeta Boston Boston y luego una vez que termina esto como siempre lo creamos una variable un Data frame una variable datos y hacemos un Head y vemos el botón que ya conocemos el conjunto de datos que ya conocemos hacemos un shape para cerciorarnos de que existe la cantidad de filas que ya sabemos que tiene la cantidad de columnas y la cantidad de observaciones características 506 y 14 y vamos a preparar los datos para el entrenamiento como hicimos antes con las cinco características que tenía el conjunto de datos Iris donde poníamos las cuatro para el conjunto de predicciones perdón y la quinta como Target en este caso son 14 características como vemos aquí Así que tomamos las 13 primeras para el conjunto de predictores y la última como Target ejecutamos eso y luego separamos conjuntos de entrenamiento y test como ya sabemos vemos Cuántas observaciones han correspondido a cada caso 379 y 127 y creamos nuestro Random poniéndolo en una variable que hemos dado a llamar Ford red y luego hacemos el entrenamiento una vez hecho esto podemos hacer las predicciones Esto no es obligatorio Es simplemente para si alguno quiere ver como ya hemos visto en otras oportunidades ejecutar esto y ver qué valores tiene este array de ipred bueno es una forma de hacerlo concretamente lo que hacemos a continuación sí es medir la precisión lo ejecutamos y obtenemos 95 y 86 95 para el conjunto de entrenamiento 86 para el conjunto de Test es una distancia considerable es un 0.9 0.09 Perdón este y bueno no sería lo ideal sería ideal que el conjunto de Test fuera cercano al detalle o inclusive superior pero lo vamos a comparar con el caso de clase 8 de El árbol de regresión y en aquella oportunidad teníamos 087 066 el problema era mucho peor Aún es decir que hemos mejorado no solamente el valor del conjunto de entrenamiento sino también la distancia que existe entre el score del conjunto de entrenamiento y el conjunto de Test paso seguido vamos a graficar el bosque con la misma lógica que aplicamos antes cuando era el problema de clasificación es decir un Ford que me arroje la información de todos los estimadores Sí y por cada uno de ellos que es un árbol graficar cada uno de los árboles lo ejecutamos bien aquí vemos entonces el gráfico resultante que como ven es bastante complejo Sí porque obviamente al ser un problema de regresión la cantidad de nodos y el contenido de los nodos en cuanto a las decisiones que toma cada o que están registradas en cada uno de ellos es obviamente totalmente diferente a un problema de clasificación la visualización obviamente es muy complicada están muy superpuestos justamente en virtud de la cantidad de nodos que se trata un nodo encima del otro hasta el punto que en algunos casos como aquí lo tapa Pero bueno Esto lo vamos a tener que configurar justamente con el size que le pongamos con el fix size que le pongamos a este a esta estructura de Sub notes Pero bueno Ahí tenemos nuestro gráfico lo que vamos a hacer a continuación es un tema también muy importante que vimos en la clase pasada que es ver la importancia de las características en este caso nosotros tenemos 13 variables de entrada será necesario tener un modelo con 13 variables de entrada habrá Abraham algunas que serán más importante que otras y en ese caso poder tomar solamente las importantes y dejar de lado las que no lo son bueno para esto teníamos un código que lo vamos a volver a ejecutar aquí porque la situación es la misma y tenemos justamente en este caso la información de Cuáles son las características más importantes y sigue siendo RM o rooms cantidad de habitaciones la característica más importante en este caso no vamos a tomar las características más importantes para generar un nuevo modelo y Buscar un modelo con mejor score o más eficiente sino que vamos a tomar solamente una de ellas con la intención de buscar un gráfico para visualizar las cosas también de un modo muy parecido al que utilizamos cuando vimos el primer algoritmo de Machine learning es el algoritmo de regresión lineal Cómo es esto vamos a tomar en principio como una extracción del conjunto de entrenamiento con lo cual voy a tomar solamente del conjunto de entrenamiento que está conformado por 13 características solamente la característica RM lo transformó en un array porque justamente Eso es lo que me va a requerir más adelante el gráfico esto ya lo hemos visto en otros casos cuando yo tengo una sola variable de entrada tengo que transformarla en a través de nampai en un array para que pueda ser graficada y luego obviamente la variable de entrenamiento la pongo en una variable que he llamado itr por y traen Y luego el conjunto de Test en lugar de tomarlo tal cual también fuera compuesto por las 13 características lo voy a circunscribir solamente a este campo sí bien entonces tengo en lugar de tener el viejo y traen y x traen y X3 e3 voy a tener xr y trxts e its lo generamos y con ello creamos un nuevo modelo obviamente ahora una variable de entrada y obviamente siempre la salida con esto lo que vamos a hacer es un scat no sé si se acuerdan que teníamos el scatter que era un gráfico de dispersión de puntos sí Solamente basado en el conjunto de Test con lo cual Este es el conjunto de Test sobre el cual va a ser probado el modelo se acuerdan que en el caso de lineal dibujamos una línea y veíamos si esa línea era haríamos abarcativa de muchos puntos o de pocos y obviamente si nosotros trazáramos una relación lineal aquí imaginemos una línea obviamente pasaría por mucho de los puntos estaría muy cerca de mucho de los puntos pero también muy lejos de muchos de ellos en este caso vamos a hacer lo mismo solamente que no va a ser una recta el dibujo que represente un algoritmo de búsqueda aleatorios sino que va a ser otra figura totalmente diferente más no va a representar una figura geométricamente conocida sino que algo totalmente diferente y para ello voy a escribir este código si x grid va a ser justamente la grilla en la cual voy a dividir ese esa estructura que voy a dibujar de dos coordenadas y le voy a decir que vaya desde el valor mínimo del valor de Test al valor máximo Alba brotes y haga divisiones desde a 0.1 luego voy a hacer un reclade de lo que es la longitud del equilibrio también para estipular la estructura de puntos ideal para dibujar no los puntos que ya lo pude dibujar sino esta figura que va a representar el modelo del Bosque aleatorio finalmente hago el scatter en Blue para dibujar de nuevo los puntos y en rojo voy a dibujar esta figura que ustedes van a poder comprobar Ahora cuando ejecute este código ven esta línea roja totalmente alejada de cualquier figura como decía recién geométricamente conocida representa el formato del modelo de bosques aleatorio para un problema de este tipo o sea estos conjunto de Test que lo estoy probando esta figura fue inspirada en el conjunto de entrenamiento conjunto acotado acuérdense que en este caso solamente está conformado por la cantidad de habitaciones y el valor de la vivienda tal cual conducimos a que el problema de regresión lineal y fíjense lo curioso de la figura que dibuja en base a la lógica de este bosque bueno con esto cerramos la clase de hoy Espero que se haya entendido todo como siempre todos estos recursos van a estar a partir del Campus y estoy atento cualquier consulta que tengan sobre esto y si no nos vemos en la clase número 10 gracias Aquí finaliza la clase número 9 ahora ya tenemos los conceptos básicos de los bosques aleatorios y sabemos también como programarlos e implementarlos los esperamos en la próxima clase Titulo: Clase10 (parte 1) del Curso de Inteligencia Artificial \\n URL https://youtu.be/7MbNubZixhg  \\n 1843 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 10 del curso de Inteligencia artificial de ifes en ella Vamos a hacer un repaso de todos los algoritmos vistos hasta aquí incorporando algunos conceptos nuevos    Hola a todos Cómo están esta es la clase número 10 del curso de Inteligencia artificial de ifes seguimos en el módulo de Machine learning y esta clase número 10 la vamos a separar en dos partes en la primer parte vamos a trabajar con una práctica integral donde vamos a repasar dos de los algoritmos que ya hemos visto antes el de regresión lineal y el de regresión logística y vamos a agregar algunas cosas nuevas que van a formar parte de esa práctica pero que también aplican a otros algoritmos como vimos también el de árbol y el de bosques y en la segunda parte de esta clase vamos a hacer un mouse incorporar un tema Mejor dicho que se llama reducción de la dimensionalidad es un tema muy importante que afecta mucho al rendimiento de los modelos y justamente vamos a dedicar toda la segunda parte de esta clase para ver es importante el tema vamos a empezar entonces con la primer parte de ellas que es la parte de la práctica integral bien ya estamos ubicados aquí en el Notebook de la clase 10 y justamente Aquí vemos el título que va a tratar el primer tema que vamos a abordar en él el manejo de Data set con variables categóricas lo vamos a aplicar como dice aquí a un algoritmo de regresión lineal pero este tema puede aquejar a cualquier tipo de algoritmo y a cualquier tipo de intención de construir un buen modelo con ese algoritmo Qué es una variable categórica una variable categórica es una variable que representa una categoría de así su nombre por ejemplo podemos decir el género que puede ser masculino o femenino o por ejemplo el tipo de documento dni o pasaporte bien Eso es un dato que por lo general está representado por un texto y Que obviamente como tal no permite poder construir con esa categoría con ese dato un modelo porque los modelos necesitan datos numéricos por lo tanto lo que tengo que hacer es convertir esa categoría en un valor numérico Y ese es el tema que va a abordar en esta primer parte de esta clase por lo tanto lo primero que vamos a hacer va a ser incorporar como siempre las librerías que vamos a necesitar para desarrollar esta práctica tenemos pandas nampai tenemos línea recreation de sydler train test split de cycle Y tenemos dos librerías que vamos a usar por primera vez porque justamente tienen que ver con el propósito de esta clase es decir make Transformer y One Hot encoder ya vamos a ver para qué sirven cada una de ellas y finalmente warnings es decir que vamos a arrancar ejecutando esta celda para incorporar todas esas librerías bien lo que vamos a hacer ahora es abrir un Data set desde una URL o sea Data set que vamos a tomar como ejemplo lo vamos a estar desde una URL de un kitkap yo les he puesto aquí que Recuerden el cola clase 2-1 Sí aquí lo tengo lo vamos a abrir donde justamente abordamos ese tema estamos hablando de la primer parte de la clase número 2 o sea hace rato que ya trabajamos con este tema de tomar desde github un tengo un conjunto de datos en ese caso bueno aquella oportunidad fue el Titanic csb ahora vamos a hacer lo propio con otro que tiene los siguientes datos tiene digamos la información relativa a las transacciones de un sitio web de tipo de ventas electrónicas con el cual Ese Conjunto de datos tiene los siguientes datos el número de transacción la edad del consumidor el número de ítem en el carro de compras cuántos ítems va comprando esa persona y están en el carro de compras el sueldo mensual del consumidor el tiempo que duró la transacción la cantidad de veces que el cliente compró en ese sitio el género del consumidor aquí ya tenemos una variable categórica justamente el tier de la ciudad que es el tier de la ciudad esto es una base de datos que proviene de la India donde a determinadas de conglomerados ciudadanos digamos urbanos los categorizan con justamente un indicador que se llama y que justamente identifica si es una población más o menos este densa o más o menos importante o desarrollada bien es un dato también categórico que no va a ser justamente para bueno implementar esta idea de pasar una variedad de categórica una variedad numérica y finalmente el valor total de la compra bueno todo eso está justamente en este conjunto de datos que voy a importar aquí que se llama ecomex ccd bien entonces tomamos desde este conjunto de datos de esta dirección perdón de esta URL de itcap y creamos un Data set que le vamos a llamar datos y luego hacemos un Head para reconocerlo como habitualmente hacemos ejecutamos eso y Aquí vemos que tenemos los datos que estamos referenciando recién dispuestos en este ejemplo de los cinco primeros oraciones y vemos concretamente Que justamente género es una variable categórica Y si te Pierre también es una variable categórica con lo cual lo que vamos a hacer como ese título aquí es transformar las variables categóricas en para ello vamos a usar las librerías que importamos hace un rato y de allí vamos a tomar en principio tres elementos make colon Transformer que lo que hace es crear un transformador de columnas es decir va a haber un objeto que va a tener toda la información lo que tiene que ver con el propósito de transformación de columnas por qué transformación de columnas porque justamente estas dos columnas las tengo que transformar de categóricas anométricas bien seguimos One Hot encoder el tipo de transformador que voy a usar para hacer esa transformación porque el tipo porque las transformaciones no son solamente del tipo que vamos a hacer nosotros en esta clase nosotros ya hicimos una transformación cuál fue escalar los datos cuando nosotros hacíamos que los datos dejarán de tener su valor original para hacer o ser transformado justamente en un valor de tipo escalado bueno allí aplicamos un concepto de transformación entonces esa transformación puede ser pueden ser distintos tipos de transformaciones en este caso la que vamos a elegir es aquel lo que hace es convertir una variable categórica en numérica y finalmente es lo que pone En rigor la transformación en este caso o la ejecución de lo que quiero hacer bien Por lo tanto Aquí vemos el código que tiene que ver con toda esta aplicación que vimos recién en principio lo que vamos a hacer vamos a crear un Data frame con solamente las dos columnas que queremos transformar se entiende Es decir DF es un nuevo que proviene de datos pero no de todo datos sí datos acuérdense que este es esto es decir voy a crear un Data frame solamente con la columna gender y citiar sí qué es lo que he puesto aquí datos Y luego lo que voy a hacer es un Drop na porque un Drop na porque presumo presumo que genderick en alguna observación pueden tener valores nulos para asegurarme de que yo no ocurra lo que puedo hacer es un Drop Leal y si me quedo tranquilo digamos de alguna manera de que ese Data frame tiene esas dos columnas y que ninguna de Las observaciones esas dos columnas tiene un valor bien luego lo que hago es crear el transformador lo que hablábamos Recién con que como dijimos con mail colon Transformers le pongo el tipo de transformador que voy a hacer según el tipo de transformación que quiero hacer que es One Hot encoder le pongo los dos y las dos características que quiero que se transformen que en realidad son las únicas porque lo voy a trabajar justamente sobre DF y de F tiene esas dos columnas bien finalmente hago la transformación con fit Transformers sobre DF Insisto que es el Data frame que tiene dos columnas creo sí el Data frame Transformer DF que es Transformer DF es DF transformado sí es decir con DF tengo los datos sin transformar y en transformar de F tengo los datos transformados y hago un Head para ver qué pasa con ello lo ejecutamos Y qué es lo que pasa con eso fíjese Cuál es el efecto que provoca lo que hace es dividir Mejor dicho crear tantas columnas como columna barra tiene cada una de las columnas originales es decir hender tiene dos opciones femenino masculino Entonces qué hace Crea una columna hender femenil y hender made en el caso de tier tiene 1 2 y hay un tercer elemento no lo podemos ver aquí existe si ponemos un Head con más valores lo vamos a ver que son tres elementos tres opciones lo que hace es crear un citier-17-2 Y citier-3 sí bien entonces bajamos para verlo bien y un 3-2-1 es decir que de dos columnas creo 5 porque porque la primer columna tenía dos opciones y la segunda con una tenía tres opciones bien Ahora lo que voy a hacer con este resultado es verificar Cuál es el contenido ya tengo Claro que hizo con los nombres que hizo con las columnas pero qué le pone al dato ya que el dato original no está más bueno lo que hace es ponerle un uno cuando ese dato se corresponde con el título de esa columna y un cero cuando no lo es por ejemplo vamos a la primera observación hender es email con lo cual lo que hacen la transformación es poner un uno en la columna finger en la columna mail lo mismo la segunda pero la tercera se invierte porque porque seguramente ahora voy a la tercera observación y justamente es masculino sí lo mismo hace con el cityer fíjense que el primero es uno y el segundo dos y el tercero es 2 vamos a verlo abajo qué fue lo que hizo bueno justamente lo que hizo es poner un 1 en el tier 1 y un cero entre dos y obviamente en el 3 en el 3 también un cero en el segundo caso en el segundo caso Perdón vimos que tier era 2 fíjense lo que hace por un cero en uno y un 1 en 2 y continúa un cero en tres si en algún caso hubiese sido un tier 3 que va a ser va a poner un cero en la columna 1 un 0 en la columna 2 y 1 en la columna 3 una vez que logramos esto cuál sería el objetivo del Data set que imaginamos queremos tener para empezar a hacer esta práctica Bueno sería este mismo conjunto pero sin la columna hender y sin la columna citier y qué pondría allí pondría justamente el Data frame que Acabo de crear donde hice el one Hot encoder bien así llevamos con lo cual lo primero que tengo que hacer es Drop eliminar las dos columnas de datos Y luego Con este código a conocer poner axis 1 que indica que lo que quiero eliminar son columnas más no filas bien lo ejecutamos y hacemos un columna si habiendo hecho un datos con un valor en tu list para poder ver la cantidad de columnas que me quedaron y cerciorarme que justamente la cantidad de columnas que me quedaron son las que quiero es decir la que veo todas menos la que nos quería tener que hender y sitiar bien luego vamos a crear un nuevo Data frame que lo vamos a poner datos 1 donde vamos a agregar a este conjunto datos que ya no tiene estas dos columnas todas las otras columnas de este Data frame producto del One Hot encoder bien Entonces qué hacemos datos 1 es igual a datos punto join ligar juntar que voy a juntarle a datos transformar DF que era transformar de F era este Data frame que tenía el resultado del coda entonces lo que hago ejecutarlo y nuevamente Crea una variable column names donde lo que hago en este caso similar a lo que hice recién datos 1.colums.vales to list Recuerden que tu list tiene que ver con que esa información me la pase una lista para poder visualizarla y manejarla como corresponde si no es una lista no lo voy a poder hacer porque es un objeto otro tipo y no me serviría bien y finalmente justamente con columnas visualizamos al ejecutarlo como lo hacemos ahora que la cantidad de columnas que tengo en este nuevo Data frame datos es la que quiero las anteriores de datos y las nuevas que he creado luego del One Hot encoder Qué pasa el nombre de estas columnas no es muy amigable y digamos muy largo también para tenerlo presente para referenciarlo y escribirlo con lo cual lo que voy a hacer es justamente datos 1.coms sub 7 sub 8 sub 9 sub 10 y sub 11 que son estas cinco columnas igual a un nombre que en este caso le puse eso podría poner cualquiera que me de alguna manera me represente una forma más amigable de poder este nombrar una columna femenino masculino si te quiero uno dos y tres y luego a un Head para ver constatar de que justamente he logrado el Data set que yo quería con las columnas pasadas según quoten cover de categóricas a numéricas Y a partir de ahora poder empezar a trabajar mi algoritmo de regresión lineal bien lo primero que vamos a hacer para este algoritmo es dividir la x y la y aquí justamente como esto es una clase de repaso Por decirlo de alguna manera o integradora de conocimiento ya vistos dejo esta pregunta por qué necesitamos dividir las variables de entrada y de salida en X e Y bien la respuesta es porque necesito tener separadas las variables de entrada y las variables de salida u objetivo para después pasar a dividir los datos en conjunto de entrenamiento y test en este caso no voy a tomar todas las variables de entrada para formar la x Solamente voy a tomar algunas el sueldo la hora de transacción el género ahora que tengo separado femenino y masculino y los tres las tres columnas que quiere recién de citiar y la variable de salida Obviamente el total gastado bien hacemos este paso creando la x y la y luego hacemos lo que dijimos recién la separación de los datos en conjunto de entrenamiento y una vez que tengo eso pasamos a crear el modelo como ya lo sabemos hacer y vamos a verificar la precisión que como dice aquí el título es pésima porque realmente es extremadamente baja con lo cual lo que vamos a hacer para intentar mejorarla es agregar una característica más Al conjunto de entrada la característica que vamos a agregar es como ese título aquí la característica récord que es el columns número 5 fíjense que aquí yo tenía 4 y 7 y ahora agregó la columna 5 con esto voy a recrear la variable x no lo voy a hacer con la variable y porque en ese caso no cambia nada sigue siendo el mismo la misma variable Target y lo que voy a hacer va a hacer volver a separar el conjunto de tres entrenamiento volver a crear el modelo volver a entrenarlo y medir su score lo hago y veo que bueno el score crece considerablemente 91 91 cuando tenía 0 19 y 017 pero voy a intentar seguir en la misma idea el mismo procedimiento agregando una nueva característica y lo voy a hacer con la característica age que es la columna es número 1 fíjense que aquí tenía 3 4 5 ahora tengo 1 3 4 5 Sí y hago Exactamente lo mismo solamente que vuelvo a recrear la variable x los otros pasos son los mismos que hice recién intentando ver qué pasa ahora con la precisión y veo que la precisión en realidad ha mejorado sobre todo en el conjunto de Test lo cual no es malo ahora luego de esto les acerco a la siguiente pregunta qué conclusiones podemos sacar respecto de toda esta estrategia que hicimos de agregar una característica más para mejorar como vimos mejoramos mucho el score y agregar una nueva característica a la ya agregada en el caso anterior para obtener una mejora en score pero no tan significativa la conclusión sería que si bien hay una mejora que la vemos fundamentalmente en el conjunto de Test la diferencia no es tan grande la mejora no es tan grande y quizás el hecho de agregar una nueva característica implicaría un gasto computacional muy grande que no ameritaría justamente ese esfuerzo para poder lograr ese pequeña o esa pequeña diferencia de score la segunda conclusión que podemos sacar Es que así como el hecho de agregar nuevas características es una forma de mejorar el modelo también puede serlo quitar características Nosotros hemos visto en clases anteriores en las prácticas que hemos hecho que podemos determinar la importancia de las características a la hora de definir un modelo con lo cual los invito a que en este caso pensemos si quizás alguna de las características que están podemos quitarlas en Pos de mejorar el modelo justamente midiendo la importancia de los predictores Es decir de la variable de entrada con esto Terminamos el ejercicio a partir del cual abordamos qué tema el tema de la transformación de las variables categóricas en numéricas lo que vamos a hacer a continuación es otro ejercicio no con un algoritmo de reelección lineal sino con un algoritmo de revisión logística buscando que lo que dice aquí en el título ver una salida en formato probabilístico ya vamos a entender a qué nos referimos Recuerden que el algoritmo de revisión logística lo que hacía era darme un valor o en un formato binario o un formato de clases es decir blanco negro 01 alguien que tiene una enfermedad o no la tiene y lo otro que tenía que ver con determinadas características que podían calificarse en tres o más variantes pero siempre en una cantidad finita no una cantidad infinita entonces lo que vamos a ver ahora es cómo analizar esa salida de una manera probabilística y no de una manera concreta es decir no por su valor concreto sino por la probabilidad de que ese valor sea una de las clases que está definida en las opciones de salida bien para eso vamos a adelantar mucho el tema pero vamos a empezar por ver como siempre las librerías que vamos a usar para este caso para este ejemplo que son las que están aquí donde vamos a volver al escalamiento de datos Y luego las que ya usamos en cualquier ejercicio vinculado a un problema de radiación logística Así que las incorporamos rápidamente y vamos a utilizar para este caso un Data set que ya usamos que es el de Iris con lo cual lo cargamos collamos este lo hemos hecho anteriormente bien ya está así cargado creamos un Data frame de nombre Iris y acá lo tenemos ya este lo conocemos y el visualizar este Data frame de alguna manera no recuerda tenemos el shape luego la separación de la x y la y que recién preguntamos porque lo hacíamos bien volvemos a hacerlo aquí obviamente donde ponemos en el caso de la x todas las variables de entrada y en el caso de la variable Target que en este caso es la especie de la flor bien y ahora vamos a escalar los datos Perdón no hice la separación de conjunto de tres entrenamientos Ahora sí bien decía vamos a escalar los datos Y aquí nuevamente aprovechamos este repaso para poner una pregunta por qué escalamos los datos Y esto es importante hacerlo bien respuestas en principio Por qué escalamos los datos los datos los escaladamos en principio porque lo que tratamos que debitar digamos con el escaneamiento de datos es que un valor de entrada una característica de entrada pueda ser más importante y más significativa solamente por el tamaño de su cifra no por su importancia a la hora de determinar en este caso si la especie setosa u otra sino porque tiene un número muy grande desde la expresión desde la cifra respecto de otros aquí obviamente el largo del sépalo y el ancho del sépalo tienen una una cifra más grande que en su mayoría de los casos de el largo del Pétalo y el ancho del pétalo la diferencia en este caso no parece ser tan significativa digamos que este problema se agudizaría si por ejemplo una columna de estas tuviese Bueno cuando todos tienen una expresión con una sola cifra que tuviesen expresiones con tres cuatro ciclos y o que tuviese un valor decimal muy pequeño Entonces en este caso Este no es tan grande pero igual vamos a escalar los datos Y es importante en este caso bueno justamente la respuesta está en lo que dijimos en este caso concreto no sería tan importante escalarlo vamos a hacer Igual para repasar pero sí es importante que entiendan de que esto es mucho más importante en la medida que justamente la diferencia del tamaño de la cifra sea muy grande entre una característica de entrada y otra así que bueno escalamos los datos de la forma que ya lo habíamos hecho Recuerden que en el caso de x-30 hago fit y en el caso este no hago fit porque porque justamente x-train son los datos que yo voy a usar para entrenar por eso es transform en el caso de Test simplemente es para comparar para evaluar con lo cual lo voy a hacer sin bien creo el modelo de regresión logística como ya sabemos y medimos la precisión bueno vemos que la precisión es de 0.96 Y 093 sí esto recuerden siempre que puede variar porque porque si yo vuelvo a hacer esto que es justamente generar el separar este los datos en módulo de entrenamiento y en conjunto de entrenamiento conjunto de Test puede hacer que este valor cambie De hecho si en este caso particular lo volviese hacer probablemente pasar a eso tenemos 0 96 0 93 lo ejecutamos nuevamente ven que cambia Sí bueno obviamente esto es parte del entrenamiento por eso es importante que aquí no lo he puesto el Random State las semillas para que me fije esa separación de conjunto de datos y entrenamiento como algo que fue la manera de distribuir que me dio el score más alto bien Ahora vamos a hacer las predicciones para el conjunto de Test esto ya lo habíamos hecho es decir con el modelo predigo todo el contenido del conjunto de Test y pongo las predicciones en resultados son los siguientes Es decir para el primer caso determinó que la predicción según el primer la primera observación del conjunto de Test debe ser versicolor la segunda virginica la segunda la tercera setosa etcétera etcétera sí Ahora qué pasa esto es un dato concreto es un resultado concreto Pero la realidad el algoritmo de la región logística no maneja las cosas de ese modo si nosotros recurrimos a este slide que fue parte de lo que nosotros vimos en la introducción teórica del algoritmo de reacción logística habíamos hablado de que en realidad esto que atienda un problema de ceros y unos como dos posibles salidas de un problema de reacción logística no son todos los valores o cero o uno sino que están ubicados en una función que se llama de tipo sigmoide y sobre esa curva hay valores que están más cerca del uno como hay valores que están más cerca de cero con lo cual estos cuatro valores azules no son 0 concretamente cero ni tampoco estos tres valores verdes son completamente uno sino que son probabilísticamente cero o probabilísticamente uno porque como vemos en este gráfico este cuarto valor azul que corresponde un 0 es el valor que tiene la menor probabilidad de ser un cero pero aún así su probabilidad es mayor de que sea un cero de que sea un 1 lo mismo con este valor verde es el que tiene la menor probabilidad de ser uno pero aún así Esa menor probabilidad lo acerca más al uno que al ser con lo cual por la probabilidad de ser es que el valor se lo asocia a una salida o a otra al 1 o al 0 Entonces eso es lo que nosotros vamos a tratar de ver para este caso de este conjunto de Test que hemos visto recién que tiene definiciones concretas pero están asociadas a definiciones probabilísticas cómo lo logro el lugar de usar model predict o sea el nombre del modelo login model predict voy a usar el método predict guión bajo proa ven que es muy similar la expresión solamente que en lugar de usar predique uso predic pro con lo cual lo voy a ejecutar y fíjese lo que me arroja me arroja una probabilidad para cada observación para cada una de las variantes es decir estos tres elementos es la probabilidad que tiene la primera observación de ser o una cosa o la otra o la otra o una cetosa o una versicolor o una virginica en este caso fíjense que el primer caso fue una versicolor Y por qué determinó que era una versicolor porque justamente el segundo elemento 8.8 elevado a la menos 10 es el valor que tiene la mayor probabilidad respecto de los otros dos es decir este elemento de aquí y este tienen una probabilidad menor de ser un acetosa una virginica que esta probabilidad de ser un versículo por eso dice esta primera observación de las tres probabilidades Cuál es la más alta la que corresponde versículo Entonces el resultado es que la salida es una versicolor en el caso de la virginica lo mismo la virginica es la tercera fíjense la segunda observación en este caso el valor más alto de probabilidad lo tiene la tercera columna es decir la de química entonces determina que por esa alta probabilidad el resultado final es virginica de esta manera funciona del modo probabilístico el resultado que determina de manera concreta como aquí con predique un algoritmo de reacción logística esto es muy importante que lo entendamos Por qué Porque a futuro vamos a empezar a trabajar con esto cuando nos incorporemos a las redes neuronales para terminar este ejemplo si esta práctica vamos a les dejo aquí un código para ver este resultado lo que hablamos recién de una manera un poquito más clara más transparente y justamente lo que hago es un ciclo for que cicle Tantas veces como elementos tenga el conjunto de Test sí es decir aquí veo que el conjunto de Test recordamos que tenía 30 elementos era 150 Las observaciones que tenía el Data set y así tomaba el 20% eran 30 que son las 30 que están aquí y son las 30 que están aquí de manera probabilística aquí de manera concreta Aquí bien Entonces qué voy a hacer con cada ciclo de Ford lo que voy a hacer en principio es imprimir por 100 y redondear cada uno de estos valores Sí para que en lugar de verse expresados así se vean expresado con un número más fácil de visualizar entonces este este y este en el primer ciclo lo voy a multiplicar para ver para verlo de esta manera lo mismo con la segunda observación lo mismo con la tercera con cada una de las 30 luego de hacer esto con np round y multiplicado por 100 lo que voy a hacer es imprimir un texto donde diga que la predicción determinada como en este caso su y que es ipred Recuerden que era esto de aquí y luego en la probabilidad lo que me va a mostrar a mí es la probabilidad de que sea lo primero la probabilidad de que sea lo segundo la probabilidad de que sea la tercero Qué es en este caso es virginica virginica era la tercera Bueno es lo que veo aquí 005 374 86 22 los tres suman un 100% de probabilidad Cuál es la probabilidad más alta la tercera Entonces el resultado en la tercera en versicolor lo mismo 9 09 84 26 y 655 cuál es la probabilidad más alta la de la segunda alternativa que es color aquí versicolor virginica de nuevo en la última verse color la segunda versículo aquí tenemos 12 tosas que la primera Bueno ahí se ve mucho más claramente esta cuestión de cómo determina en base a la probabilidad más alta Cuál es la clase de elegir bien hasta aquí llegamos a esta primera parte de esta clase vamos a vernos ahora en la segunda parte de la clase número 10 hasta aquí llegamos con esta primera parte te invito a seguir practicando e incorporando conceptos nuevos en la segunda parte Titulo: Clase10 (parte 2) del Curso de Inteligencia Artificial \\n URL https://youtu.be/urDhXUtxGWM  \\n 1390 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 10 del curso de Inteligencia artificial de ifes te invito a que sigamos repasando y aprendiendo conceptos nuevos en esta segunda parte empezamos   Hola nuevamente vamos a empezar la segunda parte de esta clase número 10 y como dijimos al principio el tema de esta segunda parte es el título que está aquí la reducción de la dimensionalidad de los datos para ello vamos a recurrir a una parte de la teoría de la clase donde abordamos el tema de regresión lineal concretamente cuando hablábamos de la reelección lineal múltiple y este gráfico que está aquí teníamos Bueno un caso donde teníamos tres variables de entrada la inversión en ventas y en tv y en radio La mención publicitaria y lo que aquí hacemos relacionábamos los tres puntos o sea las tres inversiones En Radio TV y ventas y cada una de estas relaciones nos daban un punto lo que intentamos Buscar En aquel momento fue un plano que pasará lo más cerca posible de cada uno de esos puntos para poder hablar o reconocer un modelo bien aquí tenemos algo parecido con este caso que tenemos aquí tenemos tres variables de entrada que hemos graficado en un gráfico de tridimensional Y estos son los puntos que representan las uniones de cada una de esas observaciones y los tres puntos de cada una de ellas encuentro un plano sí que está aquí y este plano que está aquí me permite pensar en una posibilidad que me acerca al objetivo de reducir la dimensionalidad De qué trata esto fíjese que este plano que está aquí justamente en esta suerte de puntos que están muy cerca pero no están tan cerca como poder para poder ser atravesados de pleno con un plano me permite pensar en un plano que por lo menos toca muchos puntos y pasa muy cerca de otros con lo cual Esto me lleva a mí a la posibilidad de transformar esta figura de aquí en esta figura que está acá a la derecha porque supóngase que lo que yo hice aquí es este plano Tomar todos los puntos que están sobre el plano y dejarlos tal cual están y los planos que están por encima del plano o por debajo del plano aplastarlo o sea como si los quisiera agarrar con la mano y llevarlos hacia el plano con lo cual habiendo hecho esto paso de esta forma que está aquí a esta forma que está aquí obviamente esto no representa la realidad al 100% porque Porque evidentemente estos puntos que no están sobre el plano tiene una distancia respecto del plan pero haciendo transformación ya tengo una pérdida de información porque insisto la información no es la real porque está en un lugar donde originalmente no estaba pero está muy cerca del lugar donde originalmente estaba entonces Pongo aquí en la balanza la pérdida de la información versus prescindir de una dimensión es decir pasar de trabajar en lugar de con tres dimensiones con dos dimensiones en que impacta esto que Obviamente el algoritmo puede trabajar de manera más rápida en un paralelismo lo que vimos antes en la primer parte de esta clase donde hablábamos de quitar una una variable de entrada para que el algoritmo fuera más eficiente o más rápido en este caso no estamos quitando lo que estamos haciendo es transformando sigo teniendo tres dimensiones se sigo teniendo tres variables de entrada pero al transformarlas pasadas de un esquema de un plano en un eje de tres dimensiones a esto que estamos viendo aquí a la derecha que es la distribución de puntos en un eje de dos dimensiones y quitado una dimensión con lo cual he reducido la dimensionalidad sin perder o tratando de que la pérdida de información sea la menor posible este caso no solamente puede darse una situación como la que vimos recién sino que los puntos pueden tener otra forma muy curiosa pero es posible que tengan esta forma y es normal que puedan tenerla que es una forma de rollo Ven aquí están los puntos distribuidos en una forma de rollo y aquí lo que intentamos reflejar con los colores es que cada uno de estos puntos que tiene un color diferente está identificado con una clase o con una categoría diferente sí se acuerdan lo que hablamos respecto de la radiación logística en cuanto Por ejemplo si era un tipo de planta un tipo de flor suponga ser que al amarillo son todos los puntos que representan el color los naranjas setosa y los rojos Virginia bueno acá hay más casos que eso pero para tomar un ejemplo de referencia lo que hicimos antes qué pasaría aquí o qué resultados se produciría si intentáramos aplicar la misma lógica que hicimos aquí arriba Bueno si nosotros aplastamos estos puntos que están en este rollo para llevarlos a un plano esto que está en el gráfico del centro o sea obtendría una reducción de la dimensionalidad Pero qué pasa los puntos estarían muy mezclados con lo cual si yo a partir de ahora lo que quiero hacer es poder identificar en estamentos bien claros diferenciar los puntos de una categoría respecto de los otros recuérdense que la categoría la estamos representando con los colores evidentemente gráfico me muestra que eso es imposible porque todos los colores están mezclados y Establecer un algoritmo que diferencia una cosa de la otra va a ser muy complicado para este caso existe otra solución que cuál es es la de desarrollar el rollo Aunque parezca cómico a la expresión es decir en lugar de aplastarlo lo que hago es estirar toda esa sábana de puntos Y lograr una imagen como la que está aquí es decir fíjense que con esta otra estrategia lo que hice fue reducir una dimensión pero que los puntos estén bien separados de modo de que además de reducir una dimensión me sea fácil encontrar un modelo que pueda diferenciar un color una categoría de otra hasta ahora hemos visto casos donde pasé de tres dimensiones a dos dimensiones Eso es todo en realidad la reducción de la dimensionalidad también se puede aplicar para pasar de dos dimensiones a una dimensión vamos a ver un caso tenemos aquí un conjunto de puntos sobre un gráfico de dos ejes recién Cuando tenemos un problema de pasar de dos dimensiones a tres dimensiones dijimos que queríamos Buscar un plano Qué pasará lo más cerca posible del conjunto de puntos ahora que queremos pasar de dos dimensiones a una dimensión El problema va a ser encontrar una recta sobre la cual se recuesta en la mayoría de los puntos Yo propongo aquí en este gráfico tres tipos diferentes de rectas en un caso una recta de tan sólido en otro caso una recta de trazo de puntos y en nuestro caso una recta de trazo de guiones voy a graficar cada una de estas tres casos en estos tres elásticos que están aquí a la derecha en el caso de la línea sólida si yo aplastase todos los puntos sobre esa recta vería un resultado como este si por el contrario hiciera lo propio con la línea de trazos obtendría un resultado como este y si hiciese lo propio sobre esta línea de puntos donde los puntos va a ganar redundancia están muy lejos y yo los aplastase obtendría este resultado que está aquí debajo la pregunta ahora es cuál de las tres rectas Será la mejor para la reducción de la dimensionalidad de dos dimensiones a una dimensión Obviamente el primer caso y por qué Porque es el que conserva el mayor nivel de varianza Qué quiere decir esto el que controla la mayor cantidad de variedades de puntos que se acercan a la realidad nosotros tenemos que tener siempre en cuenta que lo que queremos lograr aquí es reducir la dimensionalidad en este caso de dos dimensiones a una perdiendo la menor cantidad de información real posible fíjense vamos al caso si vemos el caso menos conveniente si quieren el peor el que corresponde a la línea de puntos vamos a suponer que yo quisiera aplastar todos los puntos sobre justamente esta recta de trazo de puntos me pararía aquí para poder mirar los puntos en forma totalmente perpendicular Y entonces Podría tener esta imagen reflejada mirada desde arriba desde aquí arriba para poder aplastar los puntos contra esta recta de puntos pero yo tengo por ejemplo un punto que está aquí al medio que puede ser este puede ser este o puede ser este fíjense el nivel de diferencia que existe entre este punto este punto y este punto mirado desde arriba cuando yo lo voy a aplastar sobre esta línea son lo mismo pero en la realidad Está muy dispersos con lo cual Aquí tengo una enorme pérdida de información Porque estos puntos que mirados de aquí son lo mismo en la realidad están muy dispersos vamos al caso más conveniente que elegimos como más conveniente justamente fíjense que yo si quiero recostar todos los puntos sobre esta línea sólida Me podría parar aquí arriba o aquí arriba y Si miro ahí voy a ver puntos que aquí parecen la misma cosa y que no lo son como por ejemplo si quiero ver un punto por aquí puede ser uno de estos este o este pero fíjense que la diferencia entre este punto y este por tomar los extremos más alejados no tienen la misma diferencia que en el caso anterior que tomamos como ejemplo este y este con lo cual la pérdida de información no es tan grande Por ende en nivel de varianza se conserva y Por ende Este es el mejor modelo para este caso de transformación de un escenario y dos dimensiones en una dimensión bien habiendo pasado toda esta introducción teórica vamos a ir a la práctica concretamente de poder probar este efecto de la reducción de la dimensionalidad vamos a hacerlo con un caso de regresión logística y para ello vamos a utilizar un Data set que no hemos utilizado antes que está dentro de las opciones de Data set que me ofrece la librería esto ya lo hemos utilizado justamente en la clase de regresión logística donde si ustedes recordarán justamente de cycle Data set sacamos un Data set que hablaba sobre casos de cáncer de seno y una vez que habíamos incorporado ese Data set veíamos la esquilas claves en las cuales venía ese dataset preparado digamos de antemano justamente a través de la librería de Shakira en este caso lo mismo yo lo que voy a utilizar es justamente a partir de fetch Open nml un Data set que se llama guión 784 este Data set lo vamos a incorporar luego de vamos a ejecutar las librerías las otras ya las conocen región logística warnings y vamos a proceder a abrir este conjunto de datos va a tardar un buen tiempo ya vamos a ver por qué Porque es un conjunto de datos con muchas observaciones concretamente con 70.000 observaciones bastante superior a todo lo que hemos venido trabajando hasta ahora bueno como ven ha tomado un minuto este y aquí tenemos ya el dataser incorporado el mismo se llama menist y en realidad es considerado el Hello World de la visión artificial esto para quien no está en el mundo la informática Hello World es la típica expresión con la que se busca desarrollar las primeras líneas de código cuando uno aprende cualquier tipo de lenguaje en este caso la visión artificial es una de las cosas que vamos a ver cuando hagamos el módulo de Deep learning y largamente vamos a ver porque los módulos más importantes desarrollados Dentro de este curso este conjunto es un conjunto de 70.000 observaciones como ya les había adelantado que tiene ya preparado un conjunto de entrenamiento de 60.000 y un conjunto de pruebas de 10.000 En qué consiste este conjunto de datos en números manuscritos de 0 a 9 como se logró Esto bueno convocando a personas estudiantes de High schools y de empleados de la oficina de descenso del Estados Unidos para que dibujaran números manualmente y luego los escanearon lo que hicieron fue transformar cada uno de estos este números en una digitalización de 28 dígitos por 28 dígitos por eso justamente se llama 784 por el resultado de la multiplicación de 28 por 28 bien luego que hicieron esto también lo interesante es que convocaron a otras personas totalmente diferentes para generar el Data set de prueba es decir que a diferencia de lo que hemos hecho hasta ahora donde nosotros somos quienes separamos el conjunto de entrenamiento el conjunto de Test pero todo proviene de la misma fuente en este caso como ya viene preparado el conjunto de entrenamiento con 60 mil observaciones fue desarrollado en base a la escritura de un grupo de personas y los restantes Diez mil observaciones fueron incorporadas a partir del registro manual de otras personas diferentes lo cual hace mucho más rico justamente el valor del conjunto de Test porque representa lo que podría ser la realidad nuestra o sea nosotros entrenamos y creamos un modelo con los datos que ya tenemos y luego hay que probarlo con los datos futuros que van a aparecer bueno acá el papel de los datos futuros lo desarrollan estas personas distintas que generaron el conjunto de Test respecto de las que generaron el conjunto de entrenamiento dejando las explicaciones de lado Entonces pasamos por separar los conjuntos y ya no usamos el 30 split porque como dijimos antes ya vienen separados los conjuntos de entrenamiento y test 60.000 por y disminuirse por otro con lo cual lo que pongo aquí es Ni más ni menos que eso generó X3 con 60.000 y perdón ítems y XT con las restantes 10.000 observaciones bien lo que estamos creando o lo que buscamos crear aquí es como dice el título un modelo de relación logística para salida no binaria por qué no binaria porque no son dos opciones de salida Cuántas son 10 porque son números que van de cero al nueve Sí por eso es fundamental como hicimos recordarán y volvemos aquí a la clase de regresión logística cuando usábamos salidas múltiples Aquí está el múltiples multinomial Bueno aquí tengo que volver a hacer lo mismo y como recordarán también de aquella clase utilizábamos un solver para mejorar la u optimizar Esa es la palabra correcta el algoritmo que estábamos desarrollando el modelo que estábamos desarrollando aquí lo mismo y vamos a usar el solar más común que es lbs bien qué más incorporamos aquí porque luego lo único que tenemos que hacer es hacer el feed con el grupo de entrenamiento de X ahí que es lo que venimos haciendo habitualmente vamos a incorporar justamente en la librería aquí incorporamos Time porque gracias a ello lo que vamos a intentar hacer es fijar la hora que tiene la computadora cargada Al momento de empezar a entrenar el algoritmo y la hora que tiene la computadora cargada Al momento de terminar ese proceso para qué Para poder medir el tiempo que duró la generación del modelo Bueno vamos a hacer eso y observamos que el tiempo fue de 43 segundos con 50 esta información la tengo aquí también por supuesto estas otras formas de poder verla poder capturarla y ponerla en una variable o el destino que yo quiera darle bien Ahora medimos la precisión para el conjunto de Test y entrenamientos y veo que arroja estos valores 093 y 092 55 son muy similares uno como otro y son buenos valores ahora lo que vamos a ver es una forma adicional de medir el score Por qué Porque hasta ahora lo que hacíamos era medir el score con el conjunto entero de entrenamiento xtrain e-train o con el conjunto entero de 3x3 también podemos hacerlo justamente con Akira score de sitetrix en donde lo que hacemos es cruzar el itest es decir la salida del conjunto de Test con el hiper es decir con las predicciones que es la comparación más concreta que hemos hecho en muchos de los gráficos que hemos desarrollado hasta ahora entonces para eso lo que voy a hacer es primero generar las predicciones y luego comparar hites e ipred lo ejecuto y tengo un score igual fíjense qué casualidad al del conjunto de esto no siempre es así pero en este caso ha sido escéntrico bien Ahora vamos a pasar a el tema justamente de esta segunda parte implementar la reducción de la dimensionalidad para eso vamos a incorporar pca pca es justamente una de las formas más conocidas y más implementadas de poder reducir la dimensionalidad con lo cual lo primero que tenemos que hacer es crear una instancia de pca pero incorporando este hiper parámetro es fundamental este hiper parámetro porque lo que le estoy diciendo aquí es que quiero que busque una reducción de la dimensionalidad pero con una conservación de la información de El 095 es decir que quiero apenas un 5% y no más que eso de pérdida de información si supieron ese score o ese nivel de pérdida no me interesa una reducción de la dimensionalidad es decir buscaría otra o si no existe no lo aplicaría se entiende porque por lo general Este es el parámetro de referencia típico que se toma para estos casos sí Buscar una reducción con una pérdida mayor no tendría sentido porque lo que gano por un lado lo pierdo por el otro bien luego lo que generó es la transformación del conjunto entrenamiento del conjunto de Test Recuerden que esto es parecido a lo que hacíamos con el escaneamiento de datos hacíamos un escalet fit transform y un escale transform en este caso es pese a fit transform y pese a transform en el caso de x-trait transform en el caso del transporte y tenemos dos variables que le hemos puesto bien con esas dos variables que son versiones reducidas de X3 voy a crear mi modelo como lo creo con multinomial como trabajamos antes con el solver como trabajamos antes y le cambiamos aquí el perdón no le cambiamos el Random deberíamos hacerlo porque si no nos vamos a confundir con el otro sería 43 y el anterior es 42 mido el tiempo antes del entrenamiento mido el tiempo después del entrenamiento y le pasó los valores en este caso no X3 sino XT reducida e itrain bien ejecutamos la transformación toma un tiempo también y ahora creamos el modelo y medimos el tiempo que ya lo vi un poquito por aquí arriba 977 mido la precisión y la Score y fíjense la precisión que tengo en este caso 092 1 y 0 92 01 comparada con el caso anterior es bastante similar no guardan una gran diferencia Cuál es la gran diferencia que tengo aquí Cuál es el tiempo que tardé para generar el primer modelo 43 segundos Cuál es el tiempo que empleé para generar el segundo modelo 9 segundos casi 10 segundos es decir Prácticamente la cuarta parte de lo que en plena otro modelo esto en realidad con tiempos como los que tenemos aquí que se cuentan en segundos no es obviamente significativo la diferencia tampoco significativa con lo cual aquí podría aplicar cualquiera de los dos modelos con reducción o sin reducción Lo importante es que toman este caso sencillo que lo podemos ver rápidamente para que ustedes lo lleven a casos mucho más complejos como los que se nos van a presentar a futuro y vamos a estar hablando de diferencias de horas y quizás porque pensar en más de horas sino en días porque realmente entrenar modelos como los que vamos a ver en Deep learning lleva muchísimo tiempo obviamente de mucho dependiendo del tipo de recursos informáticos que tengamos finalmente les dejo una apéndice con un ejercicio sencillo para que ustedes prueben en su casa donde la idea es probar varios algoritmos para un mismo caso para un mismo conjunto de datos Y luego verificar los scores Aquí tengo estos tres algoritmos y el conjunto de datos y lo que hago es crear un algoritmo otro algoritmo al otro algoritmo y luego medir justamente los score en Los conjuntos de entrenamiento en el conjunto ítems Y compararlos esto es parte de lo que ustedes van a tener que hacer muchas veces esto obviamente es una versión muy sencillo pero la idea de fondo es la que ustedes van a tener que aplicar muchas veces donde ante un mismo problema van a tener que utilizar diferentes algoritmos Probar con diferentes algoritmos y luego dentro de cada uno de esos diferentes algoritmos cambiar su ciper parámetros buscando siempre mejores con también ver todo lo que es previo a el entrenamiento del modelo dependiendo independientemente perdón de El algoritmo que se trate Qué quiere decir esto bueno empezar a escalar datos empezará a transformar datos con One hotcoder como hicimos hoy con el tema de pasar variables categóricas a numéricas y también con lo que vimos hoy como tema importante esta clase la reducción de la dimensionalidad un conjunto de herramientas todo en busca de el mejor score y el mejor modelo nos vemos la clase que viene hasta aquí llegamos con esta clase gracias a ellas podido consolidar y ampliar los conceptos adquiridos anteriormente Te espero en la próxima clase para aprender un nuevo algoritmo nos vemos  Titulo: Clase 11 (parte1) del Curso de Inteligencia Artificial \\n URL https://youtu.be/3Ssh1SsiW6s  \\n 1183 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 11 del curso de Inteligencia artificial de ifes en ella Vamos a aprender un nuevo algoritmo de Machine learning máquinas de soporte vectorial empezamos  Hola a todos Bienvenidos a la clase número 11 del curso de Inteligencia artificial de ifes seguimos en el módulo de Machine learning y seguimos con el propósito justamente de conocer hoy un nuevo algoritmo de Machine learning que se llama máquina de soporte vectorial que es un algoritmo de máquinas de soporte vectorial es un algoritmo de aprendizaje automático de tipo supervisado recordarán en la primer clase cuando introdujimos justamente el concepto de algoritmo de Machine learning y dimos un listado de todos los de tipo supervisado Este era uno de los últimos justamente este algoritmo se puede usar tanto para problemas de clasificación como de regresión Aunque claramente tiene su potencial mayor en los casos de clasificación en este algoritmo si se trazan los puntos en un ámbito unidimensional y luego se realiza la clasificación de segregación entre ambos puntos con un hiperplano que es el que mejor separa ambas clases para entender este concepto vamos a utilizar no n dimensiones sino dos dimensiones x y cada uno de estos puntos que están en ese espacio unidimensional en este caso va a ser en un ámbito de dos dimensiones tenemos Aquí estos puntos rojos que están por un lado y estos puntos verdes que están por el otro bien justamente ese hiperplano que mejor separa las dos clases en este caso dado que tenemos un ámbito de dos coordenadas va a ser una recta y la recta justamente es lo que tienen gráfica Ahora aquí esta recta es la mejor recta mejor hiperplano en un ámbito de dimensiones que separa este conjunto de este conjunto Pero qué son los vectores de soporte y Qué es el hiperplano bueno justamente el hiperplano hablamos recién en este caso en la recta puede ser trasladado si tenemos un ámbito de una dimensión mayor a un plano o un hiperplano en el caso que sean más de tres dimensiones y los vectores de soporte son estos puntos que fíjense que están referenciados aquí es decir este rojo y estos dos verdes Por qué son vectores de soporte porque son los puntos de cada una de las clases que están más cerca del hiperplano plano o recta depende la dimensión que mejor separa ambas clases vamos a ir analizando cómo funciona este nuevo algoritmo de máquina de soporte vectorial ya nos quedó claro que lo que buscamos es un hiperplano que separe de la mejor manera posible grupos distintos de puntos ahora Cómo podemos identificar cuál es el plano correcto porque justamente pueden haber distintas formas de distribución de puntos como justamente pueden haber muchas situaciones es que nosotros vamos a tomar para esta clase cinco casos diferentes de distribución de puntos cinco problemas diferentes para ver justamente en cada uno de ellos Cuál sería la solución óptima de cómo es la forma de encontrar el hiperplano correcto en este primer caso tenemos dos grupos de puntos por un lado de las estrellas celestes y por el otro lado los puntos rojos vamos a suponer tres propuestas diferentes de rectas ya que este caso el hiperplano corresponde a un espacio bidimensional que podrían ser las ideales para separar esos dos conjuntos de puntos Tenemos aquí la rectada la recta b y la recta c Cuál sería la mejor recta que separa estos dos conjuntos de puntos se debe recordar una regla general antes de dar esta respuesta que es seleccionar el hiperplano que mejor separa en este caso las dos clases en esta situación el mejor hiperplano que cumple con esa Consigna es el hiperplano B ahora vamos a ver un caso más en el cual vuelvo a tener las estrellas por un lado y los puntos rojos por el otro Y supongo que una recta dispuesta en diagonal puede ser la mejor forma de separar esto estos dos grupos de puntos Bueno pero me propongo dibujar esa recta Y supongo que puedo tener tres rectas que pueden cumplir con esa Consigna por eso Aquí tengo la recta a la recta b y la recta c que cumplen con esa Consigna pero no del mismo modo porque evidentemente no están dispuestos en el mismo lugar Cuál será la mejor recta que cumpla justamente con la Consigna de ser el mejor hiperplano que separan ambas clases la que cumple esa Consigna del mejor modo es la recta c porque porque si bien tanto la B como la Seo como la separan ambos grupos Es evidente que la B está mucho más cerca de las Estrellas y Por ende podría haber un margen de error mucho más chico la a lo mismo pero en el sentido contrario hacia los puntos rojos y la c tiene una distancia muy buena hacia un grupo y hacia el otro con lo cual el margen de error Sería mucho menor y Por ende c es la mejor recta el mejor hiper plano para separar estas dos clases luego tenemos otro caso más caso 3 vuelvo a tener los mismos grupos pero fíjense esta Estrella Azul Aquí está dispuesta de un modo bastante alejado del resto del grupo tengo como propuesta la recta a y la recta B Cuál será la mejor uno puede presumir en primera instancia que la recta de es la que mantiene la mejor distancia con el grupo de las Estrellas y con el grupo de los puntos teniendo en cuenta que tengo una estrella que estaría mal clasificada la recta a por el contrario no tiene el mismo nivel de margen respecto de las dos clases pero clasifica bien porque en este caso la estrella a pesar de estar alejada del resto Está bien clasificada como estrella y no como punto rojo en este caso de nuevo surge la pregunta Cuál será la mejor de las dos opciones la mejor es la opción a porque porque en esta disyuntiva que puede presentarse qué voy a priorizar la distancia respecto de las dos clases o la correcta clasificación lo que ganas esto último yo primero tengo que cerciorarme de que el hiperplano sea un buen clasificador y después obviamente lo ideal sería que tuviese una buena distancia respecto de las dos clases ahora tenemos el caso 4 que es una situación más o menos similar a la de recién Por qué Porque si bien la estrella está separada de la clase del resto de las estrellas está tan alejada que parece que formará concretamente parte del grupo de los puntos rojos en este caso Cuál será la mejor opción la mejor opción es esta recta que está aquí pero ustedes me van a decir Bueno pero no clasifica bien esta estrella bueno esta estrella es considerada un valor atípico y justamente este algoritmo de máquina de soporte vectorial tiene una forma de ignorar justamente los valores atípicos para poder clasificar bien y Por ende esta recta para este caso es la mejor solución y finalmente tenemos el caso 5 que es el más complejo fíjense que en este caso tengo las estrellas como si tuviesen una coordinación o formaran un grupo pero en un sentido circular y en el mismo sentido del grupo están de una manera agrupada en sentido más circular pero más juntas todo el grupo de los puntos rojos lo que yo voy a hacer en este caso es introducir un algoritmo que lo que hace es crear una dimensión Z la dimensión Z que no existe hasta hoy porque está la x y la y lo que hace es a través de un algoritmo Elevar al cuadrado la x y elevar al cuadrado la y como está esta fórmula que está aquí abajo y sumadas crear Z eso produce un efecto que cuál es los puntos que están en este gráfico distribuidos de esta manera se distribuyen en este otro gráfico de esta otra manera aquí la coordenada z que está aquí arriba y la coordenada x que la que está aquí al costado estos puntos son los mismos de aquí pero al haber sido producto de aplicar esta fórmula se distribuye de este modo concretamente a partir de esa realidad lo que voy a obtener es la posibilidad de separar ambos grupos quizás con una recta que pase por esta zona que no tengo en este gráfico donde es imposible trazar una recta que separe correctamente estos dos grupos de puntos entonces lo que hice aplique un truco a través de un algoritmo donde transforme los puntos para que se distribuyan de otra manera teniendo como resultado una manera distribuida como esta que está aquí que sí me permite dividir las dos clases cosa que no podía hacer con este otro gráfico tomando como base lo que vimos recién vamos a pasar un problema mucho más complejo nosotros tenemos en realidad todos los ejemplos que hemos visto hasta ahora en un gráfico de dos dimensiones y nos podemos llegar a preguntar si siempre Esta división la tengo que hacer como lo hicimos nosotros hasta ahora es decir de una manera visual La respuesta es que no obviamente yo tengo un algoritmo que es quien toma esa ese papel ese rol Pero si yo llevo esto ya a problemas donde tengo más dimensiones que dos puedo tener un problema que es el siguiente de que justamente los puntos parecido al problema que vimos en el último caso en el caso 5 este distribuido de una manera que sea imposible encontrar un hiperplano que lo separe con lo cual lo que tengo que recurrir es a un concepto a un truco kernel así se llama este concepto que lo que hace justamente es poder redistribuir los puntos de una manera diferente para que eso favorezca la posibilidad de encontrar un hiper plano que separe Dos clases para lograr eso lo que yo tengo que hacer es incorporar una dimensión más cada que justamente esa separación pueda darse de un modo de que yo justamente encuentre esa solución esto parece un contrasentido respecto de lo que hablamos en la clase pasada donde lo que buscamos era reducir la dimensionalidad Pero bueno con un propósito de poder tener un algoritmo que fuera tan eficiente como el que tenía una dimensión más pero fuera más rápido en este caso es un sentido totalmente inverso es decir lo que yo busco es justamente aumentar una dimensión porque con la dimensión que tengo la separación es imposible aquí tenemos algunos gráficos que pueden representar fácilmente esa idea fíjense que yo tengo un primer caso donde tengo todos estos puntos que están aquí y cómo se ven están mezclados los puntos rojos con los negros de esta manera es imposible encontrar en este caso en un ámbito dedimensional bidimensional Perdón una recta que separa ambos puntos yo voy a llevar esta estructura de dos dimensiones a una estructura de tres dimensiones y el resultado se transforma en este gráfico que está aquí a la derecha y si me da la posibilidad de en este ámbito de tres dimensiones donde los puntos sean distribuidos de otra manera encontrar un hiperplano que separa ambas clases en un sentido similar tengo otro ejemplo como este que está aquí donde tengo un grupo de puntos que están concentrados en un círculo y otros que están concentrados en un círculo pero más grande y que abarca la anterior de nuevo no puedo encontrar en este espacio bidimensional una recta que separa ambas clases con lo cual incorporo nuevamente una dimensión más y llego a una situación como la que está aquí en este gráfico en donde justamente esos puntos se separan claramente y me permiten a mí encontrar un hiperplano que pueda separar ambas clases como tu algoritmo de Machine learning y en línea A lo que hemos venido tratando en Las dos últimas clases desde el momento que incorporamos el concepto de tuneo de hiper parámetros el algoritmo de Machine learning este algoritmo de máquina de soporte vectorial también tiene sus hiper parámetros el primero de ellos es el carnet lo que acabamos de ver recién esta cuestión del truco kernel de esta manera de redistribuir los puntos agregando una dimensión tiene tres opciones posibles luego vienen los hiper parámetros gama y C Que funcionaba habitualmente en conjunto el valor de gama es un valor numérico y ese valor en tanto sea más alto intentará ajustar exactamente el algoritmo a las características de conjunto de entrenamiento tema que ya vimos Es peligroso porque justamente nos lleva a un sobre ajuste que es uno de los tipos de resultados que no queremos tener por lo tanto hay que ajustar el valor de Gama de manera cuidadosa Para no caer en ese error un tema importante este hiper parámetro gama no se usa si la opción de carne elegida es la lineal luego viene el parámetro de penalización c del término de error Ese es el nombre que tiene este hiper parámetro set este preparametro lo que controla es el equilibrio entre los límites de decisión y la clasificación correcta de los puntos de entrenamiento para entender un poco mejor esto vamos a recurrir al caso 3 que vimos hace un rato donde teníamos una estrella que estaba del lado del resto de las estrellas pero un poco alejada del grupo mayoritario y por otro lado el grupo de los puntos rojos nos preguntamos en este caso si lo mejor era la recta b o la recta viendo que priorizamos la clasificación en el caso de la recta o la buena separación de los grupos con una distancia muy buena respecto de un grupo o de otro que proponía la recta B esto es una cuestión que tenemos que equilibrar y justamente Este es el rol que tiene este hiper parámetro C para poder encontrar un equilibrio entre el nivel óptimo de separación y el nivel óptimo de clasificación Finalmente y al igual que en el caso del hiper parámetro Gamma el hiper parámetro c no se usa si la opción elegida de kernel es la lineal a los fines de terminar de entender estos dos últimos siete parámetros y esta cuestión de verlos en conjunto hemos puesto aquí cuatro gráficos donde lo que hacemos es ir variando los valores de ambos desde valores bajos a valores exactos Qué pasa cuando tengo un gama que aquí se representa con un valor similar con una letra similar a una y tengo un gama bajo y un sebajo bueno la división que se produce es esta Sí para este ejemplo fíjense que la división propone una buena separación pero no copia tal cual la realidad del conjunto entrenamiento porque digo esto porque fíjense que hay triángulos verdes del lado que no corresponden y hay cuadrados azules también del lado que no corresponde en el siguiente gráfico paso a pasar a el c a un valor muy alto y el gama lo dejo el mismo valor que antes Cuál es la diferencia con el caso anterior esa recta si esa línea pasa a ser una curva Y esa curva lo que intenta hacer es hacer una separación tal que esos elementos que estaban del lado incorrecto ya no lo estén fíjense la diferencia es muy clara respecto del caso anterior ahora todos los triángulos están de un lado y todos los rectángulos están del otro en el siguiente gráfico paso a subir al gama a un valor alto y vuelvo hacia atrás el valor de c que produce Esto bueno acuérdense que el gama alto lo que nos llevaba era un problema de sobreajuste y aquí está claro porque fíjense que el grupo Trata de hacerlo dejando la menor cantidad posible de valores del lado incorrecto como recién vimos con el caso de El c alto Pero cuál es la diferencia fíjense que en el caso del sea alto buscaba que esa división fuese lo más estricta posible pero abarcaba un montón de lugares donde no había ningunos de estos cuadrados Azules es decir sectores donde no había datos en este caso eso no lo hace es decir se cierra exclusivamente hacia los lugares donde están esos valores pero tiene un efecto aún fíjense que hay aquí cuadrados del lado que no corresponde y un triángulo también del lado que no corresponde Cómo se usan a eso bueno poniendo justamente un sea alto nuevamente con lo cual esto sería el mejor modelo probablemente para el conjunto de entrenamiento mejor modelo porque fíjense que ahora esta nueva figura No deja ningún cuadrado azul del lado incorrecto ni ningún triángulo verde del lado del correcto esto obviamente es un modelo que va a dar un score muy bueno pero seguramente me va a llevar a un problema sobre ajuste que no va a generalizar para el futuro Cuál es el método ideal Bueno tengo que buscar algo que sea equilibrado entre los valores de gama y C y por eso tenemos que usar como ya lo hemos hecho antes un algoritmo como gris se ve que nos sugiera Cuál es el valor más equilibrado para tener el mejor score pero con una mayor probabilidad de generalización a futuro para terminar esta primer parte de esta clase Vamos como siempre a las ventajas y desventajas de este nuevo algoritmo máquina de soporte electoral por el lado de las ventajas funciona muy bien con un claro margen de separación es decir cuando los conjuntos están bien separados Sí bueno este conjunto Funciona muy bien aunque tenga que hacer un hiperplano recto o con alguna forma Curva pero que estén esos grupos separados favorece que este algoritmo funcione bien es eficaz en espacio de Gran dimensión acuérdense que hablamos de espacios unidimensionales estamos graficando espacios de dos coordenadas y tres coordenadas justamente para poder visualizarlo de un modo que lo podamos entender mejor pero evidentemente esto puede llegar a otra dimensiones superiores a 3 utiliza un subconjunto del conjunto de entrenamiento en la función de decisión llamado vectores de soporte recordarán el gráfico el primer gráfico de esta clase donde hablábamos de los vectores de soporte que son los que están más cercanos a la línea o curva que me separa los grupos Es decir si yo tomo solamente esos valores como referencia ya suficiente porque sé que los otros que están más alejados esa curva si se para eso que está más cerca también lo va a hacer con el resto con lo cual solamente haciendo referencia a esos puntos sería suficiente para poder Establecer un buen modelo y sin tener que tener en cuenta a los restantes puntos y por el lado de las desventajas dos cuestiones por un lado no funciona bien cuando tenemos un grupo grande de datos Porque porque justamente aquí si usamos esta opción del truco kernel voy a tener que agregar una dimensión Y si eso lo hago para un conjunto muy grande de datos Obviamente el entrenamiento va a tardar y mucho finalmente tampoco funciona bien cuando el conjunto de datos tiene más ruido es decir cuando las clases que en este caso estoy intentando Separar superponen en algún caso Aunque yo utilice el truco kernel e intente agregando una dimensión separar esos conjuntos quizás esa separación no sea efectiva y ambas clases tengan puntos que siguen mezclados Bueno si eso sucede evidentemente Esta es una de las desventajas de este algoritmo y Por ende este algoritmo nos va a funcionar bien Bueno hasta aquí esta primer parte de la clase número 11 ahora como siempre nos vemos en la segunda parte para implementar todos estos conocimientos a través de cola en la práctica hasta aquí llegamos con esta primera parte los esperamos para poner en práctica todo lo aprendido en la segunda parte  Titulo: Clase 11 (parte2) del Curso de Inteligencia Artificial \\n URL https://youtu.be/3vfEIhqZCnY  \\n 1380 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 11 del curso de Inteligencia artificial de ifes en ella vamos a poner en práctica todo lo que aprendimos en la primera parte vamos por ello  Hola nuevamente Bienvenidos a la segunda parte de la clase número 11 donde vamos a poner en práctica todo lo que vimos recién en la primer parte de esta clase donde abordamos los conceptos teóricos de este nuevo algoritmo de Machine learning máquinas de soporte vectorial lo primero que vamos a hacer luego de haber hecho la conexión como siempre de El cola es importar las librerías pandas nampai sbm es Support vector Machine que es la forma que se llama movimiento en inglés este nuevo algoritmo de máquinas de soporte vectorial y obviamente siempre de la librería Side Land luego vamos a seguir utilizando otras librerías que ya hemos utilizado antes la que sirve para separar el conjunto De tez y entrenamiento gritch ser cb y confusión Matrix y finalmente warnings que bueno son ya clásicas en todas las prácticas que estamos haciendo Así que empezamos por esto y luego inmediatamente hacemos lo propio para incorporar el conjunto de datos bien ahí lo estamos importando ya ha sido importado y creamos el Data frame con el nombre de iris y hacemos un Head bueno Nada nuevo hasta aquí y ya este conjunto lo hemos usado mucho y lo que tenemos que hacer como siempre separar las variables de entrada de las variables de salida Solamente vamos a usar dos variables de entrada dado que vamos a tener que trabajar más adelante con el truco kernel para lo cual vamos a usar justamente dos variables para poder verlo bien y vamos a usar para este caso el largo del sépalo y el ancho del sépalo y obviamente la variable y la especie como ya lo hemos hecho antes luego de esto separamos en el siguiente paso el conjunto de entrenamiento del conjunto de Test y aquí empezamos con lo nuevo que sería bueno crear nuestro primer modelo de soporte vectorial en el cual vamos a incorporar los hiper parámetros que hablamos en la teoría como recordarán donde tenemos kernel que vamos a usar el linear el c lo vamos a fijar en 0,01 y el Gamma una vez logrado esto Bueno voy a entrenar el modelo como lo he hecho antes con los otros algoritmos y medimos su precisión lo ejecutamos bien vemos que el modelo de entrenamiento y el modelo de Test tienen escorts muy bajos obviamente esto tiene que ver con el tipo de terno el que he elegido el tipo de carne lineal va a proponer una solución en la cual la separación pueda ser lineal Y esto no siempre va a poder ser factible dado que si la solución sugiriera que por la forma que tienen los puntos hay que hacer una curva O tiene que tener una cierta cobertura esa hiper plano tridimensional en este caso va a ser un plano porque tengo un problema de tres dimensiones dos variables de entrada más la variable de salida bueno el problema que voy a traer es que esa linealidad que obtengo con el lineal no va a darme como resultado un buen algoritmo y aquí se ve en el score con lo cual lo que voy a hacer va a ser cambiar el kernel y a su vez también voy a cambiar los valores de c y de gama el carnet que voy a utilizar es el rdf si vamos a recordar la clase teórica veíamos que ese era el tipo gausiano Recuerden que tenemos el lineal que ya lo usamos recién en la práctica el gauciano que lo vamos a usar ahora y el polinómico obviamente para este caso que no se adapte a alinear el polinómico y de océano son las mejores opciones de esas dos vamos a usar Esta última volvemos a la práctica bien entonces también tenemos un salto y un gama alto Recuerden que eso nos puede traer el problema del sobre ajuste bueno el resto es igual a lo que hicimos recién lo ejecutamos y medimos la precisión cuando mido la precisión veo que tengo una muy buena precisión para el conjunto de entrenamiento pero una muy mala para el conjunto de Test Por qué será eso bueno vuelvo a la teoría se acuerdan que dijimos que si tengo un gama alto como está este gráfico aquí de la derecha y un c alto pero probablemente copie también la realidad del entrenamiento que me va a dar una muy buena solución del conjunto de entrenamiento pero con un nivel de sobreajuste tal que justamente después no va a generalizar bien Esto se ve claramente aquí cuando miro los score el 04 el conjunto de Test me habla de que es un modelo muy sobre ajustado dado que el score que tengo para el modelo de entrenamiento es muy Superior y muy bueno a su vez y el conjunto de Test es muy inferior y muy malo dado que con valores bajos he obtenido scores bajos para ambos conjuntos y con valores altos he obtenido un buen valor para el conjunto de entrenamiento sea un mal valor para el otro conjunto vamos a usar el gris ser se ve para que nos sugiera como ya sabemos y conocemos de esta herramienta el mejor valor de c y el mejor valor de gama para tener buenos scores para ambos conjuntos dado que voy a usar grip ser cb para que me Determine automáticamente como ya sabemos el valor adecuado de ese y de gama para este modelo voy a crear un nuevo modelo que le voy a poner como nombre sbsg por la G de grid y solamente voy a especificar Cuál es el tipo de truco kernel que voy a utilizar el cual voy a utilizar el truco de tipo gausiano que es el mismo que usamos en el ejemplo anterior una vez creado el modelo Lo acabamos de entrenarlo con los conjuntos de entrenamiento y test como es habitualmente utiliza ahora vamos a armar un diccionario con el conjunto de posibles valores para cada uno de los hiper parámetros que quiero que se incorporen a Gris ser cb para que con ellos grid me Determine Cuáles serían los mejores o los más adecuados para este modelo en el caso de c voy a poner 0,0011 10 y 100 En el caso de dama 0,11 y 5 los valores los pueden poner ustedes de acuerdo a lo que les parezca Y también poder usar la función que usamos antes de nampai Land space para que Determine un conjunto de valores en base algunos parámetros que le damos a esa función luego de ello generamos justamente el Grisel se ve en donde le especifico Qué modelo quiere usar que es el que Acabo de crear acá arriba y cuáles son los parámetros que son los que recién acabamos de referenciar lo hacemos ya tengo mi grid creado a partir del Grisel CV lo que voy a hacer ahora es con ese grid entrenarlo con los conjuntos de entrenamiento y texto y con todo ello ahora poder averiguar bien qué determinó como valores adecuados de gama y DC el glister cb y veo que los valores adecuados son 1 y 0,1 que son justamente algunos de los valores que yo ya le había dado aquí en el conjunto de posibles valores miedo a la precisión y veo que tanto en el conjunto de entrenamiento como el conjunto de Test no llegan al 0.9 y un poco más que sería lo ideal para este caso Sí pero por lo menos 0.9 pero son mucho mejores que los dos casos anteriores Y más allá de eso es importante que ven aquí que el conjunto de entrenamiento no tiene tan buen score como antes Si 080 versus 094 pero el conjunto de Test es mucho mejor y no está tan alejado en su score respecto del conjunto de entrenamiento como era antes que había 0.5 de diferencia y ahora hay 0.1 de diferencia de todos modos lo ideal sería Buscar aquí una solución para que este score no solamente sea mejor sino que sea superior al del conjunto entre para terminar de validar la precisión del modelo además de hacerlo con el método score sabemos que tenemos una herramienta que ya venimos usando las últimas clases que es la matriz de confusión recordemos que antes de aplicar la matriz de confusión tengo que establecer las predicciones Es decir para el conjunto de Test saber usando el modelo grid en este caso que es el producto de haber utilizado la herramienta griser cb y el modelo bid con predic obtener las predicciones para ese conjunto de Test y luego compararlas con las variables de salida de ese conjunto de Test lo ejecutamos y vemos el resultado Recuerden que lo que estamos viendo aquí es la precisión para tres salidas para el caso de cetosa versicolor y virginica por eso en la diagonal principal veo que tengo seis aciertos en la primera variante cetosa 7 en versicolor y 8 en virginica en el caso de versicolor tengo dos valores que no han sido predecidos de manera correcta y en el caso de Virginia que es la última tengo Siete valores que no han sido predecidos de manera correcta con lo cual aquí es donde tengo el mayor nivel de desaciertos y por eso justamente si saco el valor total voy a poder saber sobre la cantidad de aciertos versus la cantidad de errores justamente este 07 evidentemente el 30% de esta información que está en la matriz de confusión es desacertada por eso el 07 ahora vamos a ver otro caso de aplicación de El algoritmo de máquinas de soporte vectorial con kernel donde vamos a utilizar un conjunto de datos mucho más grande y que ya hemos utilizado la clase pasada recordarán el conjunto mnist sí que tenía dígitos escritos a mano entre 0 y 9 y que tenía una composición de 70.000 observaciones de las cuales 60.000 ya estaban configuradas o preconfiguradas para el conjunto de entrenamiento Y 10.000 para el conjunto de Test vamos a tomar este caso para ir incorporando algunos conceptos y adelantándonos un poquito a lo que tenga que ver con visión computacional que bueno aplica este caso y ya es una buena oportunidad para ir manejando algunos conceptos que vamos a utilizar a futuro como siempre lo primero que vamos a hacer es trabajar con la incorporación de las librerías y verías que son similares a las que usamos anteriormente solamente que en este caso vamos a usar fecha Open nml que la que usamos para incorporar este Data set del cual estamos hablando y estándar escáner la vamos a sumar también dado que vamos a probar lo que vamos a hacer ahora en el ejercicio también en una parte con el escalamiento de datos bien una vez hecho esto voy a incorporar en la cassette como dije recién y ya sabemos que es muy grande por lo tanto este proceso de la incorporación de este conjunto de datos al cola va a tardar mucho más que en los casos anteriores bien allí terminó y tardó 27 segundos como dice aquí luego imprimo el diccionario de las claves de todas las Kiss y las dos más importantes son el Data y el target por lo tanto voy a hacer un shape del Data y un shape del Target que lo que estoy haciendo aquí en el primer caso veo que tengo Las 70.000 observaciones que ya hablamos y 784 características Cómo es esto vamos a tomar este gráfico como ejemplo Este es un número 5 la información de este número 5 está dispuesta en una estructura matricial que tiene 28 por 28 píxeles 28 por 28 son 784 píxeles que en esta estructura están dispuestos en una raid unidimensional es decir que cada una de estas 28 filas de 28 píxeles están todas juntas en una sola tira por decirlo de alguna manera en un array unidimensional Por eso sumadas cada una de estas 28 filas de 28 píxeles me dan los 783 784 Perdón características que menciona y luego está el target que tiene 70.000 Qué información tiene el target bueno cada uno de estos dibujos qué número corresponde en este caso yo tendría la información de estos 28 por 28 y el tar te diría 5 porque esto es un 5 bien esa es la información y por eso tenemos de esta manera una explicación de estos números que vemos aquí ahora para ver un poquito más de cómo guarda esta información vamos a tomar la primer fila de estas 70.000 y la vamos a mostrar por pantalla no hace falta poner un print simplemente poniendo mnist punto Data sub 0 que sería la referencia de la primer fila y aquí está lo que le decía recién el array de los 784 píxeles de esta imagen similar a esta que está aquí bien cada uno de estos píxeles es un valor que va entre 0 y 255 que es la nomenclatura que tienen los colores en el formato rgb y con esto tengo toda la información si yo ahora quiero imprimir esta información para poder verlo como estoy viendo aquí esta imagen qué debería hacer debería a toda esta tira de 784 píxeles reformatearla en una estructura matricial de 28 por 28 es decir como esto que tengo aquí por lo tanto lo que voy a hacer a continuación justamente es Eso es imagen la variable digo que es igual a MX de Data sub 0 que es m&s Data sub 0 es lo que imprimir acá sí o sea todo este conjunto de valores lo quiero imprimir y para imprimir lo tengo que hacer un shape de 28 por 28 Luego de eso lo que hago es un plt o sea imprimo Qué cosa imagen con un semeaba que es un mapa de tipo binario y finalmente lo que voy a hacer es imprimir el target es decir que número corresponde con ese dibujo que estoy transformando en esta estructura de 28 por 28 dispuesto a imprimir bien lo ejecutamos y vemos que al igual que el caso que veíamos recién en el ejemplo es se trata de un 5 Entonces tenemos este número 5 que está aquí arriba es la impresión del Target y todo este dibujo que está aquí es la impresión de imagen de la variable imagen que es la resultante de haber tomado la primer fila Data Sub Zero insisto es todo este conjunto de números que está acá era una estructurada de 28 por 28 todo lo que tengo aquí es un cuadrado de 28 píxeles por 28 píxeles y que en este caso me muestra un 5 como en otro caso Me podría mostrar otra cosa podemos hacer una prueba acá y cambiar un poco esta Consigna esperemos que nosotros cinco imprimimos bueno en este caso es un cero un poco torcido pero es un cero fíjense que de nuevo aquí arriba está el target y aquí la impresión bueno Esto Pueden seguir ustedes también jugando todo lo que quieran cambiando este valor siempre que sea un valor que esté entre 0 y no 70.000 sino 69.999 bueno pueden poner el número que quieran y ver bueno tuvimos suerte que en todos los casos es un valor diferente en este caso el dibujo representa un 4 y el target Me dice que esos son cuatro Aquí es importante entender que la clave de esto es encontrar un algoritmo que viendo esta estructura de 28 por 28 y la forma que tiene puede interpretar que esto responde a un patrón de un número determinado en este caso lo que determina Es que este patrón a pesar de que es un número muy regular es lo más parecido a un 4 si eso es lo que o de esa manera trabaja el algoritmo no nos adelantemos ya la vamos a a ver bien en Visión computacional pasemos Entonces ahora a crear el modelo de máquina de soporte electoral antes debería hacer como sabemos en este caso dividir el conjunto de entrenamiento y test Recuerden que en este caso particular de este conjunto de datos que ya viene preparada la separación de datos de entrenamiento Entonces no voy a usar la librería 30 split sino que directamente voy a hacer la asignación según el número de orden de la muestra de la observación es decir los 60.000 desde el principio hasta el valor anterior al 60.000 para los primeros 60.000 datos Y desde el 60.000 hasta el final para las 10.000 restantes observaciones que corresponden Al conjunto de Test una vez que hago eso creo Ahora sí el modelo que en este caso lo voy a hacer sin manejar los hiper parámetros que vimos antes gama y s simplemente lo único que voy a poner es el tipo de capa bueno Esto obviamente por la magnitud de el conjunto de datos va a tardar bastante tiempo Bueno ahí tenemos que ha terminado la el entrenamiento del modelo y ha tomado cuatro minutos saben que cuando empezamos a trabajar con datasettes más grandes y con Data set que tienen alguna cuestión como los que manejan imágenes a pesar de que estas imágenes son blanco y negro de 28 x 28 que es una porción digamos una muestra muy chica respecto de otro tipo de imágenes que tiene más colores y más dimensión ya con eso vemos que el tiempo que invertimos en crear modelo va cambiando radicalmente respecto de los ejercicios que veníamos haciendo antes bien Ahora vamos a medir entonces la precisión del modelo para el conjunto de entrenamiento de Test como hacemos habitualmente bien como Ven aquí la medición del score también tardó mucho y mucho más aún que el entrenamiento del modelo 13 minutos la precisión es muy buena 0.98 y 097 la idea podía hacer ahora probar escalar los datos Porque porque justamente lo que hablábamos de la diferencia de las cifras en este caso tenemos 784 características y sus valores van entre 0 255 puede ser que esa ese conjunto de posibles valores marca una distancia muy grande con lo cual podría ser una alternativa escalar los datos y ver si la precisión con esa técnica mejora o no vamos por ello bueno en este caso se han invertido 7 minutos para el escalamiento de datos y la obtención del modelo con el escalamiento de datos es decir que tomó cuatro minutos que cuando generamos el modelo sin escalar los datos y ahora vamos a medir la precisión y bien hay concluyó la medición del score volviendo a repasar un poquito todo este tiempo que nos está llevando este Data set importante vemos que tenemos 4 minutos para la generación del modelo y 13 minutos para medir su precisión luego con el modelo escalado tenemos 7 minutos para la generación del modelo y 17 minutos para la medición de la presión la precisión no ha mejorado significativamente respecto del anterior hay una pequeñísima diferencia en el caso del modelo de entrenamiento y una pequeñísima diferencia en el caso del modelo de Test pero en menos es decir no he mejorado el score en este caso tengamos presente que no utilizamos recordemos lo que pusimos aquí arriba no utilizamos hiper parámetros se puede mejorar este score se puede mejorar a pesar de que es muy bueno justamente lo podríamos probar incorporando los hiper parámetros Gamma y C el tema es que aquí obviamente cada prueba nos va a llevar un tiempo muy importante si utilizásemos el grid se ve obviamente mucho tiempo más pero bueno Esto es parte de lo que tenemos que empezar a aprender a manejar y a tener paciencia porque obviamente todas estas cuestiones llevan mucho tiempo hay algunas herramientas inclusive en el Google que ya vamos a ver más adelante que pueden hacer que este proceso sea más rápido y dos herramientas que tiene también Google que son pagas que también aceleran los tiempos de desarrollo de este tipo de procesos Pero bueno lo concreto que sabemos que esta es nuestra primera experiencia con el tema del tiempo así que bueno Los invito si alguno quiere probar el hacer este alguna prueba incorporando algún hiper parámetro para mejorar este algoritmo y bueno tener una primera experiencia de lo que se trata de empezar a trabajar en el mundo de el desarrollo de inteligencia artificiales y algoritmos y cada tiempo que lleva digamos en la búsqueda de este tipo de perfeccionamiento bien hasta aquí llegamos con esta segunda parte de la clase número 11 y cerrando la clase número 11 como un todo Solo me queda despedirme y decirles que nos vemos en la próxima clase número 12 hasta entonces hasta aquí llegamos con esta clase con ella ahora podés manejar un nuevo tipo de algoritmo máquinas de soporte vectorial Te espero en la clase 12 donde vamos a comenzar a aprender los algoritmos de tipo no supervisado nos vemos  Titulo: Clase 12 (parte1) del Curso de Inteligencia Artificial \\n URL https://youtu.be/YOdajMsWZAQ  \\n 1150 segundos de duracion \\n Hola bienvenidos Esta es la primera parte de la clase número 12 del curso de Inteligencia artificial de ifes en ella Vamos a aprender los conceptos básicos de los algoritmos de tipo no supervisado empecemos  Hola a todos Bienvenidos a la clase número 12 del curso de Inteligencia artificial de ifes seguimos en el módulo de Machine learning pero hoy vamos a empezar a ver Otro aspecto del Machine learning que son los algoritmos no supervisados hasta ahora vimos lo contrario los algoritmos supervisados y vimos varias opciones las más importantes de ese ámbito ahora vamos a empezar a desalar este otro camino de los algoritmos no supervisados y dentro de ellos vamos a elegir uno de los ritmos más populares que es el de kang mean clustering hemos podido recorrer como dijimos recién un largo camino de los algoritmos de aprendizaje supervisado y al haber podido asimilar muchos conceptos sobre ellos así como también haber hecho muchas prácticas con las más importantes opciones de ese ámbito creo que estamos en condiciones de hacer algunas reflexiones sobre ese tipo de algoritmos de qué hablamos cuando hablamos de aprendizaje supervisado cuando hablamos de aprendizaje supervisado nos estamos refiriendo a un tipo de aprendizaje que se basa en descubrir la relación existente entre unas variables de entrada y una variable de salida o lo que es lo mismo decir que el aprendizaje surge de enseñarle a estos algoritmos Cuál es el resultado que queremos obtener para un determinado valor y con ello se genere un modelo que pueda predecir información de lo que puede pasar en el futuro concretamente tras mostrarles muchos ejemplos de lo que pretendemos el algoritmo pueda predecir y si se dan las condiciones para poder encontrar un patrón el algoritmo va a ser capaz de dar un resultado correcto incluso cuando le mostremos valores que no haya visto antes ejemplo de aplicación de esto ya hemos visto muchos a lo largo de todas estas clases pero vamos a ahondar en algunos ejemplos más para seguir reflexionando Por ejemplo si quisiéramos encontrar Cuál es la relación entre un correo electrónico y su clasificación como spam o correo deseado seguramente seríamos capaces de leerlo y poder clasificarlo sin problemas pero la pregunta es seríamos capaces de explicar cuál es el patrón que Define si un correo sea o no spam si le damos a un algoritmo de aprendizaje supervisado muchos ejemplos de correos y cuál es su clasificación es seguro que es el algoritmo va a tener la respuesta y va a encontrar un patrón con el cual definir un modelo de predicción más ejemplos un algoritmo de aprendizaje supervisado en el año 2016 pudo aprender a diagnosticar Si una persona sufriría o no de depresión a partir del contenido de su cuenta de Instagram con una eficiencia mayor a la que tendrían los doctores como lo hizo bueno simplemente tomando la información de miles de ejemplos de usuarios que usaban esa herramienta y padecían esa enfermedad de allí encontró un patrón y pudo justamente con esa información establecer la posibilidad de diagnosticar esa enfermedad estos dos ejemplos más todos los ejemplos que vimos en las prácticas anteriores nos permiten aseverar que si le mostramos a uno de estos algoritmos suficientes datos de entrada y de salida y si existe una relación o patrón es algoritmo va a ser capaz de aprenderla y darnos los elementos para crear un modelo por esto mismo el aprendizaje supervisado ha sido el paradigma que más aplicación práctica ha tenido durante las últimas décadas liderando nuevamente la corriente al alza que ha vivido en los últimos años la Inteligencia artificial como ya sabemos lo de encuadrar estos algoritmos como algoritmo de tipo supervisado viene del hecho de que al Mostrar los resultados que queremos al algoritmo estamos participando de la supervisión de su aprendizaje ahora La pregunta sería si esto es el aprendizaje supervisado Cuál es o qué es el aprendizaje No supervisado el aprendizaje No supervisado es el paradigma que consigue producir conocimientos únicamente de los datos que se proporcionan como entrada sin necesidad en ningún momento de explicarle al sistema Qué resultado queremos obtener bien quizás esta definición sea un poco compleja y no nos permita entender cómo se puede aprender sin recibir ninguna Pauta previa pero para entender un poquito mejor vamos a ver un ejemplo vamos a suponer que yo quiero hacer una fiesta y voy a invitar a 100 personas seguramente muchas de esas personas son amigas y Por ende dentro de la fiesta va a haber un alto número de subgrupos que van a estar conformados por las personas que se conocen previamente Mi idea es otra Mi idea es que no haya más de 10 grupos y Por ende lo que voy a buscar es que haya personas que estén en cada uno de esos grupos que tengan elementos común o que tengan afinidades por eso lo que voy a hacer es a cada persona que entra hacerle una serie de preguntas por ejemplo que profesión tienen en dónde trabajan Qué edad tiene su situación conyugal si una serie de preguntas no importa Cuáles pero que me permitan saber cuál es el perfil de esa persona y con ello conformar estos 10 grupos de qué manera bueno buscando que cada uno de los integrantes de estos 10 grupos un promedio tengan una respuesta a esas preguntas que sean similares a la de los otros integrantes de ese grupo de esta manera voy a tener 10 grupos en donde haya una gran homogeneidad en las respuestas en los perfiles de sus integrantes y ese grupo va a ser bien diferente de otros esos grupos no tienen nombre es decir Yo puedo ponerles número puedo identificarlos con una letra pero no van a tener una identificación con lo que reconocemos el aprendizaje supervisado como etiqueta en este caso no hay etiquetas Sí sé que ese grupo está integrado por gente que tiene un patrón común y ese grupo es diferente de los otros nueve pero no tiene una identificación Es más si en la fiesta concurriese una persona 101 102 haciéndole las mismas preguntas que le hice a los anteriores voy a saber en cuál de esos 10 grupos debería ubicar a esa persona este problema que acabamos de resolver se llama clasterización y es un tipo de problema muy importante dentro del campo de Machine learning como vemos sin la necesidad de darle información de salida ni tampoco de que alguien supervise la respuesta del modelo en relación a esa información de salida hemos podido generar un tipo de conocimiento de valor solo con la información que le dimos de entrada hasta aquí son todas buenas noticias ya que ahora tenemos un tipo de algoritmo que me permite crear modelos con menos trabajo de parte nuestra que lo que implicaba un modelo de tipo supervisado Pero y siempre hay un pero la dificultad de los algoritmos supervisados es que no tienen ningún ejemplo de respuesta con el que saber si el algoritmo está actuando correctamente utilizando un sistema de medición de score que nos permita valorizar la precisión del modelo y poder saber si el mismo Será o no exitoso por el mismo motivo tampoco podemos saber si en el ejemplo de la fiesta que necesitamos recién la cantidad de grupos que se definieron para el modelo es la más adecuada no obstante y volviendo a las ventajas de este tipo de aprendizaje los conjuntos de datos para entrenar son menos costosos de conseguir pensemos el siguiente ejemplo si quisiéramos entrenar a un algoritmo para clasificar animales como usamos de ejemplo recuerdan en la primer clase de este curso necesitaríamos no solo las imágenes de entrada sino también a alguien concretamente un ser humano que vaya visualizando imagen por imagen etiquetando las con el nombre del animal al que corresponden y esto en de datos que como mínimo suelen superar los 100.000 ejemplos Las 100.000 observaciones habla de una tarea que no es fácil o barato de hacer y que el aprendizaje de tipo no supervisado No nos exige un tema muy importante lo representa el hecho de que los algoritmos más potentes de este tipo no supervisado son capaces de descubrir a la perfección Cuál es la estructura interna que han generado dichos datos a ver si pensamos en el ejemplo de las figuras de animales podemos entender que cada grupo responde a un tipo de dato totalmente diferente del resto pero totalmente igual al resto de los miembros de su grupo Esto va a ser siempre así todas las imágenes de los animales que vemos en un grupo tienen que ser siempre iguales para que este algoritmo entiendan que pertenecen a un mismo cluster definitivamente no y ahora vamos a ver un ejemplo para tratar de entenderlo mejor volvamos Por un instante al datasette Méndez que usamos en la clase pasada todos sabemos que este Data set tiene imágenes digitalizadas de números manuscritos por ejemplo Estos son uno Estos son uno esto también es un uno y esto también lo es si bien esta información la podemos conocer ya que cada uno tiene un valor con una etiqueta cada uno de estas imágenes tiene un valor con una etiqueta en el Data set pero si no tuviésemos dichos etiquetas podríamos identificar como seres humanos con una simple observación a Qué valor numérico del 0 al 9 corresponde una imagen Obviamente que sí a pesar de que todos los números no son exactamente iguales un ser humano puede diferenciar una representación numérica de la otra ahora preguntémonos si no tuviéramos dichas etiquetas podríamos identificar con un modelo de Machine learning a Qué valor numérico de 0 a 9 corresponde una imagen justamente con un algoritmo de tipo no supervisado podemos hacerlo ya que esto buscará similitudes entre valores que no son idénticos pero representa un patrón a partir del cual los ubica en un grupo u otro en este caso en el grupo de los números estas estructuras conceptuales son denominadas espacios latentes y una vez que construimos este espacio las máquinas consiguen capacidades tan interesantes como las de saber si una cosa es similar a otra cosa el aprendizaje No supervisado señala un camino Muy prometedor en el futuro de la ia la segmentación de clientes que se utiliza en el campo del marketing es solo uno de los muchos casos de éxito de aplicabilidad de este tipo de técnicas de Machine learning sobre la cual ahora veremos uno de los algoritmos más conocidos el decammings clustering para comenzar a hablar de este importante algoritmo y empezar a conocer cómo trabaja un proceso de aprendizaje No supervisado vamos a empezar como otras tantas veces con un caso que tomaremos de ejemplo de aplicación imaginemos que trabajamos en una empresa implementando procesos de ia que efectivicen el funcionamiento de la misma y el jefe de marketing nos solicita la segmentación de los clientes en los siguientes grupos clientes de nivel bajo promedio o platino en función del comportamiento de gasto con fines de marketing dirigido y recomendaciones de producto sabiendo que no existe tal etiqueta histórica asociada a esos clientes cómo será posible categorizarlos bien aquí es donde el algoritmons clustering puede ayudar ya que es una técnica de aprendizaje automático no supervisada que se utiliza para agrupar datos no etiquetados en categorías o grupos similares pero específicamente cómo funciona este algoritmo la idea detrás del agrupamiento de camíns es dividir un conjunto de datos en un número específico de conglomerados Ese es el número k de allí el nombre de este algoritmo Cummins todos los puntos dentro de cada conglomerado son similares entre sí y aquellos en diferentes conglomerados también son diferentes Cuáles serían los cinco pasos claves que lleva un proceso de agrupamiento de tipo camisa primero especificar el número de cluster segundo determinar aleatoriamente los centroides para cada cluster tercero asignar cada punto del conjunto de datos al cluster más cercano cuarto recalcular los centroides de cada cluster en base al promedio de todos los datos que forman parte de cada uno de ellos y quinto y final repetir los pasos 3 y 4 hasta que las asignaciones de cluster dejen de cambiar o se alcance el número máximo de alteraciones la explicación fue demasiado rápida Bueno vamos a ver este ejemplo esta animación que tenemos aquí de 14 iteraciones paso a paso para entender mejor estos cinco últimos puntos que referenciamos para entender mejor estos cinco puntos que recién mencionábamos vamos a recurrir a la animación que vimos mientras explicábamos esos cinco puntos pero lo vamos a ir viendo detenidamente iteración por iteración Cuáles eran los dos primeros puntos que mencionábamos justamente de estos cinco el primero de ellos era establecer la cantidad de conglomerados recordemos El ejemplo que tomamos en donde decíamos que un jefe de marketing nos pedía segmentar a nuestros clientes en tres categorías bajo medio o platino o alto bien Esto no es una etiqueta que quiere decir con esto eso es una idea que nos ha solicitado nuestro jefe pero no es tan etiquetado los clientes los clientes en realidad son toda esta nube de puntos sí que en principio no están tipificados de ningún modo el segundo punto es elegir un centroide para cada uno de los conglomerados es decir si hay tres conglomerados habrá tres centroides los centroides eligen de manera aleatoria es decir que en el primer punto puede estar en cualquier lugar de esta 9 puntos en este caso elegimos este centroide este centroide y este centroide paso siguiente lo que hace el algoritmo es determinar la distancia de cada uno de los puntos es decir los clientes respecto del centroide e identifica a cada uno de ellos con el centroide más cercano es decir que todos estos puntos que están pintados de naranja están pintados de naranja porque entiende el algoritmo están más cerca de este centroide que de los otros dos lo mismo con los amarillos y lo mismo con los azules pero aquí no termina el proceso porque porque lo que tiene que buscar este algoritmo es la mejor ubicación de cada uno de los centroides por eso se habla de que este algoritmo tiene un proceso de iteración que lo que hace iteración por iteración como dicen justamente los pasos 3 y 4 ir acomodando los centroides en un nuevo lugar por qué no un nuevo lugar vamos al caso el centro de naranja como ustedes podrán observar el centro de naranja tiene una distancia respecto por ejemplo este punto que es el más alejado menor que la que tiene respecto del punto más alejado que está hacia abajo lo ideal sería que este centroide estuviese en un lugar más central que tuviera una distancia equidistante con cada uno de los miembros de su cluster bien Eso es lo que va a buscar este algoritmo en cada una de las iteraciones y por eso observemos en la iteración 1 el punto naranja ya no está en el mismo lugar que antes volvamos hacia atrás ven que está en esta posición ahora se corre a otra posición que está más central respecto de la nube de punto naranja Lo mismo sucede si ustedes observan con el amarillo fíjense Perdón la posición anterior y la nueva y el azul la posición en la iteración 0 y la nueva posición en la iteración 1 este ciclo Qué pasa genera una nueva posición y Por ende una nueva necesidad de reverer si los puntos que pertenecen a ese conglomerado siguen teniendo la menor distancia respecto de su centroide o ahora que los centroides se movieron de lugar Quizás esté más cerca de otros centroide y Por ende pasa a pertenecer a otro cluster fíjense que aquí en este caso veamos en el donde están los puntos amarillos tiene muchos puntos naranja es decir que son anteriores miembros de El cluster naranja y ahora están más cerca del cluster Amarillo también en el caso azul tiene muchos puntos amarillos que antes le pertenecían al grupo amarillo y alguna naranja lo mismo que antes le pertenecían al grupo Naranja es decir que en este caso hay una reubicación de centroides y encuadramiento de los puntos en relación a el cluster que le pertenece Y al nuevo cluster al cual le puede pertenecer porque puede cambiar o no Si vamos a la iteración 2 vemos que el ciclo se sigue repitiendo y que los centroides van cambiando de lugar Si observen que cada vez que pasa una iteración la alteración del lugar del centroide ya no es tan radical ya no es tan grande como los casos anteriores y Esto justamente habla de que cada iteración me va a llevar a buscar el lugar de mejor ubicación del centroide fíjense que inclusive respecto de los casos anteriores la división de cada uno de los clusters Y tomó como referencia esas líneas negras también va cambiando porque justamente hay una redistribución voy a pasar a la iteración 6 voy a pasar alteración 7 y fíjense que también lo que va disminuyendo entre una comparación y otra son los puntos que antes pertenecían a un cluster Y que ahora pasan a otro es decir todo va siendo más depurado los cambios van siendo más sensibles y terminarán cuando bueno como dice el punto 5 de los cinco puntos que anunciamos que describen este algoritmo cuando la cantidad de iteraciones si alcance lo que yo especifique para el cluster porque obviamente cada iteración lleva a un consumo computacional y la idea es que esto no esté indefinidamente funcionando o bien antes de llegar a la última iteración ya el algoritmo se dé cuenta que por más que itere la posición que tiene el cluster era ideal y esa posición ya no conviene que cambie bien hasta aquí llegamos con esta primer parte de la clase Ahora ya sabemos de qué se tratan los algoritmos no supervisados y en particular este algoritmo también classing ya estamos en condiciones de empezar a programar nuestras primeras líneas de código para ello Así que lo vamos a hacer como siempre en la siguiente parte de esta clase hasta aquí llegamos con esta primera parte los esperamos para poner en práctica todo lo aprendido en la segunda parte Titulo: Clase12 (parte2) Curso de Inteligencia Artificial \\n URL https://youtu.be/_hinMLX5VnI  \\n 2221 segundos de duracion \\n Hola bienvenidos Esta es la segunda parte de la clase número 12 del curso de ia de ifes en ella empezaremos a practicar con el algoritmo camins  Bueno Hola a todos nuevamente Vamos a continuar con la segunda parte de esta clase número 12 donde vamos a ver una práctica con kamins que es el algoritmo en el cual nos empezamos a incorporar al mundo de los algoritmos no supervisados que tratamos en la primer parte de esta clase así que ya pasamos al colar Y empezamos con esta práctica bien entonces aquí estamos en el cola cola número 12 en el cual vamos a ver lo que titula justamente este documento algoritmo no supervisados kamin clustering lo primero que vamos a hacer como siempre es incorporar las librerías que vamos a usar en esta práctica pandas nampai matter y aquí aparece kamins dentro de side in the cluster si es obviamente la librería específica que vamos a usar para este algoritmo nuevo que estamos aprendiendo Y también vamos a ver algo de pre procesamiento de datos algo que charlamos hace dos clases atrás donde hablamos del escalamiento de datos y la necesidad de eso cuando es necesario y porque es necesario Bueno vamos a volver a abordar este tema un poquito más en profundidad aquí con dos opciones que tienen que ver con el objetivo del escalamiento de datos Y que por ahí a veces suenan como sinónimos cuando no lo son tienen un mismo objetivo final pero no son Exactamente lo mismo así que bueno vamos a aprovechar esta clase para involucrar como siempre un nuevo tema a desarrollar y luego obviamente warnis bueno incorporamos estas librerías y pasamos a trabajar a traer digamos el conjunto de datos con el cual vamos a hacer esta práctica que es el de los vinos rojos los vinos tintos digamos no que ya usamos algunas clases hacia atrás lo vamos a volver a utilizar Así que vamos hacia ello como hacemos siempre aquí está y lo traemos al cola como siempre lo verificamos que aparezca Aquí bien aquí tenemos obviamente aparece dos veces porque obviamente ya lo hice en el marco del repaso de esta práctica la preparación de esta práctica Pero bueno Nos volvemos a hacer para demostrar justamente como siempre paso a paso cada una de las cosas que hacemos en la práctica y luego creamos un Data frame DF con esos datos y hacemos un Head recordemos un poquito de qué se trataba este conjunto de datos datos de vinos obviamente como dijimos Recién tintos con características que tienen que ver con la composición y con algunos aspectos químicos digamos no de la composición del vino pH sulfatos alcohol densidad Bueno este residuo azúcar ácido cítrico Bueno una serie de elementos que tienen que ver con las características del vino y aquí recordemos que el valor Target u objetivo era el Quality es la calificación del la calificación tiene que ver con bueno la determinación de un profesional con catador digamos de vino que le da una determinada calidad en virtud de las características del vino con los conocimientos que tiene la gente que se dedica a esto y le pone un número de calificación de acuerdo a la calidad que es especialista determina el vino bien Esto lo podemos este conjunto de datos podemos ver la composición que tienen son 1599 observaciones y 12 columnas que vimos recién y podemos justamente hablarlo del tema de la composición de la columna Quality Aquí vemos Algunos números el 5 al 6 pero no sabemos Cuántas variedades hay entonces lo que hacemos es utilizar de DF Quality es decir en Data frame DF punto Quality porque la característica que vamos a analizar es Quality el método única con Unique ya lo usamos Sí justamente en la clase análisis de datos donde podemos ver los valores únicos de una característica decir bueno cuántas variantes tiene este una determinada característica este caso Quality y cuando lo ejecuto me da como resultado una raíz donde me da todos y cada uno de los valores que tiene esa columna es decir que las calificaciones de los vinos van de 3 a 8 5 6 7 4 8 y 3 sí Esas son las variantes Así que lo único lo que uno podría llegar a presumir viendo estos números que podría ser de uno a cinco o de 1 a 6 Perdón o de 0 a 6 supongamos bueno no es así la calificación mínima es 3 y la calificación máxima es 8 bien a partir de eso vamos a involucrarnos en el tema de transformación de datos escalamiento y estandarización qué es lo que poco comentaba al principio de la clase Porque es conveniente o necesario bien para incorporarnos en este tema primero vamos a tratar de reconocer el contexto que tenemos para después encontrar la justificación de la transformación de datos para eso vamos a recurrir a un método que también usamos en las clases de análisis de datos que se llama discribe y distrive si ustedes recordarán se aplica obviamente sobre el Data frame que quiero analizar en este caso DF lo ejecutamos me va a dar una serie de indicadores muy utilizados en el ámbito de la necesidad y la estadística que bueno los puedo visualizar en una sola vista como aquí veo y por cada uno de los las características de este conjunto de datos Sí qué es lo que me da me da la cantidad de elementos que hay de cada una de las características obviamente son en todos los casos igual 1599 el promedio la dimensión estándar el valor mínimo de esa característica el valor máximo de esa característica y los percentiles 25 50 y 75 en este caso porque hago esto porque quiero ver el rango de valores numéricos entre los entre las distintas características de este conjunto de datos es decir Quiero ver el valor mínimo de todas las características y el valor máximo de todas las características esta herramienta me sirve mucho para ver ese tipo de información con lo cual voy a ir el caso de la fila mínimo y voy a recorrer todas y cada una de las características y voy a ver que por ejemplo tengo 460 0 12 0 0 90 0 0 12 16 29 274 033 843 es decir que la expresión mínima en este caso es esta que está aquí 0.012 es la mínima de esta característica Pero además es la mínima porque estoy justamente en la fila de los valores mínimos y si es la mínima de la fila de los valores mínimos quiere decir que es el valor mínimo de todo el conjunto de datos del mismo modo y con la misma lógica voy a la fila de los máximos y veo recorriendo cada una de las características el mismo proceso 1590 158 1 15 0611 bueno todos los valores que estamos viendo aquí y veo que este 289 me doy cuenta que es el valor máximo de nuevo es el valor máximo de esta característica pero es el valor máximo de todo el conjunto de datos con esto puedo ver que yo tengo una diferencia entre el valor mínimo de todo el conjunto de datos que recordemos era este valor 0012 y este valor máximo 289 esto representa una distancia muy grande en el medio obviamente todas las variantes de todos los valores que están entre esos extremos no esto representa un Rango muy grande de valores y puede llegar a distorsionar el modelo que voy a generar Por qué Porque puede entender que el valor más grande representa a una característica más importante y el valor más chico lo contrario esto en realidad no es así si yo tengo una característica como en este caso que tiene una cifra muy grande eso no debería implicar que esta característica es más importante que otra tiene más peso que otra a la hora de encontrar un patrón dentro de el modelo que estoy generando en el caso contrario lo mismo o sea la que tiene la cifra más chica no Debería ser como en este caso la menos importante esa distorsión se puede generar justamente por la magnitud de las cifras y lo que se busca transformando los datos con alguna de las tres variantes que ya vamos a ver es justamente imitar esa gran diferencia y que justamente el modelo no se equivoque en ese sentido y pueda tener una percepción más apropiada en virtud de la importancia de cada variable de entrada en virtud del algoritmo de predicción y no en relación a la magnitud de la cifra que representa recién hablábamos de Tres formas o tres métodos de transformación de datos Aquí vamos a ver dos de ellos pero vamos a mencionar Cuáles son los tres estandarización de datos es uno de ellos escalamiento de datos es el otro y el tercero es normalización de datos como dije recién de los tres vamos a ver aquí dos de ellos el primero el que titula aquí el punto 2 1 estandarización de datos es una transformación de datos en el cual se determina una media cero y una desviación estándar 1 generando si pudiese graficar esos valores una estructura de tipo acampanada que justamente lo que demuestra es que existe un nivel similar de datos hacia la izquierda y hacia la derecha del cero obviamente esto arrojaría valores que están positivos al cero O sea a la derecha del cero o a la izquierda del 0 negativos si el rango sería entre un valor negativo y un valor positivo lo más cerca posible del cero porque justamente la desviación estándar es 1 este método como dice aquí no es conveniente cuando el conjunto de datos tiene muchos outliers sí los outliers Recuerden que son los puntos O valores atípicos sí que están muy alejados del resto y que muchas veces nos hacen pensar si es conveniente que permanezcan en el conjunto de datos o directamente quitarlos porque distorsionan el modelo o pueden llegar a distorsionar el modelo Entonces en este caso si usamos esta forma de transformación de datos y Tenemos muchos que puede darse la situación y quizás no podamos quitarlos a todos ellos bueno no sería conveniente utilizar este método Bueno pero salvo ese caso vamos a aplicarlo en esta situación para poder conocerlo y practicar con él y aquí tenemos el código para eso por eso lo primero que hacemos Es crear una variable que le llamamos estándar como siempre cualquier variable que querramos y lo hacemos a partir de el método estándar que yendo un poco hacia arriba es uno de importamos aquí de cyclecic bien abajo y lo que hacemos a continuación es crear ese mismo conjunto de datos pero estandarizado bien Luego de eso que transformó creo el Data frame y le hago un Head y vemos el resultado que los valores son muy chicos las diferencias no son tan grandes como antes y tengo valores como dijimos recién algunos negativos y algunos positivos siempre centrados en el cero vamos a tomar por ejemplo uno de los valores que teníamos Aquí vamos al Data frame original tenemos 747878 11 12 11 2 Perdón 74 sí a ver si recordamos un poquito el primero y el último son iguales el segundo y tercero son iguales y el cuarto es el más alto esa lógica debería trasladarse después de haber sido transformado si a través del método de estabilización Sí este los datos de este Data es decir fíjense que en el caso este como dijimos el primero y el último son iguales el segundo y el tercero también y el cuarto es más grande que todos los otros bien el otro método es el escalamiento de datos en este caso el método funciona del siguiente modo le da el valor cero al valor más bajo de todo el conjunto de datos Y uno al valor más alto del conjunto de datos el resto de los valores están entre 0 y 1 de manera proporcional este método no es afectado a diferencia del método que vimos recién por la presencia de outliers Bueno lo aplicamos con una lógica similar a la que vimos recién el otro caso simplemente que en esta situación usamos min Mac Scanner que es uno de los este elementos que incorporamos aquí está si de la misma librería que sacamos estando en escáner al principio de esta práctica bien volvemos al lugar y bueno el proceso es similar porque tiene un Data frame que en este caso le pongo DF guión bajo scale y a un Head para verificar los datos al verificar los datos puedo hacer la misma comprobación que hice En el caso anterior solamente que en este caso tengo que entender que los valores no van entre un valor negativo y un valor positivo centrado en el cero o con desviación estándar uno sino que son valores que van entre 0 y el uno pero tiene que mantener esta cuestión de la proporcionalidad como en el caso anterior y puedo chequearlo yendo a comparar este conjunto de datos con el conjunto original no lo vamos a hacer para acelerar la clase pero lo dejo para que lo puedan hacer ustedes vamos a crear Entonces ahora el modelo camins habiendo visto toda esta introducción al procesamiento de datos y vamos a usar de las dos transformaciones que hicimos los datos que han sido escalados bien en el caso de camín tengo hiper parámetros como en todos los algoritmos que hemos visto últimamente y los sirve parámetros son los siguientes tengo el número de clusters que si no lo fijo va a ser definido en 8 por defecto luego n y net que por defecto es 10 que es el número de veces que se ejecuta el algoritmo camis con semilla de sectores diferentes es decir recordarán en la teoría que lo que hacía era tratar de buscar la mejor ubicación del centroide tratando de medir todas las diferencias o todas las distancias que existe entre cada punto y el centroide propuesto bueno eso lo hace Cuántas veces antes de pasar a la próxima iteración 10 veces por eso habla de 10 veces por cada semilla Luego pasamos a maxter que es la cantidad de iteraciones que en la animación que vimos en la clase teórica habíamos visto un proceso de 14 alteraciones Que obviamente es muy chico Porque por defecto fíjense que aquí el valor prefijado es de 300 O sea que es bastante mayor la situación solamente que vimos un gráfico para poder entender rápidamente Cómo funcionaba visualmente este nuevo algoritmo pero el maxi pero en este caso se puede fijar con el número que ustedes quieran pero por defecto será 300 y finalmente el hipermetro en it que fija el método de inicialización de los centroides las opciones pueden ser camíns o Random en el caso de camines que es lo que usa por defecto establece una un primer punto candidato por llamarlo de alguna manera que guarda cierta relación con la estructura de los puntos y en otro caso es Random Que obviamente es aleatorio y no guarda ninguna relación o no observa la distribución de los puntos Para proponer unos primeros candidatos bien Sabiendo esa información ahora vamos a crear nuestro primer modelo con kamins y vamos a fijar el número de cluster en 6 porque bueno viendo Que cuando vimos el conjunto de datos teníamos seis calidades de vino podemos tomar ese parámetro para intentar generar un primer camis con seis clusters luego en el hiper parametriums en enit 10 maxter 300 y Random State Nou es Si no vamos a usar en este caso un Random State Así que creamos el modelo y ahora vamos a ver la información importante justamente de este método que es a donde se ubicaron los seis centroides de cada uno de los clusters esto lo podemos ver justamente con model que es el nombre del modelo y cluster guión bajo centers ejecutamos y ahí vemos en el método de array las coordenadas de los seis centroides fíjense que tengo 6 conjuntos Sí se ve claramente y cada uno de estos conjuntos que tiene tiene 12 valores porque porque justamente estamos en un espacio de 12 variables y Por ende tengo un valor para cada una de las variables y ese valor me da las coordenadas con las cuales va a estar ubicado cada uno de los centroides fíjense que los valores que tengo aquí son los valores que vienen del producto de haber sido escalonados sí es decir él sí el método de transformación que dijimos que utilizamos aquí o sea no son los valores originales pero justamente esto lo que me describe insisto son los valores finales es donde este proceso de modelo de camines ubicó a cada uno de los seis centroides pero también me gustaría ver además de la ubicación de los centroides luego este proceso de creación del modelo finalmente cada uno de los puntos de este conjunto de datos a qué cluster fue asociado Y eso lo puedo ver con el método bajo de model lo ejecutamos Y tenemos una raíz obviamente vamos a ver algunos valores porque es un array que Cuántos elementos tiene tantos como observaciones tiene este conjunto de datos recordemos cuando hicimos el shape que la cantidad de observaciones que tiene este conjunto de otras 1599 Por ende obviamente tenemos solamente una parte pero lo podríamos poner en otro tipo de estructura para poder visualizarlo mejor y justamente es lo que vamos a hacer a continuación vamos a crear un Data frame que le vamos a llamar DF guión bajo View al cual le vamos a dar todos los datos del Data frame DF y luego vamos a agregar una con una nueva que vamos a poner cluster 6 y justamente en esa columna voy a poner estos valores que acabo de muy parcialmente aquí pero ahora los voy a poder ver de mejor manera porque porque justamente van a estar como una columna más del Data frame original en un nuevo Data frame obviamente para poder no confundir un Data frame con otro no alterar el original y voy a poder a través de esta visualización solamente ver los primeros registros pero puedo verlos todos por supuesto poniendo una relación entre el valor de la calidad que presumir podía tener que ver con la identificación del cluster y el número de cluster que finalmente le dio bien hago eso lo ejecuto me voy hasta el final y aquí tengo Quality y cluster 6 obviamente es difícil que sean iguales porque ustedes Recuerden que Quality va de 3 a 8 y los cluster empiezan desde cero pero aquí Lo importante es poder tratar de visualizar si hay una relación entre la calidad del vino y el cluster que no se altera por ejemplo acá a las 5 le da el cluster 4 a las 5 al 4 a las 5 al 4 sé que hay una relación Más allá de la parte numérica que dijimos Nova coincidir porque son distintos rangos de valores Pero esto es importante a las 6 El 3 a las 5 el 4 es que fíjense que hay una relación aquí la relación no está marcada 71 74 554 pero fíjense que en el caso de el 5 con el 4 es bastante marcada la relación y eso es lo que podemos investigar a ver si existe esa relación pero ahora viendo este resultado de esta comparación que mencionábamos recién podemos llegar a preguntarnos si adoptamos un criterio lógico a la hora de determinar la cantidad de clase tomando como referencia la cantidad o la cantidad de opciones de variantes de calidad de vino que existían eso lo podemos verificar justamente con una técnica que se llama técnica del ya vamos a entender porque se llama técnica del codo porque tiene que ver con una forma gráfica que vamos a ver a partir de este código que tenemos aquí en este caso voy a crear una matriz inercia ya vamos a ver por qué y luego voy a crear un Ford que va a alterar de nueve veces a partir de una variable que le he dado Llamar n cluster Cuál es la idea de esto es la idea de poder crear 9 modelos diferentes cambiando el valor de la cantidad de clutch en el primer caso va a ser un cluster de uno nada más y después de allí un modelo con dos cluster tres cluster el resto de los hiper parámetros están con los mismos valores que he utilizado antes voy a volver a crear un modelo con el conjunto y una vez que lo haga voy a incorporar a esta matriz inercia un valor que se llama justamente Inercia de cada uno de los modelos y lo ejecuto y a continuación voy a graficar la resultante de esta matriz que Acabo de crear aquí pasamos a la parte del gráfico voy a crear justamente un gráfico donde Ven aquí voy a utilizar al valor de los valores perdón de la matriz inercia le digo que voy a imprimir un Rango de una y este concepto final o este elemento final se llama marker es el tipo de dibujo en el cual voy a poner cada uno de los puntos está OK quiere decir que va a dibujar un círculo en cada uno de los puntos Sí el resto son los títulos y bueno el formato del tamaño de el gráfico que voy a dibujar bien a partir del gráfico esto se ve mucho mejor vamos a ponerlo en un tamaño más chico para que se vea mejor ahí lo tenemos que me muestra Esto me muestra un gráfico que me dice en cada uno de los casos Cuál es la inercia para cada uno de los modelos que he creado desde el cluster número uno al cluster número 9 este valor de Inercia que está dentro de este Rango de valores que está aquí me indica algo que me va a permitir encontrar un elemento que se llama codo aquí se llama codo qué es lo que se busca como codo es la parte que se asemeja digamos el codo de un brazo de un ser humano es la parte donde la inercia empieza a dejar de tener fíjense que en este caso Cuando paso del valor 1 al valor 2 Tengo una línea que va muy marcadamente hacia abajo de 2 a 3 eso va siendo un poquito menos intenso de 3 a 4 de 4 a 5 Es decir va a llegar un momento en el cual Este cambio o este nivel de cambio no va a ser tan abrupto Por ende en ese punto que por él no puede ser uno puede ser uno o dos que en donde se marque ese cambio va a ser la referencia de la cantidad de cluster que debería ser la adecuada para ese modelo en este caso podemos presumir que esta inercia deja de tener efecto en el 5 y en el 4 si quizás en el 6 ya vemos que ya esta tendencia no cambia mucho ya el nivel de graduación o de dependiente no tiene una gran alteración prácticamente es del 5 pasa eso en el 4 ya hay un cambio un poco menos significativo pero significativo al fin y ya fíjense que prácticamente del 5 al 9 parece que es una recta con lo cual esa tendencia no tiene una alteración al menos una fuente con lo cual Esto me indica que probablemente más que el 6 los valores candidatos mejores para generar un buen modelo en relación la cantidad de cluster puede ser cuatro o puede ser Cinco más que 6 con lo cual de estas dos opciones el 5 y vamos a crear un modelo ahora en lugar de con 6 cluster como decimos antes con 5 plazas viene aquí está entonces lo mismo que hicimos antes pero solamente que en N cluster en lugar de poner 6 le ponemos 5 Así que lo creamos al modelo vemos ahora los centroides obviamente tengo un centro de menos si tengo o sea un conjunto de valores que identifica un centroide menos que antes pero siempre referido con 12 valores Porque sigo teniendo las mismas 12 variables que antes sí puedo ver rápidamente las etiquetas como antes pero prefiero hacerlo también como lo hicimos antes a partir de DF View con lo cual ahora en DF View voy a agregar una nueva columna que le voy a poner cluster 5 con este modelo o los Label de este modelo que Acabo de crear que es el modelo el 5 a diferencia del otro que tenía otro nombre lo ejecutamos y acá tengo Entonces ahora si me voy hasta el final para cada observación el valor de la calidad el número de cluster que le asignó con el modelo de clusters y el número de cluster que le asignó con el modelo de cinco plast obviamente acá en algunos casos como en estos tres primeros casos hay una relación bastante marcada Aquí también 540 Así que se mantiene con un patrón pero en el momento ese patrón se va a alterar Porque bueno porque tengo un cluster menos obviamente no puede haber una linealidad si tengo una opción de cluster menos pero bueno es interesante para poder insisto como antes investigar justamente Cuál fue el resultado de una opción o de otra lo importante que este gráfico que tenemos aquí nos permite poder saber cuál es la cantidad de clusters ideal para bueno el modelo o el conjunto de datos que intentamos modelar ahora vamos a ver cómo hacer una predicción con este modelo camins y lo hacemos como hicimos antes con el método predic sobre el modelo el último modelo que hicimos que le hemos puesto model 5 y luego lo ponemos una variable resulte a partir de la cual vamos a ver qué cluster le corresponde a ese conjunto de datos que le voy a pasar para que haga la predicción estos datos los voy a sacar de el datasette original lo voy a tomar el primer registro y tengan presente que si yo he armado un modelo en base a datos escalados pues entonces los valores que tengo que ponerle de entrada para que arme la previsión También tienen que estar escalados sí Entonces esto es importante tenerlo presente por eso estoy tomando el primer registro los datos de primer registro que tenía aquí de el conjunto original después de haber sido escalado Entonces tenemos 024 039 0 0 6 8 bueno son todos los datos que estoy poniendo aquí justamente como valores de entrada para establecer la predicción y luego imprimo la variable resulte nos da como resultado 0 es decir que este conjunto de valores correspondientes a esta primera observación tiene pero asignado pero como predicción el cluster número 0 si vamos a la el Data frame que habíamos armado aquí donde teníamos el valor original recuerdo a la calidad digamos que había determinado el especialista el valor en el caso de 6 cluster y el valor en el caso de cinco cluster vemos Que en la primera observación el cluster que le corresponde es el 0 Obviamente que esto lo podríamos variar con otros valores diferentes y ver a qué cluster le asigna Pero bueno quise tomar Este primer elemento para ver justamente que lo que de alguna manera había sido estipulado de entrada cuando se definió el modelo sigue teniendo vigencia cuando lo hago dentro del contexto de una predicción o sea los mismos valores que usé y que se determinó cuando se creó el modelo que correspondían al cluster 0 al ponerlos como predicción también arrojan el valor con lo cual es una buena manera de testear este este conjunto de datos Y este modelo bien Ahora nos preguntamos si esto de algún modo así como lo vimos en algoritmos anteriores en el caso de el bosque del árbol y de otros algoritmos podemos verlo en un gráfico bueno el problema aquí que no se olviden de que tenemos una gran cantidad de variables de entrada Sí y eso hace de que es complicado poder representarlas en un gráfico ya que tenemos Claro que un gráfico de más de tres dimensiones no se puede visualizar entonces la respuesta a esta pregunta es si lo podemos ver en un gráfico pero aún teniendo esta cantidad de variables de entrada no esta cantidad de dimensiones pero para eso tenemos que aplicar un concepto que ya vimos que es el tema de la reducción de la dimensionalidad Sí pero aquí no con los propósitos que no habíamos hecho antes que era para poder desarrollar un modelo que fuera más rápido sino para poder graficarlo es decir vamos a cambiar una estructura de 12 dimensiones a una estructura de solamente dos dimensiones para poder graficarlo como corresponde así que bueno vamos a tomar el caso del modelo de cinco clusters y vamos a aplicar el psa que ya no vimos antes que es esta librería que me permite justamente poder este hacer una reducción y aquí lo que le voy a poner es en lugar de como hicimos en otra oportunidad de decirle que quiero un cambio una reducción de dimensionalidad que no me dé una pérdida superior al 005% aquí le pongo Cuántos componentes y cuántas dimensiones quiero que tenga en este momento este traspaso esta transformación Así que está la información que le pongo aquí luego le pongo justamente el conjunto de datos que deseo transformar y finalmente lo que hago es crear justamente este ese modelo transformado para poder determinar a partir de ahora cuáles son los nuevos clusters Por qué Porque ya no voy a tener la cantidad de cluster que tenía antes en 12 dimensiones voy a tener otros clusters porque mi realidad ahora es de dos dimensiones entonces lo que tengo que hacer es también redimensionar la distribución de los clusters sí acuérdense Con 12 dimensiones en una cuestión con dos dimensiones va a ser otra completamente distinta y finalmente un Ford de 5 ciclos porque tengo no se olviden este cinco cluster y en cada uno de ellos voy a generar las siguientes acciones en principio la distribución de los puntos en dos dimensiones Sí para esta nueva este nuevo formato que adquirido luego se establece un color diferente para cada cluster si con esta línea que está aquí se determina Qué tipo de dibujo quiero Que aparezca en el punto que es de nuevo esta o que representa un punto un círculo Sí luego si ese ese círculo lo quiero con un borde Y en este caso le digo que sí que quiero que ese círculo tenga un borde y que ese borde sea de color negro y finalmente una leyenda que ya vamos a ver que va a aparecer donde se va a indicar el número de cluster como cuando se hace una gráfico en Excel que aparece una tabla con la referencia de cada uno de los elementos esta gráficando en este caso la referencia de los ciclo bien Luego de que hago ese dibujo para poder representar porque hago un Recuerda que son cinco cluster y tengo que hacer estos cinco veces con cinco colores distintos y con cinco números y referencias de cluster distinto después lo que hago es representar cada uno de los centroides lo que grafiqué hasta el momento hasta ahora son los puntos es decir cada una de Las observaciones que están insisto en un gráfico de dos dimensiones ahora lo que tengo que hacer a continuación es poner el centro aire que le corresponde a cada caso ese centro Lo voy a hacer con un marker asterisco que lo que va a hacer va a dibujar una estrella y este lo que quiero que esa estrella sea de color negro y justamente la idea también es bueno tengo que referenciar la ubicación de cada centroide y finalmente la leyenda centroides que va a aparecer insisto en esa sección donde se bueno identifica cada uno de los componentes del gráfico explicado todo esto pasamos a dar finalmente el gráfico y vemos aquí la resultante de todo lo que explicamos el título general que lo último que hicimos el gráfico de dos coordenadas los puntos fíjense que cada uno de los cinco grupos de puntos tienen un color diferente Violeta azul rojo verde y naranja cada uno de estos de estas referencias están aquí en esta tabla ven que dice que las terceras y el color cluster 1 en color así hasta el cuarto que era lo que yo les había comentado antes y también me dice que las estrellas van a representarnos fíjense que en cada cluster existe una estrella negra que representa al centroide por eso dijimos recién todos estos puntos no estaban en un gráfico de dos dimensiones los pasamos a un gráfico de emisiones por eso hicimos un pc una transformación de la ubicación de cada punto y con los centroides también Hicimos lo mismo si una transformación de la ubicación de centroide Porque si esos puntos estaban en una estructura de 12 dimensiones que es difícil imaginarla pero este es así tenían esos centros de 12 coordenadas ahora tendrán que tener dos coordenadas por eso también tienen que hacer haber pasado por un proceso de capacidad de transformación y de ese modo cada punto tiene dos coordenadas y cada centroide tiene dos coordenadas bien en este caso todo lo que explicamos antes se ve claramente Pero puede ser que nos surjan una duda como esta pregunta que tenemos aquí este gráfico me muestra muchos puntos superpuestos especialmente en nuestros colores azul y violeta sí azul y rojo también este No tanto digamos el rojo con naranja rojo con verde fundamentalmente rojo con azul vamos a decirlo Entonces esto me puede hacer pensar de que quizás la distribución que yo he tenido aquí no es la más acertada sin riesgo a equivocar Me podría decir que el Violeta está bastante bien representado en naranja también el verde también Pero quizás esta distribución de los puntos rojo y azul con tanto nivel superposición nos llevaría a pensar que quizás el rojo y el azul podría ser solo cluster y Por ende tener un modelo de cuatro cluster y no de cinco ojo con esto este gráfico no nos tiene que confundir esto es una forma de intentar poder visualizar Este modelo en un gráfico Que obviamente tiene una pérdida de información muy grande pasé de 12 dimensiones a dos una imaginen ustedes esto con lo cual esta superposición en un ambiente de 12 dimensiones no es tal esto pasa Simplemente porque transforme algo de 12 dimensiones a dos y entonces a partir de ese aplastamiento de datos justamente hay datos que superponen bastante bien ha salido porque fíjense que hay tres cluster que son bastante diferentes tienen muy pocos puntos que superpone quizás lo más este representativo en cuanto a superposición lo representan los grupos azules los que nazcan Azules y rojo Pero insisto esto es simplemente una cuestión de percepción porque es una transformación en la realidad estos grupos son independientes y no se mezcla como aquí finalmente les dejo un apéndice como siempre algunos ejercicios adicionales para que los vean ustedes es un ejercicio muy interesante porque permite ver cómo puede utilizar un modelo de kamins para reducir la dimensionalidad de una imagen es muy interesante el ejercicio véanlo después me consultan pero está explicado obviamente paso a paso para que ustedes lo puedan ejecutar bien hay un archivo que tienen que levantar desde esta URL y se trata básicamente de esta imagen que tiene una dimensión muy grande es transformada en otra imagen que tiene una dimensión mucho más chica que ustedes la Ven aquí y la dimensión no pasa por el tamaño de la imagen sino por la cantidad de píxeles que tiene que obviamente aquí ustedes no lo pueden percibir pero lo van a percibir cuando termine el ejercicio porque van a poder comprobar que esta imagen original tiene una dimensión de cabeza que no es la misma que es la que tiene esta otra bien hasta aquí llegamos con esta clase y Por ende nos vemos en la próxima clase hasta aquí llegamos con esta clase Te espero en la próxima y última clase del módulo de Machine learning  Titulo: Clase 13 (parte1) Curso Inteligencia Artificial \\n URL https://youtu.be/NDor8fLB2fg  \\n 1900 segundos de duracion \\n Hola bienvenidos Esta es la clase número 13 del curso de ia y última clase del módulo de Machine learning en ella Vamos a analizar proyectos publicados en sitios web de esta forma nos vamos a preparar mejor para la evaluación final de este módulo vamos por ello    Hola a todos Bienvenidos a la décimo tercer clase de este curso de Inteligencia artificial de ifes y última clase del módulo de Machine learning en esta clase no vamos a ver algún contenido nuevo algún conocimiento nuevo sino que vamos a reforzar los conocimientos adquiridos aquí fundamentalmente en este módulo de Machine learn para ello lo que vamos a buscar es investigar o inmiscuirnos en el mundo de los proyectos que ya han sido desarrollados y publicados con algunos de los algoritmos que nosotros aprendimos En ese sentido en esta clase vamos a trabajar con esa idea yendo a algunos sitios que justamente son los que publican este tipo de proyectos para tomar uno de ellos y paso a paso seguir un proceso de cómo hacerlo de esa información y cómo traerlas a nuestro entorno para poder ejecutar la practicarlas y aprender un poco más de esto así que sin más que agregar empezamos con la clase bueno como decíamos al principio de la clase entonces aquí lo que vamos a tratar de hacer es ir indagando una información que vamos a tomar de uno de los sitios que mencionamos al principio y ir haciéndolo paso a paso como si estuviéramos juntos digamos no para ir recorriendo esta experiencia de un modo más real al que les pasaría a ustedes si lo hiciesen por su cuenta en el caso de esto que vamos a hacer vamos a buscar un algoritmo o información del ejemplo que tenga que ver con Random Forest con bosques aleatorios y aquí tenemos Bueno lo que mencionábamos hoy el sitio cabe y el sitio harring Face lo estoy poniendo a título de ejemplo tenemos presente que información para estudiar y bueno o Fuentes para para estudiar en muchas podemos recurrir a libros podemos recurrir a publicaciones que están en otro sitio web los que se llaman regularmente papers que son publicaciones de investigaciones que hacen muchos este muchas personas que gracias a Dios publican sus cosas en sitios públicos también podemos recurrir a bueno las fuentes como videos hay mucha información en vídeos de YouTube en canales de YouTube exclusivos este que se dedican digamos a este tipo de temática y bueno un conjunto de herramientas que siempre están disponibles para nosotros los voy a olvidar de eso que es muy importante que vamos a usar un poco algo eso aquí Bueno son infinitas y desde dos muy variadas las fuentes de información solo aquí nos vamos a centrar básicamente en estos dos sitios web puntualmente en cable pero vamos a recorrer un poco estos sitios aquí tenemos cable y bueno como verán Aquí estoy logueado con mi nombre y vemos que tenemos bueno algunos secciones digamos del sitio donde la primera es competiciones que es un poco la filosofía de Call porque propone bueno temas a desarrollar o temáticas a bordar con premios y bueno elige los mejores proyectos vinculados al tema que propuso como como elemento de competición después tenemos obviamente Data set que podemos recurrir y bajar entonces que pueden estar vinculados a un proyecto o no modelos código de discusión si vamos a Face También tenemos modelos Data set documentos bueno espacio de discusión etcétera etcétera En realidad lo que vamos a hacer aquí a título de muestra vamos a tratar de buscar información vinculada a Random Forest Sí porque justamente el tema que voy a proponer para esta clase y bueno como verán aquí cuando yo pongo en el buscador la palabra me aparece un montón de cuestiones vinculadas a ese tema notebooks en otros casos discusiones bueno en otros casos aparecerán solamente el conjunto de datos vinculados a un proyecto y no el proyecto en sí mismo en otro caso pueden aparecer más del Notebook una explicación muy pormenorizada y detallada de Por qué la persona que escribe ese Notebook y trabaja sobre ese tema ha hecho lo que ha hecho paso a paso como si fuese una clase bueno Y también tienen Este no es menor el tema de cursos es importante que hay pequeñas a sus cursos que también está subidos a estos sitios bien si voy hago lo mismo también pongo Random Forest insisto el tema que vamos a elegir nosotros pueden poner cualquier cosa y Bueno aquí tenemos en principio bueno proyecto Pero bueno pues este que sean muchos más si voy a modes también tengo bueno ya rubro digamos del tipo de cosas que quiero ver y no con el buscador obviamente encarguen lo mismo Bueno este todas esas cuestiones es lo que ustedes tienen como herramientas para ir buscando y van a encontrar proyectos que tienen que ver con el interés que ustedes buscan la temática que abordan es la que ustedes les interesa o por ahí no están buscando nada y les aparece un tema y le parece interesante y bueno van a encontrar proyectos muy complejos por ahí con algunas herramientas que ustedes no manejan bueno bienvenido que puedan investigar nuevas herramientas y bueno en otros casos va a ser simplemente el código sin mucha aplicación en este caso nosotros vamos a trabajar sobre algo que encontré que me pareció muy interesante que es un algoritmo de Random Forest para predecir la posibilidad de que una persona pueda tener una cardiopatía en virtud de la serie de parámetros que bueno tienen o forman parte de un conjunto de datos sobre lo cual trabaja la base de este ejemplo Este ejemplo que tenemos aquí trabaja por Random Forest y también con un algoritmo de regresión logística que también vamos a ver la idea aquí es poder empezar a trabajar con este código llevándolo a un colap nuestro a un nuestro e ir copiando y pegando cada una de las partes esto muchas veces no es necesario porque muchas de estas publicaciones tienen un gift Card con lo cual yo puedo bajar el github o clonarlo y ya tengo todo mi máquina sin tener que estar copiando y pegando Lo que pasa que en principio no vamos a tomar como dije antes todo el código de este ejemplo sino parte con lo cual eso va a ser que vayamos de a poco pero también vamos a ir copiando y pegando como una fase de ir aprendiendo paso a paso algunas cuestiones importantes que tienen que ver con ir viendo cada una de las cosas que componen este proyecto sino también Ver algunos truquitos que tienen que ver con el día a día del trabajo en cola Sí en Notebook que no por menores son no es importante digamos abordarlos y tenerlos presentes si es parte de la práctica diaria y es una forma también de aprender esas pequeñas cosas que nos facilitan el trabajo en su sin mucho más que agregar a todo esto que hemos dicho vamos a empezar con la práctica concretamente como decíamos recién si vamos aquí a este menú de los tres puntos puedo ver que puedo abrir este documento este código de este documento en Google notebooks es decir con Google o bien Download es bajar todo este código a mi disco también tengo la sección vamos a cerrar aquí la acción input en la cual puedo acceder a un conjunto de datos que es el que vamos a usar en esta práctica pero vamos a hacer una suposición para poder aprender algo más y trabajar un poquito más con github que tantas veces lo hemos mencionado y vamos a suponer que ese conjunto de datos no lo tenemos accesible con lo cual lo que yo voy a hacer va a ser Tomar nota bien del nombre de ese conjunto de datos que se esté que está aquí y lo vamos a ir a buscar a Google aquí y lo buscamos y vamos a buscar a un github aquí Tenemos uno que pueda tenerlo entonces entro este veo que aquí está el csv me meto en él y acá me ofrece una View se acuerdan que esto lo mencioné muchas veces especialmente en la en varias prácticas Digamos si desde las primeras este las primeras clases de análisis de datos donde yo podía abrir un conjunto de datos a través de una URL que me ofrecía github que empezaba con estaba en formato r aw bien Vamos a hacer aquí clic en este que me ofrecen y voy a ver que aparecen los datos en el formato que habitualmente conocemos como csv Esto me da dos opciones primero yo puedo copiar esta URL que es lo que vamos a hacer Sí vamos a copiar esta y la vamos a abrir desde el cola que vamos a fabricar o bien Yo podría pararme aquí con el botón derecho y darle guardar como y guardarlo en la parte que yo necesite de mi disco si lo que me aparece por el nombre que yo quiera poner y la extensión SD bien no vamos a hacer eso vamos a copiar esto que está aquí y no lo vamos a llevar al Notebook en el cual vamos a trabajar toda esta práctica si por el momento lo voy a copiar acá sin mucho más y vamos a en principio antes que esto ver todo el tema de las librerías que vamos a necesitar para esta práctica con lo cual voy a abrir una celda nuevamente pero la quiero poner arriba con lo cual como está abajo hago clic en la flecha y la pasó arriba me voy al cable y vamos a tomar las dos primeras librerías que son nampai y pandas y también tengo más abajo estas otras librerías y me la llevo también al cola bien este mapa lo vamos a sacar por ahora y ahí tenemos las librerías que vamos a usar para este proyecto le vamos a quitar estos comentarios de aquí que no me hacen falta Y tenemos dos libres nuevas que ya la vamos a Bueno a comentar más adelante Así que empiezo por importar libres y ahora tendría que importar este este conjunto de datos con el formato de URL que tiene pandas Así que vamos por eso ahí tenemos entonces la URL puesta en una variable que se llama URL y abrimos un Data frame con los datos de esa URL y le ponemos tarjeta s bien acto Seguido lo que me sugiere es hacer un Head Así que vamos con eso lo tomamos de aquí y lo pegamos y ahí tenemos ven los mismos datos que tenemos aquí obviamente esta línea la he salteado porque esta línea considera que yo tengo el conjunto de datos en el disco y lo levanta porque he usado la forma de la este URL con el formato bien Ahora tengo la misma situación habiendo tomado los datos de un lado de otra manera de una manera o de otra Perdón este tengo ya los datos en mi colap a partir de haberlos podido ver con el gel Así que pasamos a la siguiente instrucción que es un shape para ver justamente si la cantidad de observaciones y de características es la que corresponde Así que ahí tenemos 319.795 y 18 características voy al cable y efectivamente tengo eso tengo también aquí un título Sí así que vamos a también copiar los títulos de acá con lo cual voy a tomar esto lo pinto lo copio y me lo llevo a mi cola tengo que ponerlo en una celda de tipo texto no una celda de tipo código Entonces lo pongo aquí lo voy a poner en negrita con lo cual lo pinto y hago clic aquí y lo voy a poner en con un tamaño que se puede regular del siguiente modo yo pongo tres numerales eso me da un tamaño chico con dos uno mayor y con uno uno más grande que todas las opciones anteriores con la lógica que esto implica así que bueno ya tengo aquí mi título pero lo quiero poner arriba como está Sí en el cable arriba del shape Así que lo que hago nuevamente es usar la flecha y lo paso sí bien a continuación nos vamos a Carl y lo que hago ahora es Buscar a través del método info justamente información de las características de cada una de justamente las características que tiene este Data frames Así que lo copio lo pego agregó una celda más de código y lo ejecuto y tengo la información donde veo que muy pocos datos son de tipo numéricos sí BM y fiscal gel mental healthday el resto son de tipo objeto mayoritariamente son datos categóricos sí es decir que son datos que deberíamos transformar Si queremos usar esa característica para nuestro modelo a continuación vemos que lo que propone es trabajar con la categorización de los grupos de edades de qué se trata esto Bueno vamos un poquito más arriba y vemos Que justamente está la característica categoría de edad que está descrito de modo muy particular con rangos de edades y aquí el autor de esta publicación propone reemplazar eso por textos que digan joven adulto viejo o muy viejo entonces a través de cada una de estas instrucciones dice que reemplace esos rangos por cada una de estas correspondencias de tipo texto sí es decir que Entonces vamos a aplicar eso vamos a ir aquí a copiar este texto como venimos haciendo lo pegamos en el cola agregando un elemento de tipo texto sí como siempre lo ponemos en Evita y le ponemos tres signos numerales y ahora vamos el código lo pegamos aquí Y de paso también pegamos el distrite que me muestra a continuación hecho esto hacemos las dos acciones ahí la primera ahí la segunda y con el display puedo ver lo que vimos en las clases pasadas una descripción completa de las características de tipo numérica donde vemos la cantidad al promedio a la desviación estándar el mínimo el máximo y los percentiles 25 50 y 75 que me propone ahora el autor bien ver si hay valores nulos si esto ya hemos visto mucho así que tomamos el título lo llevamos al colar agregamos un texto bien vemos los valores nulos y no hay ninguna característica que tenga valores nulos bien Vamos al cable que lo quisiera a continuación verse duplicados esto es importante porque muchas veces la información duplicada se considera redundante y Por ende se puede eliminar concretamente aquí el autor lo que hace es eso proponer eliminar los registros duplicados así que vamos con el código lo ponemos lo ejecutamos Y tenemos la información de que de la totalidad de observaciones Perdón que tiene este conjunto de datos que dijimos es 319.795 hay 35.830 registros duplicados con lo cual lo que propone aquí el autor es eliminar esos duplicados Así que lo que vamos a hacer Bueno lo hacemos con esta instrucción que es Drop duplicate y la ejecutamos bien lo que deberíamos haber ahora y que acá el autor no propone pero yo sí Les propongo hacerlo es ver cuántos registros quedaron dado que eliminé los duplicados Así que vamos a volver a hacer un shape y ahora vemos que ya no hay más la cifra anterior que repito 319.195 sino que ahora habiéndole restado el duplicados me quedaron 283.965 observaciones veamos que propone el autor a continuación la visualización de la distribución de datos esto es muy común y es muy importante para ver cómo están distribuidos y para eso vamos a recurrir a una herramienta gráfica donde nos va a mostrar justamente si los distintos valores que tienen cada una de las características Están bien distribuidos están más concentrados en un valor respecto de otro bien copiamos el título primero y luego el código como venimos haciendo hasta ahora bien acá tengo entonces la variable categórica fixtures que lo que hace es justamente tomar del Data frame de effect todos los tipos de variables de entrada sí de características que son de tipo objeto Por ende de tipo categórica luego marco Cuál es el formato el tamaño el size del gráfico que voy a hacer y finalmente el Ford en el cual voy a poner de categórica al features primero la i que va a ser lo que me va a permitir usar como una variable que va a indicar cada uno de los ciclos empezando de Cero en adelante y luego con feature tomar los nombres de cada una de las características ahora categórica al feto tiene un montón de elementos que son los nombres de cada una de las variables de entrada tipo categórica enumerate lo que hace es enumerar justamente la cantidad de elementos que están dentro de esa variable y Por ende esa cantidad de elementos es lo que me va a dar la referencia para que está ahí vaya Sabiendo desde cero hasta Qué valor tiene que asignar luego hago el suplots y luego uso aquí sns que si vamos arriba recordemos que es una de las librerías es sibon alias que importamos al principio y que justamente sirve como patrollip para hacer gráficos el resto de las referencias que tiene que ver con el resto del código se la dejo aquí impresa en la pantalla para poder seguir rápidamente con la ejecución de esto y Ver el resultado como vemos aquí tenemos para cada una de las variables categóricas los valores posibles en este caso por ejemplo no Y sí y la cantidad de elementos no Y la cantidad de elementos sí Ahora voy por ejemplo con fumador sí o no veo que tengo cuántos son fumadores y cuántos no son fumadores aquí si la persona es bebedor o no tengo no y tengo sí vamos a tomar estos dos casos por ejemplo en el caso de los fumadores está bastante proporcionada la cantidad de las personas que fuman y no fuman en el caso de los alcohólicos son mucho más significativos los No que los sí En el caso de vamos a tomar otra característica es un poquito más abiertas sí bien en este caso de la edad se acuerdan se irá viejo muy viejo adulto joven fíjense que está distribuida de una manera en la cual obviamente entre el viejo y el joven hay una diferencia bastante importante en el caso del sexo el género sí está Igualmente distribuido igual esto lo podemos analizar para cada uno de los casos para entender la posible influencia que puede tener en cada una de las de los modelos que vamos a desarrollar a continuación bueno la variedad de tipos de datos en el caso de las variables categóricas de sí o no o si son más variantes en cada uno de los casos como aquí también tenemos bueno bueno muy bueno pobre excelente bueno Esto es un elemento muy común para poder analizar los datos antes de empezar a desarrollar el modelo ahora vamos a hacer lo mismo pero para las variables no categóricas que es lo que propone el autor a continuación ver lo mismo pero para las numeric fitos si no para las categóricas fitos Así que también copiamos este código que tiene una lógica exactamente similar a la de recién solamente que el gráfico es de otro tipo así que vamos a agregar el código aquí no pegamos lo ejecutamos en el caso anterior el gráfico era de tipo count plot en este caso es de tipo East lot estos son los famosos histogramas que ya hablamos alguna vez y fíjense que bueno puedo ver si los datos están muy concentrados o guardan este dibujo que dijimos que en la forma de la distribución normal o sea la forma acampanada bueno en el caso bmmi tiene una distribución de este tipo en los otros restantes casos está muy concentrado respecto de los valores mínimos sí en este de cuánto tiempo duerme este bueno está como con un gráfico bastante raro no con algunas puntas muy marcadas pero un poquito más distribución ahora el autor nos propone ver el ratio de bueno gente que tuvo problema de corazón entonces este tomamos el título lo llevamos a nuestro cola como venimos haciendo subimos y vamos a traernos el código que tiene que ver con un gráfico de tipo país ese de tipo torta Sí entonces típico gráfico de torta muy conocido y podemos ver cuántas personas han tenido problemas de corazón Eso Perdón 9,54% Cuántas no la han tenido que es el 90.46% . bien volvemos al cable y ahora lo que tenemos que hacer es el Level coding de qué se trata esto esto ya lo vimos no de esta manera llamado o ni la de la forma que lo vamos a hacer En parte sí en parte no tanto pero tiene que ver con la transformación de las variables categóricas en No categóricas ya hicimos el análisis de todos los valores los categóricos no categóricos el ratio de la variable en este caso Target objeto que es si la persona tiene o no una cardiopatía y en este caso vamos a utilizar primero vamos a hacernos el título Antes que nada Ahora sí vamos a traer la librería de processing Never vamos más abajo Acá hay mucho comentado por lo tanto obviamente no vamos a tomar eso esta línea que crea la variable l de tipo Level encoder justamente y ahora sí el resto del código que es lo que hace bueno obviamente lo que hace es generar un Ford para que cicle Tantas veces como variables categóricas existen sí bien acá Bueno este código es el que justamente me dice en la variable categoría de calcos cuáles son las variables de tipo categórica lo que va a hacer aquí es definir si el unic de esa columna supongamos en el caso de tomar la primera columna mido la dimensión del Unique es decir cuántos valores únicos o diferentes tiene esa característica si esa característica tiene menor o igual a 2 quiere decir que es de tipo binaria como acá dice la el comentario Sí con lo cual yo voy a transformar eso en un cero o en uno depende Si es una opción u otra y en el caso que las opciones sean más de dos o sea que no sea de tipo binario que vamos a aplicar vamos a aplicar un wat hot encoder pero no justamente la librería que se llamaba Así que usamos otras veces sino que este esta librería que se llama get dames que hace un proceso muy parecido al que hicimos pero bien es otra herramienta que eligió aquí el autor en lugar del que usamos nosotros y luego hacemos un Head para ver el resultado lo ejecutamos bueno y aquí vemos el resultado obviamente vamos a tomar un caso de una variable que tenía dos valores posibles Por ejemplo si era fumador no tenemos sí y no vamos a ver si lo encontramos aquí fumador acá tiene uno y cero uno y cero uno sí tenemos otros casos donde tenía más de dos valores posibles por ejemplo este que está aquí en hertz vamos a buscarlo bueno Y aquí tenemos el proceso similar a lo que nosotros habíamos logrado con el water que es en el caso de gengels excelente justo bueno pobre y muy bueno y hace el mismo trabajo acá son cinco valores posibles el primer la primera observación es muy bueno con lo cual tiene uno es muy bueno y cero en nosotros cuatro restantes Sí vamos al caso del tercero por ejemplo es justo tiene uno el justo y cero cero cero sí Ese es la misma mecánica solamente de acá uso se ve aquí arriba get damise y no el método What holtencoder Más allá de que el nombre que le usa para identificar es justamente porque ese es el nombre del proceso después hay método de llama así y otro que no bien hasta aquí con la transformación de las variables y ya estamos entonces en condiciones de poder empezar a generar nuestro modelo para el cual previamente como bien propone aquí el autor tengo que empezar por dividir nuestra x y nuestra y aquí tenemos quien lo hace con hilok donde justamente recordarán y lock ya lo hemos usado en las primeras clases justamente de análisis de datos donde en el primer caso toma a la variable Target con hilo referenciando el elemento número 0 y el resto de los elementos de uva al final los toma como el resto de las variables de entrada Sí porque es cero porque justamente el primer valor Sí el 0 es el valor Target y el resto son el resto de las variables de entrada tengo mi y mix Bueno lo ejecuto y ahora vamos a pasar a hacer lo que ya sabemos también que es la generación de nuestro modelo a partir de incorporar vamos a sumar algunas librerías pandas ya la tenemos metrix no la tenemos vamos a llevarla qué más 30 Split estas dos como les dije es parte del código que no vamos a utilizar por lo tanto no me la llevo estándar estas cuatro sí la de la que ahora sí estándar escáner porque vamos a escalar los datos Forest justamente le he hecho el ejercicio y la de regresión logística que es la que vamos a usar que también es parte de este ejercicio como les adelanté al principio bueno veamos esta y nos la llevamos bien ahora ejecutamos todo esto y vamos a pasar a hacer aquí tareas que ya conocemos bastante que es el tema de separar el conjunto de entrenamiento el conjunto de Test y descargar los datos aquí lo he puesto en una sola celda Bueno lo copiamos en una celda no hay problema son todas cosas que ya no son familiares Así que no hace falta que estén separadas las ejecutamos bien y creamos finalmente nuestro modelo bien en la siguiente celda llevo toda lo que hacemos es primero entrenar el modelo luego hacer las predicciones siempre con los datos separados porque fue lo que hicimos anteriormente medimos el aquiura Sí sí el score en base a los datos escanados el x-train y el que traen instalado Y luego medimos el Arquero así entre y finalmente lo imprimimos para ver los resultados bien como verán tardó bastante y aquí tenemos el resultado final tenemos una iglesia muy alto para el conjunto de entrenamiento y uno no tan alto y bastante diferente para el conjunto de Test lo que vamos a hacer ahora es también se acuerdan que vimos que una forma de visualizar El discord digamos de un modelo es esto que usamos y también la matriz de confusión solamente que la matriz de confusión la vamos a hacer de un modo gráfico porque justamente es lo que propone aquí el autor de esta publicación Bueno fíjense qué manera tan particular y tan interesante de ver los datos Recuerden que la matriz de confusión lo que hacía era en este caso tenemos valores 0 y 1 es decir cuántos ceros los predijo como 0 que sería esto Cuántos cero lo predijo como uno mal predecidos Cuántos unos los predijo como cero mal predecido también y cuántos uno lo predicó como uno que sería lo correcto si pagamos que los elementos la diagonal principal son los que marcan los aciertos y la otra diagonal y lo contrario en este caso fíjense que poco lo que me predice a mí aquí el bajo score conjunto de Test Es que tengo un alto número de unos que han sido mal predecidos fíjese que son más los uno mal predecidos que bien predecidos y no más sino mucho más y prácticamente aquí estamos hablando de casi ocho veces más este valor que este valor lo cual me habla de que los casos de predicción en valor 1 digamos que sería la persona que sí Tuvo una cardiopatía Bueno Este modelo no ha sido eficiente si en el otro caso bien con esto cerramos esta parte y vamos a ver a continuación lo que interpone el autor también que es la alternativa de usar el método de regresión logística el modelo perdón de regresión logística para intentar otra forma de ver si podemos lograr un mejor hasta aquí llegamos con esta primera parte de la clase Te espero en la segunda parte nos vemos Titulo: Clase13 (parte2) Curso de Inteligencia Artificial \\n URL https://youtu.be/2jAJjJ2WO3Y  \\n 1095 segundos de duracion \\n Hola bienvenidos Esta es la clase número 13 del curso de ia de ifes en ella Seguiremos con lo experimentado en la parte anterior y daremos las pautas para la evaluación final del módulo  Bueno Hola a todos nuevamente continuamos con la clase número 13 de este módulo de Machine learning última clase de módulos segunda parte donde en la primer parte habíamos visto un análisis de un proyecto en Google y vamos a continuar con ello justamente En esta segunda parte rápidamente vamos al cable y allí teníamos recordemos un poquito lo que habíamos visto en la primer parte donde habíamos hablado de modelo armado con un algoritmo de Random Forest para predecir la posibilidad de que una persona pudiese tener o no cardiopatía esto lo habíamos hecho en base a un conjunto de datos este que está aquí donde bueno teníamos características de salud digamos de algunas características de la salud de cada una de estas personas y en base a ello teníamos la variable que Target donde decíamos si esa persona tenía o no una cardiopatía y con eso encontrar un modelo para poder predecir que cualquier persona de la cual tengamos todos estos datos podamos saber si puede o no tener esa esa patología bien avanzamos con el proyecto justamente con todo esto que ya recordamos bien lo que hemos hecho hasta llegar al final en donde teníamos un modelo que nos daba una curacy de el orden de aquí lo tenemos 99,44% para el conjunto de entrenamiento y 88,49% para el conjunto de Test que propone el autor hacer con esto bien podemos ver que aquí lo que busca es un improvement es mejorar este a través de algunas técnicas que justamente entiende el autor que pueden hacer que obtengamos un mejor score que este que está aquí pero antes de eso vamos a ir a implementar un algoritmo de regresión logística y estas partes que estamos viendo aquí que estamos pasando de largo Bueno ya lo vamos a ver más adelante porque los pasamos de largo no es una cuestión que vamos a dejar por fuera sino que va a tener otro contexto en el cual lo vamos a utilizar por lo tanto Vamos hasta la sección donde el autor aborda el análisis de los institutos regresión model para justamente ver si se puede mejorar el álgebraid implementando este otro modelo el logístico recreation se ve es un algoritmo que bueno como su nombre lo indica se diferencia de el de regresión logística que vimos nosotros anteriormente en que tiene incluido un modelo de validación Cruzada en este caso en autor usa este en lugar del modelo que usamos nosotros igual el algoritmo que usamos nosotros y vamos a incluir en este caso el solver que sí lo utilizamos como hiper parámetro cuando implementamos nuestro modelo de relación lógica luego bueno en la separación de los conjuntos de entrenamiento y de Test con trayentes split y el escaneamiento de los datos a continuación se genera o Se entrena el algoritmo se hacen las predicciones se hacen no sé que Brasil y se imprimen los hay que ir así y los valores que se obtienen son obviamente mucho más bajos y los obtenidos con Random Forest 76.176.1 en los dos casos es bueno que coincidan pero es malo como Score t luego tenemos la matriz de confusión donde vemos bueno obviamente aquí en la diagonal principal los aciertos y en la diagonal opuesta como siempre los valores que fueron predecidos de manera incorrecta Bueno pero no obstante haber utilizado Este modelo de regresión logística aún con Esta técnica que incluye la validación Cruzada no ha dado buen resultado puesto que el score ha sido muy bajo Comparado con lo que habíamos obtenido antes con Random Forest por eso y volviendo a la sección que salteamos previamente vamos a ver cuáles son los intentos del autor por mejorar ese modelo justamente de Random Forest bien Por eso aquí habla de un mejoramiento de ese modelo donde habla justamente de los problemas de las clases desbalanceadas y va a utilizar una técnica que se llama Smooth que sirve justamente para trabajar con eso nosotros algo habíamos hablado de las clases de balanceadas recordemos en esta parte de la primer parte de esta clase justamente donde abordamos el problema de que había características que tenían valores optativos opcionales que tenían una gran diferencia en cantidad de apariciones de un valor o cantidad de apariciones del otro valor sí y no por ejemplo en algunos casos como mostrábamos aquí bueno avanzando con lo que aquí hace buscando abordar este problema con una herramienta que se llama Smooth que se está que está aquí giga a un score muy bueno aquí lo vamos a ver del orden del 99.67% en el conjunto de entrenamiento y un 89.5% en el conjunto de Test luego bueno muestra también esta esta performance a través de la matriz de conducción y usa una herramienta se llama un mapa de calor sí un hitmap de correlación esto es este mapa que vemos aquí que analiza algunas cuestiones y luego también intenta mejorar aún más el Akira y del modelo con la extracción de características a través de la herramienta psa cosa que ya hemos visto nosotros en clases anteriores bien Por lo tanto vemos todo este desarrollo aquí y llega a una quiero decir mejor todavía del 99.67 al 99.69 en el caso del conjunto de entrenamiento y también mejora el anti del conjunto de Test pasando a 91.32% ustedes se preguntarán por qué esta última parte la pasamos tan rápido y porque no le explicamos como hicimos en las etapas anteriores bueno la respuesta a esta pregunta es que esta Va a ser una de las instancias que van a tener que hacer ustedes como parte de la evaluación final de este módulo de Machine learning en particular y de todo el cuatrimestre en general es decir lo primero que van a tener que Investigar qué es esto de las clases de balanceadas que hablamos en la primera parte de esta clase pero profundizar un poquito más este concepto pero fundamentalmente Esta técnica de Smooth La idea es que investiguen de qué se trata si puedan documentar esto puedan defenderlo por supuesto y poder llevar todo este código que está aquí al colap que venimos desarrollando desde el principio de esta clase bien luego vamos a hacer lo propio también con esta herramienta que se llama mapa de creador Para qué sirve un mapa de calor por qué usamos un mapa de calor y cómo se usa como está aquí tratando de tomar conocimiento de este código que no es muy largo ni muy complejo y lo pueden hacer tranquilamente y luego de nuevo llevarlo al cola para poder implementarlo en la práctica que venimos llevando a cabo finalmente también vamos a hacer lo mismo para esta extracción de este características con psa está este conjunto de datos tenía 38 características y se reduce a 27 bueno Esto no va a tener que hacer una una gran explicación desde la fundamentación de la extracción de características porque ya un tema dado en la clase pero sí la idea es poder llevar este código al collage y poder Bueno de nuevo tener todo este coral completo tal cual lo propone aquí el autor y hay un tema más que formaría parte y cerraría esta instancia evaluativo en este caso fíjense que Esta técnica es utiliza un concepto de canaibos negros son vecinos y este concepto de canews implica algo así como vecinos más cercanos Este es un tema que en este caso es como un hiper parámetro para Esta técnica y me trae a colación otra cuestión que no está relacionada directamente con esto sino desde el concepto pero no desde lo que les voy a pedir que investiguen y es justamente un algoritmo que se llama kesinos más cercanos sí en este caso es parecido al concepto esto es solamente aplicado no como un hiper parámetro sino como un modelo Así que la última parte de la investigación va a ser justamente investigar de qué se trata este algoritmo de campesinos más cercanos que si ustedes hacen memoria recordarán que en la primer clase entre la lista de los algoritmos supervisados estaba justamente el último justamente era este cabecino más cercanos bien este último elemento ustedes tienen que investigarlo documentarlo desde la teoría y también con alguna aplicación práctica que ustedes puedan encontrar y presentar con un kolab algo parecido a lo que vemos aquí con este algoritmo de Random Forest bueno pueda llevarlo ustedes a un colap sobre esta temática de cabecinos más cercanos explicado paso a paso para que lo puedan demostrar Bueno ya habiendo terminado todo esta cuestión que tiene que ver con la evaluación de este módulo de Machine learning y como dije antes de todo el cuatrimestre vamos a terminar la clase poniendo otro ejemplo más que lo que vimos justamente en todo el resto de la clase pero en este caso de un modelo no supervisado en cambio de lo que vimos antes que era un modelo supervisado con algoritmo de tipo supervisado justamente volvemos acá y tenemos aquí otra opción le dejo aquí arriba la URL para que puedan entrar a ella y se trata de un kamiz aplicado a un problema de segmentación de clientes La idea es que exploremos juntos este proyecto no tan paso a paso como hicimos en el caso anterior yo creo que ya pueden tomar un poco la la guía que usamos en el caso anterior para aplicarla aquí pero sí por lo menos describir un poquito los las etapas los componentes de este proyecto para que puedan hacerlo mejor en principio tienen aquí una descripción muy importante que es como una guía de todo este proyecto donde bueno habla de la importación de librería de la exploración de datos la visualización de datos bueno la creación del modelo de kamins la selección de los clusters si acuérdense que siempre tenemos que con la técnica del codo ver cuál es la cantidad ideal de cluster para este modelo de camisas y finalmente un ploteo de cada una de las telas de los cluster digamos que hemos logrado y sus respectivos centroides Bueno si vemos esto tenemos justamente aquí la primer parte la importación de las librerías la exploración de datos en este caso los datos Recuerden que en el caso anterior habíamos simulado la situación de que no nos teníamos a mano y vamos a buscarlo a un raw de github en este caso si los tenemos vamos a la sección input y aquí está el dato con lo cual lo puedo bajar Al conjunto de datos con esta opción de Download vuelvo al notebook y bueno una vez que ya lo tenemos en nuestro disco lo levantó como hacemos habitualmente no como está aquí sino como hemos usado en las experiencias anteriores de las prácticas en cola Bueno me voy a encontrar con este conjunto de datos bueno hago un shape el distrive veo los tipos de datos de cada una de las características verificó cenus y Empiezo con la visualización de datos en este caso fíjense que existe aquí siempre una Bueno una herramienta que busca justamente Mostrar o esconder el código si yo escondo el código voy a ver nada más que los gráficos que busco justamente generar sin ver todo el código anterior si lo abro Aquí voy a poder ver cuál es el código que yo escribí para generar estos gráficos en principio lo primero que hace es hacer un gráfico de despliegue para ver cómo están distribuidos los datos de la edad del ingreso anual y del gasto promedio luego hace lo propio con el conteo de observaciones por género la cantidad de masculinos y la cantidad de femeninos a continuación hace un ploteo de la relación entre la Edad el ingreso anual y el gasto promedio Aquí está oculto el código lo va a abrir y veo que hago un gráfico muy interesante una matriz de 3 por 3 de su plots donde obviamente relaciono cada una de las características consigo misma Por lo cual siempre me dibuja aquí aquí aquí esa recta a 45 grados y después hago lo propio con las restantes age con gasto anual con ingreso anual perdón y es con gasto promedio lo mismo con los otros tres casos bien luego hago lo propio pero distribuyendo por género es decir fíjense acá Este es age versus anual inconvent voy hasta aquí arriba ya que tengo age versus es este mismo gráfico solamente que este gráfico tenía todos los puntos pintados del mismo color porque no diferenciaba género y aquí si lo hace otra forma distinta de presentar los datos fíjense que aquí está el Celeste como masculino y el naranja como femenino bien lo propio hace con la relación de las otras variantes sí de variables de entrada Aquí tengo la otra y luego lo que hace es buscar una distribución de los valores pero con otro tipo de gráficos de box blog donde me muestra cómo están distribuidos los géneros en las características de la edad en las características del ingreso anual y en las características del gasto bien Todo esto le permite hacer un preanálisis que ya lo van a poder ver bien en detalle de las características de los datos para luego pasar a crear el modelo Aquí está el cluster Inc y lo hace en principio acuérdense de la técnica del codo de recorrer a través de un Ford distintos valores de posibles cluster aquí va de 1 a 10 y bueno genera entonces 10 modelos diferentes cada uno con una cantidad de cluster diferente para qué para lograr el gráfico del codo que se esté que está aquí que me dice que para este modelo de camisas lo ideal es un cluster es un modelo perdón de 4 clacs aquí el codo acuérdense que me demuestra eso el codo cuando cambia acuérdense abruptamente la inercia sí de la recta bien aquí tenemos entonces con 4 crea el algoritmo sí gráfica la resultante a través de este código que está aquí arriba de bueno cómo se distribuyen estos cuatro sí acá tengo 1 2 3 4 donde están con un color los clusters con un color los puntos de cada uno de esos clusters y con un naranja todos y cada uno de los cuatro centrales bien Esto lo repite también para el caso de la segmentación usando el ingreso anual y el gasto promedio sí lo mismo genera la técnica del code en este caso concluye que 5 es el codo por lo tanto crea un modelo con cinco planos y genera de nuevo el gráfico en este caso va a ver obviamente un cluster más pero con la misma lógica de distribución de colores de los puntos de los cluster de los cluster y de los sectores bien y finalmente Busca poner las tres características edad ingreso anual y gasto promedio y busca generar otro modelo de camines en el cual concluye que el codo está ubicado en este caso en el 6 y crea un modelo justamente de site claster en este caso el gráfico es diferente porque es un gráfico de tres dimensiones y aquí van a poder comprobar justamente los seis Class Sí este este este Y estos dos que son parecidos por el color pero este tiene una tonalidad más clarita y es una tonalidad más oscura bueno Esto también no pueden usar ustedes de una forma de cambiar los colores para que sean a la vista mucho más significativas las diferencias les dejo este proyecto para que ustedes lo puedan implementar llevarlo su cola y tratar de seguir desarrollando esta experiencia de investigación a partir de la cual Espero que lo puedan hacer y en caso que tengan algún problema ya saben cómo pueden recurrir al campo virtual para consultarme lo que necesite Bueno hasta aquí llegamos con esta última clase quedo en contacto con ustedes y a la espera de sus trabajos finales con lo cual Espero que aprenden este módulo con éxito nos vemos hasta aquí llegamos con esta clase número 13 y también finalizamos con el módulo de Machine learning Te espero en el próximo módulo el de Deep learning nos vemos  Titulo: Clase14 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/9-iZjJwEk3c  \\n 960 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 14 Hola a todos hoy vamos a empezar con este tercer y último módulo de este curso que es el para empezar con esta clase vamos a remontarnos a dos conceptos importantes que vimos justamente en la primer clase Qué es una red neuronal una red neuronal es un modelo de Inteligencia artificial que está inspirado en el comportamiento del cerebro humano es decir en una estructura de neuronas y conexiones entre ellas Qué es el Deep learning el Deep learning es su campo dentro del mundo de las redes neuronales que tiene una estructura de muchas neuronas y muchas capas de neuronas y Por ende tengo una red muy profunda de allí el concepto de Deep learning o aprendizaje profundo este concepto de neuronas y capas y profundidad es algo que tranquilo ya vamos a llegar a verlo bien en elche déjame contarte antes que hoy por hoy el Deep learning es justamente lo que está manejando toda esta evolución de la Inteligencia artificial de todas las aplicaciones que vos ves hoy en día reconocimiento y seguimiento de objetos reconocimiento de voz predicción bursátil generación de textos traducción de idiomas pronóstico de enfermedades a través de imágenes análisis de sentimiento prevención de fraude conducción Autónoma análisis genético son algunas de las muchas utilidades de estas redes como ves es una familia de algoritmos muy potentes que hoy por hoy pueden hacer cosas increíbles Pero cómo es esto de que estos círculos y estas conexiones pueden darme hoy este nivel de aprendizaje profundo y este lugar que hoy tiene la ia mi cerebro el tuyo y el de todos los humanos tienen decenas de miles de millones de neuronas interconectadas entre sí Y gracias a esas neuronas interconectadas podemos ver entender sentir crear imaginar y tomar decisiones esas neuronas funcionan de la siguiente manera cada neurona recibe estímulos eléctricos de otras neuronas la neurona procesa sus estímulos y en ciertos casos se activa y dispara para estimular a su vez a otras neuronas a la que está conectada esas otras neuronas reciben ese estímulo lo procesan y en ciertos casos también se activan y disparan estímulos a otras neuronas y así sucesivamente continúan en esta gran red neuronal supongamos que vos un día quería hacer un cambio importante en tu vida y querés empezar a hacer cosas que aún hoy no sabes hacer por ejemplo vamos a suponer que querés aprender a jugar al padre y pintar son dos habilidades que no tenés para lo cual hoy tu cerebro no está preparado cuando empiezas a hacer este tipo de cosas o intentar hacer este tipo de cosas turno no tienen el conocimiento para poder llevar adelante esa actividad por lo tanto esas conexiones No están entrenadas para ello y las neuronas no están activadas o preparadas para activarse cuando vos querés hacer eso pero vos empezás a practicar supongamos padre Eso hace que tus neuronas empiecen a tener conexiones para esa nueva habilidad y empiecen a ser cada vez más potentes para poder justamente desarrollar la habilidad que vos necesitas para hacer eso esto es el proceso de aprendizaje estamos hablando de un Proceso biológico que sucede en tu cerebro y eso toma tiempo Pero conforme avanzas con la práctica las neuronas no solo van reforzando sus conexiones sino que generan nuevos procesos en los cuales Cuando se dispara una neurona se disparan automáticamente las otras necesarias para la acción que está llevando a cabo esto es parte de lo que llamamos memoria muscular y nos sucede frecuentemente con cosas que hacemos todos los días Como por ejemplo levantarnos y lavar los dientes hace casi 70 años se intentó emular artificialmente este comportamiento del cerebro humano primero con recursos eléctricos y luego con procesos computacionales pero no fue una tarea sencilla Cómo lograr en principio que se puede artificialmente tomar la decisión de la misma manera que lo hace un ser humano normalmente cuando tomamos una decisión no lo hacemos de manera espontánea ni racional si no utilizando una serie de elementos que son parte nuestra como nuestros conocimientos nuestras experiencias anteriores nuestra Situación actual nuestros principios nuestras creencias etcétera para poder tomar lo que creemos Será la mejor decisión aunque no siempre es así  vamos a la situación en la que vos querés aprender a jugar al padre vamos a tomar tres factores para que vos puedas tomar la decisión si lo vas a poder hacer o no en principio supongamos vas a buscar un buen Club segundo vas a ver si los horarios que te ofrece ese club son compatibles con los tuyos y tercero si tienes dinero suficiente para poder pagar ese club en base a esos tres factores vos vas a tomar la decisión de si vas a empezar a jugar al pal o no vamos a intentar simular esto con un proceso de computacional y para ello Vamos a darle a cada uno de estos tres factores los nombres x1 x2 y X3 cada factor solo tiene dos opciones sí o no para tomar la decisión vamos a considerar la siguiente regla con que dos de esos factores sean Sí vos vas a tomar la decisión de empezar a jugar al padre caso contrario no lo vas a hacer conectemos estos tres factores a este círculo este círculo debe tomar los tres factores como entrada y por medio de una operación de suma de valores los tres factores como esta x1 + x2 + X3 se envía resultado como salida de ese círculo siguiendo la regla citada anteriormente Y con esta fórmula matemática vamos a tomar lo siguiente si la salida da como resultado dos o tres vamos a tomar clase caso contrario si el resultado es 0 o 1 no vamos a tomar clase es decir que fijamos un valor de referencia igual a 1 si la suma es mayor a 1 vamos a empezar a tomar clases de si no no lo vamos a hacer ahora nuestro círculo ya nos brinda una herramienta para tomar una decisión en este caso si puedo o no jugar al padre vamos a suponer el siguiente ejemplo vamos a suponer que vos has podido conseguir el dinero suficiente y que los horarios son compatibles con los tuyos pero el club no es tan bueno como a vos te gustaría que fuera hay dos de los tres factores que son Sí Por ende vas a empezar a jugar al padre normalmente nuestros razonamientos no son tan estructurados ni tan matemáticos seguramente ante esta cuestión de que el club no es lo que yo esperaba yo voy a resonar quizá del siguiente modo voy a empezar igual Aunque no me guste voy a buscar Mientras tanto otro Club y cuando consigue un club mejor me voy a cambiar otro ejemplo podría ser que conseguiste un buen Club Los horarios son los apropiados pero no tenés dinero para pagar la cuota con esta regla de decisión que tenemos en este círculo vas a empezar a jugar al pádel porque la suma es mayor a 1 pasando nuevamente de este razonamiento computacional a un razonamiento más humano se podría suponer que yo podría decir No tengo dinero suficiente Pero no importa lo voy a pagar con la tarjeta de crédito o le voy a pedir dinero prestado a alguna persona conocida pero vos decidí no hacerlo porque no pensás así y pensás de otro modo Como por ejemplo diciendo no quiero usar la tarjeta de crédito porque quizás después no voy a tener dinero para pagarla o bien no quiero pedir dinero prestado por lo tanto vos decidís que no vas a empezar a jugar el padre yendo totalmente en contra de la regla de decisión del proceso computacional Pero qué pasó acá es decir hay una incompatibilidad entre el razonamiento artificial y razonamiento humano porque porque vos decidiste darle más importancia al factor del dinero es decir en este caso para vos el dinero es un factor mucho más importante que los otros dos es importancia no la vemos incluido en nuestro proceso computacional vamos a hacerlo ahora pero antes déjame decirte que es importancia especial que vos le diste a uno de estos tres factores en el mundo de las redes neuronales se llama por ello ahora vamos a cambiar la suma y suponiendo por ejemplo que el factor del dinero es el doble de importante que nosotros dos la suma quedaría de este modo aún no x1 a 2x2 a 3x3 en donde las a son los pesos y en este caso particular aún no sería igual a 2 y a dos y a tres serían igual a uno esto que tenemos ahora ya no es una simple suma sino que es una suma ponderada suma ponderada te hace acordar algo ese concepto claramente Esto no es otra cosa que el algoritmo de regresión lineal múltiple que vimos en la primera clase de Machine learning y ahora vas a empezar a entender Por qué decimos que era muy importante que antes de empezar con el módulo de it pasáramos por el módulo de Machine por ello a esta suma ponderada que estamos asociando con un algoritmo de regresión lineal múltiple vamos a agregarle como en aquel caso también el sesgo concretamente lo que teníamos hasta acá era una suma x1 + x2 + X3 que me da cuatro resultados posibles 0 1 2 y 3 dos de los cuales daban la decisión por sí y los de los cuales daban la decisión por no ahora con esta nueva suma que acabamos de crear tenemos cinco resultados posibles 0 1 2 3 y 4 y Por ende vamos a tener que cambiar también la regla de la decisión si el resultado de la suma ponderada es 0 1 o 2 no vamos a empezar a jugar al pad caso contrario si lo vamos a hacer lo que hemos hecho también aquí es cambiar el valor de referencia antes era 1 ahora el nuevo valor de referencia es 2 podemos ver que la regla de decisión siempre implica una salida binaria sea Comparado con el valor de referencia 1 o Comparado con el valor de referencia 2 siempre la salida va a ser sí o no o cero o uno nuevamente Te invito a pensar este proceso de salida binaria nos hace recordar algo evidentemente sí aquello que hablamos en la segunda clase de Machine learning cuando abordamos el algoritmo de regresión logística en aquella oportunidad representamos al algoritmo de revisión logística con una función que llamamos sigmoide que tenía la forma de una s eso por ahora no lo vamos a aplicar vamos a aplicar otra función pero tranquilos ya vamos a llegar a utilizar a la función sigmoide lo que vamos a usar ahora se llama función escalonada o Step y con ella Vamos a graficar la regla de decisión de nuestro círculo rojo de este modo Aquí vamos a codificar la respuesta de salida porque ya sabemos muy bien que en el mundo de el Jeep learning y el Machine learning por supuesto no se pueden usar valores categóricos vamos a representar el Sí con un 1 y el no con un cero es por ello que los valores de la suma ponderada 0 1 y 2 están asociados al valor cero y los valores de la suma ponderada 3 y 4 están en el valor 1 ambos de la variable Y para esta distribución de valores de y hacia el cero hacia el uno hemos vuelto a utilizar el mismo valor referencia de nuestra Norma fijando como dijimos antes el valor 2 bien este valor de referencia en el mundo de las redes neuronales se llama umbral y finalmente en este proceso de ponerles a las cosas el nombre que corresponde vamos a dejar de llamarle a este círculo de decisión que usamos hasta aquí justamente la nomenclatura círculo y vamos a llamarlo perceptrón suma suma ponderada regresión múltiple regresión logística umbral peso funciones escalonada perceptrón Quizás mi explicación hasta aquí fue Clara y todos estos conceptos has podido relacionarnos o por ahí todavía los tenés un poco desconectados sea como sea lo importante es que hagamos un repaso y podamos sintetizar todas estas cosas que hemos visto hasta ahora y relacionarlas como corresponde tenemos un perceptrón que nos va a ayudar a tomar una decisión para ello le damos al mismo valores de entrada esos valores de entrada van a ser utilizados dentro del perceptrón para llevar a cabo una suma ponderada el resultado de esa suma ponderada va a ser el input para una función escalonada que determinará si el valor de salida del percentrón será un 1 o será un cero esa decisión la va a tomar en base a un valor denominado umbral que tiene justamente esas funciones cargadas Espero que esta explicación te haya servido que haya sido un buen resumen de todo lo que vimos hasta aquí porque justamente aquí es donde terminamos la primer parte de esta clase aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte  Titulo: Clase14 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/La9JEBk6ZVU  \\n 742 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella Hola a todos nuevamente continuamos con la segunda parte de esta clase en la primer parte de esta clase vimos como emular un proceso de razonamiento humano a través de un proceso computacional pero todos sabemos que en este ejemplo que tomamos de referencia de si voy a jugar o empezar a jugar o no al pádel hay tres factores que intervienen pero en nuestra vida no es tan sencilla Por lo general tenemos mucho más que tres factores para tomar una decisión por ejemplo en este caso podemos pensar que si bien están esos tres factores pueden haber otros que son de emergencia y tienen que ver a partir del momento que yo vaya a jugar al padre por ejemplo si hay buen clima si tengo el vehículo para poder movilizarme y poder llegar u otros similares estamos hablando de factores que son diferentes de los anteriores los anteriores son más programáticos en el tiempo los que mencioné recién son más emergentes como por ejemplo también podría pensar si ese día tengo un compromiso o no Bueno puedo diferenciar esos tres factores de estos otros tres factores y pensar en una estructura que ya no solamente se resuelva a través de un perceptrón sino a través de un segundo percepción el segundo perceptrón va a estar ubicado en una misma estructura vertical la cual se va a llamar pero también una decisión requiere muchas veces una cadena de elementos que están conectados para justamente llegar a esa decisión final vamos a suponer que decidimos viajar ese día porque tenemos un problema en el vehículo con un colectivo bueno en principio sé que si Quiero viajar en un colectivo que tengo que tener una tarjeta para poder hacerlo bueno lo primero que tengo que ver es si puedo encontrar esa tarjeta segundo una vez que la encuentro sé que esa tarjeta tiene que tener un saldo en dinero para poder pagar ese pasaje con lo cual lo segundo será ver si esa tarjeta tiene dinero se entiende que lo que estoy hablando son de un montón de cuestiones que están enlazadas y que van a llevar hacia el final voy a poder ir a jugar o no al padre es decir que hay una cadena de decisiones enlazadas en donde las primeras decisiones generan salidas que son entradas para la siguiente decisiones y potenciando este problema podemos pensar en nuevos perceptrones que se dispongan en otras varias estructuras verticales es decir en otro conjunto de capas en donde todos los perceptrones están conectados con sus pares esta estructura compleja a la que Hemos llegado se llama perceptrón multicapa y los perceptrones multicapa son reconocidos hoy en día con el nombre de redes neuronales y los perceptrones con el nombre de este gráfico va a terminar en una estructura final que tendrá dos salidas si es el caso de una salida binaria como el ejemplo que tomamos o bien en una salida que tenga más de dos opciones como en el caso cuando intentamos adivinar el número que estaba representado en una imagen de nuestro Data set Eminem teniendo en cuenta todo esto y este conjunto de capas que forman parte de esta estructura que ahora llamamos red neuronal podemos identificar tres elementos una primera línea de capa que se llama capa de entrada una última línea de capa que se llama capa de salida y todas las capas intermedias se llaman capas ocultas esta estructura de capas ocultas nos permiten como hablamos recién llevar a cabo una decisión capa a capa con diferentes niveles de complejidad esto en este nivel de decisión complejo se llama conocimiento jerarquizado este conocimiento jerarquizado será cada vez mayor en la medida que nuestra red neuronal tenga más capas de neuronas más neuronas por capas y en consecuencia un mayor nivel de conexiones entre ellas generando una estructura profunda de decisión y dando lugar a una red de aprendizaje profundo es decir una red de tipo Deep learning vamos a suponer el caso de cómo actuaría una red neuronal para intentar dilucidar Cuál es el número que está en una imagen tal cual como lo hicimos con nuestro datasette Méndez cómo sería el proceso las primeras capas pueden centrarse en cosas básicas como los ejes y líneas de esa imagen la siguiente capa pueden tomar esa entrada y centrarse en uniones entre los ejes para formar figuras simples luego las siguientes capas pueden unir esas figuras simples para definir si lo que están viendo Parece ser un número y tratar de dilucidar qué número es hasta acá vamos bien con esta idea de la red neuronal y el Deep learning Y parecería ser una estructura sencilla pero lo real es que lograr que esas neuronas sus capas tuvieran un funcionamiento jerarquizado como el que vimos y que cada una de ellas abordaran diferentes tipos de decisiones profundas y niveles de complejidad no fue tarea sencilla y algunas teorías que se desarrollaron a los fines de los 60 recién pudieron empezar a aplicarse 20 años después en lo que se llamó el invierno de la Inteligencia artificial concretamente los científicos de aquella era se encontraban con tres grandes problemas el primero de ellos el manejo de los pesos de las neuronas ya definimos que dentro de cada neurona hay una suma ponderada esto es similar a lo que vimos en un algoritmo de regresión lineal múltiple de esta manera se operan los valores de entrada y sus pesos además de ello Tenemos también la función escalonada para definir el valor de salida a través de un elemento básico y esencial que se llama umbral todos estos valores pueden ser alterados buscando que la red neuronal pueda tener el resultado con el mayor nivel de precisión posible el problema en aquel entonces fue que esos valores sólo se podían ajustar manualmente para hacer que la red funcionara mejor y eso era una tarea prácticamente imposible para graficar esto imaginemos que tuviésemos en una sala de grabación donde están esas consolas gigantes que tienen millones de perillas vamos a suponer que cada una de estas perillas me permitirá cambiar el peso de cada una de las sumas ponderadas que hay en cada una de las neuronas de mi red neuronal sería prácticamente imposible pensar que manualmente yo pudiera encontrar la mejor combinación para que la red de ese modo manual tuviese su mejor rendimiento Ese fue el primer gran problema que tuvieron los científicos y que le llevó 20 años poder llegar a que alguna persona pudiese encontrar una solución a ese problema bien el segundo punto era la necesidad de contar con funciones de activación como ya vimos y recién mencionamos las neuronas necesitaban de una función de tipo escalonada que transforme resultado lineal de una suma ponderada en una salida no lineal como Proponen las funciones escalonada recordemos que esta función me da solamente dos posibles valores de salida ceros y unos estas funciones vital pero más allá de ello Lo importante es el rol fundamental de esta función rol que cumplía con resultados muy pobres las funciones escalonada y por ello fue necesario pensar en funciones que cumplieran con el mismo rol pero con resultados muy superiores el tercer problema fue la volubilidad que tenían esas redes por aquel entonces Te acordás del ejercicio de nuestra clase número 10 en él hablamos tomando como ejemplo un algoritmo de regresión logística que en el caso de un problema de clasificación de flores Iris la salida no era un valor concreto setosa versicolor O virgica sino un porcentaje de posibilidad para cada uno de los tres valores posibles de salir bien esta lógica no aplicaba a los perceptrones ya que la salida del mismo era simplemente un cero o un uno y eso solamente es aplicable o es Útil para algunos casos pero si tenemos una red normal grande un pequeño cambio en solo uno de los parámetros puedes encadenar una serie de modificaciones no deseadas En las siguientes capas y cambiar por completo el resultado de salida es decir tengo una red muy voluble lo que se necesitaba es que los perceptores tuvieran una respuesta más real es decir no que me digan si es una serie o una clase de flor o no sino que me dijera Qué probabilidad había de que fuera cada una de las tres clases posibles otro ejemplo si tomamos el caso de Géminis que ya usamos no necesito que la red neuronal me diga concretamente qué número es el de la imagen sino un porcentaje de posibilidad para cada una de las opciones de 0 a 9 de que esa imagen corresponde a ese número de esta manera cuando cambia un parámetro en lugar de cambiar totalmente la serie de decisiones es decir abruptamente pasa a cero o pasa a uno cambia sutilmente sólo lo necesario y Por ende el impacto en las capas posteriores es mucho más controlado la falta de solución a estos problemas más una considerable merma en las inversiones que había para estas investigaciones En aquel momento produjeron lo que recién mencionábamos como el invierno de la Inteligencia artificial hasta que en el año 1986 salió esta publicación que provocó un resurgimiento de la Inteligencia artificial impulsado también por el aumento de la capacidad computacional y la disponibilidad de grandes conjuntos de datos con logros que no pararon de crecer hasta lo que conocemos hoy en día lo que nos queda por saber ahora es cómo se resolvieron a través de este artículo estos tres problemas que mencionábamos Pero esto es un tema que queda para la próxima clase hasta la próxima clase aquí termina esta clase los espero en la próxima clase nos vemos Titulo: Clase15 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/UNZz4H_Bx0s  \\n 665 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 15 Hola a todos empezamos con esta primer parte de la clase número 15 donde Nuestro objetivo va a ser dar respuesta a lo que nos quedó pendiente de la clase anterior los tres problemas que quedaron pendientes de resolver en la casa anterior fueron los siguientes funciones de activación manejo de peso de las neuronas y volubilidad de las redes problemas que se solucionaron con el artículo que mencionamos al fin de la clase pasada uno de los temas más importantes sobre los que hubo que trabajar en aquel momento fue el tema de las funciones de activación nosotros ya trabajamos con una de ellas que estaba en el perceptrón que era las funciones escalonada Pero luego se descubrieron muchas más y mejores que la vamos a nombrar a continuación muchos de los problemas que resolvemos constantemente con nuestro propio cerebro podemos verlos como funciones de entrada y salidas Por ejemplo si estamos viendo un número escrito manualmente como el del Data set Eminem y Queremos saber que número es podemos entender a este problema como una función la función de entrada es la imagen y la salida la respuesta A dilucidar qué número es el que corresponde esa imagen como dijimos en la clase pasada el propósito de los científicos que crearon las redes neuronales era emular el conocimiento del cerebro humano con una red computacional que tuviese el mismo nivel de aprendizaje profundo para alcanzar ese aprendizaje profundo dijimos que teníamos que crear una estructura con múltiples neuronas conectadas de forma secuencial y si recordamos también lo que dijimos en la clase pasada lo que hace cada una de estas neuronas es un problema de regresión lineal es decir que lo que estamos haciendo si lo planteamos matemáticamente es concatenar diferentes operaciones de regresión lineal el problema aquí es que sumar distintas operaciones de regresión lineal va a dar como resultado inexorablemente un valor lineal es decir una recta y un espacio de dos dimensiones un plano en un espacio de tres dimensiones o un hiperplano en un espacio unidimensional bajo este problema no importa la estructura de la red no importa Cuántas capas haya no importa Cuántas neuronas haya porque en realidad si toda esta concatenación de neuronas lineales dan resultado lineal toda esta estructura es lo mismo que tener una sola neurona los problemas del mundo real son mucho más complejos que una simple función lineal por lo que una red así no podría aprender nada interesante para conseguir que nuestra red No colapse necesitamos que esta suma de aquí dé como resultado algo diferente a una línea recta y para eso necesitaríamos que cada una de estas líneas sufra alguna manipulación no lineal de las distorsiones Cómo conseguimos Esto justamente con las funciones de activación esta sencilla función escalonada que nos ayudó la clase pasada a resolver con un umbral una decisión de si vamos a empezar o no a jugar al padre es ahora un elemento fundamental en todas y cada una de las neuronas de una red neuronal para que gracias al aporte de su distorsión no lineal podamos encadenar de forma efectiva la computación de varias neuronas obviamente que la forma en que la salida cambia según las entradas depende de qué función de activación estemos utilizando y por eso vamos a ver en detalle Cuáles son los tipos de funciones de activación más importantes que hay hoy en día Comencemos con la más simple que es la que ya venimos usando la función escalón o Step esta función realiza cero para todos los valores que están debajo de un valor llamado umbral y uno para todos los que están encima de ese valor llamado umbral nos da el componente no lineal que necesitamos sin embargo no es apta para el aprendizaje profundo actual ya que se limita a problemas muy sencillos de salida de tipo binario Esta es la función sigmoide y como vemos la distorsión que produce hace que los valores muy grandes se saturen en uno y los valores muy pequeños por tanto con esta función sigmoide no solo conseguimos Añadir la deformación que estamos buscando sino que también nos sirve para representar probabilidades que siempre vienen en el rango de 0 a 1 seguramente la forma de la figura te debe parecer familiar sí es la misma que usamos para representar el comportamiento de un algoritmo de revisión logística en el módulo de otra función similar a la chimoide es la función tangente hiperbólica o tan cuya forma es similar a las inmóviles pero cuyo Rango varía de menos uno a uno softmax es la función más utilizada para las redes de clasificaciones no binarias y muy necesario como ya dijimos para que las salidas sean de tipo probabilístico Por ejemplo si la entrada a una red es un número 5 escrito manualmente y digitalizado como en el caso de lo que tenemos en Data de salida tiene 10 neuronas como salida de 0 al 9 Entonces Esperamos que la neurona que representa al 5 sea la que probabilísticamente tenga Las mayores posibilidades y finalmente esta función de activación que es la más utilizada hoy en el campo del dip de Darling la unidad rectificada lineal o básicamente se comporta como una función lineal cuando es positiva y constante acero cuando el valor de la entrada es negativo se representa con esta expresión o esta fórmula que está aquí no obstante Los investigadores siguen buscando mejores funciones de activación y algunas de las que hoy están en pleno proceso de investigación son estas que están aquí o lrelu tiene una función con esta forma es muy similar a relu pero en lugar de dar cero para los números negativos multiplica el valor por un número muy pequeño 0.01 generando una gráfica como esta parametric reloj o prelu es prácticamente lo mismo que el relu pero en lugar de multiplicarse ese valor por 0.01 se utiliza un parámetro que puede ajustarse Hello la cual es muy similar sin embargo es más suave y es usada por el hoy ya mundialmente conocido gpt hay muchas opciones más que hoy están en instancia de investigación pero los científicos aún no la consideran formalmente parte de todas las que te mencioné antes ahora Es lógico y encuentro razonable que te preguntes después de ver tantas funciones de activación cuando aplica uno y cuando aplicó otra la función logística o sin móviles se recomienda solamente en la capa de salida cuando requerimos un resultado binario 0 o 1 verdadero o falso la tangente hiperbólica o tan es superior a la función logística para capas ocultas sin embargo en muy pocos casos da resultados superiores a Reno relu es la que normalmente vas a utilizar A menos que quieras experimentar mucho con otras funciones y Por ende puedas experimentar con el reloj o PR elu es muy nueva pero de manera general tiene mejores resultados que relu especialmente en redes muy grandes y finalmente está softback que es una variante muy importante que no mencionamos antes en realidad como dijimos antes conviene usar la función sigmoide Cuando tenemos una salida binaria es decir solamente dos valores posibles pero si tenemos más de dos valores posibles la función para la salida es softmax con esto terminamos la primera parte de esta clase en la segunda parte de esta clase vamos a ver los dos temas que nos quedan pendientes la volubilidad de las redes y el manejo de los pesos nos vemos en la siguiente parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte Titulo: Clase15 (parte2) Curso Inteligencia Artificial \\n URL https://youtu.be/NUlWcTWsfR8  \\n 1388 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella Hola nuevamente seguimos en esta segunda parte de la clase donde vamos a hablar de los dos temas que nos quedaron pendientes de la primer parte la volubilidad de las redes y el manejo de los pesos de cada una de las neuronas de nuestra red neurona la medición de la precisión de una red neuronal no es muy diferente a lo que vimos ya en el Machine learning es decir le voy a dar a esta red neuronal un montón de valores de entrada va a pasar por todas y cada una de las capas y va a dar un resultado final ese resultado final se mide después y se ve que también o qué tan mal puede haber andado con la predicción que hizo solamente que antes le llamábamos Sport a este valor de la mediación de la precisión y ahora le llamamos función de costo según lo bien o mal que nos haya ido con la predicción es que vamos a tener que ajustar los pesos de cada una de las neuronas de nuestra red neuronal para saber Cómo ajustar los pesos y los sesgos calculamos la derivada o gradiente de la función de costo a cada uno de los pesos y sesgos de la red y lo hacemos capa por capa hacia atrás una por una hasta llegar al inicio este proceso es llamado propagación hacia atrás o Bad para ello la explicación es esa solamente que aquí hay dos conceptos que aún vos no manejas el censo de gradiente y va a propagayo dos conceptos que fueron fundamentales en el artículo que se publicó en 1986 y revolucionó el mundo de la Inteligencia artificial vamos a empezar con el concepto de Back propagation como dijimos antes podíamos suponer que estamos controlando a través de una enorme consola de sonido todas las perillas que modifican los pesos de cada una de las neuronas de cada una de las capas de una red neuronal qué nos proponemos tocar Las perillas cambiar los pesos de cada una de las neuronas o de alguna de ellas capa por capa y medir la función de coste para ver cómo nos puede Tenemos que tener presente que cambiar cada uno de los pesos de una neurona no implica un cambio solamente en ella sino que su cambio afecta a todos y cada uno de los caminos que se generan de las múltiples conexiones que tiene esa neurona con el resto como ya dijimos antes pensar en esta solución sería realmente una locura dado que dar de ese modo con la mejor combinación de pesos para que una función de coste arroje los mejores resultados sería francamente una Misión Imposible el sistema de vapor variation propone analizar la función de coste capa por capa y en cada una de ellas analizar diferentes niveles de responsabilidad de las neuronas de esa capa veamos yo puedo tomar una capa y medir no solo la función de coste sino además ver cuál o cuáles fueron las neuronas que mayor incidencia negativa tuvieron en el resultado de la función de coste e implementar cambios en los pesos de ella o de ellas este proceso se repetiría hacia atrás capa por capa hasta llegar a la primera este análisis de determinar Cuánta responsabilidad tiene cada neurona en el resultado final se reconoce como retro propagación de errores y tiene sentido real que se ejecuta hacia atrás ya que en una red neuronal el error de las capas anteriores depende directamente del error de las capas posteriores si por el contrario yo observaba que era una capa una neurona no tiene responsabilidad en la función de coste no tendría sentido cambiar los pesos de esa neurona ni tampoco de todas las conexiones precedentes para entender esta cadena de responsabilidades hacia atrás vamos a suponer un problema de logística o de entrega de productos vamos a suponer que tenemos un cliente que está disconforme porque un producto que compró por internet Le llegó muy tarde vamos a suponer una red neuronal que está compuesta por capas que tienen que ver con distintos tipos de actores que intervienen en esa entrega de ese producto tenemos la capa de los transportistas la capa de los despachanques la capa de los empaquetadores y la capa de los vendedores obviamente ese cliente recibió el paquete de parte de uno de los transportistas con lo cual si ese transportista tuvo culpa en esa demora yo tengo que ajustar el peso de ese transportista y no de los otros transportistas que están en la misma capa de transportistas si por el contrario la culpa es del despachante yo tendré que ajustar los pesos de la neurona de ese despachante que está relacionado con el transportista que entregó ese paquete obviamente este análisis hacia atrás Me permite determinar Dónde está el problema sabiendo que cuando yo corrija el peso de una neurona todas las otras neuronas que están relacionadas con esta también va a mejorar y van a aportar para que la función de coste sea mejor en un sentido genérico vamos a suponer que todos tuvieron algún problema que hizo que esa entrega se demorara demasiado bueno pues entonces habrá que ajustar un poco los pesos de cada una de las neuronas para que de esa forma la función de coste final sea la mejor una vez realizado esto habría que volver a hacer el mismo proceso que hicimos en Machine learn es decir volver a entrenar el modelo volver a probarlo volver a testearlo y volver a medir su función de costes hasta acá está todo muy claro la teoría está Clara pero aún no sabemos cómo vamos a modificar de manera no manual sino automática los pesos de cada una de las neuronas para mejorar la función de coste Bueno ahí aparece el otro concepto importante que mencionamos recién que acompaña el Back propagation que es la función o el algoritmo de descenso de gradiente el método de descenso al gradiente es un algoritmo clave dentro del campo del Machine learning y que se encuentra en el corazón de la gran mayoría de los sistemas de Inteligencia artificial que se desarrollan hoy en día el objetivo con su uso es reducir la diferencia entre resultado obtenido y el que se busca obtener a través de este algoritmo de descenso de gradiente los pesos de la neurona de la red son modificados para que la red logre ese objetivo supongamos un sistema de dos variables una variable de entrada y una variable de salida de la neurona el valor de salida es igual a la función de activación que se aplica sobre la suma ponderada los resultados se pueden visualizar en una función de tipo convexa como esta que está aquí encontrar el valor mínimo en esta figura sería Encontrar el punto donde la derivada es igual a cero empezando el proceso desde cualquier punto determinado automáticamente de manera aleatoria algo así como encontrar el fondo de este Valle una vez que hayamos encontrado eso tendremos el valor del peso en el caso del ejemplo que hemos usado el único peso de nuestra suma ponderada y Listo ya tenemos el peso ideal de esta neurona para que la función de coste sea la mínima la pregunta ahora es como pasamos de ese punto definido manera aleatoria al punto mínimo donde la derivada es cero El Punto de partida es solamente un punto arbitrario para que podamos evaluar el rendimiento desde ese Punto de partida encontraremos la derivada o pendiente y Desde allí podemos usar una recta tangente para observar la inclinación de la pendiente la pendiente va a informar las actualizaciones de los parámetros es decir los pesos y el sesgo la pendiente en El Punto de partida será más pronunciada Pero a medida que se generan nuevos parámetros la pendiente debe reducirse gradualmente hasta alcanzar el punto más bajo de la curva conocido como punto de convergencia es decir el punto donde la derivada es cero para explicar esto de una manera más clara vamos a usar un ejemplo vamos a suponer que estamos en la cima una montaña y que estamos en un día muy nublado el cual nos permite solamente una visibilidad de no más de un metro nosotros queremos llegar rápidamente a la base de la montaña y entendemos que mirando a nuestro alrededor aún con esa visibilidad de un metro vamos a buscar la parte más pronunciada ya que ella seguramente nos va a llevar más rápido a esa base de la montaña esa observación que hemos hecho en nuestro alrededor para intentar Buscar Cuál es la parte más pronunciada es justamente lo que hace el algoritmo de descenso del gradiente yo voy a bajar ese metro en esa parte más pronunciada y cuando esté allí voy a tener nuevamente un metro de visibilidad y voy a repetir el proceso buscando desde ese lugar también la forma más pronunciada de ese modo paso a paso podré ir a partir de la información que me dé la función o algoritmo de descenso de gradiente llegando finalmente a la base de la montaña pero aún nos queda un tema más Cuál es la magnitud de ese avance que yo voy a ir haciendo paso a paso pensemos un poco si avanzamos a grandes pasos puede que la selección de los puntos vaya de un lado a otro de manera tan significativa que llegar al punto mínimo sea algo que nunca ocurra lo más lógico sería hacerlo con pasos muy pequeños para que no corramos ese riesgo pero el problema ahora sería El excesivo tiempo que esto conllevaría y Por ende un alto nivel de gasto computacional este valor es uno de los más importantes parámetros del mundo de Deep learning y se llama tasa de aprendizaje y como vimos es uno de los valores que debemos manipular a la hora de buscar la mayor eficiencia de la red y de su entrenamiento ahora bien con el ejemplo de la función convexa hemos tomado El ejemplo más sencillo ya que la curva resultante podría ser algo como esto en donde tenemos un mínimo global pero también tenemos mínimos locales y ese es uno de los mayores problemas del algoritmo del descenso del gradiente ya que no permite diferenciar un mínimo de otro y Por ende puede estimar pesos entendiendo que un mínimo local es en realidad un mínimo global no obstante ellos seguimos parados en un ejemplo sencillo dado que estamos trabajando por una sola variable de entrada y sabemos bien que en una red neuronal está lejos de trabajar de ese modo sino con un número muy grande de variables de entrada y Por ende con una gráfica n dimensional y no hablamos del valor de un peso sino de un vector unidimensional formado por cada uno de esos Todo queda a manos ahora de nuestra imaginación ya sabemos que lo que podemos graficar solamente puede llegar hasta tres dimensiones con lo cual este mundo en unidimensional siempre lo hemos repetido muchas veces que adentro nuestra imaginación pero les voy a dar una herramienta justamente para que se divierta se entretengan y puedan tratar de cambiando alguno de los valores de los conceptos que hemos visto de aquí tener una representación gráfica de cómo cambia una red neuronal cuando voy tocando esos hay una aplicación que se llama playground de la empresa tensorflow tensorflow es hoy por hoy una de las empresas desarrolladoras de soluciones que más utilizan en el campo del Deep learning en este caso es una aplicación que ahora esta clase va a estar acompañada de una locución que le va a mostrar Cómo manejarla y realmente es muy práctica para empezar a aplicar insisto todos estos conceptos que hemos visto acá bien aquí estamos en esta aplicación playground de tensorflow Aquí vamos a observar primero esto que está aquí arriba donde me dice si quiero abordar un problema de clasificación en regresión vamos a dejar clasificación y tenemos aquí cuatro posibles escenarios de distribución de puntos para resolver distintos tipos de problemas vamos a este que es el más básico y vamos a elegir una función de activación lineal Así que esto prácticamente lo hemos usado porque es muy particular el uso que podemos usar pero fíjense que en este caso como tenemos dos grupos bien diferenciados probablemente una línea sea una forma de clasificación que pueda funcionar bien Así que hago clic aquí y ya veo que ya tengo miren pasaron rápidamente esto debería pasar más lento pero acá hay dos cosas para observar Bueno aquí está la cantidad de épocas si esto va a seguir tanto como yo le di hasta que yo le dé stop digamos no no hay un límite que yo prefijo de cantidad de épocas Aquí ya está en el gráfico estipulada Cuál es la división correcta para una clasificación o sea este modelo lineal Funciona muy bien y fíjense que aquí está la curva de pérdida del conjunto de Test y la curva de pérdida del conjunto entrenamiento bueno esto por ahora Funciona muy bien pero vamos a ver qué pasa Yo cambio el escenario y voy a poner este escenario esto son de puntos es totalmente diferente bueno ejecuto nuevamente y fíjense lo que pasa miren acá Este esta curva ven que hay una que baja y otra que no O sea no es una buena función de pérdida porque porque está teniendo enormes dificultades de encontrar una línea que vivía estos dos conjuntos que deben evidentemente nunca la va a conseguir porque no hay ninguna línea que pueda adaptarse a ese problema Bueno lo paramos con este volvemos todo para atrás con este botón como si fuera una especie de rifles y voy a pasar a la sigmoid se acuerdan la función de activación Sigma y Bueno pero vamos ahora a ver qué pasa fíjense las curvas de pérdida como se van almoldando pero aún así no consigue un modelo paramos volvemos hacia atrás vamos a tan se acuerdan la tangente hiperbólica bueno aquí fíjense que vayan mirando la curva esta yo lo voy deteniendo un poquito para que ustedes puedan ver esto cuando ambas curvas coinciden y tienen ya un movimiento que deja de tener efecto fíjense que ya se deja de mover allí es porque ya definitivamente Considero que este es el modelo ideal para este problema vamos a otro si este que tiene como dos este grupos diferentes como que estuvieron en extremos diferentes de este cuadrado ejecutamos Bueno yo sé que tarda un poco en encontrar la forma ideal aquí la curva no lo va mostrando sí los spots y será el enorme cantidad de pues no Bueno ahí ya parece que no avanza más Y eso es todo lo que puede hacer Qué pasa si voy a reloj que siempre decimos que es la mejor bueno ejecutamos rápidamente a conseguido un modelo Si puedo seguir un poquito más fíjense las curvas de pérdida bueno y ya ahí determinó que este es el modelo Cuál es la estructura de la red bueno fíjense que aquí tenemos dos variables de entrada una capa oculta de cuatro neuronas y una capa de salida de dos neuronas bien Esto lo que viene determinado de ahora justamente lo vamos a cambiar bueno fíjense que aquí me va mostrando en cada neurona como que va abordando un problema diferente acuérdense que las novedades siempre tienen que tener una resolución una función diferente si fueran todos iguales como dijimos antes todo parecería que fuera una sola neurona gráficamente aquí veo que justamente todos los gráficos son diferentes con lo cual a medida que esa neurona aborda una posibilidad de división diferente Cuál es la posibilidad de visión final Bueno aquí tengo en las dos finales esta y esta puedo agregar más capas ocultas Sí con este botón y a cada capa oculta le puedo agregar más neuronas Sí con este botón es el que ahora tengo dos capas ocultas y no solamente una fíjense que los dibujos de las capas ocultas no son iguales a las capas anteriores van resolviendo temas más específicos que tienen que ver con la forma final sí Bueno lo volvemos a ejecutar Bueno ya rápidamente encontró la solución ya ahí ya estamos Casi casi por la forma de la curva de pérdida la situación ideal fíjense los dibujos que hacen estas neuronas y los dibujos que hacen estas otras no resuelve el mismo tipo de gráfica que tiene que ver con la Gráfica final de de que va marcando la separación de un conjunto del otro y eso me da la idea de que como siempre dijimos en la teoría las capas van abordando temas cada vez más complejos de aquí ya lo más complejo sería aquellos que está cercano a la realidad que si esta división con esta forma la primer capa de neuronas No atiende aún la división parecida a lo que sería la final la segunda capa sí Entonces esto quiere decir que las últimas capas que van llegando a la salida se encargan de definir más en fino más precisamente Cuál va a ser la salida final yo aquí los dejo para que ustedes sigan jugando porque acá varias cosas con las que pueden trabajar en principio cambiar clasificación por regresión y me va a dar otros otras distribuciones de puntos y después trabajar con el learning rate es decir con la tasa de aprendizaje fíjense que en algunos casos podría ser esta vamos un caso de clasificación como los que teníamos antes un poco más sencillo bueno Esto va a ir muy lentamente porque el nene right es muy chico entonces va paso a paso a Pequeños pasos avanzando en base a lo que yo estipule como tasa de aprendizaje Aprendiendo a pasos muy pequeños sí bien volvemos le cambio esta tasa por una más grande bueno fíjense la aceleración que va tomando esto lo paro vuelvo vamos en el raid de nuevo vamos bueno fíjense que va resolviendo mucho más rápido bien Ustedes se preguntarán para que voy a elegir la opción más chica si estaba ahí más rápido Sí no se olviden que esto no siempre es así la más chica de alguna manera puede llevarme mucho gasto computacional pero como sé cuál es la ideal como sé que poner una mayor no me va a llevar al otro problema que vimos que ese problema donde justamente lo optimizador va de lado a lado de la curva convexa y nunca llega al punto ideal Bueno ese problema se puede presentar yo elegí ese una tasa muy grande como ésta fíjense lo que pasa no encuentra la solución no la encuentra porque porque justamente pasa esa cuestión gráfica que les decía recién va de un lado al otro de esa curva convexa y no encuentra la optimización de este modelo para que resuelva lo que tiene que resolver vamos a otro supongamos 03 volvemos bueno va más rápido piensa también las formas de la curva es muy interesante analizar eso bueno concretamente es una aplicación muy buena que realmente le pueden sacar mucho provecho para ir imaginando el efecto que tienen cada una cada una de estas cosas que hemos aprendido es una opción que me parece muy muy interesante muy importante bien después de haber hablado largamente de todos estos conceptos a lo largo de estas dos clases y poder haber jugado un poquito con esta aplicación que les mostré recién termina la clase me despido y ya en la próxima clase empezamos a programar hasta la próxima clase aquí termina esta clase los espero en la próxima clase nos vemos Titulo: Clase16 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/NJKUPY31Ihw  \\n 852 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 16  Hola a todos Bienvenidos a la clase número 16 del curso de Inteligencia artificial de ifes seguimos en el módulo de Deep learning tercera clase de este módulo y luego de Dos clases teóricas muy importantes vamos a empezar con la práctica a programar Nuestra primer red neuronal les recuerdo la necesidad de que las dos clases anteriores las hayan visto bien porque es muy importante que tengan muy presentes esos conceptos para justamente aplicarlos no solamente en esta clase sino en todo lo que venga de ahora en más así que sin más palabras pasamos al cola para programar Nuestra primer red neuronal bueno acá estamos en el colap de la clase 16 como recién mencionábamos dispuestos a programar Nuestra primer red neuronal Aquí vamos a hacer una paralelismo Si se quiere entre nuestra experiencia cuando probábamos algoritmos de regresión lineal simple y nuestra primera Nosotros sabemos siempre que en el caso de Machine learning a un algoritmo le damos valores de x y de y obtenemos con ello Mejor dicho el algoritmo genera un modelo en base a esos datos que le dimos encuentran un patrón generando modelo con lo cual genera una fórmula como esta que está aquí en la cual los valores de x y los valores de y bueno tienen relación con esta expresión matemática es decir cada vez que yo le dé un valor aquí de X lo va a multiplicar por 5 lo va a sumar por dos y va a obtener una y igual a la que yo le di como valor de entrada junto con las x para que genera el modelo bien Esto digamos no estoy diciendo más nuevo digamos es lo que ustedes han visto durante el primer módulo de Machine learning pero ahora la idea Cuál es Es que ustedes puedan comprobar que una red neuronal muy sencilla Nuestra primer red neuronal va a generar un proceso similar y va a obtener los mismos resultados que este algoritmo de regresión lineal simple aquí la diferencia es que esta fórmula la conocemos desde el vamos porque de hecho nos va a permitir justamente generar los valores de X e y quedamos a utilizar para generar bueno Nuestra primer red neuronal vamos a empezar Entonces por importar las librerías como siempre y aquí aparece Bueno un actor importante que vamos a ver o vamos a utilizar de ahora en más así como fue sidekick learn para Machine learning tensorflow es una librería muy importante de las más importantes del mundo de Deep learning también vamos a sumar a nampai y vamos a sumar también aqueras es una Api de tensorflow que nos propone una forma más sencilla de manejar las redes neuronales al menos en esta instancia que estamos empezando a programarlas Cuál es la situación tiene una estructura que por ahí puede ser un poco compleja para quienes empiezan a programar este tipo de algoritmos por lo tanto quieras es una Api que se desprende de tercio Flow tiene propósitos similares De hecho están íntimamente ligadas solamente que tiene herramientas más amigables para quien empieza a programar redes neuronales así que bueno explicado esto vamos a empezar a generar nuestro primer modelo Antes que nada es importante que Bueno les marque que en los libros de trabajo que vamos a traer en cola vamos a poner bastantes explicaciones de cada una de las etapas que vamos llevando adelante y bueno los porqués los conceptos Más allá de que bueno obviamente muchas de estas cosas han sido abordadas en las clases teóricas Bueno poco a poco vamos a ir bueno ampliando o consolidando todos estos conceptos que vimos en las primeras clases y lo vamos a ir poniendo en los comentarios de los notebooks esto no lo habíamos usado tanto en la primer parte de este curso pero visto que ahora vamos a ir un mundo un poquito más complicado un poco más complejo Bueno vamos a ir este bueno utilizando este método para que más allá de la explicación teórica que yo dé en la clase y ustedes puedan repasar viendo Este vídeo Tantas veces como quiera Bueno también tengan este apoyo en el marco del mismo Notebook así que bueno vamos entonces a crear nuestro primer modelo como decíamos recién y nuestro el modelo va a ser el tipo secuencial esto qué quiere decir que va a ser o va a estar preparado para ser formado por una secuencia de capas en este caso lo hacemos a través de secuencias de keras y queras de tensorflow y creamos un modelo que le hemos puesto un móvil Como tantas veces lo hemos hecho sabiendo que Puedo ponerle el nombre que yo quiera que le informo digamos aquí a esta creación de este modelo a través de este método secuencial bueno en principio que va a ser una estructura o una red tipo densa sí es decir aquí llamamos densa sí lo importante bueno básicamente las redes cuando tienen muchas capas formadas por muchas neuronas tienen que estar todas conectadas con todas y esa conexión múltiple digamos esa full conexión se llama o se reconoce como una red de tipo Dance bien y esta red de tipo densa Cuántas capas va a tener una como dice aquí y Cuántas neuronas va a tener esa única capa también una es decir que esto se reduce básicamente a un escenario parecido al que teníamos cuando arrancamos la clase teórica con un perceptrón bien obviamente esto tiene que ver con con hacer una primera red y hacerla de la manera más simple posible para después ir bueno complejizando los ejemplos bien creado Este modelo lo que tenemos que hacer a continuación es compilar nuestra red a través de compay sobre model porque insisto le pusimos a nuestro modelo móvil en el compay vamos a manejar tres hiper parámetros y esto nos va a traer justamente a recordar a conceptos que vimos en las clases teóricas el primero es el del optimizador recuerden todo lo que hablamos del algoritmo del descenso del gradiente que justamente nos permitía ir poco a poco optimizando los pesos y los sesgos de cada una de las neuronas para ir obteniendo una mejor función de coste una menor valor de función de coste en este caso justamente el optimizador que estamos eligiendo es el del descenso del gradiente solamente que en este caso se llama descenso de gradiente estocástico O sea no hay un solo una sola forma de algoritmo de descenso de gradiente este es uno de ellos luego lo que se especifica es la función de pérdida que es la función de pérdida lo que hablamos recién como función de coste Exactamente lo mismo aquí justamente se llama función de pérdida atiende lo mismo tenemos que buscar la menor función de pérdida y se especifica con este hiper parámetro de nombre los qué es lo que pongo en los bien para medir la función de coste o de pérdida no hay una sola forma también hay muchos métodos como recién mencionábamos tipos de optimizadores nosotros elegimos aquí este mini Square error que justamente es lo que dice aquí en la explicación método error cuadrado medio bien y finalmente con metrix especificamos que necesitamos que vaya dándonos referencia de la cura si decir el nivel de precisión que se va obteniendo paso a paso en el entrenamiento ahora vamos a crear dos array con todos los conjuntos de valores para x y para y tomando como referencia la función que ya dijimos hoy y = 5x + 2 por ejemplo de esta manera si x fuese menos 1 y sería menos 3 y así sucesivamente buscando otros valores que en este caso vamos a tomar nada más que seis valores los arrays están aquí Este es el array de las x y este es el array de ahora viene el entrenamiento del modelo fíjense que es exactamente el mismo método que usábamos en Machine learning fit ponemos las x las y el parámetro hipoch sí que esto sí es un concepto nuevo que lo que refiere es la cantidad de entrenamientos que quiero que haga este algoritmo para generar el mejor modelo posible buscando Qué cosas Bueno las dos cosas que aquí estamos poniendo en principio mejorar la precisión a través del optimizador del descenso del gradiente estocástico y bajando la función de pérdida o los fucsia vamos a entrenar el modelo y lo vamos a hacer Online para que ustedes puedan ver justamente Cómo se va mostrando la información que recién referencia vamos bueno y rápidamente me voy para abajo porque lo hace bastante rápido dado que es un modelo muy sencillo fíjense que me va mostrando el entrenamiento número de tanto de tanto sí 455 4 No ya terminó rápidamente no este me voy un poquito para arriba y fíjense que me va diciendo 488 de 500 487 500 y así desde el 1 de 500 Así que como empezó todo esto voy a ubicar bien aquí vamos a ir hasta arriba de todo bien aquí estamos en el primero fíjense la función de pérdida 161.0618 era que iba así en cero yo me voy a ir ahora hacia abajo y van a ver que la función de pérdida Ven aquí ya cambio bajó a 126 luego 99 luego 78 luego 71 luego 48 y va a seguir así bajando qué es lo que estoy buscando justamente que como el optimizador va generando mejores valores de pesos y sesgos la función de pérdida tiene que ir bajando hasta llegar Bueno a la mínima expresión posible si la idea que fuera cero Pero bueno este Esto va a llegar si nos vamos hacia abajo Buscando el entrenamiento 500 de 500 que sería el último de todo al valor 2.36 por 10 a la menos 6 bien se entendió O sea la función bajó desde aquel valor Al toque mencionamos al principio a este pero fíjense la quiura si está en cero o sea nunca tuvo un valor voy a subir un poquito aquí siempre fue cero por qué pasó eso bueno él aquí ahora sí va a funcionar siempre y cuando yo tenga un conjunto de entrenamiento en conjunto de tres aquí como no tengo conjunto de entrenamiento ni conjunto de Test porque solamente es un conjunto de seis valores de X ese valores de Y bueno pues entonces no hay altura así no hay problema ya en el próximo ejemplo que vamos a hacer vamos a tener los dos conjuntos y vamos a poder ver cómo así como va bajando la función de pérdida va subiendo el nivel de precisión o sea de atura sí finalmente vamos a probar el modelo y lo vamos a hacer justamente a través de predic que ya lo hemos usado mucho en Machine learning de el modelo que tenemos puesto model y vamos a imprimirlo dándole a predique el valor 10 con lo cual esto fácilmente podemos predecir Cuál va a ser el resultado el resultado como decimos aquí en las conclusiones finales debería terminar 152 y en realidad fue 52.00449 por qué pasó esto Bueno en realidad Nosotros le hemos dado en principio nada más que seis valores si para generar modelo lo cual obviamente es extremadamente escaso por eso el modelo no está tan seguro ni se afirma en que la fórmula sea igual a 5x + 2 por eso lo hace de una manera muy aproximada casi exactamente el valor 52 que debería haber sido pero justamente con un margen mínimo de error porque porque justamente esa muestra es muy pobre y demasiado bien se las Ha para generar un modelo bastante cercano a la realidad pero evidentemente no va a ser 52 sin decimales O sea la presión entera Como debería haber sido si yo hubiese usado esta fórmula concretamente bien hasta aquí llegamos con esta primer parte de esta clase la idea era generar un modelo muy sencillo para que recorramos todos los componentes y los pasos que hacen a la conformación de una red neuronal artificial conterson Flow y con keras de ahora más vamos a empezar como ustedes ya imaginarán a complejizar la situación y ya por lo pronto en la próxima parte de esta clase vamos a ir un modelo un poquito más complejo y más parecido a lo que nos va a tocar en la realidad de las próximas clases Bueno nos vemos en la siguiente parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte Titulo: Clase16 (parte 2) Curso Inteligencia Artificial \\n URL https://youtu.be/qgoA7M8msSs  \\n 1734 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con  ella bien seguimos con la segunda parte de la clase 16 ahora vamos a implementar nuestra segunda red neuronal un poco más compleja que la anterior y en este caso vamos a hacerlo usando el dataset Fashion mnist el nombre les va a sonar familiar porque justamente el dataset mnist lo usamos eh mucho en el módulo de Machine learning dado que teníamos allí bueno algunos los primeros ejemplos de lo que era visión computacional con números escritos a mano y digitalizados en matrices de 28x 28 bien acá Este dataset es algo Bastante parecido solamente que en lugar de tener variantes de número de 0 al 9 tiene variantes de prendas de vestir o calzados que están codificados de 0 al 9 según esta tabla que aquí les muestro cero camiseta uno pantalón bueno etcétera etcétera hasta llegar a bota que es la número nueve la estructura del conjunto de datos es similar es decir tiene 70,000 observaciones como era el emnis eh las matrices son de 28 por 28 y tiene dentro de las 70,000 10,000 para conjunto de Test o sea 60,000 para entrenamiento y 10,000 para test Exactamente igual al modelo mmis obviamente aquí tenemos alguna referencia gráfica Para que vean el contenido eh obviamente Estos no son todos los elementos pero sí para que tengan un pantallazo del contenido de de este conjunto de datos ven que tenemos tres filas por cada tipo de prenda sí las primeras dijimos que eran camisetas segundo pantalón pulover Bueno aquí tenemos tres filas de camisetas tres filas de pantalones tres filas de pullovers y así hasta llegar a las botas que era el último producto bien eh evidentemente estas imágenes se ven aquí muy pequeñas pero cuando las agrem van a poder comprobar que una imagen de 28x 28 es poca calidad no esto es obvio no pero bueno está bueno referenciarlo así que vamos a trabajar entonces una Inteligencia artificial de tipo reial con este conjunto de datos antes vamos a ir a esto que está aquí que es importar las librerías vamos a usar tensor Flow map lip y esto que puse Aquí es importante para que puedan constatar la versión de tensor flow que están usando el tema de las versiones es un tema muy importante y que vamos a empezar a a observar mucho en todas las prácticas que vamos a llevar adelante ahora en más dado que bueno conforme pasa el tiempo aparecen nuevas versiones de de todas las librerías que vamos a usar y Bueno hay librerías que cuando cambia la versión desafectan algunas cosas o las cambian por lo tanto puede que alguna práctica que con una versión funcione con otra No lo haga y simplemente habrá que hacer algún cambio para poder adaptarlo a la nueva versión bien Así que aclarado esto vamos concretamente al primer paso que es poder dividir el conjunto de test y de entrenamiento Recuerden que ya lo habíamos hecho esto porque así como el conjunto de datos mnist venía ya con el conjunto de entrenamiento y el conjunto de Test preparado De antemano Bueno aquí va a pasar exactamente lo mismo solamente que lo vamos a obtener cargando el conjunto de datos de esta mod sí tfas dataset Fashion en nist este ya es un conjunto de datos que al igual que en nist ven Tena ya preparado dentro de una librería en este caso dentro de la librería dataset de queras de tensor Flow y creo aquí la variable FM nombre que se me ocurre Como siempre digo puede ser el que ustedes quieran bien creo ahí y ahora sí lo que hago es separar los conjuntos de Test y entrenamiento De qué modo bueno con el método lo datata sobre fnis es lo que Acabo de crear aquí arriba bueno tomo image training o training image training labels test image y test l qu corresponde si tomo El ejemplo de aquí arriba a lo que era antes la variable que había creado como xtrain como itrain como XT y como it sea he puesto este comentario aquí arriba para tratar de hacer algún tipo de de de correlatividad entre lo que hacíamos antes y lo que hacemos ahora concretamente es exactamente lo mismo solamente que al cargarlo lo a Data lo hago de este modo y no como lo hacía antes que como lo expuesto aquí arriba bien cargamos los conjuntos y vamos a visualizar para aprender un poquito más de cómo se cargan esta información Dentro de este conjunto de datos que tiene una matriz de 28 por 28 vamos a hacer un print de train images sub cero es decir la primera observación Ya lo hemos hecho antes sí así que hacemos un print y vemos que acá aparecen todas las filas de 28 por 28 es decir 28 filas por 28 elementos cada uno vamos a hacer lo propio ahora con la etiqueta training Level sub cero también y me dice que el resultado es nu Qué quiere decir nu que corresponde con una etiqueta de tipo nu9 que justamente representa bota sí es decir alguno de estos elementos que que está aquí debajo bien al tener esa información también vamos a imprimir justamente Esa esa bota justamente con image trining subcero y con semap grade lo que le digo es que me imprima esto en tonos de grises que es realmente cómo está cargada la información sí en este este conjunto de datos bien hago esto y obtengo esta imagen sí ven es una bota de muy baja definición como imagen pero nos va a permitir mirar algo muy interesante fíjense la primer fila de este conjunto de valores le dirá las tres primeras filas tienen todos ceros los ceros representan el color negro si ustedes ven la imagen vamos aquí abajo van a poder ver que las tres primeras filas son todas negras Por eso cada uno de los valores que tienen estas tres primeras filas son Cero en la medida que vamos bajando en la imagen obviamente van apareciendo otro tipo de píxeles que no tienen color cero no tienen color negro bueno Y esto Si miran Aquí van a ver que los ceros en la medida que van bajando van desapareciendo van dando lugares a otros valores numéricos no cero y que representan distintos tonos de gris sí y así hasta llegar al final donde al final fíjense que tengo una dos tres dos líneas completamente de ceros y la tercera eh empezando de abajo hacia arriba por supuesto con este otro valor ven que aquí hay algunos tonos que son oscuros pero ya no son el negro puro HM esta en la tercer fila empezando y si trabajo hacia arriba es algo que veo reflejado aquí bien Esto simplemente a título de curiosidad Me parece que ha algo interesante para que puedan entender este tema de los valores y los colores que representa lo próximo que vamos a hacer es un proceso de normalización ya tenemos en claro ese concepto es tratar de que los valores vayan entre 0 y 1 y en este caso en particular como los valores van entre 0 y 255 la manera de normalizar que vamos a utilizar Es simplemente una expresión matemática que es esta que está aquí que es dividir Al conjunto de entrenamiento de imágenes por 255 y el conjunto de test de imágenes también por 255 con lo cual de esa manera he logrado normalizar el conjunto de entrenamiento y de test de imágen o sea los valores de entrada del conjunto de entrenamiento los valores de entrada del conjunto no los valores Label sí los valores de entrada una vez que hacemos esto pasamos a diseñar el modelo bien aquí tenemos alguna particularidad respecto a lo que habos hecho antes por qué bueno porque acá ya no tenemos una estructura de una sola capa y una sola neurona por eso le hemos dado como nombre a este modelo model 2 para diferenciarlo del model que habíamos utilizado antes no obstante seguimos usando sequential de models de queras Por qué Porque justamente es una red de tipo secuencial múltiples capas tenemos en este caso Cuántas capas tres capas la primera es de tipo flatten esto es muy común si cuando este entra al modelo una imagen o cualquier estructura que sea de 28 por 28 sea que sea una matriz No importa De cuánto sea Perdón en este caso 28 por 28 yo tengo que transformar esa matriz en un vector unidimensional es decir que lo que tengo que hacer de alguna manera como dice aquí flat es aplanar esa matriz para que cada una de esas o filas si conformen una única fila de 784 valores bien Esto es lo que hace flaten y siempre va a ser en este caso este lo que deba hacer si yo recibo insisto una matriz y necesito como entrada un vector luego viene la primer capa densa por eso hago keras layers dens y esta capa densa tiene dos cosas interesantes primero ya le digo Cuántas neuronas quiero que tenga 128 y le digo la famosa función de activación de la que hablamos en la teoría qué tipo quiero y es la que mencionamos se acuerdan como que dijimos que la que más íbamos a usar Perdón es relu h y finalmente la otra capa es una capa densa de 10 neuronas y tiene otra función de activación que es sofm por qué porque esta capa Es la capa de salida y esa capa de salida lo que tiene que darme son tantas neuronas como posibles valores de salida pueda tener esta en este caso 10 porque son 10 tipos distintos de de prendas de vestir y la función de activación es softmax se acuerdan que dijimos en la clase teórica que en la capa de salida voy a tener dos opciones mayoritariamente de tipo de funciones de activación si la salida es binaria yo voy a usar la sigmoide y si es de tres o más variantes la z qué es lo que hecho aquí concretamente repasando Este modelo esta capa es la capa de entrada esta capa es la única capa oculta pueden haber más en este caso es la capa oculta de este modelo y esta capa Es la capa de salida o sea que ya tenemos un modelo completo más parecido a lo que hablamos en la teoría capa de entrada capa oculta y capa de salida bien avanzamos con el próximo paso ahora con la compilación del modelo donde van a aparecer dos elementos nuevos y que respaldan de alguna manera lo que dijimos en la primera parte de esta clase porque digo esto Aquí está el optimizador como habíamos usado hoy solamente que no vamos a usar el sgd como usamos recién sino el Adam porque es más propio más conveniente y más rápido para este tipo de eh modelos que trabajan con imágenes esto por qué digo respalda lo que dijimos antes porque yo antes dije que cuando pusimos cgd no era el único tipo de optimizador sino que había otras variantes Bueno aquí hay otra variante y dije lo mismo con la función de pérdida usamos el método de mínimos cuadrados aquí usamos spars categorical Cross entropy que en realidad es más propio para cuando tenemos valores de tipo categórico es decir que en este caso voy a usar esta función de pérdida sí no es binario acd que no tengo un problema binario como antes Tengo un problema de distintos tipos de categoría que son las las 10 tipos de prendas que hay de de vestir o o de calzado y el acur lo dejamos tal cual estaba pero fundamentalmente quiero que presten atención y que en este caso usé otra función de pérdida y usé otro optimizador con esto que después ya más adelante vamos a ver cuando nos conviene uno u otro que no es el tema de este momento sí lo importante que quiero que vean aquí es que hay más de un optimizador y aquí está el ejemplo y hay más de una función de pérdida y aquí está el ejemplo ahora vamos a entrenar el modelo Sí aquí tenemos model 2. fit y le pongo la variable de entrada o los valores de entrada que son los eh la definición de cada uno de los píxeles de la variable de entrada son todas las x que entran y la variable de salida que sería ahí que venimos trabajando habitualmente que es el Label y le voy a poner nada más que un entrenamiento de cinco epoch sí Y esto a diferencia de antes que yo no había puesto est objeto aquí o sea eh entrenaba el modelo pero no asignaba el resultado de ese entrenamiento una variable lo voy a asignar Ahora sí una variable ya vamos a ver para qué sí entrenamos el modelo y después vemos eso Bueno ahí terminó el entrenamiento como habrán visto son solamente cinco entrenamientos y tardó mucho Sí ya estamos trabajando insisto con un conjunto de datos mucho más grande que el caso anterior que dan solamente seis valores y eh lo que tenemos aquí es el descenso de la función de pérdida de 04 997 a 02944 Obviamente que si tengo más entrenamientos va a bajar mucho más y el acuras que va de 08241 a 0899 queé es lo que no habíamos visto en el ejemplo anterior porque no teníamos separado conjunto de entrenamiento y test y ahora sí Por lo tanto Ahora sí vemos los datos de ambas cosas sí Entonces con esto puedo ver de que tengo bueno una queas del orden del 89 con solamente cinco entrenamientos eso en cuanto al conjunto de entrenamiento Qué pasará con el conjunto de Test acuérdense que al igual Perdón que Machine learning tengo que evaluar para el conjunto de entrenamiento y para el conjunto de Test lo hago con el método evaluate de model dos si y le mando la x de Test y la y de Test si las viejas x e de Test es decir test images aquí y test levels para este caso lo ejecutamos y comprobamos que en este caso la el accuracy para el conjunto de Test obviamente es un poco menor que el conjunto de entrenamiento Y es de 0877 o sea 8707 por. bien Ahora vamos a desvelar eh Por qué hablábamos recién de que cuando entrenábamos el modelo lo igualamos a una variable a esta variable que le hemos dado en Llamar histórico lo que voy a destinar lo que a poner dentro de ella es todos y cada uno de los valores de las pérdidas y de los acuras con lo cual voy a tener la posibilidad de como vemos aquí poder acceder a crear una Ray de acasis y una Ray de pérdidas y con ello lo que voy a hacer es y todo lo que está aquí abajo graficar ambas curvas lo ejecuto y fíjense lo que obtengo lo voy a reducir un poquito en tamaño para que se vea mejor un poco más bien Ahí está tengo la precisión del entrenamiento que desde el principio y hasta el último epoch va de 0 a cu obviamente Este es el epoch número uno fíjense va creciendo creciendo creciendo creciendo pasando De nada al 083 que era el primer caso al 089 que es el último y la pérdida tiene un efecto completamente contrario arranca arriba y termina o bajando conforme van pasando los entrenamientos Bueno este tipo de gráfica de una precisión y de la pérdida tienen que siempre tener esta figura si no tenemos estas figuras sí es decir que la precisión no sube y la pérdida no baja bueno es evidentemente o será evidentemente un mal modelo Pero qué pasaría si el entrenamiento fuese no de solamente cinco épocas fuera de 50 épocas sí obviamente la primera respuesta virtud del tiempo va a tardar mucho más obviamente pero qué va a pasar con la precisión va a mejorar mucho poco no va a mejorar Bueno vamos hacia atrás a simular porque a este modelo ya lo entrenamos al modelo donde al punto Perdón donde creamos el modelo nuevamente lo compilamos nuevamente y nos salteamos el entrenamiento de cinco épocas y vamos directamente al entrenamiento de 50 épocas bueno acá no lo igual un histórico podría haber hecho eso y para poder ver los gráficos como recién y hubiera visto gráficos con en lugar de cinco entrenamientos con 50 Pero bueno vamos a ejecutarlo Entonces ahora Este modelo con insisto 50 Bueno ahora vemos Que bueno finalizado este largo proceso me dio una curiosi muy bueno de 0.96 96,52 por. ahora vamos a hacer como antes la medición con el conjunto de Test y vemos que el resultado que me arroja Es muy inferior 88,55 por. Esto me puede llevar a presumir que se ha hecho probablemente un sobreajuste Bueno no importa no obstante vamos a ver otro tema importante que es el tema de la deducción O la visualización del resultado de manera probabilística O sea no quiero que me diga que la el resultado de eh Este modelo digamos de de evaluar una predicción con este modelo sea un nueve como en el caso anterior se acuerdan que habíamos probado que la predicción del primer caso me daba que era un nueve que representaba la la bota digamos no sí esta bota que estaba aquí arriba sino que lo que quiero hacer en todo caso es ver probabilísticamente cómo me da el resultado de la primera observación Entonces vamos a imprimir este código que está acá qu lo que hace es imprimir la clasificación sí de la primera observación como con predict hago un predict de todo el conjunto de Test y los resultados de las predicciones las pongo en una variable que se llama classification es como antes le poníamos el ipred se acuerdan Bueno es lo mismo ahora le cambiamos el nombre podríamos Haber puesto ipred también no bueno y después imprimo la la etiqueta sí original de eh de esa observación veo que la etiqueta original es nueve es la bota obviamente como dijimos el primer elemento pero vamos a mirar el resultado demostrado de manera probabilística como ya lo hemos hecho alguna vez en alguna práctica de Machine learning fíjense que acá tengo una probabilidad de cada uno de los elementos de que ese sea en este caso el elemento predecido correcto bueno evidentemente eh la predicción es nu porque de todos estos 10 valores el más alto es el último y como va de 0 a 9 el más alto es el noveno perdón el décimo en la ubicación pero el número de índice número nue por qué Porque evidentemente ya fíjense con las expresiones menos tanto que quiere decir 10 elevado a la menos tanto el el valor que tiene ese coeficiente menor es este que es 10 a la men1 sería 9,99 10 a la men1 que quiere decir 99 99% Sí 99,999% así que hay una altísima probabilidad de que sea la bota y de hecho es la bota bien y para terminar la clase vamos a usar el mismo modelo que utilizamos hasta ahora pero vamos a ponerles algunas pequeñas variantes Para que vean justamente bueno como ese modelo puede tener distintas versiones con distintas opciones lo primero que nos proponemos aquí es cambiar significativamente el número de neuronas de la capa oculta cambiando los las 128 neuronas que teníamos en el modelo anterior por esta que tiene ahora 1024 neuronas Cómo se hace simplemente yendo al lugar donde yo ponía la cantidad de neuronas de la capa oculta 128 y cambiándolo por 1024 el resto del ejercicio está todo en una celda pero es Exactamente igual que el caso anterior donde se termina midiendo el acuras del conjunto de entrenamiento el acuris del conjunto de Test vemos aquí 89,6 6% y 87 51 y haciendo una predicción probabilística y luego comparándola con el resultado final que siempre es la bota que siempre decimos aquí lo que tenemos que observar es que con este nuevo modelo también tenemos una predicción certera dado que el valor ubicado en la décima posición de índice número nu es el que finalmente tiene la mejor probabilidad y Por ende es el valor que ha sido designado como valor siguiente que tenemos aquí es la posibilidad de no tener una capa oculta sino tener dos capas ocultas donde la primera es de 512 y la segunda es 256 esto es muy común que la cantidad de neuronas vaya decreciendo hacia la salida eh como parte de lo que es la estructura clásica de una red neuronal artificial bueno no hay mucho más cambio que ese simplemente poner dos capas en lugar de la que teníamos antes de 128 o la última de 1024 tenemos una de 512 y una de 256 ambas capas obviamente densas luego insisto como recién el resto de los ejercicio es Exactamente igual que en el caso anterior y los casos precedentes y el último ejemplo que quizás sea el más interesante es de la aplicación o utilización del método callback para detener el entrenamiento de qué se trata esto Nosotros sabemos que cuando hemos podido comprar justamente porque hicimos un ejemplo con cinco os y otro con 50 epods en la medida que haya más entrenamientos obviamente va a haber una mayor posibilidad de tener un mayor score un mayor aquí H Sí con lo cual puede hacernos llevar a pensar que yo cuando tengo que llevar a desarrollar un modelo y quiero que sea exitoso voy a poner un número muy alto altísimo de cantidad de entrenamientos porque de esa manera me voy a asegurar tener un buen score un buen acac bien obviamente como siempre decimos todas estas decisiones grandilocuentes o exageradas son innecesarias muchas veces con menos se puede lograr el mismo objetivo y seguir persistiendo en un accionar de ese tipo buscando un mejor score cuando ya tengo un muy buen score es algo que seguramente puede ser que crezca digamos ese accuracy pero no va a ser significativo con lo cual no es no se justifica hacer ese esfuerzo con este método lo que vamos a hacer es decirle al método sí que va a ser una clase como Tenemos aquí y una función que vaya cuando va terminando cada entrenamiento verificando Cuál es el accuracy Y si el accuracy es eh o llega a los niveles que yo quiero automáticamente el entrenamiento se va a detener Aquí voy a poner un una que s un objetivo bastante bajo no que es del 90 por nada más Pero más que nada para que se vea el efecto y luego lo que hacemos el resto es Exactamente igual volvemos a un modelo de una sola capa Comparado con el ejercicio que hicimos recién y el resto de este el ejercicio Exactamente igual solamente que qué va a pasar aquí aquí va a pasar que cuando yo ejecute esto se va a parar no en los 100 que le he puesto aquí fíjense que puse un número muy alto sino que se va a parar cuando la el acuras llegue a ser mayor al 90 por lo ejecuto y veo que justamente tengo en el epos 1 083 087 088 088 85 089 53 09021 en el sexto entrenamiento por lo tanto no se llega al séptimo entrenamiento porque que bueno evidentemente esto ya ha llegado a 09021 y con eso es suficiente como dijimos recién el proceso se cancela en el epoch número 6 y eh vamos a ver aquí que eh podemos que no lo hemos hecho aquí arriba medir la evaluación del conjunto de Test y vemos que nos arroja un 088 30% bueno habiendo hecho tantos modelos y habiendo obtenido tantos scores lo que vamos a hacer para terminar esta clase es hacer una tabla viendo todos y cada uno de los modelos con las variantes que hemos tenido y cuáles son los valores de act Así que hemos tenido en cada caso para el conjunto de Test y para el conjunto de entrenamiento y sacar algunas conclusiones bueno Y aquí finalmente como dijimos recién una tabla con las cinco variantes de modelos que trabajamos más la columna donde tengo todos los acacy train y todos los acacy test sobre el conjunto de entrenamiento sobre el conjunto de test bueno eh 128 neuronas con 5 ort 198 neuronas con 50 1024 neuronas con 5 ort la que tenía dos capas 502 y 256 con c epoch y la última con el colb que se cortó automáticamente con el sexto epoch porque le pusimos que a los 090 de acur train se cortara si lo hubiésemos puesto a tumbral bueno quizás hubiese llegado más lejos eh una conclusión que podemos tener aquí Más allá de la cantidad de capas y más allá de la cantidad de neurones que es muy significativa la diferencia cuando entreno muchas más veces sí con lo cual los invito a que por su cuenta con paciencia eh utilizando digamos la opción que nosotros no hemos utilizado aquí pero que justamente al finalizar esta clase les quiero mostrar que es alterar el entorno de ejecución yo voy aquí a la opción entorno de ejecución y le pongo cambiar el tipo de entorno de ejecución y activo la opción gpu eso van a acelerar bastante este los procesos de entrenamiento de los modelos h en algunos casos hasta puede ser que tarde tres veces menos sí eh Lo que pasa que esta opción está puesta de manera como todo cola de manera gratuita o de pago con lo cual obviamente la gratuita Tiene ciertas limitaciones depende del momento que nos conectemos hay veces que por ahí nos vamos a conectar y no nos va a dar el servicio eh o a veces este nos lo va a cortar utamente bueno obviamente es algo grao Hay que tomarlo de este modo pero hagan el intento de entrenar los otros modelos que los entrenamos con pocos epod con esta herramienta de gpu simplemente pones gpu y ahí me me dice Bueno obviamente yo estoy conectado sin gpu me propone volver a conectarme con gpu no se puede cambiar sin cortar la conexión eso es importante que lo sepan con lo cual si cambian el entorno tienen que volver a ejecutar todo lo que anteriormente no se ejecutaron porque pierden la conexión y Es como empezar nuevamente todo ese bien aquí se lo voy a poner Cancelar y eh digo de esta manera pueden probar que todos los modelos que solo usamos cinco o se epoch como en el último caso Bueno aquí pasar de 5 a 50 Aquí también Aquí también y en este caso que usa el callback ponerle más que la cantidad de poch más alta el umbral más alto pongámosle por ejemplo como este 094 095 096 lo que ustedes quieran digamos porque justamente Esto está puesto como referencia para que ustedes puedan probar próximos modelos Y a partir de esta tabla puedan darse cuenta si la cantidad de superior hizo que esos valores mejoraran teedo que sí pero bueno cuánto má Cuanto más y por eso si se justifica hacer ese cambio Bueno hasta aquí llegamos con esta clase terminado toda esta práctica de la clase número 16 en la parte un y la parte dos nos vemos en la próxima clase hasta entonces aquí termina esta clase los espero en la próxima clase nos vemos Titulo: Clase17 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/1rBuE0-juks  \\n 1773 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 17  Hola a todos Bienvenidos a la clase número 17 del curso de Inteligencia artificial de ifes seguimos en el módulo de Deep learning y vamos a ver un nuevo tema justamente Deep learning que son las redes neuronales convolucionales la clase pasada empezamos a ver nuestra primera red neuronal justamente programandola con python en el ámbito de cola en el cual vimos primero Un ejemplo muy sencillo y luego un ejemplo un poco más complejo en el cual tratábamos imágenes de un Data set que llamaba Fashion Eminem en esa última red neuronal tratábamos de dilucidar en un problema de clasificación si una imagen que entraba a esa red era un determinado tipo de prenda o de calzado u otro con esa red logramos buenos objetivos Tuvimos una buena precisión y realmente con las pruebas que hicimos logramos en todos los casos por ejemplo tomando el ejemplo de la bota poder dar que la imagen que entraba representaba una bota pero en la vida real las imágenes no siempre van a venir como están preparadas en ese conjunto de datos dado que las imágenes probablemente no sean exactamente iguales Ni estén ubicadas en la misma posición en todos los casos ese zapato está ubicado en una misma posición y representaba una idea estéticamente similar bueno Esto no va a pasar siempre Por ende es que las redes neuronales convolucionales implican una evolución en el tratamiento de las imágenes y el reconocimiento de las mismas a través de características o patrones que forman parte de un conjunto de imágenes es por ello que ahora yendo al colap vamos a empezar a hablar de este tipo de redes y vamos a lograr justamente implementar al igual que en la clase pasada un problema sencillo y en la segunda parte de la clase un problema más complejo vamos con ello Bueno aquí estamos en el cola 17-1 que es el que corresponde a la primera parte de esta clase en donde se hace un abordaje teórico de lo que deseamos recién en la introducción de esta clase básicamente las limitaciones que tiene la red que diseñamos en la clase anterior en cuanto al reconocimiento de imágenes que son muy avanzadas acá tenemos un pullover y acá tenemos una bota que es lo que usamos como ejemplo en la clase anterior cuál es el problema aquí supongamos que yo quisiera comparar esta bota con otra imagen de entrada que fuese esto evidentemente Esta imagen más allá de que tiene una calidad diferente porque tiene una definición que no es de 28 por 28 aún así yo llevase esta imagen a una definición de 28 por 28 esta representación y todas las otras con las cual la red se entrenó Y a partir de eso identifica que cualquier cosa que se parezca a Esto va a ser clasificado como una bota Esto va a ser muy diferente y difícilmente ocurre la misma suerte dado Que obviamente las perspectiva en que está mostrada está gota es radicalmente diferente esto más allá insisto de la cantidad de píxeles que tenga como definido Entonces esto es un simple ejemplo y esto se puede llevar a cualquier otra cuestión como puede ser el pullover o el resto de los componentes que forman parte de El Data set Eminem y correr la misma suerte con lo cual Es evidente que esta red que hicimos la clase pasada para la identificación de imágenes tiene serias limitaciones es por ello que para tratar de tener una versión más avanzada de red neuronal que pueda trabajar con imágenes se crearon las redes neuronales convolucionales esta redes neuronales trabajan con dos conceptos fundamentales que son las convulsiones y los publiques o agrupaciones Pero antes de avanzar con la descripción de la estructura de este tipo de red vamos a poner un ejemplo de cómo trabaja el concepto de red neuronales convolucionales desde lo conceptual después vamos básicamente a lo estructural supongamos que queremos una red neuronal que pueda reconocer en una imagen si algo que se está visualizando en ella eso no un rostro bien lo que queremos es una red que no Determine a partir de todo un conjunto sí como un todo si algo es o no un rostro a partir de como hicimos en la red neuronal de la clase pasada ponemos un montón de rostros y detecte un patrón con ello sino que ese patrón esté establecido a partir de un conjunto de características que tiene eso que conforma un rostro por ejemplo empezar a detectar si hay ojos si en nariz lleven la boca etcétera etcétera es decir si todo ese conjunto de elementos determina que eso es un rostro la determinación de ese conjunto de elementos si va a ser a partir de características es decir yo voy a determinar que para que algo sea reconocido como rostro tiene que tener un montón de características que lo que dijimos recién algo que tenga una forma de ojo otra cosa que tenga una forma de nariz otra cosa que tenga una forma de boca y cada una de esas características cuando es imagen pase por toda esa características y tenga una devolución positiva reconociendo que esas características están en esa imagen pues Entonces será determinado que eso es un rol Así trabajan las redes neuronales convolucionales aquí tenemos un gráfico que lo vamos a ir tomando como referencia en cada uno de los pasos que tengamos en esta clase en primer lugar Tenemos aquí la imagen natural si la cual va a ser pasada por un proceso de convolución Qué es una convolución una convolución como dice su definición aquí es un filtro que pasa sobre una imagen la procesa y extrae características que muestran una similitud en la imagen Es decir si vamos a gráfico de aquí aquí tengo la imagen original y le aplicó un filtro que es esto que está aquí en Amarillo a partir de eso género un mapa de características así se llama el conjunto de características que a través de distintos filtros yo extraigo de una imagen con la que voy a entrenar a la red con lo cual esta característica representan esto que hablamos recién de el caso del rostro y los componentes del rostro y tengo aquí una muestra obviamente solamente de cuatro características se trabajan con mucha más características obviamente simplemente Porque queremos simplificar para que este gráfico se vea bien Vamos a ver aquí una cuestión que es importante que tiene que ver con las dimensiones de lo que entra las dimensiones del filtro y la dimensiones de lo que sale luego de la coagulación para eso vamos a un ejemplo primero de explicar cómo se aplica a través de un ejemplo muy sencillo cómo sería una convolución aquí tenemos un ejemplo supongamos que estos 9 píxeles son parte de una imagen obviamente pensamos en una imagen de tono de grises y por eso cada Pixel está representado por un número que va de 0 a 255 como ya vimos en el caso de los Data set Eminem y Fashions bien cada uno de estos números que está justamente a propósito con un tono de gris representa un píxel lo que voy a hacer con este Pixel es aplicar un filtro el filtro es esto que está aquí en Amarillo menos 101 menos 202 menos 1 0 1 obviamente Este es un ejemplo muy sencillo pero los filtros los va a determinar automáticamente la red neuronal convolucional en este caso estamos tomando un ejemplo de un filtro del 3x3 muy usado digamos en las redes en general y lo que hace aquí justamente es aplicar una cuenta que la realiza multiplicando cada uno de los valores de los filtros por su correspondiente en posición de El Pixel que tomamos como ejemplo así de este modo multiplicamos 0 por -1 64 por 0 128 por 1 tal como tengo la cuenta aquí al costado y justamente cada una de estas nueve multiplicaciones las voy a sumar y el resultado me va a dar un número ese número que yo obtenga 86 en este caso va a ser el que va a reemplazar al 192 original este proceso se va a repetir por casi todos los píxeles de una imagen y ya vamos a ver porque decimos por casi todos y justamente va a hacer que todos los valores vayan teniendo un nuevo valor a partir de que se aplicó ese filtro ese filtro una vez que pase por toda la imagen va a ir generando paso a paso un mapa de características luego otro filtro hará lo propio recorrerá la imagen y generará otro componente de todo este conjunto de mapas de características a diferencia del caso que vimos aquí de un filtro de tres por tres aquí tenemos un filtro de 9 * 9 y he generado a partir de imágenes 36 por 36 4 mapas de características obviamente a través de cuatro filtros de 9 x 9 de 28 por 28 para ver un ejemplo de esto y que sea lo más gráfico y animado posible le he dejado aquí una URL que apunta a una animación vamos a hacer clic y la vamos a ver aquí tenemos el filtro donde en esta animación que está muy interesante van a ver que en este caso con centro en el 7 apk el filtro y en la misma posición Ha logrado un 9 aquí está la cuenta no que pueden ir siguiendo paso a paso luego hace lo propio y genera un nuevo valor en reemplazo del 6 que es el 8 y bueno ahora acelera la animación y van viendo que al recorrer el filtro toda la imagen va generando un nuevo valor para cada uno de los pasos de esto que está aquí que es el mapa de características bueno esta animación la pueden ver Tantas veces como quieran a quitar a cuenta que también es muy interesante para que se entienda la lógica que no es muy compleja Pero bueno aquí vuelve a aplicar un filtro tres por tres como el ejemplo que tomamos nosotros a diferencia de el gráfico que tenía un filtro a 9 por 9 y ahora vamos a ver un poco algo relacionado con la importancia de la determinación del tamaño del filtro bien Ahora veamos lo siguiente Porque si tengo una imagen de 36 por 36 el mapa de características es de 28 por 28 bien la razón es el tamaño del karland o del filtro Sí el filtro también se le llama kernel en este caso fíjense lo siguiente si yo tengo un canal de 9 por 9 ese carnet tiene que estar aplicado en una situación en la cual el centro tiene que tener de 9 por 9 4 vecinos a la izquierda cuatro vecinos a la derecha cuatro vecinos arriba y cuatro vecinos abajo justamente porque es de 9 por 9 Por ende ese filtro nunca se puede aplicar a otro elemento que esté en la imagen que no tenga esa cantidad de vecinos por ejemplo no podría ser el 00 la posición cero cero porque no tiene vecinos tiene cuatro abajo cuatro a la derecha pero no tiene arriba ni a la izquierda entonces Esto hace que yo deba dejar cuatro filas arriba cuatro filas abajo cuatro columnas a la derecha y cuatro columnas a la izquierda pasando en consecuencia de una estructura de 36 x 36 a una estructura de 28 por 28 como tienen cada uno de los mapas de características generados por este filtro de 9 x 9 ahora para hacer un código que también nos permita entender esto desde algo que sea lo más visual posible dado que después empezamos con las redes neuronales y toda esta cuestión obviamente va a quedar muy oculta en justamente las capas ocultas es que tenemos una práctica simulativa con python de Cómo aplicar filtros en una imagen que tomamos como ejemplo la imagen que tomamos como ejemplo la importamos desde una librería que se llama sai pay y justamente a través de Miss tomamos lo que es una escalera Sí una imagen de una escalera por eso se llama ascent Sí y justamente empiezo por con mapley plt haciendo un ploteo básicamente de cada uno de los componentes de la imagen y me da la resultante que es esto que veo aquí bien luego lo que vamos a hacer es tomar una copia de esa imagen generando una variable que se llama img Transformers y vamos a tomar a través del método shape el 0 y el 1 Cuáles son las dimensiones de esa imagen si los sites y me da que el size es 512 por 512 bien Luego hacemos un código a través del cual vamos a generar una matriz de tres por tres que es Ni más ni menos que el filtro y luego en este código hacemos un Ford donde vamos a recorrer todos los puntos de esta imagen de aquí sí decir cada uno de los X y los y de esta imagen que está aquí evitando como dice aquí los bordes por eso arranca de uno y no de Cero y llega a 6 menos 1 si al ser de 3 por 3 lo que estoy dejando como vecino a diferencia del caso 9 por 9 es solamente uno uno arriba uno abajo uno izquierdo en la derecha y recorro los X dentro de los X recorros y en cada uno de estos casos lo que hago es generar una cuenta similar una cuenta acumulativa similar a la que hicimos aquí Este ejemplo y vimos también en la animación fíjense que lo que hago Ni más ni menos es multiplicar el filter 0 0 el filtro 01 el filtro 02 es decir cada uno de los elementos de este filtro menos uno el menos dos el menos uno el cero el cero cero el uno el dos y el 1 y multiplicarlo por cada uno de los píxeles de cada una de las partes de esa imagen Y obtener un nuevo Pixel y ese nuevo Pixel lo voy a poner en el mapa de características que lo que estoy haciendo aquí es decir en cada xy que tomaba de antes Ahora hay un nuevo x y por eso la salida es img Transformers que es la imagen original y poniéndole convolución que convolution convolution es el resultado de esta cuenta fíjense que es importante aquí lo que estoy poniendo esta parte de aquí la suma me puede llegar a dar más que 255 mucho cuidado con esto el filtro puede dar un valor Superior y sabemos que si pongo un valor superior 255 no va a representar ningún color y me va a dar un error Por ende trata de evitar de que el valor no sea mayor que 255 o algo muy importante que sea negativo también puede pasar que sea menor que cero por eso este If de acá es importante finalmente hacemos la impresión y vemos que el gráfico sí que en este caso no es una imagen como la original sino ya es un mapa de características resalta de alguna manera todas las líneas verticales Sí si pueden comparar la imagen original Ya ven que tiene una distorsión respecto de la imagen original porque ya no se trata de una imagen se trata de un filtro Entonces en este filtro lo que destaca es eso luego aplicó otro filtro y este que está aquí y hago exactamente la misma lógica que recién y me da otro mapa de características y luego hacemos uno más con otro filtro y tengo otro mapa con lo cual ustedes pueden variar este filtro con los números que quieran y justamente ver de alguna manera interesante Cuál es la resultante la imagen resultante simplemente aplicando este código y variando el valor del filtro que van a elegir en este caso primero después ese luego este otro que está destacado aquí y en este tercer caso este otro habiendo entendido de qué se trata el concepto de convolución es momento ahora de ver de qué trata el concepto de bullying Bueno vamos aquí a la imagen tenemos Entonces estos cuatro mapas de características producto de la convolución le aplicamos un proceso de pulin y pasamos estos cuatro mapas de características a cuatro mapas pero del tamaño 28 por 28 a un tamaño de 14 por 14 Qué hizo el bullying acá hizo una compactación una reducción de la dimensionalidad concepto que hablamos mucho en la parte de Machine learning siempre con el concepto que también dijimos en aquellas clases de no perder calidad de información es decir tratar hacer una transformación en donde reduzco la dimensión fíjense que en este caso a la mitad pero tratando justamente de mantener las características principales de esto que insisto no es una imagen sino un mapa de características ahora tenemos Entonces esta pregunta que está de alguna manera tiene una definición que respalda lo que explicábamos recién resulta que no existe una sola forma de hacer bullying hay varios métodos para hacer pulin nosotros vamos a usar uno de ellos que es el Max y justamente Aquí está una explicación que de una manera siempre gráfica intenta explicar De qué trata esto vamos a suponer que yo tengo aquí este conjunto de píxeles sí de cuatro píxeles por 4 píxeles sí lo que vamos a hacer en este caso es dividir este conjunto de 4x4 en cuatro conjuntos de dos por dos Ven aquí tengo el primero que lo he puesto aquí arriba el segundo aquí el tercero aquí el cuarto aquí qué voy a hacer con cada uno de estos casos voy a determinar Cuál de estos cuatro valores es el máximo y lo voy a seleccionar Lo mismo hago con el otro lo mismo hago con el otro lo mismo hago con el otro con lo cual con estos cuatro voy a conformar en la misma posición en que estaba ese Cuarteto es decir 92 corresponde a este Cuarteto que estaba en este vértice superior izquierdo lo voy a poner aquí luego el 128 aquí el 255 aquí y el 168 aquí de esta manera hemos hecho una reducción de la dimensionalidad porque pasamos de una matriz de 4x4 a una matriz de 2 por 2 haciendo una reducción del 50% al igual que como teníamos en el gráfico de aquí arriba donde pasábamos de una figura de 28 por 28 a una figura de 14 por 14 este proceso se va a repetir en distintas capas acá tenemos una capa oculta es la única capa oculta que tiene una red neuronal convolucional puede tener muchas en este ejemplo tiene dos y el proceso va a ir creciendo justamente en la reducción es decir luego de la primer capa oculta que está conformada por un proceso de convolución y un proceso de pulin va a haber una nueva una capa oculta en la cual el input van a ser estos cuatro mapas de características que han sido pasado por el proceso de pullying bien nuevamente yo voy a pasar de 14 a un grupo de diez por diez por qué porque vuelvo a perder cuatro características como el primer caso de arriba de abajo de la izquierda de la derecha conformando en este caso un grupo de más mapas de características Esto es algo que habitualmente se hace de manera creciente es decir pase de cuatro mapas a 8 mapas porque porque aprovecho que como se reduce la dimensionalidad yo puedo tener más mapas sin ocupar más espacio con ellos porque ahora yo tengo mapas más chicos pasé de mapas de 28 por 28 a mapa de 10 por 10 si sacan la cuenta van a ver que aún teniendo más mapas tengo un consumo de espacio menor y luego vuelvo a aplicar un proceso de bullying pasando en este caso como antes de 28 por 28 a 14 por 14 en este caso de 10 por 10 a 5 por 5 finalmente esto se repite tantas capas ocultas como yo haya determinado y termina en una layer donde tengo todas las salidas propias de una capa densa parecida a la que hicimos justamente en nuestra red neuronal artificial que la primera que hicimos y que me van a llevar a en este caso supongamos cuatro valores posibles de salida o los que haya que tener como consecuencia o características del ejemplo que estoy tomando yo para aplicar en este caso Este modelo de red neuronal convolución habiendo abordado todos los conceptos teóricos relativos a una red neuronal convolucional es que vamos ahora a empezar a programar como dice título aquí nuestra primera red neuronal convolucional lo primero que hacemos Es importar nuestra ya conocida librería de tensorflow y crear la variable de menist justamente con los datos del dataset que ya conocemos también Fashion luego creamos los conjuntos de entrenamiento y test como lo hicimos también la clase pasada y aquí tenemos el primer elemento que cambia respecto de lo que veníamos trabajando antes me refiero al reformateo de las imágenes del conjunto de entrenamiento y las imágenes del conjunto de Test en ambos casos a través del método de rehave y le especificamos cuatro parámetros el primero de ellos es la cantidad de observaciones 60.000 el primer caso 10.000 en el segundo como ya conocemos luego como ya también conocemos las dimensiones de ancho y alto 28 por 28 en ambos casos y el último parámetro especifica la cantidad de canales de colores es decir cuando tenemos tonos de gris como en este caso el parámetro se fija en uno cuando tenemos colores de rgb es decir que son los tres colores primarios rojo verde y azul el canal de especificarse con el número 3 luego lo que tenemos que hacer también para ambos conjuntos es normalizar los valores que corresponden Al conjunto de entrenamiento y Al conjunto de Test simplemente como ya hemos explicado en otras oportunidades dada la característica que tienen los números que son propios de los canales de grises que van de 0 a 255 lo que hacemos Es dividir ese valor por 255 justamente Entonces de esa manera yo tengo valores que en lugar de ir de 0 255 van a ir de 0 a 1 y finalmente la creación del modelo el modelo lo vamos a crear consecuencia el model keras de tensorflow igualandolo a una variable que le vamos a poner model igual que como hicimos Nuestra primer red neuronal la clase pasada vamos a poner dentro de esta estructura dos capas de tipo convolucionales y dos capas de tipo pulin es decir vamos a tener dos parejas así como funcionan en este caso voy a no puede ir una convolucional sin su respectiva pulin en este caso tenemos las primeras aquí y la segunda es aquí luego el resto ya vamos a volver a esa parte luego el resto va a ser Exactamente igual a lo que hicimos la última clase pero que sólo definimos como una red neuronal artificial común sí o densa como se llama en estos casos tengo una capa fluten como tenía en aquella oportunidad tengo una capa densa oculta y tengo una capa de salida también densa si la capa flaten la capa densa va a tener 128 neuronas y la capa de salida también densa va a tener 10 neuronas porque justamente son 10 los valores posibles como ya sabemos los 10 tipos de prenda que tiene el modelo fashion emmy volviendo al tema propio de lo que estamos agregando o codificando por primera vez en relación a esta red neuronal convolucional tenemos la primer Capa convolucional en la cual definimos en principio 32 mapas de características elegimos un filtro de tres por tres elegimos una función de activación relu y le definimos dado que la entrada es en este caso es la primer capa de convolución pero a su vez es la entrada del modelo Cuál es la el formato que va a tener la imagen que va a entrar que justamente es de 28 por 28 con el 1 que como dijimos recién le especifica que es una imagen con tonos de grises luego viene la capa de bullying que utiliza una reducción de 2 por 2 con lo cual va a reducir como el ejemplo que vimos en la teoría a la mitad la dimensión del mapa de características que va a lograr esta otra capa anterior y luego la segunda el segundo par de capas una convulsional y su Bull de nuevo tenemos relu no ponemos impulsar porque obviamente Esta no es de entrada definimos un filtro de tres por tres pero aumentamos la cantidad de mapa de características de 32 a 64 la duplicamos la cantidad de mapas Y al igual que lo que hicimos o lo que vimos en el dibujo en la teoría así esto insisto no es ninguna regla se puede trabajar del modo que ustedes les parezca mejor o más conveniente y luego nuevamente otra capa de bullying con el mismo criterio que el anterior con una reducción de dos por dos y finalmente lo que hablamos antes estas tres capas que se parecen completamente a lo que era por sí sola la primera red neuronal que hicimos en la clase pasada luego con pilor modelo uso nuevamente el optimizador Adam uso esta función de pérdida y le pido que por favor me registre el akiu Brasil con el cual voy a medir la eficiencia del modelo Finalmente y luego aplicó el método submari del model que me da una información muy valiosa o muy digamos este interesante para poder visualizar que es toda la descripción del modelo que Acabo de crear donde me Indica cuáles son las capas que lo que yo había antes y la convolucional primera la Pull en primera la conversión segunda y segunda después la flaten y las dos dedos es decir está la capa de entrada perdón y es la capa de salida en el medio tenemos algunos detalles que son importantes que ustedes vean la imagen que entra Es de 28 por 28 pero ya me muestra aquí en la primer capa convolucional que la dimensión que va a tomar va a ser de 26 por 26 porque porque hacer como vimos antes el filtro de tres por tres le saca un este una fila al principio una fila al final una columna derecha y una columna izquierda con lo cual hay una reducción que tiene que ver con el no usar como dijimos antes los píxeles de los vértices porque no se los puede filtrar luego se aplica la primera capa de bullying en la cual se reducen la dimensión de 26 por 26 a 13 por 13 a continuación la segunda capa de convolución que pasa de 13 a 11 por el mismo motivo que pasó de 28 a 26 porque se quitan los valores de los extremos y la capa de public reduce sus once fíjese a 5 Es decir aquí la mitad literal no puede existir porque es un número impar con lo cual redondea hacia abajo y justamente la dimensión que queda es 5 por 5 no 5,5 por 5,5 queda totalmente imposible y luego viene la capa flaten que tiene 1600 neuronas porque porque toma la salida de esta capa que tiene 5 por 5 por 64 da esta dimensión Y luego como dijimos antes la capa densa de 128 y la capa de salida de 10 que omití acá un detalle importante cuando mostra el modelo que seguramente ustedes lo habrán visto que la función de activación para la salida es softmax acuérdense que como vimos en la teoría es softmax y son en más de dos salidas posibles y si no sería de tipo logística o sigmoide habrán observado una columna aquí que dice paran y son la cantidad de parámetros que maneja cada una de las capas fíjense que si quieren entender De dónde sale este número Yo aquí les he dejado un comentario justamente debajo del sumar y para que entiendan Cuál es la forma de cálculo que aplican cada caso para entender porque muestra cada uno de estos parámetros y al final muestra el total de parámetros que no es Ni más ni menos que la suma de toda esta columna y para terminar entrenamos el modelo y hacemos la evaluación de la función de pérdida con lo cual es un entrenamiento muy cortito de sólo cinco épocas y llegamos a una 092 y la función de pérdida me lleva a una pérdida final de 0 19 71 habiendo partido de 0.45 17 en la evaluación del conjunto de Test me lleva a 0 90 52 que son buenos aquí así entiendan siempre que para una cantidad de pochos muy atípica porque en realidad debería haber hecho un entrenamiento de por lo menos 100 épocas Bueno hasta aquí terminamos esta primer parte de la clase quedamos para aplicar entonces en la segunda parte de la clase un ejemplo también de red neuronal convolucional pero con una propósito o un ejemplo un poco más avanzado nos vemos en la segunda parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte  Titulo: Clase17 (parte 2) Curso Inteligencia Artificial \\n URL https://youtu.be/frbxynGYOSQ  \\n 1831 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella   bueno seguimos ahora con la segunda parte de esta clase y vamos como dijimos antes a un programa más complejo como bien aquí la complejidad a ver nosotros estamos tomando muchas veces y hasta ahora fundamentalmente para poder ir avanzando poco a poco conjuntos de datos que ya vienen hechos Sí este en realidad todo lo que hicimos con Géminis y Fashions son conjunto de datos que ya vienen preparados vienen preparados del punto de vista que ya está definida la cantidad de imágenes que corresponden Al conjunto de entrenamiento Cuántas son las que corresponden Al conjunto de Test y vienen debidamente separadas las imágenes vienen todas formateadas iguales de 28 por 28 y tienen algo muy importante un etiquetado el problema que vamos a tener muchas veces con las imágenes y que nosotros vamos a querer clasificar cosas que no van a abrir siempre de ese modo seguramente vamos tener que hacer un trabajo muy delicado y muy llevadero de conseguir las imágenes para entrenar a la inteligencia Y luego etiquetarlas para que pueda generar su modelo en realidad aquí hay algunas cuestiones que pueden ponderarse por sobre otras es decir si yo quiero entrenar a una red para clasificar determinadas tipos de imágenes tengo que tener las imágenes si no las tengo porque no consigo de dónde sacarlas bueno las tendré que buscar y crearlos yo manualmente es decir voy a tener que crear todas las imágenes en esa creación de toda ese Ese Conjunto de imágenes que no es un conjunto de datos Y es un grupo de imágenes que están imagínense dentro de una carpeta bien Voy a tener que bregar porque las imágenes tengan la misma dimensión dentro de lo posible Esto no es una cuestión se puede reformatear después dentro del código como ya la hemos visto pero si pueden tener un formato igual sería lo ideal sí bien Cuántas imágenes voy a lograr para entrenar estamos hablando de que fallan en tiene 70.000 imágenes usted se imaginan que juntar 70.000 imágenes no es una tarea sencilla Así ya vamos a ver en próximas clases que hay métodos de poder sacar por ejemplo a través de Google Pero insisto no es una tarea poco llevadera y luego verde etiquetar las imágenes para poder entrenar esa inteligencia si yo tengo imágenes por ejemplo de perros y gatos sí tendré que tener todos los perros etiquetados como perros y todos los gatos etiquetados Como gatos bueno Esta última parte puedo guiarse y hay mecanismos que justamente vamos a ver en esta clase para poder tratar de evitar esa parte pero la primera parte que es Juntar todas las imágenes no queda otra por lo pronto yo aquí les voy a pasar a ustedes a través de el campus virtual un conjunto es una carpeta insisto no quiero decir la palabra por conjunto porque se van a confundir con el conjunto de datos ya viene pre hecho una carpeta sipiada con imágenes de caballos y otra carpeta sipiada con imágenes de seres humanos en ambos casos Si tengo una división de un grupo de imágenes que han sido están preparadas o están etiquetadas con la carpeta no etiquetadas que están preparadas dentro del nombre de la carpeta como destinadas a entrenamiento y otra que están en una carpeta que está destinadas a validación sí Pero insisto las imágenes son simplemente archivos que van a ver que el nombre del archivo va a decir caballo 1 o humano 1 Pero eso no quiere decir que eso sea un conjunto de datos donde cada una de esas imágenes está etiquetadas Sí bueno por eso vamos a ver cómo trabajar en este caso suponiendo que estas imágenes que yo les doy usted la van a tener que lograr por sus propios medios para diferenciar otras cosas que no son caballos y humanos como el ejemplo que vamos a ver aquí supongamos autos de motos por decir algo bueno ustedes van a tener que conseguir las imágenes de los autos las imágenes de los motos la van a tener que poner en dos carpetas diferentes y vas a tener que sacar algunas imágenes de autos y algunas imágenes de motos para ponerlas en otras carpetas destinadas a validación y así tener de alguna manera dividida las carpetas de entrenamiento con autos y motos y las carpetas de validación o test con autos y mundos bueno esa es la base de la ejercitación que vamos a ver en esta clase Así que pasamos sin hacer más larga esta aplicación alcohol bueno acá estamos en el cola clase 17-2 de la segunda parte de esta clase donde vamos a ver algunas cuestiones que son de antes de ver el código propiamente dicho es muy importante para el tipo de trabajo que vamos a llevar a cabo fundamentalmente para lo que hablamos recién en relación a tener carpetas con una gran cantidad de imágenes de lo que quiero clasificar en este caso de caballos y de seres humanos da la particularidad de este caso a través del campus virtual yo le voy a brindar a ustedes como les adelante dos archivos zip que contienen estas imágenes que van a estar en ese campo virtual y les pido que cuando las bajen la pongan en una unidad de Drive la que ustedes quieran esto obedece a que a partir de esta clase les voy a enseñar Cómo vincular una sesión de colap con una unidad de Drive de esta manera yo cada vez que inició una sesión de cola puedo vincular a través de un proceso que se llama montaje de la unidad de Drive a la sesión de colap y directamente tomar los datos de allí sin necesidad de estar subiendo los como lo hacemos antes con algún Comando llevándolos a nuestro espacio transitorio donde está espacio de almacenamiento transitorio que nos ofrece cola sino directamente con Drive otro tema importante que tenemos que empezar a ver aquí como novedad y que seguramente nos va a acompañar de ahora más es la opción de usar el entorno de ejecución de tipo gpu que me ofrece justamente Google cola el acelerador por Hardware de lo que sería habitualmente una plaqueta aceleradora de gráficos que tienen muchas computadoras de alto nivel Pero quizás nosotros no tengamos acceso a un recurso calificado de ese tipo Y en lugar de usar la opción Non que aparece siempre cada vez que abre una sesión de cola de manera predeterminada uso la opción gpu luego le doy guardar y lo que hace yo no lo voy a hacer aquí pues ya tengo la sesión iniciada lo que hace es generar una nueva sesión Iniciar una nueva sesión Pero ahora con la modalidad de ejemplo Esto va a ser que todo proceso de tratamiento de imágenes se va a celebrar considerablemente versus la opción estándar de cola ahora en sí Entonces vamos a el montaje de la unidad de drive y lo hago justamente con from Google kolab import Drive y Drive Mount y la carpeta donde voy a poner justamente o voy a conectar voy a montar mi unidad de Drive una vez que lo haga justamente me dice que ya está montado porque lo acabo de ejecutar voy aquí a la carpeta y veo que aparece justamente la unidad de Drive la abro dentro de así aparece la unidad mi drive y allí aparece la misma carpeta que yo tengo si voy acá a la unidad de Drive Ven aquí tengo la carpeta archivos dentro de la cual tengo los dos zips que ustedes deberían bajar de el campus virtual y tener su propio Drive bien si yo voy a esta parte de aquí voy a acceder exactamente a la misma información es decir aquí tengo un zip y acá el otro abrimos un poquito más esta ventana para que se vea bien el la carpeta sipiada que va a tener las imágenes para entrenamiento y la carpeta sitiada que va a tener las imágenes para validación lo que tengo que hacer a continuación es descipear todos estos archivos que bueno conecte que están en mi unidad de drive y conecte ahora mi cola y lo primero que voy a hacer es crear un pad luego voy a abrir el archivo zip de esta manera y finalmente voy a poner el contenido de todas las imágenes ahora desde el archivo zip las voy a descifrar y las voy a poner en esta carpeta dentro de la carpeta tmp que ya la vamos a ver es una carpeta estándar de la unidad de almacenamiento de kolla y una carpeta que yo le voy a poner horse original sí voy a hacer exactamente lo mismo luego con el archivo zip destinado de validación con lo cual creo el local zip luego abro el archivo zip y finalmente hago el extracto y finalmente Cerramos el ZIP que es el recurso que hemos importado aquí esto es muy similar a lo que hacemos en el explorador de Windows Sí cuando justamente tomamos un archivo que está sipeado o comprimido y lo abrimos y lo colocamos a partir de un proceso como puede ser un archivo rar o de ese tipo a la carpeta o el directorio que yo quiera cómo veo todo esto Bueno voy a la unidad de almacenamiento drive y voy a ir a abrir a este icono de aquí y se me despliegan todas estas carpetas y voy a ir justamente a la carpeta tmp que es la que asignado aquí horse original y validation acá tenemos la primera works todas las imágenes de Los caballos son todos archivos de tipo png y más lo mismo otro tanto con todas las imágenes manos no confundan esto que está aquí que son los nombres de los archivos con etiquetas insisto estas imágenes no están etiquetadas A diferencia de lo que teníamos en el conjunto en Miss o Fashions sí bien nos falta ver también validación horse Exactamente lo mismo tengo las dos carpetas y dentro de cada carpeta tengo las cimas que solamente que las imágenes no van a ser entrenamiento sino que van a ser para validar la precisión de este modelo que estoy generando bueno en el siguiente paso Vamos a crear unas variables en las cuales vamos a guardar la ruta o el pass de la ubicación en este caso de los las imágenes para entrenamiento imágenes de caballo para entrenamiento imágenes de humanos para entrenamiento imágenes de caballos para validación e imágenes de humanos para validación y vamos a poner un print en cada caso para ver cuántas imágenes hay dentro de cada una de esas carpetas lo ejecutamos y vemos el resultado que me dice que hay 500 imágenes de caballos para entrenamiento 527 imágenes de humanos para entrenamiento y 128 y 128 imágenes para validación a continuación gracias a mclop y ya lo que manejamos como estructuras para imprimir justamente imágenes vamos a imprimir una tira de 5 imágenes de caballos y de seres humanos simplemente para visualizar y tener una idea aproximada de qué tipo de imágenes son las que tienen estas carpetas que acabamos de bajar y descomprimir aquí tenemos cinco imágenes de caballos y cinco imágenes de seres humanos fíjense que en realidad no son fotos son imágenes a color pero son dibujos y todas están formateadas en 300% justamente con la referencia que tienen al costado de abajo cada una de las imágenes y ahora viene la definición del modelo como lo hicimos antes primero contra sunflow importando Flow y luego siempre consecuencia el modo del que era tf le volvemos a poner el móvil al modelo creamos una capa de convolución donde digo que el input van a ser este imágenes de 300 por 300 pero ahora no de uno que sería un canal de grises sino de 3 porque son imágenes a todo color bien en este caso defino una primer capa de convolución de 16 y un canal del 3 por 3 y una función de activación de tipo relu luego la capa de pooling de 2 por 2 hago otra capa de convolución donde incremento la cantidad de mapas de características sigo usando un filtro de 3x3 y la misma función de activación y hago otro pulin de dos por dos y en la última capa de convolución incremento nuevamente la cantidad de mapa de características sigo usando el filtro de 3x3 y el Max puliendo por dos y finalmente la capa flaten que ya hemos visto anteriormente una capa densa de 512 y siempre la función de activación relu y la última capa que la capa de salida una capa densa de una neurona Por qué de una neurona Porque aquí tengo dos opciones o es caballo o es hombre por eso justamente la función de activación ya no es softmax sino como ya dijimos antes cuando es un problema de tipo binario es una función de activación de tipo sigmoide bueno ejecutamos el modelo para crearlo y con su marido podemos ver como ya habíamos visto antes todas las características de este modelo que acabamos de que a diferencia del modelo que creamos anteriormente tenemos tres capas de convolución y finalmente las que ya conocemos floten la densa y la densa de salida obviamente en este caso al ser en más capas de convolución tenemos una cantidad más grande de parámetros Más allá de Que obviamente Tenemos también imágenes de 300% y no de 28 por 28 como teníamos antes fíjense simplemente Este detalle que las imágenes no son 800% sino son de 298 Perdón 98 por el mismo problema que vimos antes que tampoco eran de 28 por 28 ya deben ser por 26 porque Saca los bordes de la imagen como ya sabemos bien lo que vamos a hacer a continuación es compilar el modelo como ya lo hicimos en el caso de los ejercicios anteriores Y aquí vamos a incorporar una novedad en realidad hemos venido trabajando con dos optimizadores sgd y Adam hoy vamos a incorporar un tercero que aquí lo estamos importando también de trazos louqueras y lo incorporamos como en los casos anteriores en el parámetro optimizar y vamos a estar agregando un concepto que es muy importante tanto que lo vimos en las clases teóricas que es la tasa de aprendizaje en este caso Lenin raid lo vamos a fijar con el valor 0.001 es decir una tasa de aprendizaje que va a trabajar de manera muy lenta respecto de otras opciones que pueden ser 0,01 y 0.005 que aquí se las dejo en el comentario de abajo que luego de hacer esto que vamos a hacer con esta tasa ustedes podrían probar cambiar esa tasa volver a compilar el modelo volver a entrenarlo y ver son los score que nos arroja bien luego también vamos a hacer un cambio en la función de pérdida vamos a usar binary Cross entro Y si recordamos lo que usamos en la primer parte de esta clase en la misma situación de Conspiración habíamos usado otra que es esparce categórical por qué la diferencia porque en este caso hacemos referencia a un modelo que buscaba dar solución a un problema de múltiples salidas no era binario Era muchas salidas tantas como elementos tenía el conjunto de datos Fashions que eran 10 en este caso volvemos tenemos una salida que atiende a resolver si una imagen es un caballo o un ser humano con lo cual es una salida de tipo binaria y por eso en la función de pérdida se elige esta opción a continuación voy a hacer un proceso que vas a llamar justamente procesamiento de datos por qué recordemos cómo estamos en este momento con lo que tenemos como datos nosotros tenemos carpetas con imágenes que no están etiquetadas si están en carpetas que tienen un nombre los humanos por un lado los caballos por el otro Pero eso no quiere decir que las imágenes están etiquetadas este conjunto de imágenes aún no es un Data set no tiene forma de Data Zeta ahora con este proceso es lo que vamos a hacer justamente vamos a crear como dice aquí este texto tensores que se llama tensores o a qué se refiere son estas estructuras que vimos aquí en vuelvo al caso de El ejemplo de hoy cuando hicimos este se acuerdan que dijimos que esta estructura tiene en principio la definición de la cantidad de observaciones el alto el ancho y el canal de color que utiliza Bueno eso es un tensor y por eso nosotros en ese tipo de estructura es lo que tenemos que transformar nuestras imágenes para que después puedan ser utilizadas para entrenar porque si no tienen este formato no van a poder ser utilizadas ni para entrenar el modelo ni para crear el modelo ni para validar el modelo ni para hacer algún tipo de predicción con ese modelo así que bueno el próximo paso Entonces es utilizar image Data generator que es una librería de queras en la cual lo que hago es en principio escalar los datos esto es igual a lo que hicimos antes cuando lo hicimos con una simple cuenta vuelvo al caso anterior se acuerdan esto que está aquí de la primer parte de esta clase Bueno lo usamos con otra herramienta Pero esto que viene ahora no lo hemos hecho antes porque antes teníamos un Data y ahora no que es justamente generar un conjunto de datos con estas imágenes que están en una carpeta para generar este conjunto de entrenamiento con este generador hay cuatro datos importantes que debo asignarle en principio el nombre de el directorio del cual tiene que tomar las imágenes luego el formato de las imágenes 300 por 300 luego el backside que es el tamaño de lote A qué se refiere el tamaño de lote cuando yo vaya a entrenar el modelo que quiero generar eso lo voy a hacer con lotes de imágenes es muy difícil darle todas las imágenes en un solo acto para que se ejecute el entrenamiento habitualmente por el peso que tienen las imágenes y justamente por lo complicado que implica todos los recursos computacionales que hay que tener para generar Este modelo con imágenes se va haciendo de alotes bueno con este parámetro aquí te fijo el tamaño de cada uno de sus lotes y finalmente el Class binary que lo que le digo es que lo que voy a hacer aquí es una definición de imágenes tipo binaria si tengo nada más que dos diferentes tipos de imágenes es decir que en este caso le va a poner 0 a 1 y uno al otro no nos confundamos en este caso no le va a poner horse a los caballos y se suman a los seres humanos lo que le va a poner a cada uno de ellos es un número que va a permitir diferenciar uno de otro pero como cuando teníamos en el caso del fallons que sabíamos que el no era la bota Pero además que no decía bota si no decía el número nueve bueno en este caso como no es un problema que tiene varias opciones más de dos concretamente sino que tiene dos le pongo Class binary y hago exactamente el mismo procedimiento para generar el conjunto de datos para validación bueno teniendo claro todo esto ejecutamos Esta parte Y tenemos la definición aquí de 1027 imágenes que le pertenecen a dos clases tan distribuidas en Dos clases caballos y seres humanos y 256 imágenes que son el conjunto de validación que también están diseminadas en Dos clases diferentes luego viene el entrenamiento del modelo como ya lo hemos hecho antes a posterior de la compilación aquí tenemos dos elementos que no veníamos usando de este modo anteriormente porque veníamos trabajando con conjuntos de datos que ya estaban pre hechos aquí hemos usado un generador para generar nuestro propio conjunto de datos y por eso primero le paso el conjunto de entrenamiento y luego el conjunto de validación o de Test a continuación la cantidad de épocas que en este caso hemos elegido 15 este número obviamente es importante la elección de ese número por supuesto porque obviamente trabajando con imágenes con una gran cantidad de imágenes y con las dimensiones que tienen el tiempo en virtud de este número que yo pongo aquí va a variar muchísimo en cuanto también a si dispongo de la gpu o no Porque ustedes vieron que yo le sugerí elegir la opción con gpu Pero eso no siempre va a estar disponible en colapse entonces obviamente Esto va a jugar en el tiempo final de entrenamiento Si los resultados no van a variar por eso pero si el gasto computacional y el tiempo que le vamos a tener que dedicar al entrenamiento va a variar mucho de acuerdo a este valor y de si tenemos o no la gpu luego tenemos estos dos parámetros steps y validation steps en ambos casos 8 porque 8 vamos un poquito al paso anterior y hemos recordemos que aquí tenemos 128 en el conjunto de entrenamiento y batch site 32 en el conjunto de validación habíamos dicho que eso lo hacíamos para que las imágenes vayan formando parte del entrenamiento de alotes y no todas las imágenes en un solo acto nosotros tenemos 1027 imágenes en el conjunto de entrenamiento y 256 en el conjunto de validación bien lo que tenemos que hacer Entonces es dividir este valor por 128 y me va a dar 8 dividir este valor por 32 y también me va a dar 8 por eso estos dos valores que le indican al entrenamiento que tiene que ir haciendo este entrenamiento con lotes de imágenes y no con todas las imágenes le indica En cuántos lotes va a hacer ese entrenamiento y es 8 en ambos casos y finalmente este parámetro verbos 1 le indica cómo voy a visualizar el entrenamiento con verbos 1 le digo que quiero ver esta información de la cantidad de epoch uno de 15 dos de 15 y esta barra que está aquí que es la barra de avance que van a ir viendo que va a ir creciendo como toda barra de avance y me va diciendo bueno en qué parte de entrenamiento está cada uno de sus entrenamientos bien una vez que tenemos esto lo ejecutamos yo voy a acelerar el vídeo porque obviamente Esto va a tardar mucho para justamente sacar la conclusión final la corrupción final es que la precisión del conjunto de entrenamiento es muy buena y Hemos llegado al 100% si esto como ya lo venimos diciendo desde los modelos de Machine learning puede ser una buena noticia o pueden ser no una tan buena noticia porque podemos tener no un gran algoritmo sino un algoritmo sobre ajustado y esto lo vemos porque justamente el conjunto de validación tiene un valor digamos que ronda el 80% porque digo ronda porque ustedes saben que de acuerdo a la cantidad de veces que lo prueben o la experiencia de cada uno cuando lo pruebe puede ser que sea un valor un poquito inferior a eso o superior a eso pero ronda ese 80% con lo cual tengo un 20% de diferencia entre el alquiler así de un conjunto de la criolla del otro lo cual es extremadamente Claro que manifiesta un problema de sobreajuste bueno justamente hay dos formas de evitar el sobre ajuste en lo que tiene que ver las redes neuronales convolucionales Y eso va a ser un tema que vamos a abordar la clase que viene bueno Y finalmente vamos a probar el modelo nosotros ya hemos hecho algunas pruebas con obviamente los Data set que hemos utilizado eminís y fallon de menis que ya dijimos y venían preparados pero esta es la primera vez que vamos a probar un modelo con algo que hicimos completamente nosotros desde el conjunto de datos Aquí hay una función que yo he diseñado dado que la idea es probarlo de una manera muy práctica con imágenes que voy a bajar desde distintas urls sí es cierto pueden buscarlo digamos a través de Google imágenes de caballos y de humanos y empezar a probarlo he hecho una función justamente para no repetir el código Tantas veces como pruebas vayas a hacer entonces este aquí voy a hacer una función que se llama predecir que tiene una URL aquí como parámetro de entrada obviamente en la URL lo que le voy a mandar va a ser la URL que contiene la imagen o cada una de las imágenes que quiero predecir o evaluar con este modelo luego tenemos esto que está aquí que es de URL que si ustedes recuerdan la clase de web scrapping justamente era el mismo argumento que usamos en aquel entonces para traer todo el contenido de la web en este caso el contenido es una imagen si no es un sitio web sino que directamente una imagen y la pongo en una variable que le he puesto respuesta luego lo que hago es justamente volcar todo ese contenido de esa imagen a una variable img y pasarla a un formato normalizado por eso la divido por 255 y con np array que es Nampa array le doy el formato de raíz que tienen que tener las imágenes a continuación yo no sé en qué formato viene la imagen Es decir de cuánto por Cuánto es con lo cual tengo que garantizarme que como Este modelo necesita imágenes de 300 por 300 la imagen sea de 300.800 con lo cual utilizo el método Bienvenidos a YouTube Aquí está importado en realidad digo bienvenido porque es una librería que vamos a usar muchísimo de aquí hasta hasta la última clase de redes neuronales convolucionales o visión computacional bien entonces vuelvo a poner dentro de la variable img la imagen ahora garantizándome que sea de 300 por 300 luego lo que hago es hacer la predicción Pero antes hago un re-have de img para garantizarme que tenga la forma de tensor se acuerdan que esto ya lo habíamos hablado aquí le doy 300 por 300 Por 3 que es el tipo de canal en este caso las imágenes tienen tres colores y este -1 donde iría la cantidad de observaciones tal cual lo habíamos hablado antes en este caso sería menos uno saque no hay una cantidad de observaciones Es solamente una observación porque es un objeto luego predicción me va a dar un resultado como es un problema binario no es que me va a dar un porcentaje por cada una de las opciones me va a dar solamente un porcentaje Lo que pasa que yo tengo que tener un umbral en este caso el umbral que voy a poner es el 0,5 Y a partir de su umbral si la predicción es mayor a 0,5 se tratará de un ser humano y caso contrario se trata de un caballo la función devuelve justamente la clase que la clase en realidad es el texto más el valor de la predicción para que ustedes lo puedan ver bueno habiendo desarrollado esta función lo que voy a hacer es hacer uso de ella con lo cual aquí puesto tres ejemplos de seres humanos de urls que apuntan imágenes seres humanos y tres ejemplos de URL que apuntan a imágenes de caballos esto para saberlo De antemano y después ver si la predicción puede haber sido certera o no y en cada caso voy a hacer print de predecir mandándole el valor de URL puse los seis ejemplos juntos así lo ejecutamos todos juntos y vemos los resultados Bueno lo probamos entonces y vemos los resultados bien Recuerden que en los tres primeros casos son humanos y los tres últimos son caballos en el primer caso bueno determina que hay un humano con un 0.89 casi llegando al 100% de certeza de que es un humano en el caso del humano 099 el segundo manos 099 en el caso del ser humano también hay una total certeza de que es un ser humano en el cuarto caso es un caballo pero lo determina con cero simboliza que está muy cerca del umbral o sea podría haber sido tranquilamente visto como un caballo porque insisto el umbral de 0,5 y está apenas por encima del 05 no obstante es una mala predicción Pero acuérdense que nosotros tenemos un así en el conjunto de test de 0,80 es decir que tengo un 20% de posibilidad de error sí bien y aquí tenemos una prueba que hay un error no obstante en los dos casos restantes ha hecho una buena predicción que es un caballo con una predicción del 0.01 o sea está muy cerca de cero que sería la certeza absoluta o la predicción absoluta de que eso es un caballo bueno Esto ustedes pueden seguir utilizándolo sería interesante que lo hicieran buscando otras imágenes y poniendo aquí el resultado de esas urls y ver bueno Cómo funciona el modelo sabiendo que como dije antes tenemos un modelo con un 20% de error hasta aquí llegamos con esta clase en la próxima clase vamos a ver bueno este problema que hemos hablado hoy mencionado en cuanto al problema de sobre ajuste como ir solucionándolo y vamos a volver a utilizar nuestro viejo Data set en Miss para justamente ver las diferencias de Qué cosas puedo usar para ir superando el problema de sobre ajuste así que hasta la próxima clase aquí termina esta clase los espero en la próxima clase nos vemos Titulo: Clase18 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/B0Q6OW0xQpY  \\n 1046 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número    18  Hola a todos Bienvenidos a la clase número 18 del curso de Inteligencia artificial de ifes donde vamos a seguir con redes neuronales convolucionales dentro del marco de este módulo de Deep learning en principio el propósito de la clase de hoy es resolver algunas cosas que nos quedaron pendientes de la clase pasada cuando decíamos que teníamos un modelo con sobreajuste y hoy vamos a ver algunas técnicas para mejorar y superar ese problema por eso inmediatamente pasamos al colab a empezar a programar justamente estas soluciones de las que estábamos hablando bueno Y aquí estamos en el colap de la clase 18 y el título justamente de este Notebook hace referencia a lo que decíamos recién en principio cuando tenemos un problema de sobreajuste tenemos dos opciones para superar Uno de ellos es el aumento de datos Y el otro el Drop out Pero antes que nada vamos a empezar como siempre por usar un dataset para justamente demostrar todo esto que estamos diciendo y vamos a volver a cargar el dataset emist como ya sabemos que lo venimos haciendo habitualmente Así que empezamos por eso importando también las librerías como siempre lo hacemos Ahí está cargado verificamos sí que tenemos la cantidad de observaciones que ya conocemos que tiene este dataset 60,000 observaciones En el conjunto de entrenamiento y 10,000 observaciones En el conjunto de Test Y corregimos de paso aquí que est mal escrito Así que lo ejecutamos y seguimos Sí luego lo que también hemos este de alguna manera explicado en las clases anteriores que necesitamos reformatear para que justamente el input se adapte a lo que solicita una red de tipo neuronal convolucional que es hacer un resave para convertir la entrada en un formato de tipo tensor Recuerden que el tensor tiene cuatro elementos el primero de ellos describe la cantidad de observaciones el segundo la altura de la imagen el tercero el ancho de la imagen y finalmente el canal de colores aquí tenemos con el reshape len de X3 que sea medir el largo de El X3 que son los 60,000 que tenemos aquí arriba podría haber puesto 6000 directamente también por supuesto 28 por 28 las dimensiones de la imagen y uno que es el canal de color de tonos de gris H lo mismo el resap para el conjunto de Test y luego hacemos Perdón no ejecuté esto Luego hacemos la normalización como ya sabemos dividiendo por 255 bien y acá estamos con la primera de las técnicas para superar el problema de sobreajuste que es el aumento de datos Qué es el aumento de datos Bueno aquí hay toda una larga explicación que les va a venir muy bien como ya hemos hablado estamos poniendo mucha información eh de conceptos en el propio colap para que ustedes tengan bien a mano pero lo podemos explicar del siguiente modo en principio yo tengo un problema de sobreajuste que puede verse en el caso de el ejercicio de la clase anterior a una muestra muy chica de imágenes Recuerden que teníamos solamente 10000 observaciones En total en este caso del dataset emis o Fashion emis tenemos 60,000 que es un número mucho más lógico pero muchas veces sucede de que yo no tengo forma de poder hacerme de más imágenes para que justamente tenga mayor variedad y el modelo pueda generar esa mejor entonces una de las opciones es llevar a cabo un proceso de aumento de datos A qué se refiere a aumento de datos bien la idea es Buscar algunos mecanismos que generen nuevas imágenes distorsionando las originales A qué nos referimos cuando hablamos de distorsionar imágenes Bueno aquí hay un texto que es seleccionado para que leamos juntos y podamos entender este concepto mejor dice para una imagen se pueden aplicar variaciones por ejemplo rotar en 90 gr en 45 gr se podría achicar o alargar la imagen saturar los colores aumentar o disminuir el brillo o contraste cambiar la iluminación agregar ruido a la imagen y varias opciones más hay entre 20 y 30 opciones que se pueden aplicar para este cambio esta transformación de datos Ya lo vamos a ver este en en la práctica y le voy a dejar después un link para que ustedes vean toda la cantidad de variantes que existen en este método eh En este caso particular dice que si yo eh agrego cuatro variaciones sí tengo la imagen original más las cuatro variaciones paso a tener de una imagen a tener cinco imágenes con lo cual mi dataset se quintuplica y Por ende puedo generar un modelo mejor sin la necesidad de recurrir a por mis medios hacerme de más cantidad de imágenes habiendo explicado esto vamos a pasar a aplicarlo al dataset emist bien lo primero que vamos a hacer vamos a importar image Data generator de esta librería que justamente es lo que nos va a permitir hacer o aplicar este aumento de datos Antes que nada les dejo aquí una URL Para que vean todas las opciones que existen para este proceso de aumento de datos lo voy a mostrar aquí rápidamente para que vean fíjense una lista larguísima de opciones que tengo para alterar la imagen esa esas opciones están aquí en esa web que yo les dejo allí y con la explicación de Para qué sirve cada uno y ejemplo Sí este es el sitio oficial de tensor flow donde está toda esta valiosa información que se las dejo para que ustedes miren volviendo a la práctica vamos a aplicar solamente cuatro casos de estos que vamos a hacer una rotación un movimiento en ancho un movimiento en alto y un Rango de acercamiento estas son variables que yo creo con los datos que yo quiero para cada caso y lo que hago es crear una variable Data que va a ser la variable del generador la voy a crear justamente con image Data generator y voy a ponerle a cada uno de sus parámetros la variable que cree con los valores yo aquí arriba luego lo que tengo que hacer es entrenar este generador de datos Sí el entrenarlo es que conozca los datos originales porque si no conoce los datos originales cómo va a ser un buen entrenamiento conociéndolos bien para poder hacer alteraciones que correspondan bien a ese origen Así que esto es lo que que tengo que hacer y lo ejecutamos terminado esto vamos a hacer un código que va a ser muy práctico y muy orientativo para lo que eh queremos demostrar que estamos haciendo Aquí como siempre tratando de dar un efecto visual para que ustedes lo puedan entender mejor vamos a imprimir bueno con el código que usamos muchas veces con ML blip dos tiras de eh ocho imágenes es decir vamos a imprimir 16 primeras imágenes eh de el dataset original y luego vamos a imprimir la misma cantidad pero de las imágenes alteradas y ustedes visualmente van a poder comparar una cosa con  otra bien y aquí tenemos el resultado bueno Estas son las imágenes que ya conocemos con las formas que ya conocemos y tenemos 5 0 4 1 9 bueno y el resto estas son como dijimos hoy dos tiras de ocho imágenes tenemos las 16 primeras imágenes del dataset emis original y luego hacemos la impresión de las imágenes alteradas alguna de las imágenes alteradas también las 16 primeras pero se corresponden con la original fíjense las diferencias Sí el cinco original vamos a achicar esto un poquito para que se pueda ver mejor bien el cco original con el cco modificado el cer original con el modificado fíjense el cu cómo lo transformó lo saca un poquito del cuadrado sí del límite fíjense que acá hay una parte dos partes del cuatro que no aparecen el uno está muy inclinado de acuerdan que uno de los parámetros le decía la rotación bien Eso es lo que aplico aquí aquí el nu está como alejado fíjense el dos también está alejado y fuera de foco el uno el tres seguimos después con el uno con el cuatro el uno fíjese en este caso el cinco lo recostó mucho más que lo que está allá en la imagen en original el tres fíjense que hasta pareciera que fuera la parte de abajo un cinco El Seis el un bueno de esta manera es más eficiente la búsqueda de un patrón que corresponda exactamente con lo que buscamos o queremos identificar en cada caso bien Si volvemos al código vamos a ver que en este caso yo para ver esa última tira apliqué esto que está aquí que es el dat Flow Qué es Data sh Flow dat sh es lo que creamos aquí arriba el Data generator si la imagen que tiene todo el Data generator creado con las características que yo quiero y el Flow es lo que hace fluir que ya hemos visto este concepto sí que justamente trae las imágenes y las altera esto no implica que ya están alteradas las imágenes esto lo estamos haciendo solamente para ese grupo de 16 primeras imágenes para hacer esta comparación todavía no Hemos llegado a hacer la la transformación efectiva sobre el conjunto de datos ahora antes de avanzar con la transformación definitiva de los datos vamos al otro método porque acá vamos a aplicar las dos cosas juntas sí eh Para justamente Este modelo que estamos generando buscando evitar eh el sobreajuste de la red la segunda técnica es el dropout como ya lo habíamos mencionado antes Qué es el dropout es una técnica que lo que busca es Aunque parezca incoherente tratar de que la red no aprenda tanto sí Parece que va en contra de todo lo que dijimos antes Cuál es el tema es que la idea es que no aprenda tanto en el sentido de que si aprende demasiado puede llegar a idealizar el conjunto con que se entrenó como ya sabemos y después no generalizar bien cómo evitamos para que no aprenda tanto y copie tanto fielmente eh el conjunto con el que está entrenando bueno La idea es Apagar algunas neuronas es decir por cada entrenamiento no usar todas las neuronas algunas desactivarlas Entonces de esa manera en el próximo entrenamiento Esto va a cambiar o sea no siempre van a estar apagadas las mismas neuronas sino que eso va a ir cambiando de ese modo como las neuronas van a ir participando de manera alternada no va a generarse una copia o una idealización de los datos de entrenamiento y Por ende no voy a tener el problema del sobreajuste e Por lo general el Drop out se puede aplicar en todas las capas en algunas s por lo general eh es muy poco aplicado en la capa de entrada y muchas veces se recomienda eh tomar como opción ya aplicarlo en la capa de entrada eh el nivel o la cantidad de neuronas a pagar Eh puede ser también variado se puede eh arrancar desde el 20% y llegar a no más del 50% por supuesto bueno así que explicada Esta técnica vamos nuevamente a a implementarlo pero antes les dejo aquí una animación para que ustedes intenten ver de qué se trata un sistema de dropout vamos a hacer clic aquí y acá ven fíjense cómo se van apagando solamente aquí tengo una capa en la cual estoy aplicando el dropout evidentemente pero fíjense que las neuronas que se apagan no son siempre las mismas Y eso justamente es lo que le da valor a este método bueno Esto es interesante simplemente Siempre busco cuestiones gráficas para que puedan ustedes entenderme mejor los conceptos bien volviendo Aquí vamos entonces al diseño de nuestro modelo al cual le hemos agregado una capa de Drop out que es esta que está aquí antes de la capa flaten hemos puesto Aquí esta opción eh Recuerden que la puedo poner Tantas veces como yo quiera y en el lugar que yo quiera Sí el resto tengo dos capas de convoluciones la primera de 32 y la segunda de 64 tengo la capa Drop out que en este caso le he dado un 50% o sea va a estar estar apagando la mitad de las neuronas y luego la capa flatten la capa dens y la capa de salida de 10 neuronas como ya hemos usado antes Perdón por el tema de las 10 salidas que tiene el conjunto emist y luego la compilación usamos Adam spars categorical Cross entropy acuérdense que esta función de pérdida tiene que ver con un problema de categorías y no binario y el la métrica a través de el método accuracy Sí así que gener el modelo bien y ahora sí vamos a generar o sea volvemos al concepto anterior si ya tenemos el Drop aplicado tenemos que volver al tema de la generación de imágenes alterando la imagen original como dije antes lo que hicimos hasta ahora sí fue Simplemente crear el generador darle todas las características que queremos que tenga pero hasta ahora no lo hemos aplicado sí lo hemos entrenado lo hemos generado pero no lo hemos aplicado ahora vamos a hacerlo aquí donde tomo el xtrain el etrain le pongo un batch size de 32 Recuerden que esto es un concepto muy importante en el manejo de las imágenes para ir manejándola por lote y con Data Gen Flow pongo Ese Conjunto nuevo con todas esas modificaciones y alteraciones en Data gent TR lo ejecutamos y una vez que tengo todo esto lo único que me queda es entrenar el modelo bien aquí lo vamos a entrenar por 60 épocas y otro dato importante que vamos a poner aquí Bueno aquí hemos puesto en algunos casos ustedes van a ver que yo puedo crear una variable para poner el dato o poner el dato concretamente Yo podría haber puesto aquí ep 60 y el lote 32 Pero bueno creé variables que contienen ese valor son formas alternativas de hacer esto steps per epoch y validation steps se acuerdan que lo obtenían dividiendo la cantidad de observaciones por el el tamaño del lot por ejemplo 60,000 di 32 y 10,000 di 32 bueno lo hago con una fórmula matemática que es esta que está aquí np sale sirve para redondear el valor bueno es una un método de npai Sí y aquí lo mismo para el validation Step Sí así que bueno ahora entrenamos el modelo paciencia va a tardar mucho pero bueno es lo que nos toca Así que vamos con ello bien habiendo terminado el entrenamiento vemos Que logramos un 91 por en el conjunto de entrenamiento y un 98 por en el conjunto de Test lo cual habla más que claramente que hemos superado El problema del sobreajuste dado que tenemos una situación totalmente inversa a la que teníamos en la clase pasada donde teníamos un 100% para el primer conjunto y un 80% para el segundo es necesario que ambas técnicas se apliquen en conjunto definitivamente no se puede hacer solo dropout solo aumento de datos o ambas en combinación como hemos hecho en esta práctica También es importante que podamos ver la posibilidad de mejorar este 98 por del conjunto de Test como en principio por ejemplo con la cantidad de entrenamientos hemos aplicado en esta práctica solo 60 entrenamientos Los invito a poder Probar con 100 o 200 entrenamientos y finalmente bueno variar estas dos técnicas que por más que las apliquemos en conjunto por separado hemos utilizado aquí es decir más capa de Drop out variando también además de más capas el el valor de 02 o 05 por y en el caso del aumento de datos bueno poner más variantes más opciones de variabilidad ya viemos que hay aproximadamente 24 opciones para variar eh el aumento de datos o la la deformación de estas imágenes bueno eso también es otro parámetro para variar y probar a ver si ese escor del 98 por lo podemos mejorar Bueno hasta aquí la primer parte de esta clase número 18 vamos a ahora en la segunda parte ver una forma muy práctica de cómo implementar Este modelo que hemos creado Así que nos vemos en la segunda parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte Titulo: Clase18 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/Vr5mQ6m-WKA  \\n 1365 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella Bueno ahora Estamos en la segunda parte de esta clase vamos a en principio abordar todo un proceso que nos va a llevar a poder implementar Este modelo era un entorno web eso en cuanto a lo que es el contenido central de esta clase pero gran parte de esta segunda parte específicamente la última nos va a dar una buena introducción hacia todo lo que tiene que ver con las siguientes clases que nos van a llevar hasta el final del tema redes neuronales convolucionales así que bueno sin más nada que decir empecemos ya mismo con la clase bien aquí estamos en la segunda parte de este colap de la clase 18 con el propósito como dice título aquí de utilizar el modelo desde una página hay tareas que para lograr ese objetivo vamos a hacer dentro del entorno del colap y tareas Que obviamente la vamos a hacer fuera del entorno del collage hasta llegar al entorno web vamos aquí a tener una guía de Cuáles son los pasos que tenemos que hacer en un entorno Y en el otro vamos en primer orden con los pasos que vamos a hacer dentro del entorno de cola lo primero que tenemos que hacer es exportar el modelo Por lo cual vamos a usar el método save de la variable modelo y vamos a exportarlo a un formato h5 que es el formato de tensorflow justamente que utiliza cada vez que vamos a exportar un modelo fuera del collap o fuera del entorno el cual hemos desarrollado el modelo para hacer utilizado en la aplicación web por una app el nombre que le hemos puesto aquí es este nombre que está adelante del h5 puede ser el nombre que quieran y si la extensión debe ser h5 porque es la extensión como dije antes de exportación que usa tensorflow luego lo que tenemos que hacer es instalar tensors js que en realidad es una aplicación que permite desarrollar un modelo que pueda ser utilizado en el entorno de javascript que justamente es hacia donde vamos a llevar nuestro modelo lo ejecutamos paso Seguido lo que vamos a hacer va a ser crear una carpeta que le vamos a poner carpeta salida que se va a crear dentro del entorno de cola luego vamos a recurrir a tensor Flow js converter para convertir Este modelo que hemos grabado con la extensión h5 en la carpeta salida para que me formateé los archivos en el formato ideal para usar justamente en una aplicación con html y javascript en un entorno web ahora vamos a los pasos fuera del entorno de colap y lo primero que tenemos que hacer es bajar el archivo modelos nulo.rar que va a estar en el campus virtual lo vamos a poner dentro de una carpeta que puede tener el nombre que queramos yo elegí aquí modelo guión bajo nnos y vamos a poner allí dentro de esa carpeta este archivo zip para luego desviar luego vamos a ir aquí a esta carpeta de cola y vamos a buscar nuestra carpeta salida que creamos donde ya tiene que estar los dos archivos que acabamos de crear Recién con tenso flojo js converter y voy a ir a esta opción de aquí y los voy a descargar tal como dice el paso 4 que tenemos aquí en la descripción dentro de la carpeta que Acabo de crear si voy el explorador de Windows Aquí está el primer archivo y el segundo que son los mismos que estaban en la carpeta de cola es decir estos dos lo que tenemos que hacer a continuación en este paso 6 es Ingresar a esa carpeta nuevamente pararnos en ella y accionar el botón derecho del mouse y seleccionar la opción abrir terminal una vez allí vamos a escribir lo siguiente y vamos a dar enter con lo cual vamos a activar un servidor local una vez hecho esto vamos a ir a un navegador y vamos a escribir localhost dos puntos 8000 lo cual nos va a levantar automáticamente el archivo Index html que es uno de los archivos que desisteamos Y tenemos dentro de la carpeta modelo ns es decir este archivo que está aquí se va a abrir cuando yo escriba lo que comentaba recién esta aplicación es un Index html es un archivo html que tiene también algunas cosas codificadas en javascript y lo que va a estar levantando es este archivo y este archivo porque justamente son los que representan el modelo que yo exporte en principio guardé en formato h5 y luego exporte a esta carpeta y justamente me generó estos dos archivos que son los que necesita esta aplicación que estamos corriendo aquí y en la cual voy a empezar justamente a tener la posibilidad de escribir un número con el Mouse y ver si la predicción es o no correcta empecemos con una prueba con el botón para decir me aparece aquí abajo Cuál es la predicción en base a esto que lo hice a mano tal cual como los números que originalmente se colocaron en las con este botón voy a limpiar para seguir escribiendo números y volver a predecir puedo probar otras posiciones y este caso justamente me da la posibilidad de entender bueno las bondades de el aumento de datos cuando cambia la posición natural de los números o los gira o los hace un zoom para acercarlos un zoom para alejarlos bueno en este caso seguramente en en no hay ningún número que esté en esta posición aún así justamente gracias al Drop y gracias a el aumento de datos ha hecho una predicción correcta bueno como vemos cuando yo escribo algo que no va a responder a ningún valor de los que tiene el conjunto de datos obviamente la predicción será la que entiendes más parecida a esa posibilidad pero evidentemente la proyección siempre va a ser mala Bueno aquí queda la aplicación entonces para que ustedes hagan todas las pruebas que quieran y justamente puedan ver las características de cómo responde el modelo a lo que ustedes dibujen aquí en este recuadro también pueden hacer lo siguiente que es ir a abrir el archivo index html y las personas que conozcan Este lenguaje bueno poder mirar un poquito el código que está aquí y tratar de entender las características del mismo obviamente aquí hay un código que lo que hace y bueno obviamente formatear en el formato que necesita el modelo de entrada acuérdense que esto que estoy haciendo es un dibujo en un Canvas y hay que transformar en un tensor para que justamente el modelo pueda tomarlo y evaluarlo y hacer la proyección como corresponde Sí aquí está el model punto Jason que es uno de los archivos que bajamos justamente de este nuestra aplicación colapse cuando hicimos la exportación Bueno un último tema para terminar es el de dar de baja el servidor que usamos como local juegos para ver nuestra aplicación web Entonces tenemos que ir para ello a la ventana de la terminal y oprimir control c bien este sería el último tema Por lo cual vamos a pasar a la segunda parte de la clase para cerrar la segunda parte de la clase 18 vamos a contextualizar un aspecto muy importante que nos va a acompañar durante el resto de las clases vinculadas a la red neuronales convolucionales vamos a empezar por decir que hasta aquí hemos visto cómo crear nuestras propias redes en principio las redes neuronales artificiales y luego las redes neuronales convolucionales con todas las características y opciones que hemos aprendido a manejar en estas cinco clases en el mundo de las redes neuronales convolucionales existen investigadores y científicos que trabajan permanentemente en el diseño de redes más eficientes y más rápidas nosotros lo hacen por sus propias motivaciones sino que hay organizaciones que promueven desafíos para cada año poder seleccionar y premiar a las mejores propuestas una de las organizaciones que promueven este tipo de desafíos y premiación de los proyectos más importantes se llama image net large visual recommisión challenge y es en donde se evalúan algoritmos para la detección de objetos y la clasificación de imagen a gran escala Aquí vemos todos los años donde este desafío se llevó a cabo y vamos a poner aquí rápidamente la referencia de algunas redes que fueron las ganadoras de algunos años y más allá de eso fueron muy conocidas porque se usaron mucho a partir de ese evento estas redes son diseñadas como dijimos recién por investigadores y científicos y tienen estructura por arquitectura complejas como la que aquí vemos Esta que estamos viendo aquí corresponde a Álex net que es una de las más conocidas y que fue la ganadora de la contienda del año 2012 este tipo de redes logran altísimos niveles de precisión y rapidez debido a dos elementos fundamentales en primer orden son entrenadas con datasette de millones de imágenes y el segundo orden se las entrega con un número muy grande de entrenamientos obviamente para un desarrollador común como nosotros generar y crear este tipo de redes es una tarea casi imposible dado que no disponemos de los recursos humanos ni materiales para poder hacerlo pero no todo es malo dado que si bien es complejo crear este tipo de redes para un desarrollador común como nosotros estas redes que han sido seleccionadas y premiadas en estos desafíos quedan a partir de ese momento a disposición para que cualquiera de nosotros las podamos usar  en concreto estas redes son reconocidas en el mundo de Deep learning con el nombre de redes pre entrenadas como ya dijimos estas redes son entrenadas con Data set de millones de imágenes que están conformados por un grupo de determinados tipos por ejemplo en más popular de los datasets que se usan en este caso se llama coco Aquí está el sitio web justamente oficial este datasette y utiliza 80 categorías de objetos diferentes  otro dato importante es que este dataset tiene 330.000 imágenes de cada imagen hay cinco captions con lo cual en total existen 1,5 millones de objetos de instancias ahora la pregunta que nos cabría aquí es Qué pasa si yo pretendo usar una red pre entrenada para seleccionar o trabajar con imágenes de otros objetos que no se corresponden con los del datasette con que fuente nada es por ejemplo con algunos de los 80 objetos del Coco dataset que aquí vemos  en realidad los pesos de esas redes han sido entrenados para seleccionar determinados patrones de imágenes pero no Por ello se circunscriben específicamente sus objetos dado que si bien no son los mismos objetos hay patrones que pueden ser similares y poder ser utilizados del mismo modo es por ello que nosotros podemos utilizar una red pre entrenada volviendo a entrenarla con otros datasette que pueden ser otras set de mi propiedad o bien otros Data set que también son públicos como Coco y que tienen imágenes de diferentes contextos por ejemplo imágenes aéreas imágenes subacuáticas u otro tipo de objetos pero teniendo claro esto quizás ahora ustedes se pregunten si a reentrenar esa red con ese nuevo Data set vuelvo a necesitar un tiempo y recursos que no tengo para poder llevar a cabo ese entrenamiento en realidad esto no es así dado que no Se entrena completamente la red sino sólo una parte de ella ya que como dije antes todos los pesos de las capas principales han sido entrenados con imágenes que si bien no son las mismas que vamos a usar Nosotros también sirven para manejarse con nuestras nuevas imágenes  concretamente tenemos en nuestras manos una enorme oportunidad de desarrollar aplicaciones muy interesantes dado que podemos utilizar redes que han sido creadas por desarrolladores e investigadores de primer nivel y nuestro trabajo sólo se limitará a seleccionar la que más nos convenga Y entrenarla utilizando recursos que estén a nuestro alcance pero de qué hablamos cuando hablamos de seleccionar la red que más nos convenga Más allá de que hay diferentes tipos de redes que abordan distintos tipos de tareas a realizar con las imágenes así como también diversidad de tipos de imágenes con que fue entrenada lo cual nos da la posibilidad de elegir la red que más se adapta a nuestras necesidades una misma red tiene muchas veces varias opciones con diferentes tipos de precisión y de velocidad  Por lo general cuando una opción de una misma red es más veloz seguramente será menos precisa En comparación con las otras opciones y viceversa Esto se debe a que las opciones más rápidas y menos precisas de una misma red tienen una arquitectura más sencilla esto es menos neuronas menos capas etcétera y conforme vamos viendo las otras opciones esa arquitectura va creciendo Y con ello va creciendo también la precisión y Por ende la velocidad va descendiendo ahora bien En qué lugar podemos acceder a estas redes pre entrenadas para comenzar a usarlas bien hay muchas plataformas a las cuales podemos recurrir para eso en primer lugar podemos mencionar acá ya que ya la conocemos por haberla utilizado en la clase 3 en este curso aunque con otro propósito pero vamos a agregar acá y está aquí quien también como uno de los sitios más completos para buscar este tipo de redes es muy conocido y no solo con la posibilidad de acceder a ellas sino también para poder ver información de aplicaciones de estas redes  si bien Después vamos a poder ver este sitio con más detalle aquí podemos ver algunas aproximaciones alguna información que es muy interesante para ver la obra aquí por ejemplo podemos ver si queremos Buscar una red pre entrenada que tenga que ver con imágenes o contextos vamos a seleccionar imágenes que lo que estamos viendo ahora y tenemos por ejemplo un tipo de problema con imagen a tratar Y tenemos un buscador que por ejemplo nos permite Buscar una red por arquitectura buscamos una arquitectura determinada y queremos ver qué opciones de res la usan O también por ejemplo contrata set es decir queremos ver qué redes están entrenadas con un determinado Data set que justamente tiene que ver con el problema que queremos abordar  pero vamos a poner especial foco en una de las redes pre entrenadas más conocidas hoy por hoy en el mundo de las redes neuronales convolucionales que es la red io cuyas siglas significan You only look once esta consideración especial se debe a que vamos a usar mucho esta red Durante las próximas clases de redes neuronales computacionales toda la información de esta red se encuentra dentro del sitio uttralitiks que es el que estamos viendo Aquí tiene muchas variantes siendo las últimas versiones yo lo v8 y oro  cuando entramos a cualquiera de los tipos de Yolo por ejemplo vamos a mostrar a Yolo v8 vamos a poder ver lo que hablamos antes en relación a las variantes que hay de cada modelo y el tipo de actividades que se pueden llevar a cabo con ella sí aquí tenemos una de las redes y 8 una de las variantes yo lo vi 8 y podemos ver aquí la cantidad de parámetros que tienen las diferentes versiones es decir la más liviana tiene 3,2 millones y la más pesada a 68,2 millones y la velocidad también como varía en la medida que tiene menos parámetros la cantidad de milisegundos que emplea para la detección es 80 y cuando más pesada 479 es decir es más lenta Tenemos también las variantes para detección para segmentación para clasificación y para 11 es decir a los distintos tipos de problemas las diferentes variantes que existen que tiene que ver en cada caso con la cantidad de parámetros propio de la estructura más liviana más pesada y la velocidad más rápido o más lenta también podemos ver en línea lo que hablamos antes la sección Data sets En donde podemos acceder a diversos Data set para entrenar a Yolo por otro conjunto de datos que no son el coco con que fue entrenada originalmente aquí tenemos por ejemplo bistro  un último aspecto abordar como preparación para lo que vamos a comenzar desde la próxima clase es el entorno de trabajo Paisa aquí estamos viendo el sitio oficial de Paisa desde la próxima clase vamos a empezar a trabajar con este entorno y vamos a abandonar por el momento el entorno polar con que veníamos trabajando  el empezar con paichard es una alternativa muy interesante ya que quizás es más apropiado para lo que vamos a ver y además es importante que podamos utilizar otros entornos para poder conocer diversas opciones Y luego podemos elegir la que más nos gusta o más nos conviene  para ello les pido que por favor instalen este software dado que lo vamos a empezar a utilizar como dije recién desde la próxima clase para lo cual vamos a tener que ir a este botón Descargar que está aquí y luego cuidado no seleccionar esta opción que es la versión profesional Que obviamente tiene costo sino elegir Esta que está aquí abajo que es más liviana pero tiene propósitos educativos  la instalación es muy sencilla simplemente van a tener que bajar este ex y ejecutarlo y siguiendo los pasos que tiene ya predeterminados van a poder instalar este software sin ningún tipo de inconveniente Así que con este entorno vamos a empezar a trabajar desde la próxima clase con Yolo v8 y con el entorno de trabajo paisa Bueno hasta aquí Hemos llegado a esta clase 18 donde abordamos todo lo que dijimos al principio el desarrollo de los pasos para poder implementar Este modelo en un entorno web cosa que le habrá parecido muy interesante A todo fundamentalmente aquellos que manejan html y javascript y luego doble introducción hacia lo que es el contexto que tenemos que preparar para todas las clases que vienen a continuación básicamente el trabajar con Yolo que es una red entrenada de las más conocidas y bueno el nuevo entorno de trabajo que es Paisa Sí así que bueno sin mucho más que decir nos quedan muchas cosas para empezar a recorrer desde la clase 19 hasta la próxima clase aquí termina esta clase los espero en la próxima clase nos vemos  Titulo: Clase19 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/ohHO3qSj9pM  \\n 1667 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 19   Hola a todos Bienvenidos a la clase número 19 del curso de Inteligencia artificial de icfes seguimos en el módulo de Deep learning y seguimos dentro también del tema redes neuronales convolucionales el tema de hoy es como dijimos al finalizar la clase pasada en las últimas clases hicimos varias prácticas con redes neuronales que atendieron a un mismo tipo de problema es un problema de clasificación justamente hoy vamos a ver cómo abordar un problema de clasificación con una red pre entrenada nosotros hasta ahora lo hicimos con Data set que ya venían hechos en el caso de mis y con un Data set que si bien se los dicho lo Podrían haber hecho ustedes que era para diferenciar caballos de seres humanos si bien el modelo que vamos a usar es iolo del cual hablamos al finalizar la clase pasada no lo vamos a usar con su entrenamiento natural con el dataset coco que también referenciamos la clase pasada lo vamos a hacer con un Data set nuestro La idea es hacer una red neuronal convolucional que pueda diferenciar cuatro situaciones de clima y de momento del día básicamente vamos a tener imágenes de Día lluviosos de días soleados de Días nublados y de amaneceres La idea es que justamente nuestra red pueda diferenciar y calificar con certeza una cosa u otra dentro de esas cuatro posibilidades para que podamos entender mejor el objetivo de esta clase vamos a empezar por el final es decir vamos a mostrar las dos variantes o versiones de lo que vamos a desarrollar las vamos a ejecutar la vamos a ver y luego vamos a ir paso a paso haciendo el desarrollo para lograr ese objetivo aquí estamos ya en paichard que es la plataforma de la cual también Hablamos al finalizar la clase pasada y que les pedí que tuviesen ya instalada para empezar esta clase vamos a ejecutar Este programa de python que se llama predict.pay el cual lo vamos a ejecutar haciendo clic aquí y lo que vamos a hacer es ver si nuestro algoritmo acierta en la predicción de lo que tiene una imagen la imagen es un amanecer y la estoy poniendo escribiéndola sobre el mismo código obviamente es una versión muy sencilla pero como primer objetivo vale Así que ejecutamos el programa y vemos que lo primero que hace como yo puse Aquí tres prints es que me muestre Cuáles son las clases a clasificar y aquí me dice cero amanecer uno lluvia dos nublado tres soleado luego me dice Cuál es la probabilidad para cada clase tengo un 0,99 de que sea amanecer De hecho la imagen como dijimos antes corresponde un amanecer bueno 0 0 0 03 para lluvias 00006 para nublado y 00008 para soleado es decir que claramente es un amanecer y luego a una impresión concretamente del texto que corresponde a la probabilidad mayor es decir a la probabilidad cero corresponde el texto amanecer por eso finalmente aquí pone el texto de la clase que aprecio de manera correcta luego vamos a generar otra versión un poquito más avanzada donde va a tomar algunas imágenes de una carpeta de carpetas que tenemos con distintos tipos de imágenes de esta que hablamos recién y me va a ir imprimiendo en pantalla la imagen y va a etiquetar la predicción lo ejecutamos aquí apareció la primera Espera que le dé enter y pasa la próxima luego la otra y finalmente la otra es decir hice un código para Que aparezca una imagen de cada tipo y ha acertado en todos los casos el tipo de imagen a la que corresponde esa predicción bien Este es el objetivo de la clase de hoy lo vamos a hacer como siempre en dos partes Así que ya empezamos entrando a ver primero el tema de las imágenes que es el más importante y luego empezando a entrar por primera vez a Paisa y desarrollar nuestro primer proyecto en esa plataforma bien como habíamos hablado hay un conjunto de datos de imágenes que yo les voy a pasar a ustedes pero tranquilos después al finalizar la clase les voy a decir cómo podrían haberlo hecho ustedes Esto es para acelerar un poquito esta práctica el archivo zipeado o comprimido es este que está aquí dataset.ar lo van a tener en el campo virtual lo van a poder bajar y lo vamos a poner en una carpeta que le vamos a llamar clasificador este otro archivo que está aquí por ahora vamos a dejar en Stand By ya vamos a hablar del más adelante  bien como vieron lo que hice fue poner este archivo rar en esta carpeta nueva que acabamos de crear clasificador lo abrí y hice la extracción de los datos en esta misma carpeta con lo cual ahora ya no nos sirve más lo podemos borrar sin ningún tipo de problemas y vamos a entrar a ver la carpeta Data set tiene dos carpetas tren igual obviamente como ustedes ya van a suponer y correctamente una para entrenamiento y otra para validación esto es muy importante cuando lo hagan ustedes que las carpetas se llamen de este modo si no el entrenamiento con hielo no va a andar bien Sí entonces train y vals son nombres exigidos luego dentro de esto voy a poner una carpeta para cada uno de los tipos de imágenes aquí si ustedes pueden ponerle como quieran sí Entonces vamos a entrar por ejemplo en amanecer y están todas las imágenes de amaneceres sí los nombres también de las imágenes pueden ser los que ustedes quieran luego tenemos lluvia otro tanto nublado otro tanto y soleado otro tanto es conveniente que todas las carpetas de cada una de las opciones tengan la misma cantidad de imágenes Esto no es una exigencia una obligación Pero siempre es conveniente para que sea más equilibrado justamente el algoritmo que vaya a desarrollar esta red neuronal luego vamos un poquito hacia atrás y tenemos Val que tiene exactamente la misma distribución amanecer lluvia nublado y soleado la diferencia es que en el caso de Val tengo 35 imágenes sí y en el caso vamos hacia atrás de train tengo 174 imágenes esto como hacemos siempre con este equilibrio entre cantidad de observaciones para el conjunto de entrenamiento y cantidad de observaciones para el conjunto de validación en este caso hace una relación de 20-25%. eso hay que seguir manteniéndolo que es igual a lo que venimos trabajando antes bien a continuación vamos a crear el proyecto Por lo cual vamos a ver python yo aquí lo tengo abierto ya con el proyecto terminado pero vamos a crear otro proyecto en otra ventana sobre el mismo entorno de pinchard voy a venir a file y le voy a dar New proyecto una vez aquí fíjense que aparece la localización donde yo voy a crear este proyecto y voy a buscar la carpeta c dentro de ese clasificador una vez que la elijo le doy ok Y le doy create acá voy a decir una unidad fíjense que la base o el intérprete base es python tienen que tener obviamente payton instalado es una novedad porque ya venimos trabajando sirviendo trabajamos dentro de cola ya venimos trabajando con python Así que python lo tienen que tener instalado aquí tengo la versión 310 Pues a la que corresponda automáticamente le vas a poner aquí en la base o el intérprete base de python de la versión que ustedes tengan instalado creamos el proyecto y acá me va a decir si creo el proyecto desde la carpeta existente o quiero hacer una carpeta nueva No justamente lo voy a hacer con la carpeta que Acabo de crear antes así que lo de create y acá lo que me pregunta es si quiero cerrar esta ventana que tengo abierta y poner sobre esta única ventana el proyecto que estoy creando o Quiero crear una nueva ventana para el proyecto nuevo conservando la ventana anterior para el proyecto antiguo Así que le voy a decir que quiero una New Windows Bueno aquí está creando un entorno virtual que esto es fundamental para generar este tipo de proyectos Así que eso toma un tiempito pero ya está estas ventanas le van a aparecer muchas veces vamos a maximizar esto aquí que son recomendaciones o tips Sí bueno ustedes lo pueden leer en algunos casos tienen que ver bueno como dice aquí como combinar teclas Bueno no es algo que lo necesiten Así que lo cerramos y avanzamos aquí fíjense que ya me reconoce que dentro de mi carpeta clasificador existe ya una estructura conformada por datasette yo ahora voy a ir al explorador de Windows y lo voy a meter todo esto dentro de la carpeta datos Porque por ahí en algún momento quiero manejar otro tipo de datos que no son solamente y creo que queda más ordenado así es una cuestión de gusto nada más así que vamos a hacer eso justamente para tener una modalidad y una forma de cómo manejarse con este tipo de criterio  bien ya tenemos dentro de datos Entonces ahora te hace todo lo que habíamos visto antes y si vuelve un poquito hacia atrás voy a ver que también me ha creado la carpeta del entorno virtual que se está que está aquí y esta punto hay día también son dos carpetas básicas de cualquier estructura de un proyecto de tipo bien volvemos al paisa fíjense que ahora lo que hice en el explorador de Windows me lo resalta aquí así que me crea automáticamente este o sea refleja automáticamente el contenido de la carpeta que he elegido para crear Bueno lo que vamos a hacer a continuación es crear nuestro primer programa de python dentro de una interfaz de python Sí así que venimos aquí donde está el nombre de la carpeta principal y le doy botón derecho nuevo python file y vamos a poner train.ar porque traen punto Pay porque lo primero que vamos a hacer es entrenar a nuestro modelo de Yolo con nuestro propio Data bien Aquí está el código de este train punto fíjense que son solamente tres líneas y con tres líneas vamos a entrenar al modelo esta red pre entrenada Yolo de 8 que es la última o una de las últimas versiones Perdón junto con nuestro datasette en principio lo que tengo que hacer y aquí fíjese que me lo marca justamente en rojo es importar la librería ultralitiks que es la que justamente me habilita el uso para ello vamos a file settings y aquí lo que vamos a ir es a la sección donde dice project con el nombre del proyecto nuestro lo abrimos y vamos a project interpreter aquí fíjense que tiene un más y un menos evidentemente el menos va a accionarse cuando yo seleccione una librería para quitarla y si no con más vamos a agregar una hacemos clic aquí y buscamos en esta barra o sea lo podemos Buscar de arriba hacia abajo va a ser bastante complicado porque tiene muchísimas muchísimas opciones y librerías que ya vienen preconfiguradas vamos a poner Ultra buscando por aproximación y aquí lo encuentra y vamos a elegir la versión de ultralitiks que vamos a instalar esto tiene que ver para tener mayor garantía de éxito Con la práctica y que ustedes no tengan ningún problema Esto va a pasar permanentemente el tema de las versiones puede ser que ustedes hagan una práctica la misma con el mismo código sin ningún cambio con una versión posterior o anterior y haya habido algún cambio en esa librería Y ustedes tengan algún problema no va a pasar nada seguramente cuando aparecen estas cosas siempre les da algún tipo de referencia también lo pueden buscar en algún buscador bueno Cuál es la nueva versión es cuál es la variante de alguna cuestión que tenga que ver con esa librería y solucionamos el tema pero para que sea lo más efectivo posible este proceso vamos a instalar una versión determinada la versión es la 8058 la última versión no es esa es posterior a eso así que vamos a hacer clic aquí para decirle que quiero una versión específica y buscamos la que dejemos recién la 80 58 insisto Esto es para tener mayor tranquilidad puede ser que tomemos la última versión y no tengamos ningún problema pero bueno para evitarlo los propósitos del éxito de esta clase vamos a utilizar la misma que utilice yo cuando estuvimos cuando creamos esta clase Así que habiendo elegido la librería y habiendo elegido la versión le doy install va a tardar un tiempo Eso depende mucho de la librería librerías Obviamente que son más pesadas que tarda más tiempo y otras no tanto bien así terminó de instalar sutralities lo que vamos a instalar ahora es nampai así que sin salir de aquí directamente borro lo que estaba escrito y escribo la nueva librería que Quiero instalar y lo mismo install esta va a tardar menos obviamente ya está bueno son estas dos nada más por ahora después vamos a instalar seguramente más así que cerramos esta ventana le damos Ok y ya fíjense que obviamente lo que estás viendo aquí ya no aparece señalizado como antes como que estaba Había algo que faltaba instalar Bueno lo próximo es crear el modelo fíjense que es sencillo es hacerlo invoco a Yolo que es lo que había importado aquí y pongo dentro de los paréntesis el tipo de modelo de hielo que quiero no solamente me refiero a la versión en este caso es la v8 sino también a dos cuestiones a tener en cuenta primero Qué tipo de problema quiero abordar y Por ende tengo que elegir un modelo que tenga que ver con ese problema y segundo qué nivel de peso de requiero si una red liviana y rápida o pesada y más lenta pero más precisa como ya sabemos dijimos antes Bueno eso es lo que tengo que elegir ahora lo que vamos a elegir es un nuevo que sirva para clasificación por eso esta parte del nombre cls y la versión más liviana que es la n esto de dónde lo saco en realidad si yo ejecuto este programa sin que tenga esto instalado automáticamente lo va a instalar python pero vamos a verlo más prolijo vamos a ir al sitio de Yolo y lo vamos a bajar Desde allí lo vamos a poner dentro de nuestra carpeta y una vez que esté allí vamos a ejecutar y vamos a avanzar con el siguiente paso de este programa bueno acá estoy en la sección models la reaccionó si hago clic en este menú de ultralitics al sitio lo conocen porque hablamos de él en la última clase y selecciono aquí el modelo que quiero que 8 hago clic aquí y fíjense que bueno acá hay una gráfica muy interesante porque me muestra todas las versiones que existen y cuál es bueno mi nivel de rendimiento obviamente superador en cuanto lográndose mejores versiones sí lo que voy a hacer aquí es ir a esta sección Donde tengo los tipos de problemas que se pueden abordar con libros detección segmentación clasificación y voz lo que nos consideramos nosotros es clasificación Así que voy aquí y elijo lo que vimos recién si la yo lo v8 n que es la más liviana así como x es la más pesada entonces hago clic aquí y va a bajarme esa versión donde la voy a poner bueno en la carpeta que venimos trabajando dentro de clasificador es decir aquí y volvemos al entorno de Paisa donde estábamos con el tren punto Pipe que estábamos programando y constatamos de que ahora aparece aquí justamente en esta visión que me muestra esta explorador interno que tiene Paisa lo que acabo de bajar esto que está aquí es lo que vimos y ahora vamos a ir hacia allí Aquí vemos los dos archivos que están en el campus virtual que ustedes al bajar lo pueden utilizarlos y este que está acá es el que acabamos de bajar Sí yo se los paso por cualquier cosa pero en realidad lo mejor es que aprendan a bajarlo porque así van a poder bajar el modelo que quieran cuando quieran bien ya tenemos creado el modelo que tenemos que hacer ahora entrenarlo con lo cual con modo el punto 30 y entre paréntesis tengo que poner lo siguiente en principio Dónde están las imágenes están en C de barra clasificador barra de datos barra de datos esto traten de poner la ruta completa para que no tener ningún tipo de inconveniente luego la cantidad de epox O sea la cantidad de entrenamientos con que quiero que se entrene Este modelo de iolo con mi conjunto de datos y finalmente el tamaño el size de las imágenes Sí bueno hecho todo esto no me queda más que ejecutar fíjense que esto es parecido a lo que nos mostraba cola van a ver aquí cada una de las líneas que hacen al entrenamiento y me dice que los resultados del entrenamiento lo va a poner dentro de rams classify Ya lo vamos a ver bien ahí terminó el entrenamiento fíjense que la quiere decir que hemos logrado es de 0.93 6 nada mal acuérdense que estamos manejando una cantidad de imágenes muy chica sí no tenemos una tazas muy grande así que me parece que es muy buena clase pero ahora vamos a ir a esta carpeta que señala aquí rams clasify train bien aquí tenemos la carpeta runs que referenciamos recién Vamos a entrar a ella tenemos classify y dentro de ella tenemos 30 estas carpetas se crean la primera vez que yo entreno al modelo y concretamente esta carpeta que se llama train se va a llamar train la primera vez que yo entré al modelo si yo vuelvo a entrenar al modelo y no borre esta carpeta las próximas se van a llamar tren 132 etcétera etcétera y bueno Obviamente que si lo borro voy a apagar la información del entrenamiento anterior no sería lógico con lo cual creo que es una buena recomendación el conservar todas las carpetas bien entramos a aquí tenemos weiss y resulta vamos a empezar por este último archivo si lo abro van a ver que lo que tiene es Ni más ni menos que lo que habíamos visto antes en colap y lo que vimos recién en pantalla el histórico de cómo fue evolucionarlo por cada entrenamiento todas las métricas sí la curva de pérdida y la curva de precisión Es decir para hacer un paralelismo con lo que veníamos trabajando en cola justamente para hacer un paralelismo con lo que veníamos trabajando en cola ustedes recuerdan que en la clase 16 hicimos un código a través del cual estas métricas las graficábamos sí la curva de la pérdida y la curva de la precisión eso es lo que vamos a hacer ahora con un nuevo programa dentro de nuestro entorno de paisan tomando un poquito del código que usamos en cola para justamente aprovecharlo y ver que estamos haciendo más o menos lo mismo tiene una pequeña diferencia pero bueno en principio vamos a ir a lo que hicimos en ese momento en cola Aquí está recuerdan que teníamos un código que lo usamos con Maple lip y teníamos estas curvas de precisión y de pérdida ambos para el conjunto de entrenamiento la información la sacamos en base a la variable histórico que usábamos cuando entrenábamos al modelo esa va a ser la única diferencia que vamos a tener en este caso porque en lugar de tener una variable en la que yo fui guardando los entrenamientos ya lo hizo Yolo y me generó este archivo csv que va a ser la fuente de ingreso de datos es decir en lugar histórico vamos a usar ese archivo csv Así que este mismo código que está aquí prácticamente el mismo vamos a usarlo desde python creando un nuevo programa a ese nuevo programa le vamos a poner metrix punto aquí tenemos el código en principio lo que tengo que hacer es empezar por importar pandas y acuérdense que aquí ya no estamos en cola cada vez que quiero importar algo tengo que tenerlo justamente incorporado dentro del proyecto Por lo cual vamos a hacer el mismo proceso que hicimos antes para ultralitiks y para nampai vamos a hacerlo ahora para pandas y vamos a files settings y ya estamos aquí y vamos a hacer clic aquí en el más para empezar a importar la primer librería aquí está y luego pandas bastante rápido a las dos le doy Ok y ya tengo instaladas las librerías que me hacían falta bien aquí lo que vamos a hacer es crear un Data frame al igual que lo hacíamos con cola a través de pandas Rift ssv y tomando el lugar del cual está justamente este archivo en realidad es 30 Aquí tengo 321 porque lo había utilizado antes es esta ruta que está aquí fíjense Perdón aquí en rams classify train y resulta este punto implica que justamente el directorio está aquí dentro no hace falta poner la ruta completa si la quieren poner completa no hay ningún problema bien luego voy a crear una matriz Perdón los y una matriz acc lo mismo que hacía o que hicimos justamente en el colar y vamos a tomar de results la columna traen barra loss y de results la columna metrix si esta es la de pérdida Y esta es la depresión Por qué tienen Estos espacios que están acá adelante porque en el ccv está puesto de ese modo así el nombre del campo y si no lo pone así probablemente no lo reconozcan bien aquí el nombre es todo esto lo mismo que aquí por eso la idea es poner todo el contenido que está entre cada una de las comas que separan una columna de la otra luego el resto del código es Exactamente igual Exactamente igual al que teníamos en el ejercicio 16 solamente que lo que cambió aquí es que en lugar de tener cinco entrenamientos tengo 20 entrenamientos Por lo cual el rango para graficar ahora es de 20 bien habiendo hecho todo esto lo guardamos y lo probamos y aquí tengo justamente el gráfico con las dos curvas la de precisión y la de arriba hacia abajo como debe ser y la de precisión hacia arriba como debe ser bien vamos a mirar una cosita acá muy importante fíjense dónde termina el entrenamiento pero fíjense que donde terminan no fue el punto de precisión más alto el punto de presión más alto estuvo en el entrenamiento número 15 vamos a ver eso ahora volvemos al archivo csv que Les recomiendo si lo quieren lo pueden abrir con Excel para que sea más fácil de visualizarlo pero aquí vemos que el entrenamiento 15 tenemos el altura así más alto 095 714 en el 14 tenemos 0 95 pero después baja a 0,93 571 y se mantiene así hasta el último caso esto nos va a llevar a lo que vamos a ver a continuación si vuelvo a la carpeta clasificador rams clasify training está toda la información del entrenamiento donde tomamos recién esta resolución csv veo que tengo una carpeta Waze la abrimos y tengo dos archivos de tipo PT Qué son los PT son los modelos que desarrollamos siempre la extensión Pero por qué me muestra ves y Last porque justamente puede pasar lo que nos pasó aquí el último no es el mejor entonces este las representa el último modelo es decir el que se obtuvo con el último entrenamiento Y ves me da el modelo con la más alta predicción aquí podemos tomar dos posturas puedo hacer que tome vez pensando que es Obviamente el que tiene mejor score razones están de más pero las es el que tuvo el mejor entrenamiento y probablemente la cantidad de entrenamientos haga que tenga una predicción mejor Más allá de que el score no lo dijera sí pero a mayor cantidad de entrenamientos muchas veces también es algo conveniente a tener combustión las dos opciones en este caso están al alcance de la mano no siempre va a ser así probablemente la las sea la que tenga la mayor cantidad de entrenamientos y la de mayor precisión en este caso Está bueno el ejemplo porque justamente se da la lógica de porque hay un vez y porque hay un Last nosotros para este ejercicio Pero si ustedes quieren tomar el vez También pueden hacerlo Bueno ya tenemos todos los elementos necesarios en condiciones y vamos a crear nuestro primer programa para predecir para usar el modelo y ver si funciona bien o no con lo cual voy a ir de nuevo a a para crear un nuevo archivo de python que le voy a poner predict si el punto No ponerlo porque ya le estoy especificando que es un archivo de python bien aquí tenemos el código pero lo vamos a empezar a ver en la segunda parte de esta clase Así que vamos a la segunda parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte   Titulo: Clase19 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/0KCVEQp9AzE  \\n 1799 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella  bien seguimos con la segunda parte de esta clase donde habíamos terminado la primera parte de esta clase creando pay y teníamos este código que ahora vamos a pasar explicando en principio la importación de las librerías y olo y nampai y luego vamos a crear el modelo que es esta línea que está aquí situación similar a la que teníamos en Trade Cuál es la diferencia en este caso cuando lo hicimos aquí para train utilizamos el modelo clásico de iolo en este caso nosotros a ese modelo lo entrenamos con nuestro propio y obtuvimos dos archivos ves y las y dijimos que vamos a usar las para tener nuestro modelo de Yolo entrenado con nuestros datos bien Entonces vamos a crear nuestro modelo de ese modo poniendo la ruta donde está las puntopt es más lo podemos revisar aquí y fíjense cómo sería la ruta rams es decir es esto mismo que está aquí siempre con el punto para indicarle que es un directorio interno Pero insisto pueden poner a la ruta completa de la raíz luego vamos a hacer la predicción cómo vamos a hacer la predicción bueno con model y poniéndole dentro de modos el nombre de la imagen que quiero que prediga entonces para eso voy a acceder también a usar la ruta interna datos Data set amanecer y dentro de amanecer sunrise 31 puntos JP es decir esta ruta con esto lo que hace es obtener resultados de la predicción aplicando nuestro modelo sobre ese archivo de imagen sí los resultados los voy a mostrar digamos por distintas partes resulta digamos es un componente es un conjunto muy grande de datos sí que vamos a imprimirlo aquí así tenemos una una primera aproximación y se entiende bien de qué se trata esto que estoy intentando explicar vamos a poner print de results y vamos a comentar estas líneas que no vamos a ejecutar por ahora bien ejecutamos esto fíjense toda la información que tiene results tiene la skin los boxes que por ahora no vamos a ver de qué se trata más adelante si lo vamos a ver los nombres de las cuatro clases sí la configuración con los valores de rgb acuérdense que en este caso no es no son imágenes con tonos de grises sino un color bien el array completo con la imagen el tamaño original el size original de la imagen bueno como se llama la imagen donde está la ruta hacia dónde está el tensor con las probabilidades de cuál de las cuatro puede ser acá Cuál de las cuatro la primera justamente es un amanecer y luego bueno algunos datos que tienen que ver con la velocidad de procesamiento bien de todo esto este conjunto de información que tiene razón Yo tengo que sacar una parte de ellas sí Cuál es la parte que tengo que sacar la de los names que es esto que está aquí por eso volviendo al código vamos a bajar esto y vamos a desconectar esto que está aquí lo primero que hago es tomar los names de results y ponerlos en una variable que se llama Nancy en realidad una radio porque varios van a aparecer varios nombres Sí luego voy a tomar la lista de probabilidades Sí pero off esto que subimos un poquito esto acá de nuevo aquí arriba esto que es un tensor y para mostrarlo tengo que convertirlo en lista por eso es Data punto tu lista y lo pongo también en una variable de tipo lista bien y finalmente lo que voy a hacer Voy a imprimir lo primero es decir lo segundo que he generado props y tercero vamos a imprimir el nombre de la clase a la cual corresponde esta imagen para poder entender mejor Esta última línea vamos a ejecutar el programa para que se pueda entender esta secuencia de instrucciones anidadas y por qué sale el nombre de la clase que predice este algoritmo finalmente bien fíjense lo que sale aquí es este texto que dice aquí nombre de la clase predescia amanecer la palabra amanecer la palabra amanecer De dónde sale sale una raíz que tiene cuatro elementos cero amanecer una lluvia dos nublado tres soleados qué es lo que hace esta instrucción en principio me dice que voy a sacar de name is Dick Es decir de este diccionario sí uno de los nombres no sé pero tengo props también que es prox es esta raíz que está acá con las cuatro probabilidades a ese props yo le aplico np Art Max np add Max es obviamente un método Nampa que lo que hace que es elegir cuál es de todo un arranque el valor mayor pero no me da el valor mayor Si no me da la posición del valor mayor es decir que este nt armas de props no me va a devolver 0998 etcétera etcétera sino que me va a decir cero entonces con ese cero yo le digo que quiero el valor cero Deep y Por ende lo que me da como resultado en la palabra amanecer bien Por eso aquí justamente aparecen estas tres impresiones de los cuatro nombres de las clases las cuatro probabilidades donde claramente está con 090 o sea 99.81 3% es la más probable y finalmente la palabra amanecer bien con esta versión simple ya tenemos una prueba de que hemos creado un modelo que por lo menos con esta imagen haga una previsión correcta pero como dijimos al principio de la clase vamos a generar otro modelo de break que sea un poquito más Dinámico no tan estático como el que hicimos recién que teníamos que escribirle literalmente dentro del contexto del código de programación el nombre de la imagen y la ruta que quiero predecir así que vamos a crear un nuevo archivo de python y le vamos a poner predict 1 y a ese y vamos a poner todo este código que está aquí que al igual que en el caso de predic vamos a explicar paso a paso en principio vamos a importar las mismas librerías que en predique ultralitics import y ololo y luego vamos a importar os y vamos a importar en realidad es una librería que se llama opencv que es una de las librerías más conocidas de lo que es todo lo que es el mundo lo que corresponde al mundo de visión computacional Así que estas librerías no están incorporadas dentro del proyecto Así que paso seguido vamos a empezar por eso vamos a Fall settings y vamos a incorporar Open guión la instalamos y le damos ok por lo tanto lo que tenemos que hacer lo mismo que antes creamos el modelo al igual que como lo hicimos en predique la misma línea Sí y vamos a hacer lo siguiente en este caso vamos a buscar un código que tome la primera imagen de la carpeta de validación la voy a abrir aquí para que se entienda bien de esta la primera imagen de amanecer la primera imagen de lluvia la primera imagen de nublado la primera imagen del soleado luego vamos a crear una variable folder pad con la ruta de datos Data set Por qué Porque lo que yo quiero es que a través de oslistic que es un método de la librería o s recorrer y armar un array con los nombres de todas las carpetas que están dentro de este caso estas cuatro eso lo va a poner en esta otra variable pad list donde va a tener los nombres de estas cuatro carpetas qué voy a hacer ahora voy a entrar a cada una de esas carpetas y voy a tomar las imágenes voy a decir las imágenes Pues porque no es las imágenes con este Ford como bien dice aquí Ford pass es siempre una variable que me permite identificar cada uno de los elementos voy a tomar cada uno de los elementos de sí es decir amanecer Sí y todas el resto de las carpetas cuando tome la primer carpeta amanecer voy a hacer lo propio pero tomando lo que tiene adentro la carpeta amanecer es decir los archivos cómo lo hago con otro Ford image in o es el uso el mismo método siempre me permite bucear que hay dentro de una carpeta pero ahora las rutas es diferente por qué Porque la ruta sería la ruta que tenía al principio datos Data set yval más amanecer como armo esa ruta con dos partes justamente con otro método ese que se llama path joint juntar Paz sí Entonces con se pasó Ahí le digo que me hagan una ruta tomando folder pass que es este que estaba acá datos y paz que va a ir variando en este primer caso es amanecer entonces con esto le estoy diciendo con ese pad completo datos va al amanecer dame todo el contenido y andan mostrándomelo de a uno a través de la variable mg con lo cual Dicho de otro modo en la primera oportunidad cuánto va a valer img sunrise 31 cuánto va a valer en la segunda oportunidad Y así sucesivamente hasta llegar a la última imagen que será Sandra 331.j a partir de lo cual va a ser lo mismo pero para la siguiente carpeta y así está la última carpeta bien Estoy parado Entonces en este caso en esta primer archivo supongamos que estamos aquí Entonces qué voy a hacer Voy a armar una predicción pasa que lo hago de modo Dinámico antes escribía el nombre ahora bien decir que esto que yo tenía aquí el mismo archivo sí no lo voy a tomar habiéndolo escrito literalmente sino tomándolo de la manera dinámica por lo tanto esto que está aquí es lo mismo que antes lo que pasa que está armado de otra manera dinámica no está escrito como dicho recién literalmente Entonces qué le voy a poner a model para que haga la predicción le voy a poner de nuevo un passione ahora armado por tres elementos por folder path que es Data por paz que es amanecer y por ing que es sunray 31 puntos de esa manera con estos tres elementos tengo toda la ruta completa hacia la imagen que quiero utilizar con resulta hago la predicción igual que antes y estas tres líneas que tengo aquí más allá que están explicadas con este comentario son las mismas líneas que tenían exactamente la misma dónde viene el cambio ahora viene el cambio justamente a partir del uso de Civic que le da todo un contexto este estético visual que no teníamos en la versión anterior más allá esta cuestión dinámica que tampoco teníamos en la versión anterior lo que vamos a hacer a continuación es aplicar el método tomando como referencia el mismo pad que usamos para predecir si la misma el mismo archivo a la imagen y lo voy a poner dentro de img1 para no utilizar el mismo nombre y ya lo venía usando antes sí a partir de esto lo que me va a pasar Es que voy a tener dentro de mg1 una distribución de la imagen en un formato que si tú reconoce Porque después a continuación Voy a aplicar otras cosas con show lo que hago es Mostrar por pantalla la imagen Sí una imagen cualquiera en este caso la imagen que estamos viendo y hay que ponerle un nombre que es el que va a aparecer en la parte de arriba de la ventana donde se va a mostrar esta imagen y la imagen concretamente que quiero mostrar me salte dos pasos porque es porque Bueno a continuación voy a explicar qué es lo que propone estos dos pasos la idea sería en principio poder dibujar en el vértice superior izquierdo como se la muestra un rectángulo que va a oficiar de fondo de un texto que va a aparecer adelante ese rectángulo rectángulo va a ser de color rojo y el texto va a ser de color blanco Cómo se arma el rectángulo con esta línea que está acá con cybitu punto hay que poner la imagen donde quiero Que aparezca ese rectángulo esto serían las coordenadas ese sería el vértice superior izquierdo de ese rectángulo Y estos dos referencias 160 y 22 estado coordenadas es el vértice inferior derecho de ese rectángulo esto obviamente va a tener que ver un poco aprobando digamos en virtud del texto que vayamos a escribir luego le voy a especificar Con qué color quiero que salga ese rectángulo y voy a elegir el 00255 esto ojo Porque pueden confundirse de que es en realidad rgb sí con lo cual tiene que ser azul en realidad sigue tu trabajo al revés trabaja con bgr con lo cual 00 255 es rojo y con menos uno le especifico que ese rectángulo tiene que ser Sólido y no solamente marcando los bordes rectángulos sino sólido bien luego viene el texto tiene una estructura similar Obviamente el método no es el mismo en este caso es de lo primero que se pone en la imagen luego el texto completamente que quiero mostrar que es name name viene se acuerdan de lo que teníamos aquí en esta variable que era lo que en el caso anterior de predict a secas era lo que mostrábamos sí lo que mostramos como finalmente como el nombre final bien aquí eso lo tenemos al igual que con armas aquí lo tenemos con armas de aquí dentro de una variable bien con 120 le voy a fijar Cuáles son las coordenadas a partir de las cuales quiero que empiece el texto acá No hay coordenadas de fin como el caso del rectángulo simplemente diciéndole desde aquí quiero que empiece el texto el texto será tan largo como letras tenga esto es la Fuente dentro de civitud hay varios tipos de fuente este unos tipos más usados de fuente luego el color 255 tres veces quiere decir blanco así como 03 veces quiere decir negro bien y con esto le indicó el tamaño perdón de la letra y esto el ancho de la letra Sí así que bueno esto Ahora cuando lo probemos vamos a cambiar algunos parámetros para que lo puedan ver bien pero bueno Esta es la descripción completa finalmente que hago le pongo un huequito es un método que sirve para que se detenga la ejecución de lo que yo vaya a estar haciendo este caso la muestra de esa imagen Y cuando aprieta cualquier tecla Va a continuar la ejecución del programa y finalmente break Por qué Break y esto obviamente lo dejo un poco en manos de ustedes para que vean cómo lo quieren manejar yo lo que voy a hacer acá es solamente tratar de mostrar como dijimos antes la primera imagen de cada una de las carpetas Sí con lo cual cuando yo entré este Ford y recorre esto por primera vez y hago un break de esa manera le estaré indicando hace porque solamente voy a dar un ciclo y al primer ciclo se corta el Ford y ya pasaré a la próxima carpeta que será la lluvia ahora esto porque lo dejo así o lo dejo un poco a manos de ustedes como lo quieran trabajar si ustedes quieren que esto se ejecute más de una vez por carpeta Bueno pues entonces pondrán todo esto dentro de un If Irán contando sí u otro Ford Sí este y contarán cuántas veces quiere que salga esto y si quieren que salgan todas literalmente todas Obviamente que en ese caso van a tener que sacar el break no Y en este caso justamente referencia eso si quieren que aparezcan todas todas completamente las este imágenes bueno sacan el break directamente y listo Así que eso lo dejo un poquito a consideración ustedes yo pongo acá un break para que no sea tan larga la demostración Bueno vamos a mostrar la ejecución de esto que ya lo habíamos hecho al principio de esta clase pero ahora lo hacemos ya conociendo todo el código que hemos escrito para llegar al objetivo que mostramos al principio de la clase Bueno aquí ven la primer ventana fíjense que dice imagen en la parte superior porque es imagen en la parte superior porque acá en el ini show puse imagen y que me muestra img y mg es la imagen original más lo que yo le puse arriba la etiqueta que yo le puse arriba es decir la palabra amanecer porque es lo que me tomó aquí de name sí en este caso de esta imagen lo que predijo y aparece como dijimos el rectángulo sólido de color rojo y el texto de color blanco Bueno si yo a esto oprimo una tecla va a pasar a la o sea esto lo hice una vez break toma la siguiente carpeta la siguiente carpeta es la de lluvia toma la primera imagen de lluvia y realiza la proyección correcta que es imagen corresponde y así hasta llegar al final en que todas las predicciones han sido correctas y bueno cuando entre a la última imagen automáticamente el programa va a terminar vamos a jugar un poquito con este tema de el rectángulo por ejemplo vamos a quitarlo y van a ver que ahora van a aparecer las referencias pero sin el fondo rojo ven amanecer lluvia Casi ni se ve la letra nublado soleado bien lo inverso ocurría si yo ahora comento porque ahora lo que va a aparecer va a aparecer el rectángulo rojo pero no va a aparecer el texto ejecutamos para que ustedes lo puedan ver Esta es una buena manera justamente de aprender todas las variantes de síbito bien siempre aparece esto no quiere decir que no estés en la predicción Simplemente no estoy mostrando el nombre de la predicción que es otra cosa quitamos el comentario vamos a poner aquí por ejemplo un dos ejecutamos vamos a ver que la letra va a cambiar ven que inclusive escapa del tamaño del rectángulo porque porque es más grande la Fuente acá Bueno voy a detener directamente el programa haciendo clic en este botón de aquí arriba así no llegó hasta el final porque ya sabemos lo que con lo que nos vamos a encontrar vamos a volver a poner uno Aquí y ahora vamos a trabajar con el grosor de la letra vamos a poner un número más grande aquí no se detuvo Ahora sí bien volvemos a ejecutarlo ven hasta inclusive queda más lindo así que lo vamos a dejar así ahí ven justamente las etiquetas esto después hay formas de que ese recuadro se adapte al largo del texto aquí yo elegí el texto más largo que es amanecer Pero bueno eso ya vamos a ver más adelante que eso se puede este configurar para adaptarse a esto con algunos cálculos matemáticos y algunas herramientas que tiene bien Obviamente que si cambio los colores eso también este cambiaría si vamos a poner por ejemplo 255 aquí y pongo cero Aquí bueno Esto insisto para que ustedes todas estas cuestiones que voy camino también lo puedan hacer ustedes y una buena forma sea esta de aprender las variantes de vamos a volver a dejarlo en rojo y hasta aquí todo lo que tiene que ver con este programa le damos control S para guardarlo con su última versión y ya hemos terminado esta práctica bien con esto terminamos la ejercitación que nos hemos propuesto en esta clase pero vamos a dejar algunas cuestiones pendientes para que ustedes la puedan desarrollar y sacarle más provecho esta práctica que hemos hecho la primera de ellos cuál sería yo les propondría por ejemplo agregar una circunstancia más del día por ejemplo imágenes de nieve Entonces vamos a tener en este caso no cuatro clases sino cinco van a tener que bajar imágenes de nieve y colocarlas dentro de la carpeta correspondiente es decir tendrían que venir a la carpeta Data set train y poner o agregar una quinta carpeta con imágenes de nieve sí o de paisajes con nieve tantas en la misma cantidad que hay de Amanecer de lluvia nublado y soleado y tendría que hacer otro tanto dentro de la carpeta de value sí Recuerden que el nombre tiene que ser el mismo si amanecer en ambos casos en este caso nieve o Nevada o lo que fuera Sí así que esa es una buena práctica que le sugiero poder hacer para justamente poder sacarle más provecho a esta práctica ahora La pregunta sería Cómo hago para bajar toda esa cantidad de imágenes Bueno ahí vamos vamos a acceder a este programa que se llama Download all images en realidad hay muchos programas que ofician de extensiones de Google que me permiten bajar las imágenes del Buscador de Google sí esto queda como una extensión y a partir de esto yo puedo usar el Buscador de Google y con esta aplicación que insisto es una extensión de Google bajar todas esas imágenes simplemente con el solo hecho de recorrerlas en el Buscador de Google Acá tengo por ejemplo puse paisajes nevados y tengo todas estas fotos sí así que lo que voy a hacer ahora más o menos calcular que tengo cinco imágenes por fila voy a ir hacia abajo hasta más o menos llegar a la cantidad de imágenes que estoy buscando que entre conjunto entrenamiento de validación rondan Las 230 243 igual no hay problema Si no llego esa cantidad porque puedo volver a hacer este proceso otra vez o Tantas veces como quiera vamos a suponer que hasta aquí tengo eso es un cálculo muy muy muy visual no este voy aquí a la extensión elijo la extensión y fíjense lo que pasa ya las está bajando Ven aquí el contador y hasta de esa manera me entero Cuántas imágenes he logrado bajar Por eso digo que si no llego a la cantidad que quiero puedo ejecutar nuevamente este proceso bien Ahí está bajando y allí me genera un zip y este zip Bueno lo puedo poner aquí dentro de datos lo puede comprimir donde quiera Sí este y bueno una vez que lo tengo aquí vamos por dentro de la tasette lo pongo transitoriamente en este lugar y Lo pongo aquí directamente en este lugar está el propio lugar es traer aquí y bueno van a poder ver que todas estas imágenes son imágenes que las que veíamos recién corresponden a sitios nevados bien lo que tengo que hacer con esto luego es digamos seleccionar estas imágenes y ponerlas en una carpeta en nieve o Nevada dentro de tren y otra carpeta nieve Nevada dentro de distribuyendo las proporcionalmente al igual que el resto de los conjuntos que vimos hasta ahora sí es importante vamos a poner aquí iconos grandes sí que miren las imágenes porque quizás haya unas imágenes por ejemplo hasta que está acá que no van a aplicar a esto Esto es una revista por ejemplo vamos a seguir mirando puede ser que hayan dibujos por ejemplo o ven acá una tapa de una revista un interior de una revista Bueno hay que hacer un proceso de selección de imágenes no es que todas las que voy a bajar van a servir y al final seguramente voy a ver algunas imágenes que tienen este logo que son vectores sí Que obviamente no sirven para este caso Bueno hay que hacer insisto una selección cuidada Acá tengo más pase de unir un animal bueno las que realmente representen lo que ustedes están buscando no importa el tamaño sí obviamente que la mayor definición va a favorecer al modelo Pero bueno tampoco es una cuestión que todas las imágenes tienen que ser exactamente iguales bueno acá tienen otra imagen por ejemplo que es un dibujo Insisto Hay que hacer una selección cuidada a las imágenes y una vez que tengamos la cantidad de imágenes o la calidad de imágenes que queremos la distribuimos en los grupos de train y demás  Luego de eso hay que volver a entrenar al modelo iolo con este nuevo Data set que en realidad es muy parecido al anterior porque tiene una quinta clase nada más pero es otro hay que volver a entrenarlo y una vez que lo vuelva a entrenar voy a tener nuevamente un las PT y un Best PT depende Cuál de los dos quiera usar será el que deberé acuérdense que eso va a ir a parar a otra carpeta train no la anterior con lo cual van a tener que ver bien esta ruta quizás no sea tren sino tren uno Sí o sea otra carpeta de Perdón no de esta sino de rams Sí otra carpeta ven acá tienen traen 21 sí que es una nueva carpeta bueno por eso digo cada vez que entrenamos el algoritmo hay otra carpeta entonces tiene que fijarse bien de No equivocarse porque si no van a estar usando un modelo que no es el de el nuevo y solamente con ese cambio todo este código que está acá uno les va a servir porque porque justamente todo lo que hace acá es recolectar desde el vamos Cuántas clases hay O sea que eso es algo Dinámico Por más que yo le ponga dos tres cuatro cinco clases más este código está para que se adapte al dataset nuevo y a la cantidad de clases que tienen O sea que el cambio es mínimo y es una buena manera de poder empezar a probar cosas diferentes con el código que ya tenemos otra cosa que Les propongo investigar más allá que después más adelante lo vamos a utilizar vamos a suponer que acá ustedes me pueden decir bueno está bárbaro el sistema Este programa que hemos hecho porque me muestra imágenes y muestra la etiqueta pero a partir de eso yo como que tengo que memorizar Cuáles fueron las predicciones no hay una forma de que me quede registro de todas las imágenes y hasta podría pensar que será algo muy bueno que yo tengo todas las imágenes dentro de una misma carpeta no como recién que hicimos conjuntos de validación sino que ya estamos aplicando este algoritmo y tengo todo un Universo de fotos diferentes de imágenes diferentes de estas cuatro o cinco situaciones este y están todas en una carpeta y lo que yo quiero que después del algoritmo de que se ejecute Este programa con este algoritmo este programa lo que haga sea ponerme en distintas carpetas las imágenes clasificadas Sí bueno investiguen Esto se puede hacer muy fácilmente con uno de los métodos de os esta librería que usamos que justamente me permite guardar en determinadas carpetas y la acción de guardado la puedo hacer con civitud lo vamos a hacer más adelante pero aquella persona que quiere arriesgarse a empezar a investigar esto ahora puede hacerlo Bueno alterando un poco el código de predict una acotación importante para esto último que hemos dicho es que yo debería si voy a hacer esa clasificación quitarle la línea de putex y de rectángulo para que nos salga esa leyenda arriba de la imagen porque aquí el efecto que busco justamente es que las imágenes estén según la predicción que yo he hecho ubicadas en las carpetas que le corresponden bien hasta aquí todo lo que tiene que ver con esta clase Espero que les haya parecido muy interesante hemos aprendido muchas cosas nuevas y vamos a seguir aprendiendo las en la próxima clase así que hasta la próxima clase aquí termina esta clase los espero en la próxima clase nos vemos Titulo: Clase20 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/TnLbGAKQGyE  \\n 1772 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 20   Hola a todos Bienvenidos a la clase número 20 del curso de Inteligencia artificial de ifes seguimos en el módulo de learning seguimos en el módulo también de redes neuronales convolucionales y en el contexto de el uso de redes para entrenadas particularmente y hoy en la clase pasada usamos hielo para abordar un problema de clasificación de objetos en el día de hoy en esta clase número 20 Vamos a abordar un problema con hielo también de detección de objetos este tema lo vamos a abordar en Dos clases durante esta clase vamos a ver aquellas cuestiones que tienen que ver con el uso del Data set con el cual fue entrenado originalmente Yolo que es el coco de datasep que ya lo mencionamos antes vamos a ver toda la cantidad de opciones que tenemos a partir de eso y en la clase que viene vamos a usar con este mismo objetivo Yolo para detección de objetos pero con un Data set propio y vamos a entrenar a Yolo con nuestro propio como recién dijimos vamos a recorrer muchas variantes con iolo desde las más básicas como esta que está aquí por ejemplo vamos a ejecutarla donde vamos a ver el reconocimiento de objetos de una colección de imágenes y vamos a imprimir los resultados por pantalla en todos los casos lo que vimos es identificación de objetos que están encuadrados entre los 80 objetos que reconoce automáticamente Yolo gracias al dataset coco también vamos a ver algo un poco más avanzado como esto que está aquí en donde hemos hecho una detección de objetos a partir de un vídeo que tomamos como input y también lo vamos a hacer con streaming tomando como input la detección que podemos ver a partir de una imagen capturada con la cámara de nuestro propio computador Así que dicho Todo esto empecemos con cada uno de estos pasos que nos van a llevar a desarrollar cada una de estas variantes en siete programas diferentes dentro del mismo contexto del proyecto que tenemos que crear como Primer paso para empezar con todo esto así que vamos con eso Bueno como ya mencionamos antes Vamos a continuar trabajando con Yolo B8 solamente que en el caso de la clase anterior lo hicimos para un problema de clasificación y en este caso lo vamos a hacer para un problema de detección por lo tanto me vengo aquí al menú donde están las tasas que son tipo de actividades que se pueden hacer con hielo y voy a seleccionar la opción de texto bien al hacerlo ingreso a esta página y veo Cuáles son todos los modelos que existen para un problema de detección acuérdense que la n es de Nano que es el módulo modelo Perdón más chiquito y x es el modelo más grande sí acuérdense que el más chico Por lo general es más liviano pero menos preciso Aunque más veloz y obviamente el más grande siempre es el más voluminoso el más grande El más preciso pero siempre también el menos así que bueno vamos a elegir el Nano que es el más liviano y luego vamos a bajar el más grande bueno allí terminó la descarga habrán podido comprobar la enorme diferencia de tiempo que hay entre Descargar el modelo nado del modelo x y también lo vamos a poder contratar cuando vayamos ahora a la zona de descargas viendo el tamaño de archivo que tiene uno y otro Aquí están los dos archivos obviamente en la no tiene 6.382 kv y el otro tiene 133.160 grados Bueno creo que no hacen falta más pruebas para poder diferenciar uno de otro Así que vamos al siguiente paso aquí la sección de Tech a la cual he recurrido para la sección models y bajar estos modelos Tengo otras secciones también que las puedo ir descubriendo subida hacia abajo y llego a la sección predique que es un código que nos muestra muy básico para hacer una predicción muy básica también y es una de las primeras herramientas que vamos a usar para hacer nuestro primer programa justamente usando la posibilidad de detectar objetos a través de a continuación vamos a crear la carpeta para nuestro proyecto a la cual le vamos a poner detección creada la carpeta vamos a crear nuestro proyecto Recuerden que como siempre yo tengo el proyecto ya realizado y Por ende abierto con lo cual vengo acá y Busco al igual que como hicimos la clase pasada la carpeta detección que acabamos de crear y simplemente haciendo eso le doy clic y se crea el proyecto recuerden lo que dijimos la clase pasada también que voy a crear este proyecto en una nueva ventana para no cerrar la ventana del proyecto terminado que siempre lo tengo como referencia acá aparece un programa Main punto Pay que siempre crea en cada vez que crea un proyecto nuevo con lo cual este no voy a usar Este programa directamente así que lo voy a eliminar para así dar lugar a crear mi primer programa que le voy a llamar rama de python por supuesto detección como dijimos antes vamos a crear Este primer programa con el código que vimos recién en la página de ioloques un programa muy básico pero nos va a empezar a dar como una primera base para empezar a hacer detecciones de objetos a través de Yolo vamos entonces al sitio de Yolo copiamos este código y lo pegamos en nuestro primer programa como Bien bien aquí aparece esta librería ultralitiks marcándome como que no existe con lo cual recuerden lo que hicimos la clase pasada tengo que cargarla al proyecto a través de settings luego voy aquí a project detection y project interpreter genial más buscaba bueno y ya está instalada entonces allí nuestra carpeta con lo cual lo que tendríamos que hacer ahora es darle Ok y continuar y empezar a ejecutar Este programa que tenemos aquí aquí tenemos en principio el uso de nuestro modelo está usando el n acá el Nano nosotros vamos a usar obviamente las dos opciones para poder verlas si a lo largo de toda esta práctica esto que está aquí lo vamos a quitar Sí y luego vamos a predecir O sea la predicción con este modelo que estamos creando gracias a iolo con una imagen Que obviamente no va a ser esta Va a ser otra imagen que vamos a buscar nosotros qué tenemos que hacer en principio bajamos los dos modelos de Yolo el n y el X esos dos lo tenemos que poner dentro de la carpeta detección y además a través del campus virtual ustedes tienen unas carpetas que justamente vamos a usar en esta práctica algunas con imágenes y otras con videos todo eso también lo tenemos que guardar dentro de la carpeta de detección y como hicimos la clase pasada todo eso adentro a la vez de una carpeta que le vamos a poner datos vamos por ello acá tenemos los dos modelos de ídolo y lo vamos a llevar aquí a la carpeta de detección Bueno ahora vamos a crear acá una carpeta datos como dijimos antes donde vamos a colocar los datos que nos vamos a traer desde el campo virtual en el campo virtual tenemos estas tres carpetas también aparecen los archivos de hielo que siempre como dije antes los voy a poner a través del Campus Pero la idea que lo bajemos como lo bajamos recién Y de esa manera usted siempre van a poder bajar el que quieran más allá de que yo lo suba también en el campo virtual o no así que me llevo estos archivos a la carpeta detección y dentro de la carpeta de dirección la carpeta datos y ahí tengo las tres carpetas con material que vamos a usar a lo largo de esta clase cuando vuelvo a Paisa a mi proyecto veo que todos los cambios que hice por fuera desde el explorador de Windows también impactan obviamente aquí en explorador de Paisa veo los dos iolo el Nano y el x y veo también la carpeta datos que tiene adentro la subcarpitas imágenes vídeos y videos largos con la primera que vamos a empezar a trabajar es con imágenes es decir que vamos a abrir aquí y justamente aquí Veo todas las imágenes con las cuales vamos a empezar a trabajar los primeros programas de este proyecto la primera imagen con la cual vamos a trabajar es esta que está aquí arriba calle día lo vamos a hacer doble clic Y esto es importante para que ustedes puedan visualizar la imagen sobre la cual vamos a hacer la detección de objetos dado que en una primera instancia en los programas más básicos nos vamos a el resultado de esta detección sobre la misma imagen sino que la vamos a ver de modo texto en la sección de la terminal bien ya vamos a entender a qué me refiero pero esta es la imagen sobre la cual vamos a trabajar la cerramos vamos a reemplazar esta imagen que está aquí que es la que estaba colocada recuerdan cuando trajimos este código desde el sitio de Yolo por nuestra imagen calle día punto jpg con la ruta completa datos imágenes calledía punto recuerden cuando ponemos la ruta poner el punto adelante toda esa expresión tanto entender que todo este contenido está dentro de nuestra estructura de carpetas de nuestro proyecto si dentro de la carpeta detección bien ya está todo dado para programar nuestro programa Así que vamos a ello Antes que nada un tema importante que tiene que ver con este entorno de trabajo aquí arriba no siempre va a aparecer el nombre del archivo que queremos ejecutar fíjense que aquí aparece más porque este proyecto nació con un único programa llamado Main Entonces yo ejecuto Esto me va a decir que más no existe Es decir porque obviamente Porque yo lo estoy viendo que ejecute este archivo y este archivo no existe una manera muy práctica de hacer esto y no complicarse demasiado las cuestiones antes que poner el nombre concretamente el programa quiero ejecutar puedo poner la opción file Y eso se adapta al archivo que tengo abierto con eso siempre voy a estar diciéndole quiero ejecutar este archivo que tengo abierto en este momento bien lo ejecutamos un detalle no menor porque les va a pasar y no van a entender Qué pasa así que bueno está bueno que nos aparezca ahora para poder este alertarlos de ese problema y ver cómo solucionar vamos con nuestro programa Bueno fíjense lo que aparece aquí en principio obviamente como les comenté no me parece la imagen con las detecciones sobre marcadas sobre la propia imagen sino que aparece en la sección terminal que es esta zona de aquí abajo las referencias de modo texto me dice que ha detectado sobre la imagen calledía que tiene este 6 si esas dimensiones 11 personas 5 autos y cinco semáforos en este tiempo Bueno eso es importante para que ustedes entiendan que Con este código el tipo de información por ahora que vamos a recibir es esa vamos a pasar otros programas como dije antes que puedan hacer predicciones más avanzadas más logradas y visualmente más fáciles de Identificar y reconocer Bueno aquí le he hecho algunos cambios al programa poniéndole comentarios en castellano para exceptuar los comentarios que ya vienen de propio sitio de ídolo y lo que vamos a poner aquí en principio es un print de results Sub Zero punto boxing para que para que puedan ver la información que se generan con las detecciones Sí qué quiero decir con esto la información que vimos recién tiene que ver con una contabilidad de qué tipo de objetos y cuántos objetos de esos encontró 11 personas 5 autos y cinco semáforos la información que me va a dar ahora es mucho más completa porque me va a decir dónde están ubicados en el contexto de la imagen cada una de esos objetos que detectó sí así que vamos a ejecutar nuevamente el programa ahora con este agregado y miren lo que me aparece en principio de los boxes en formatos de tensor Cuántas apariciones de esta tengo en este caso tantas como objetos ha detectado 11 personas más cinco autos más cinco semáforos tenemos 21 objetos detectados y la cuenta de estos elementos que están aquí tensores que están aquí son justamente 21 Qué información tiene cada tensor en principio fíjense que tienen uno dos tres cuatro primeros elementos que son las coordenadas sí vértice superior izquierdo vértice inferior derecho lo que sigue es el nivel de confianza nivel de confianza es digamos Cuánta seguridad tiene el predictor en este caso de que ese objeto que ha detectado esa categoría la que corresponde qué categoría lo que sigue en continuación es el número de categoría Sí el 2 es un auto el 0 que es el ejemplo que sigue es una persona y el 9 es una señal de tránsito o una una de un semáforo sí esto después lo vamos a ver bien porque así bueno porque sé que el 2 es tal cosa Barceló tal Pero bueno estos son los códigos de Coco Sí de la taza de coco después lo tengan como referencia Pero bueno aquí ha detectado tres tipos de elementos y en cada caso me da repito las cuatro coordenadas el nivel de confianza aquí me dice que el nivel de confianza de la primera detección de objeto que es este un auto es del 90.97% y el valor con la clase es la clase número 2 que corresponde en el que sigue al detecta una persona porque es el valor cero Más allá de que lo pone con todos estos decimales en realidad el valor es un valor entero sí pero bueno lo expresa de esta manera por la característica del tensor y la confianza en ese caso es inferior es de 71.28 por ciento en el caso de lo siguiente que es una señal de tránsito una luz de tránsito que tiene un semáforo la confianza es aún inferior del 66% en la que sigue es inferior bueno fíjense que el nivel de confianza es totalmente variable respecto de cada una de las detecciones bueno Esto es importante porque más adelante nosotros vamos a empezar a usar esta información cuando querramos usar algunos complementos que tiene chu que todavía no lo han usado usamos la clase pasada ahora también lo vamos a volver a usar y que me permiten hacer algunas cuestiones como marcar las detecciones por mi cuenta y no que lo haga Yolo justamente con su modelo bien a continuación como dijimos antes vamos a crear otro programa para seguir avanzando en opciones que nos ofrece la detección de objetos con yolos con lo cual tengo aquí voy a crear otro python file detección 2 con el siguiente código que vamos a pegarlo y empezar a explicar nosotros recién lo que hicimos fue detectar una imagen escribiendo literalmente aquí el nombre de la imagen vamos a buscar hacer algo que sea más Dinámico que pueda tomarme todas y cada una de las imágenes de una carpeta y que yo no tenga que estar escribiendo una por una sino que el código solo lo vaya capturando ofreciéndoselo al predictor y que haga la predicción lo imprima y seguimos con la siguiente imagen hasta llegar a la última las imágenes van a ser de la carpeta imágenes que está dentro de datos Y son toda esta lista de imágenes que está aquí sí calle día calle noche electrónica esquí fútbol living mujer prerrogato 1 perro gato 2 y vegetales bien en todos los casos lo vamos a hacer con el modelo Nano nada más chico y el código lo voy a ejecutar haciendo uso de la librería Us que ya la hemos usado en este caso lo que voy a hacer es Establecer un folder pack que va a ser datos imágenes de base y luego lo que voy a hacer va a ser crear una variable y mgs de array que pretendo que a través de listir sobre folder path me traiga todo listado de archivos que hay dentro del folder es decir dentro de datos imágenes una vez que tenga acá en imgs Ni más ni menos que esta lista de archivos que está aquí la voy a recorrer y voy a ir haciendo predicciones con cada una de ellas sí es decir results de model Y en lugar de poner aquí un valor fijo voy a hacer una ruta a través de path joint sí a conocer que no me permite digamos combinar distintas partes para armar una única ruta en este caso el folder path que es datos imágenes y perdón img que es todas y cada uno de los nombres de archivos que están dentro de esa carpeta bueno aclarado esto pasamos a ejecutar Este programa acuérdense que con el file por lo que explicábamos recién y vamos a ver que ahora nos vamos a tener una predicción sino que vamos a tener tantas predicciones como archivos hay dentro de la carpeta imágenes bien Allí están todas levantamos un poquito aquí la terminal y vemos la primera que ya teníamos de antes 11 personas cinco autos cinco semáforos luego la siguiente en calle noche dos personas cinco autos bueno todas y cada una de las especificaciones de cada caso perro gato un gato un perro gato dos un gato un perro y en el caso de vegetales ha detectado tres zanahorias bien vamos entonces ahora a ir hacia un pasito más avanzado a través de propósitos que vamos a volcar en el siguiente programa detection 3 bien Vamos a crear entonces el archivo nuevo detección 3 como habíamos dicho recién como en el siguiente código Cuál es el propósito ahora vamos a hacer lo mismo que hicimos en detección 2 es decir detectar todas y cada una de las imágenes que están dentro de la carpeta imágenes pero me quiero guardar quiero guardar cada una de las predicciones en una imagen que esté alojada una carpeta para poder visualizar las Tantas veces como yo quiera y además y además verlas durante la ejecución del programa sí bien la primer parte de este programa que tenemos aquí es Exactamente igual a la detección 2 Sí todo esto que está aquí hasta el armado del source Exactamente igual Sí aquí el Surf no lo armamos se lo poníamos directamente hacia el sur sería esta parte de aquí el origen digamos de lo que va a predecir a través de resolución ahora lo vamos a hacer A través de model predict siempre con Yolo es decir que este source que yo antes ponía aquí directamente se lo daba al model ahora lo voy a poner dentro de una variable y lo voy a aplicar con model Pretty Modern Credit me ofrece lo siguiente en principio que le ponga el Surf es decir cuál es la imagen sobre la cual quiero hacer la predicción o la detección de objetos Sí pero existen cuatro parámetros más y muy importantes en principio 6 que puede ser falso lo que le digo es si quiero que me guarde la imagen con las detecciones en una carpeta o no true quiero que me las guarde con 05 es el índice de confianza yo esto lo puedo fijar es decir si hay una detección de objetos con una confianza inferior al 0,5% creo que lo ignore y si es superior o igual sí obviamente este 05 puede ser el valor que ustedes quieran si ustedes quieren poner algo que sea más exigente pondrán 07 siempre obviamente tienes un valor entre 0 y el uno por supuesto no Entonces esto en este caso le pongo es como un umbral digamos de confianza cero cinco por ciento luego viene 6 text que para que se puede poner en true and Falls para poder guardar las referencias de cada una de las detecciones en modo texto si con las coordenadas que lo que mostramos en el caso anterior que me figuraba a través de la terminal ya no lo voy a ver de ese modo sino que lo puedo recuperar a través de un archivo y verlo todas las veces que quiera porque ustedes saben que lo que está en la terminal lo ven una vez y después desaparece aquí va a quedar conservado al igual que las imágenes o sea que tras la imagen con la detección dibujada y aparte la información en modo texto y finalmente YouTube que puede ser true Falls le estoy indicando si quiero que durante la ejecución del programa me vaya mostrando las detecciones o no obviamente que tanto savetex y show pueden ser true Force Y ustedes con eso hacen lo que les parezca o resulte más conveniente y con nuestro Ford Pero puede tener el valor que ustedes también consideremos conveniente bueno la toda esta explicación no queda más que ahora ejecutarlo allí terminó lo que vamos a hacer ahora es ir a la carpeta donde guarda esta información para poder visualizarlo bien Aquí estoy en la carpeta de detección voy a encontrar ahora por primera vez por haber corrido de esta manera por primera vez una carpeta rams entro a rams y allí voy a encontrar de y dentro de Tech voy a encontrar predica que por primera vez se va a llamar predic a secas y luego va a ser previc 1 2 3 y así sucesivamente guardando cada una de las referencias de las predicciones Tantas veces como ellos ejecute Este programa u otros por supuesto No bien entramos a predict y que me encuentro aquí con las imágenes que recién fuimos visualizando con la computadora pero para poder con las predicciones también y verlas todas las veces que quiera Sí aquí tengo fíjense la primera de ellas con las señales de los semáforos Perdón las personas y los autos la segunda acá detecta una mochila persona Auto la tercera electrónicos el futbolista el living la mujer fíjense aquí el gato del perro el gato y el perro y aquí las zanahorias bien esto por un lado por otra parte tengo una carpeta labels que me guarda la información que yo les comentaba recién es decir un archivo por cada una de las imágenes pero con las referencias de todas y cada una de las coordenadas de cada una de las detecciones de cada imagen Es decir en este caso calle día tiene los 21 elementos que habíamos mencionado hoy que había detectado esta este algoritmo bien entonces tengo esta información dentro de predict las imágenes y dentro las referencias de las coordenadas de cada una de las esto que está aquí cuando me refiero a coordenadas vamos a tomar un elemento cualquiera vamos a agrandar un poquito esta imagen aquí por ejemplo esta trastic light sí un poquito más claro Sí esto tiene un vértice superior izquierdo y un vértice inferior derecho que está conformado por coordenadas Cómo le digo Cuál es la línea de marcaje de esto que llaman bonding Box Sí para que ya lo vayamos nombrando como se debe Sí con una una caja de recuadro del objeto Cómo se determina dónde empieza cómo son la forma de marcar ese cuadrado a través de este vértice y a través de este vértice es la manera de indicar cómo está conformado el dibujo que enmarca el objeto detectado para cerrar esta primer parte de la clase vamos a observar un pequeño detalle Qué modelo de Yolo utilicé yo para hacer la predicción en detección 3 a diferencia del usado en detección 2 detección 2 use el Nano Qué pasaría si yo cambiase Este modelo es si vamos a comentar esta línea y habilitamos la de arriba es decir qué pasa si uso el modelo Nano también en detección tres puntos lo primero que seguramente vamos a presumir Sin temor a equivocarnos es que va a ser más rápido el proceso pero Debería ser también menos preciso será menos preciso bueno acá hay una excelente forma de corroborarlo porque lo voy a correr ahora con este otro modelo y me va a quedar otra carpeta de predicciones y voy a poder comparar concretamente visualmente y a través de el archivo de labels las predicciones con el modelo de x y las predicciones con el modelo de Nano y ver cómo nos fue vamos a hacer eso bien terminamos de ejecutar el programa vamos a la carpeta detección de nuestro proyecto y allí tenemos la carpeta runs y dentro de runs detecto pero ahora dentro de como les dije antes ya no tengo una sola carpeta predic sino que tengo una nueva carpeta Play 2 en este caso Entonces vamos a ir a elegir una imagen por ejemplo esta perro y gato vamos a dejar abierta y vamos a volver un poquito hacia atrás vamos a ir a proyectos y vamos a elegir la misma imagen Bueno fíjense aquí comparando ambas imágenes es un ejemplo muy claro del nivel de predicción de un modelo de otro en el modelo x detecto El perro y el gato el modelo Nano solamente el perro es como que el gato estuviera fundido dentro del perro y no lo ha alcanzado a detectar vamos a ver otro ejemplo más Aquí tengo la imagen de la mujer fíjense que en el modelo Nano no detecta la cartera y en el modelo x si la detecta aquí estamos con calle día que es la primera imagen que usamos Bueno ya de un solo vistazo se dan cuenta ustedes que aquí la cantidad de personas que detecta en esta que usa el modelo x respecto a la que usa el modelo Nano Bueno hay una diferencia más que apreciable la cantidad de semáforos fíjense que es uno solo aquí son dos bueno está más que claro yo creo que no no habría que poder saber si la que está a la izquierda o a la derecha Cuál es en virtud de qué carpeta lo abrí Ya solo con el hecho de verla ya me da claramente una manifestación de Cuál es la que usó el modelo x y cuál es la que usa el mundo bueno con esta experiencia para poder comparar modelos y puedan entender mejor lo que es en términos de eficiencia un modelo y otro Recuerden que estamos en los extremos hemos elegido el menos preciso y el más preciso y el medio hay otra variante pero bueno es una buena idea para poder justamente verlo de la manera más gráfica posible las diferencias entre modelos decía que con este último tema estamos cerrando esta primer parte de la clase Así que nos vemos en la siguiente parte de esta clase aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte  Titulo: Clase20 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/6pNLOlOH7vg  \\n 1630 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella   Bueno Hola a todos nuevamente seguimos con la segunda parte de esta clase número 20 en la cual vamos a desarrollar cuatro programas más con cuatro niveles de avance o variantes diferentes de este proceso de detección de objetos a través de Yolo vamos a crear ahora el programa detección 4 venimos aquí como ya sabemos New python file detección 4 y tenemos una pequeña variante respecto de detección 3 aquí en detección 3 queríamos hacer detecciones recorriendo todas las imágenes de una carpeta que se llamaba imágenes lo que queremos hacer ahora es lo mismo pero recorriendo todos los vídeos de una carpeta que se llama videos por eso en detección 4 vemos que no cambia demasiado el código respecto de detección 3 porque la detección 3 lo que hacía recorrer con img en img es decir cada uno de los archivos de imágenes e iba haciendo la detección en cada una de las imágenes acá lo que voy a recorrer es una raíz de vídeos de cada uno de los vídeos que están dentro de la carpeta videos ahora una declaración necesaria aquí dentro de video fíjense que tengo vídeo uno corto y video uno video dos corto y video 2 porque es esto en la detección de objetos a través de hielo en un vídeo la detección se produce por secuencias que se llaman frames es algo así como si yo fuera haciendo capturas de pantalla de un video entre 15 y 30 por segundo eso genera un proceso mucho más complejo para la computadora y para los recursos de la computadora este proceso también se puede grabar en la medida que yo use el modelo x y no el modelo n es el modelo más pesado de iolo bueno Esto si compromete los recursos de la computadora puede ser que nos convenga más usar las versiones de vídeos más cortas porque acá lo importante es Ver el resultado de esta detección y no probarlo Sí o sí De punta a punta Y si tienen recursos disponibles pueden usar las versiones más largas Yo aquí voy a usar la versión más corta pensando justamente que la mayoría no va a tener recursos este digamos amplios para poder generar un proceso con un vídeo largo y usando como está aquí en este programa el modelo más grande el modelo x Entonces lo vamos a trabajar con los dos vídeos cortos  luego tengo la carpeta vídeos largos donde sí tengo un vídeo obviamente mucho más largo donde vamos a probar más adelante Este vídeo largo pero con el modelo más lonano que es el modelo más liviano lo concreto insisto para detección 4 vamos a recorrer esta lista de vídeos de la carpeta videos con las dos versiones cortas de video 1 y 2 Bueno ejecutamos entonces el programa y vemos los resultados bien allí terminó y se habrá dado cuenta que tardó a pesar de ser videos muy cortos de apenas segundos bueno tardado muchísimo tiempo ahora entonces vamos a ver los resultados de las detecciones de estos dos vídeos bien aquí estamos en la carpeta de detección de nuestro proyecto y aquí tenemos rands sabemos que tenemos detect y dentro detecte tenemos predica 2 que son las predicciones que hicimos con los programas anteriores Y ahora tenemos predict 3 que es la que hicimos con este último programa bien entramos a predic 3 y acá tenemos los vídeos con las predicciones de las detecciones sí bien fíjense que no dura nada el vídeo porque el vídeo tiene nada más que dos segundos y fíjese todo lo que tardó para un vídeo de dos segundos no se olviden que el vídeo este está acelerado perdón el video de mi clase está acelerado el proceso no duró ese ese tiempo duró mucho más y ustedes lo van a apreciar digo esto para que no se asuste cuando lo hagan ustedes en su casa Así que va a tardar mucho más que esa aceleración que yo le hice a la clase justamente para que no tarde tanto la clase pero es un vídeo de 2 segundos pero como trabajo con el modelo x de Yolo trabajando tarda mucho Perdón bien pero lo puedo recorrer manualmente Sí aquí como lo estoy haciendo y apreciar más lentamente cada una de las detecciones que ha ido marcando en cada frame en cada cuadro de este vídeo yo lo divide como les dije antes este video en varios frames y bueno va marcando en cada frame cada una de las detecciones lo mismo con el vídeo de los autos si yo lo corro normalmente dura dos segundos va a pasar rapidísimo pero lo puedo pasar también manualmente e ir viendo Caso por caso y apreciando cada una de las situaciones fíjense que en algunos casos detecta personas siempre hay personas obviamente solamente que en algunos casos por la sombra no las alcanza a detectar y cuando le da la luz de pleno las detecta ven acá tengo una persona y aquí otra persona bien y así hasta llegar al final qué más las levels fíjense que acá tengo una idea concreta Y esto es muy gráfico y lo pueden apreciar bien tengo el vídeo 1 vídeo 1 guión bajo 1 y Vamos hasta el video 1 guión bajo 61 es decir que este vídeo de dos segundos tuvo 61 frames piense por eso tarda tanto y demórate y en cada frame tengo la información de las coordenadas Y antes que nada del tipo de clase de cada uno de los objetos detectados en cada frame de ese video bien y en el caso del vídeo 2 generó algo parecido un poco más arranca de uno y 101 tiene más se ve que un poquito más largo el vídeo entonces logró más frames a lo largo del vídeo 2que a lo largo del vídeo bueno obviamente esta información después la Tendremos que recopilar en un sistema que la procese esto no es para ver entrar al archivo por archivo porque obviamente no va a ser muy muy sencillo manejar de ese modo pero sí esta información es muy valioso para que después yo haga un análisis a través de un sistema que pueda tomar es ese input sí Y procesarlo para sacar determinado tipo de conclusiones bien vamos ahora con el siguiente programa vamos a crear el detección cinco punto Pay con un código muy cortito que tiene solamente el propósito de mostrarles a ustedes una tercera opción de input para poder hacer una detección ya vemos la imagen ya hemos servido nos falta el streaming que con el caso del streaming bueno donde va o pusimos en su momento el nombre de la imagen o con su ruta y el nombre del video con su ruta en el caso de hacer un streaming se pone directamente 0 que no es cero lo que hay que poner sí o sí como es un streaming lo que va allí es el número de cámara tengan cuidado con esto siempre el 0 apunta a la situación de la cámara que ya tiene el sistema que ustedes estén usando parcés ahora si ustedes tienen una cámara además de la cámara que tiene el equipo de computación bueno y más también serán 0 1 2 Depende como estén conectadas como estén configuradas dentro del sistema de computación Pero insisto el cero no quiere decir cámara el 0 quiere decir la cámara cero si fuera otra cámara y sí Debería ser el número que corresponde bien en este caso yo voy a ejecutar esto y vamos a ver justamente Cómo puedo hacer una detección pero desde mi propia cámara habiendo terminado de ejecutar detección 5.y vuelvo a la carpeta de detección Para volver ir a rams volver a detect y ver una nueva carpeta de predicciones donde tengo bueno el vídeo que grabó por mi streaming si acá lo pasamos lentamente donde detecta el celular en algún momento lo confunde con un control remoto la botella bien y luego tengo también los labels que me generó una secuencia de 15 levels 15 frames en el espacio que yo grabé es streaming bien Ahora vamos a crear otro nuevo programa y Esto va a ser un poquito más complejo Que todo lo que hemos hecho hasta ahora detección 6 porque acá empezamos a usar civitud la idea ahora es poder tener nosotros un poco más el control de muchas de las cosas que manejo iolo con sus herramientas La idea es poder capturar nosotros los frames y lo nosotros mostrando de a uno nosotros guardando la información por cada captura o sea todo esto que hace automáticamente yo hacerlo nosotros a través de YouTube esto en realidad no no es algo un sin sentido porque uno puede decir Bueno si ya lo hace y yo lo Para qué lo voy a hacer aparte porque justamente ustedes saben que todas estas cuestiones que son tan mecánicas tan automáticas en algún momento no sirven y tenemos que abrir el código y trabajarlo un poco más nosotros bueno el civitud si me permite eso y me ofrece un límite mucho más allá de las automaticidades o las herramientas que tiene de manera predeterminada y olor así que bueno vamos a programar un poquito más ahora con si tú pero antes que nada ya que voy a usar civitud y no lo tengo instalado tengo que instalarlo con este proyecto Así que vamos como ustedes ya saben a file settings y acá Recuerden que si tú se accede con opencv- python bien Ahí está lo instalamos terminado de instalar le damos Ok y ahora sí vamos al código del programa lo primero que tenemos que hacer es importar ídolo vamos a crear y vamos a trabajar mejor dicho con el modelo iorano creamos el modelo con eso y vamos a apuntar al vídeo largo que decíamos que lo teníamos justamente para cuando usemos la versión Nano vamos a usar el vídeo en su versión larga ahora lo que tenemos que hacer es capturar Este vídeo el cual vamos a trabajar a través del método video capture de cb2 dentro del cual se pone el nombre de archivo ya que el nombre de archivo junto con la ruta lo puse dentro de una variable y después le mando la variable podría haberle mandado todo el pad con comillas y es exactamente lo mismo bueno crea un objeto cap la variable objeto cap en virtud de la captura de ese archivo con luego pregunto si ha sido la apertura de la captura perdón de ese vídeo normal no ha habido ningún inconveniente técnico Entonces si está todo bien continuo obviamente esta pregunta si no la hago y la medida que ande bien la cámara es necesaria Pero bueno es una manera de poder resguardarme y al tener algún problema técnico que después haga que el programa se clausure porque no están las condiciones previas dadas bien aquí Tenemos también algo nuevo sería leer el vídeo acá acuérdense que leer el vídeo lo va leyendo por secuencias por frame no lee todo el vídeo en un solo acto Por ende lo que devuelve ese caprit son dos parámetros primero el frame cada uno de los capturas de ese video y el saxes dice si esa captura fue positiva de nuevo es una variable de estado que dice si salió bien o mal bien luego lo que voy a hacer es trabajar ustedes Recuerda que cuando trabajamos la primera vez con las imágenes se veía muy grandes y no se alcanzaban a percibir bien hasta excedían probablemente el tamaño de la pantalla bueno en este caso eso lo vamos a evitar justamente haciendo un rezays del frame que obtengo de ese vídeo hace 40 por 480 como lo podría poner Haber puesto con cualquier otro otra dimensión no y a ese frame que lo formateo lo vuelvo a poner en la misma variable frame luego pregunto si saxe Qué quiere decir si la captura estaba esta variable de estado está bien esto como preguntar six true si está todo en orden si realmente tomó un frame de ese vídeo sin ningún tipo de inconveniente bueno fantástico entonces lo que hago es con model poder hacer una predicción una detección de objetos de ese frame y voy a agregarle algo que es interesante que es circunscribir esta detección a un solo de los 80 objetos que tiene Coco con lo cual esto lo hago con el parámetro clases y entre corchetes le pongo los números de todos los tipos de objetos que yo quiero que en este caso solamente personas con la cual le voy a poner el cero después lo vamos a variar con lo cual Esto va a detectar nada más que personas Sí bueno una vez que tengo la detección o las detecciones se resulta que pueden ser más una persona lo que hago es en una variable que le voy a llamar anotatitis frame voy a poner el resultado o los resultados que me dé en formato plot para que para que después con Ims Decide tú pueda mostrar la imagen con todas las anotaciones estos recuadros estos bondi box de cada uno de los objetos que detecta en este caso cada una de las personas que detecta sí Recuerden que este recuadro se llama bonding Box Sí bueno de esta manera yo lo que hago es garantizarme de que imprima por pantalla ambas cosas la imagen y el recuadro y con esto lo que hago es indicarle que el programa se va a detener cuando yo prima letra Q esto para no hacer clic en el botón de detener Sí entonces por qué hago esto porque quizás el vídeo lo quiera cortar antes Entonces les preocupa sino esto terminará cuando termina el vídeo finalmente con cap se libera la cámara Sí y con distroll Windows se cierran todas las ventanas que pudieran estar abiertas Sí bueno habiendo hecho toda esta larga aplicación vamos a pasar a ejecutar el programa ahí oprimí q simplemente ahora lo voy a poner el programa funcionar de nuevo Pero simplemente para que vean de que la idea es si yo quiero cortar antes del vídeo lo arrancó no Y esta vez no lo voy a detener voy a dejar que termine hasta el final bien como dije antes ahí terminó el vídeo solo y bueno algunas cosas importantes para ver aquí habrán podido comparar cuando grabamos la secuencia de detecciones en los vídeos cortos todo lo que tardó a pesar de que eran videos cortos habiendo usado el modelo X ahora tengo el vídeo más largo pero usando el modelo Nano fíjense que va mucho más rápido el río de hecho ha durado casi menos que los otros que eran más cortos Sí así que fíjense la enorme diferencia entre usar un modelo u otro sabiendo que con este modelo voy más rápido pero tengo una capacidad de detección menor que con el otro ustedes recordarán que cuando vinimos a esta línea Este programa habíamos hablado de que habíamos configurado Este programa para que solo detectara personas a través del parámetro classis ahora voy a anular esta línea y voy a activar esta otra que está aquí debajo que ya dejé preparada para que detecte tanto personas como mochilas a través del número 0 y 24 que son las que dentro del Data set coco identifican esos elementos y voy a ejecutar nuevamente el programa bueno corte el vídeo porque realmente no quería verlo de nuevo De punta a punta pero ya con lo poco que vieron pudieron comprobar que esta persona que iba de espalda en la medida que iba avanzando iba detectando su mochila cosa que no pasó antes porque estaba configurado nuestro programa para solamente identificar personas ahora puede detectar tanto personas como mochila y de hecho usted lo han podido comprobar Obviamente con esta Consigna pueden ustedes variar el tipo de objetos siempre que esté encuadrado dentro de los 80 objetos que detecta Coco y Mejor dicho que detecta hielo a través del entrenamiento con coco y el último los casos no pone el parámetro y va a detectar cualquiera de los 80 sin que se especifique Cuál es para terminar de cerrar las cuestiones vinculadas a este programa es importante decir que así como antes trabajé con imágenes y doy streaming en este programa también con Puedo trabajar con streaming y lo único que debería hacer es reemplazar como aquella oportunidad o algo similar a lo que hice en aquella oportunidad el nombre con la ruta del archivo por el número de cámara con que quiero generar el streaming con lo cual con ese simple cambio echamos ejecutar Este programa nuevamente para hacer las detecciones en este caso de persona y mochila bien pero hasta aquí hemos cambiado digamos esta este formato esta forma automática que tenía Yolo que lo que hacía era detectar y guardar siempre y cuando yo manejara los parámetros de hielo para que eso pasara Bueno aquí no lo tenemos aquí todo lo que hemos hecho es una detección online pero no ha quedado ningún registro ni de capturas de pantalla ni de labels con los txt que me dejan registro de cada una de las de los bonding box de las detecciones en cada uno de los frames eso también lo voy a tener que hacer A través de Y eso lo vamos a hacer en el último programa de esta clase que es el detección 7 creamos entonces un nuevo archivo de python detección con los propósitos ya mencionamos bueno el código no va a guardar grandes diferencias respecto de detección 6 detección 6 detectaba y mostraba aquí tengo que detectar Mostrar y guardar Por lo tanto el código va a ser muy similar y una de las grandes diferencias va a estar en esta parte en detección 6 yo hacía un m show de la imagen en detección 7 voy a tener que hacer Qué cosa el IMS y un método de civitud que se llama right va a ser el que me va a permitir a mí guardar cada captura de esas imágenes que antes solamente veía y ahora Quiero ver y guardar Qué elementos tengo que darle al imrite 2 básicamente primero la imagen Es decir anotatitis frame es esto que antes mostraba ahora lo quiero usar para guardarlo y un nombre más la ruta para el archivo donde va a ser guardado Cuál es el tema aquí que como yo voy a grabar varios frames no puedo estar poniendo un nombre manualmente Tengo que buscar una forma un mecanismo de que el nombre se vaya generando automáticamente entonces yo voy a usar en principio acá un path joint un Patch Para qué para poner en principio una ruta rams detect va a ser una carpeta que yo voy a crear específicamente para esto que he pensado ponerle XP coma y un nombre un archivo un nombre un archivo que va a terminar en jpg pero lo que esté a la izquierda del punto del jpg va a tener que ir variando para eso voy a usar una librería que se llama uid Y eso es una librería que tengo que importar cómo está importada aquí arriba va a ir cada vez que invoque al método uid 1 generando un nuevo nombre aleatoriamente con lo cual esto me va a generar un nombre y lo va a poner donde está este especie de comodín que son las dos llaves que significan eso con lo cual va a ser ese número aleatorio punto jpg otro nombre aleatorio punto jpg sí y así me garantizo que cada frame va a tener un nombre de archivo diferente hasta ahí estamos bien Ahora le agregamos algo más a título de detalle Este programa que es lo siguiente yo no sé si esa ruta de hecho sé que en primer orden esa ruta no existe rands no está ex entonces lo que voy a hacer es ahora trabajando con la librería o ese que también la importamos aquí lo siguiente si no existe ese paz que Paz Ram detect x que lo cree convertirse repasamos ahora si no existe créame esa ruta por lo contrario tomo la ruta creada Sí y lo que voy a hacer es pensando que este programa lo puedo ejecutar más una vez Eliminar todos los archivos que estén dentro de esa carpeta que sí existe se entiende entonces de esta manera lo vamos a hacer bien Dinámico Porque si existe la carpeta Borra el contenido si no existe la crea y dentro de esa carpeta los nombres de las los frames también van a ser dinámicos gracias a uu y d sí bien Vamos a ejecutar entonces Este programa y vamos a ver justamente el resultado en las carpetas de todo esto que estamos comentando bien obviamente por haber elegido el modelo de hielo más grande y haber elegido el vídeo más largo Esto va a durar muchísimo por eso lo corte con la salida sí de emergencia o abrupta que me ofrece esta instrucción aquí que como le dije antes cuando primo q puedo dar por terminado el programa pero bueno a partir de esto podemos ir a ver ahora qué cosa la carpeta ex ven que ha creado la carpeta ex aquí Así que vamos a ver Su contenido lo podemos hacer desde aquí mismo ven están cada uno de los frames en general Entonces ahora tengo algo parecido a lo que tenía automáticamente con hielo pero generado con civitud no he hecho aquí relativo con los labels sí se acuerdan con los archivos txt que me permitían guardar las coordenadas también lo podría hacer porque ustedes saben que justamente hay información justamente que me ofrece results se acuerdan result boxes que me daba todas las coordenadas Así que también puedo hacer así como un raid de una imagen puedo guardar también archivo de tipo texto y fíjense como algo muy importante lo que les mencionaba que hacía esta librería vamos a abrir un poquito hasta acá que los nombres de cada uno de los archivos son totalmente aleatorios no importa aquí el nombre lo que importa es que sean distintos para que no se pise con uno con el otro ni tenga que estar yo cargando los a mano lo cual sería literalmente imposible bien hasta aquí llegamos con la clase de hoy resumen de todo lo que hemos hecho hemos utilizado muchísimas de las variantes que tiene Yolo con el entrenamiento el Data set coco es decir todas las variantes que hemos utilizado han sido basadas siempre en la detección de los 80 objetos que tiene Coco y como Yolo está naturalmente o de manera predeterminada entrenado con coco hemos trabajado solamente con ese 80 objetos pero el tema ahora o el desafío ahora es Cómo podemos usar Yolo sin el dataser que puede ser uno de los dataset conocidos que hay hay muchos que ya vienen como Coco y se puede entrenar con ellos o bien con un dataset generado por nosotros Bueno es un tema importante que vamos a abordar desde la clase que viene así que hasta la clase que viene aquí termina esta clase los espero en la próxima clase nos vemos Titulo: Clase21 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/mdiQ8x2IB2w  \\n 1395 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 21    Hola a todos Bienvenidos a la clase número 21 del curso de Inteligencia artificial de ifes seguimos en el módulo de learning seguimos en el mundo de las redes neuronales convolucionales y esta clase es una continuidad lineal respecto de la clase anterior la número 20 dado que vamos a abordar el mismo tema bajo otro contexto  en la clase anterior habíamos abordado el tema de la detección de objetos y hemos utilizado aiolo Con el datasette coco que viene entrenado naturalmente esa red dijimos que en esta clase y vamos a pasar a otro contexto es decir utilizar la misma red iolo pero entrenándola en este caso en principio con un Data set que no es Coco hay muchos datos públicos que pueden ser utilizados para eso y yo puedo buscar por ejemplo un dataset que detecte el tipo de objetos que a mí me interesan detectar siempre y cuando los 80 objetos que corresponden a coco no se corresponden justamente con lo que yo intento detectar en mi proyecto esto a veces puede no darse puede ser que lo que yo intente detectar no esté en cuadrado en ningún Data sed público al que yo pueda recurrir con lo cual yo deberé Construir mi propio datación por lo tanto tenemos que ver estas dos situaciones en principio utilizar un dataset público que no sea coco Y en segundo orden el tema de cómo hacer y Cómo construir y utilizar mi propio vamos a empezar primero con la opción de usar un público diferente a Google bien existen muchos sitios públicos para poder bajar Data sets en este caso vamos a ver el sitio roboflow y aquí podemos ver una lista larga de datas disponibles y nosotros vamos a tomar este que está aquí el que tiene que ver con Los cascos de los trabajadores aquí tenemos bueno la descripción bueno está claro Cuál es el tipo de objeto que ayuda a detectar este conjunto de imágenes y aquí puedo ir justamente con este sitio público digamos este público a bajar bueno todo el conjunto de imágenes y sus anotaciones me voy a loguear esto me pide siempre puedo lograr con mi cuenta de Gmail o la cuenta que yo tenga Y bueno ya estoy aquí me pregunta qué tipo de formato quiero para lo que Quiero bajar en principio quiero que llegue en formato zip Y además que se corresponda con Yolo v8 que es el tipo de red que voy a entrenar con este conjunto de datos una vez que selecciono todo esto sí me dice justamente que me va a bajar las imágenes y las anotaciones y el yang ya vamos a ver de qué tratan esos nombres y bueno empieza a bajar la información aquí una vez que logró esto lo bajo en la zona de descarga de mi computadora bien luego vamos a ir a datos que está dentro de la carpeta de detección de nuestro proyecto y vamos a crear una carpeta que le vamos a poner trabajadores allí dentro vamos a colocar este zip que acabamos de bajar del sitio roboflow y lo pegamos aquí y luego lo vamos a descomprimir aquí mismo bien Vamos a borrar estos dos archivos que no necesitamos y también el archivo zip que tampoco necesitamos más porque ya hemos bajado toda la información que estaba allí bueno dentro de datos tenemos trabajadores dentro de trabajadores tenemos la carpetas train y test que acabamos de bajar cada una de ellas tiene imágenes y abierto aquí en dos solapas diferentes por un lado las imágenes y por otro lado las etiquetas Para que veamos lo siguiente vamos a abrir la imagen la primera de ellas vemos dos obreros con casco ahora voy a ir a abrir la etiqueta correspondiente fíjense que las los archivos se llaman igual el nombre de la etiqueta el nombre del archivo Bueno aquí tengo las cuatro coordenadas y las dos clases fíjense que en ambos la clase número 1 que la que corresponde al casco vamos a buscar otra imagen por ejemplo a ver Esta que está aquí la 4 donde hay todas las personas sin casco bueno Miren el nombre del archivo arriba no Ahora busco con el mismo nombre del archivo pero terminan txt en lugar de jpg la número 4 y veo que aquí tengo bueno Obviamente todos con clase 0 es la primer columna primer columna es la clase y luego de nuevo las cuatro coordenadas para cada una de las detecciones es decir primero la clase que detecta y después las cuatro coordenadas bien Podemos buscar alguna otra imagen a ver Esta que está aquí y las 23 son todas personas sin casco menos una esta persona que está aquí tiene casco y el resto no vamos a ver la etiqueta número 23 bien fíjense en la primera fila hay una detección con la clase 1 que es la única persona que está con casco y las otras son todas cero es decir personas sin casco y en todos los casos insisto las cuatro coordenadas para cada una de las detecciones lo mismo va a pasar en test voy a tener labels e imágenes Obviamente con la intención de que sea el conjunto de Test tengo las imágenes por un lado y las etiquetas por el otro con los mismos nombres en un lado terminado de jpg y el otro lado terminado con txt Vamos a entrar a la carpeta de trains y fíjense que tengo 5.269 imágenes Sí si voy a la de Test animalis tengo 1766 imágenes lo mismo las etiquetas 1766 etiquetas en el conjunto de con lo cual tengo una gran cantidad de imágenes y probablemente Ese Conjunto sea demasiado pesado para entrenar yo les dejo en el campus virtual un rar que tiene la misma estructura pero con una cantidad de imágenes mucho menor para que sea más fácil para el computador de ustedes soportar un entrenamiento con una menor cantidad de imágenes está más que claro que la media casa más imágenes de entrenamiento va a ser mejor pero bueno estamos en una estancia de aprendizaje con lo cual tenemos que tratar de ser prácticos aquí tenemos 990 en lugar de las 5000 que había para el conjunto de entrenamiento Bueno también las etiquetas 990 y en el caso de el conjunto de Test le hemos destinado 200 imágenes y sus correspondientes 200 etiquetas si pasamos de 5000 a 990 y de 1200 a 200 y con un conjunto insisto mucho más factible de manejar en un computador y aquí tenemos algunas imágenes en un conjunto de Val que la vamos a usar al final cuando ya tengamos la red entrenada para probar a ver si las predicciones son buenas o no tenemos imágenes y tenemos vídeos para probar con cualquiera de los dos elementos Aquí vamos a borrar el archivo zip que ya no nos sirve y vamos a llevarnos este archivo y alma que mencionamos antes al directorio principal y lo abrimos este archivo es un archivo que sirve para configurar el entrenamiento al futuro entrenamiento que voy a tener con Yolo y el Data c que acabo de bajar tiene referencias de dónde están los conjuntos de entrenamiento de test la cantidad de clases y los nombres de las clases aquí tenemos tres clases se llaman Head helmet y person y esta información que está aquí abajo bueno no hace falta con lo cual la podemos borrar ahora la información que tiene aquí respecto de dónde están los conjuntos de entrenamiento de Test no es la real Entonces la voy a ir a buscar ubicar bien dónde está mi información y aquí que tengo toda esta información lo que voy a hacer voy a copiar esta ruta que está aquí arriba botón derecho y le digo que voy a copiar esa dirección como texto y la voy a pegar en mi archivo Ian lo mismo voy a hacer ahora para el conjunto de Test mucho cuidado porque el conjunto de Test la palabra puede confundir si se asocia al Val del Llano no al test del yang sí fíjense que acá el test lo borro y nos quedamos solamente con tren igual a pesar de que en específico lo que yo tengo nombrado como test Sí pero no confundamos las palabras bien ya está configurado nuestro archivo Ian que insisto es el archivo que le va a indicar a Yolo Dónde están las imágenes con las cuales debe entrenarse con lo cual ahora voy a mi proyecto si el proyecto que dejamos la clase pasada el mismo proyecto y voy a entrenar a mi ídolo como escribiendo lo siguiente desde la terminal iolo el tipo de Tas que es tipo de tarea que quiero entrenar de detección bueno que quiero hacer aquí quiero hacer un entrenamiento de cinco épocas y le digo que la información respecto de dónde están las imágenes para entrenar la saque del archivo data.yam que acabamos de crear luego le digo que el modelo que voy a utilizar para entrenar va a ser el modelo Nano sí Y obviamente con que lo tengo aquí el modelo Nano ya lo bajamos sí la clase pasada luego la configuración de las imágenes tiene que ser 40 y un batch de 4 Esto es lo mismo que ya viene usando bueno así que empezamos a entrenar detenemos en el entrenamiento aquí en el vídeo específicamente un momentito porque quiero que observen lo siguiente fíjense la información que me da el proceso de entrenamiento respecto del tipo de optimizador que va a utilizar que es el Adam y la cifra vinculada a la tasa de aprendizaje esto es muy importante y lo van a ver siempre que se alargue un entrenamiento sea de ídolo de cualquier otra red de tipo pre entrenada y es importante que le presten atención a este dato básicamente porque es un concepto que abordamos muy claramente en las primeras clases introductorias de conceptos de redes neuronales bien aquí Finalizó el entrenamiento y hay información muy importante para que podamos mirar en principio la duración del entrenamiento ha sido de casi dos horas uno punto nueve nueve seis horas para solo 5 épocas Bueno me dice que tiene ellas y el ves que ya sabemos que eso lo genera cada entrenamiento con hielo y luego la información relativa a bueno la curva de pérdida digamos Sí y el nivel de score Sí el nivel de precisión fíjense en esta columna m ap50 tiene el score donde el score general que está en la fila all es de solo 065 pero también me genera el valor del score para cada una de las clases Es decir para la clase G el score es de 0,95 6 para la clase helmet es el casco es 095 7 y para la categoría personal es bajísimo es de 00 por eso es que el score general es solamente de 0 6 5 7 Cuál es la aplicación de esto si miramos el contenido de las etiquetas que tienen cada una de las imágenes vamos a ver qué hay Muy pocas personas que han sido etiquetadas Es decir de la clase persona de la clase número 2 Hay muy poca referencia de lo que son la detección de objetos en cada una de las imágenes que forman parte de este banco de datos bueno Esto no tiene una razón aparente digamos es parte de lo que ha hecho el autor cuando diseñó este conjunto porque está más enfocado en detectar si hay personas que tienen o no el casco más que identificar a las personas en sí mismo Sí entonces Esto hace que la predicción cuando sea de personas Va a ser mala pero cuando sea de personas que tienen o no casco va a ser muy bueno porque fíjense que aquí los score son de 0,95 o sea de 95,6% o 95,7% lo cual son valores muy buenos bien como recordarán cuando hicimos el entrenamiento y hielo para clasificación veíamos los resultados del entrenamiento en una carpeta que le llamaba train train 23 respecto sabía más de un entrenamiento aquí pasa Exactamente lo mismo tenemos la carpeta train con los waves con el modelo best PT y las pity y el resto de las referencias métricas que nos daban un poco el resultado de lo que había sido el entrenamiento en este caso de cinco épocas con el modelo iolo con el dataset de trabajadores bien si voy a la carpeta del explorador de Windows también puedo ver esta carpeta train y tengo bueno las imágenes que ya vimos de alguna manera en la clase que hablábamos de la clasificación del problema clasificación tengo el resur csv con las métricas de cada uno de los cinco airpods tengo también los resultados pero gráficos con las gráficas de los hacia abajo y precisión hacia arriba que es lo ideal tengo dentro de la carpeta Waze lo que dijimos recién el ves y el las y yo voy a tomar en esta oportunidad el best pity y me lo voy a llevar a la carpeta detección donde voy a pegarlo y lo voy a renombrar para poder reconocerlo bien desde mi proyecto le voy a poner yo lo veo 8 n custom con lo cual ahora lo puedo ver desde mi proyecto aquí está junto con los otros dos tenemos el n y el X que bajamos del sitio de Yolo y este que hemos creado nosotros y voy a crear un programa nuevo copiando y pegando el detección 7 creo el detección 8 muy sencillo porque son muy parecidos Así que no lo voy a escribir de nuevo directamente lo copiamos y lo pegamos le voy a cambiar aquí el nombre para que tenga una referencia más precisa es decir en lugar de decir con Yolo v8 le voy a poner con Yolo v8 que ha sido entrenado con el dataset de los trabajadores o de personal de construcción como quieras bueno Sí más cambios que tengo que hacer en este programa bueno cambiar el modelo ya no voy a usar y yo lo veo 8 ni x ni n que son los que baje digamos del sitio de hielo sino el que acabamos de crear y yo lo veo 8 n custom voy a cambiar el nombre de la ruta donde voy a poner los resultados de las detecciones para no usar el mismo que usamos en detección 7 que habíamos usado la carpeta ex ahora vamos a ponerle work entonces en cada lugar donde decía ex le voy a poner Word podría usar lo mismo por supuesto Pero la idea es dejar cada uno de estos ejercicios de manera independiente para poder visualizarlos bien bien Vamos a usar para prueba se acuerdan la carpeta Val que teníamos estos vídeos vamos a usar el primer los de los vídeos que se construcción 1 mp4 con lo cual voy a tener que cambiar aquí la ruta y sería datos trabajadores luego viene mal luego videos y construcción 1 punto y medio cuatro luego tenemos que ir hasta aquí para borrar esto que habíamos puesto para digamos de alguna manera filtrar solamente las clases cero que en su momento eran personas aquí la clase 0 representa otra cosa una persona sin casco bueno acá lo pongo para que detecte todos los tipos de clases y le cambio Aquí también que me quedo pendiente export y no me queda más nada más que ejecutar Este programa teniendo la precaución de cambiar Mine por Carrión file y ejecutarlo y voy a ver los resultados por pantalla Bueno si me detecta Los cascos no hay detección de personas sin casco aquí porque no las hay directamente en la imagen bueno la imagen son muy chiquitas es una es una afirmación de muy lejos Bueno aquí tiene más vídeos que los pueden usar hay cuatro vídeos más Perdón tres vídeos más y también puede bajar su propio vídeo Sí este filmar porque no usted es un video y literalmente lo que tienen que hacer es tomarlo en modo mp4 que es lo ideal y bueno lo usan directamente en este programa sin ningún tipo de inconveniente va a funcionar cualquier archivo de tipo mp4 insisto firmado por usted de manera casera o bajado de algún sitio público o no público también hay alternativas para ver de YouTube si después en algún momento lo vamos a ver pero bueno pueden utilizar vídeos que tengan una toma más cercana que en este caso no pero bueno Esto es importante Si la tomas lejana y los detecta bueno habla bien de del modelo Bueno lo que tenemos aquí entonces es ir a work y Ver el resultado de las detecciones grabo un frame como ya saben ustedes por cada parte del vídeo por cada captura del vídeo si voy al explorador de Windows también voy a la carpeta work y veo los frames con sus detecciones cuadro por cuadro de este vídeo que tomamos como ejemplo podemos abrir una imagen cualquiera y vemos las detecciones si voy pasando hacia atrás imagen por imagen bueno puedo ver como dije antes cada una de las capturas Recuerden que hace capturas por segundo fps frame per Second 15 o 30 depende la velocidad con que se desarrolla el modelo bien aquí tenemos como decíamos recién los vídeos que puedo cambiar pero también tengo las imágenes para probar las imágenes vamos a tomar como referencia el detección 3 y copiando y pegando vamos a crear el detección 9 se acuerda que en el detección 3 habíamos programado algo para que tomara todas las imágenes de una carpeta bien tomamos ese como referencia válida con lo cual lo primero que tengo que hacer es cambiar el modelo para usar nuestro modelo castomizado y luego cambiar obviamente la ruta ahora no va a ser datos barra imágenes sino que va a ser datos trabajadores imgs bien y eso es todo porque el resto del programa tal cual está me sirve perfectamente con mi nuevo modelo y con mi nueva carpeta de imágenes lo ejecuto acuérdense que las imágenes se ven grandes porque acá estoy usando las herramientas naturales de Yolo no la programación mía Pero después vamos a la carpeta de las detecciones y vamos a poder ver bien los resultados imagen por imagen que tengo predex 5 que es el último y Bueno tengo las imágenes que las abro de aquí directamente ahí tiene un casco detectado aquí no ha detectado una cabeza sin casco aquí detectó dos cascos aquí una persona sin casco y aquí dos personas con Bien también pueden ponerle más imágenes buscadas por ustedes dentro de esa herramienta bueno con esto terminamos la primera parte de la clase vamos a dejar para la segunda parte de la clase una experiencia muy interesante que cuál es Nosotros hemos entrenado aquí con solamente cinco épocas y nos ha llevado casi dos horas hay una forma que es utilizar el entorno colap que ya lo conocemos con la opción de gpu lo cual me va a permitir a mí no solamente acelerar el entrenamiento sino quizás en menor cantidad de tiempo poder entrenar con modelos más pesados y con más Sí así que bueno Ese es el tema que nos va a quedar para la segunda parte de esta clase aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte  Titulo: Clase21 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/1rbpnqTIXRk  \\n 1327 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella    Bueno estamos aquí en la segunda parte de esta clase dejamos el tema planteado al finalizar la primer parte de esta clase en donde decíamos que íbamos a buscar una manera de poder entrenar con mejores recursos nuestro modelo de hielo con el Data sed personalizado en principio Recuerden que lo que hicimos fue sólo cinco entrenamientos y nos llevó casi dos horas y recuerden también que habíamos reducido en la caseta original y la serie original tenía 5000 imágenes y nosotros lo bajamos a 990 O sea fíjense que a pesar de haber hecho ese esfuerzo con lo cual tenemos menos imágenes Obviamente el algoritmo va a entrenar de una peor manera tenemos aún así con esa esa ventaja que requiere menos imágenes una duración de casi dos horas existe una metodología para poder de alguna manera superar ese problema en principio básicamente si tenemos en cuenta que nosotros no tengamos una computadora con una gpu si tenemos una placa tenemos la suerte de tener una placa de ese tipo Obviamente el proceso va a ser mucho más rápido y quizás hasta con una mayor cantidad de imágenes pero en la mayoría de los casos nosotros no vamos a tener ese recurso y cola si nos ofrece la posibilidad de tener una gpu virtual esto yo ya lo expliqué cuando vimos este algunos conceptos de Machine learning pero no lo aplicamos aún y a que si lo vamos a empezar a aplicar por primera vez en donde vamos a crear un Notebook a partir del cual vamos a usarlo solamente para hacer el entrenamiento Así vamos a ir a cola a entrenar a nuestro modelo vamos a llevarnos los datos también a un Drive Vamos a hacer todo en ese entorno utilizando una gpu con lo cual lo vamos a hacer vamos a poder hacer Perdón mucho más rápido y una vez que lo tengamos vamos a llevarnos el modelo de nuevo el entorno del Paisa y lo vamos a usar como si lo hubiésemos entrenado así que bueno dicho Todo esto pasamos a desarrollar entonces nuestro modelo entrenándolo en un entorno de color Bueno lo primero que tenemos que hacer es llevar nuestro conjunto de datos el mismo que usamos en Paisa dentro de la carpeta trabajadores a una carpeta nuestra en el Google Drive en nuestro Google Drive Sí yo tengo una que le puse archivos luego Tenemos que trabajar sobre el archivo Ian y creamos un dataset colab.yang obviamente las rutas no son las mismas Más allá de que las clases y la cantidad de clases son las mismas hay que poner en principio la ruta del pas principal es en la que me lleva a mi carpeta archivos del drive y luego lo específico que me lleva a las imágenes para entrenamiento y a las imágenes para texto Sí así tal cual está escritorio bien Esto lo hago dentro del entorno de Windows Sí y después me lo llevo obviamente al cola así que bueno lo Configura acá lo guardo y me lo voy a llevar adentro de la carpeta trabajadores que ahora tengo dentro de la carpeta archivos en mi Drive en la carpeta de trabajadores que acabo de transferir a mi Drive ahora vamos a ir al notebook que vamos a utilizar que se llama clase 21 train y de 8 más trabajadores lo primero que hago lo que dijimos seleccionar la gpu con el entorno de ejecución y luego conectar bien ya estoy conectado con lo cual lo primero que hago es instalar yolo con pib install ultralitics acuérdense que la librería de hielo es ultralitiks de la importaba más Cuando usábamos el proyecto en luego voy a importar justamente para el código es una cosa es cargarla instalarla a la aplicación y otra esa poner la disposición del código bien ya está y lo que voy a hacer a continuación es conectar este Notebook con mi origen de datos del Google Drive entonces bueno esto ya lo hemos hecho antes me pide mi usuario de Gmail donde está el Drive que están mis archivos y le doy acceso y bueno si voy a la carpeta Ya los tengo que ver así bien ya está le doy refresh para que aparezcan Aquí está Drive my Drive archivos trabajadores y tengo las carpetas test y train dentro de Test tengo imágenes y etiquetas dentro de train lo mismo y finalmente el archivo ya castomizado para usarlo para el entrenamiento de cola bien Vamos a entrenar un modelo hielo Nano con el cual creamos nuestro model y luego vamos a entrenar el modelo como del punto Trail donde le pongo el archivo yang que Acuérdese que le da Referencia para que ubique Dónde están las imágenes para entrenar sería Drive my Drive archivos trabajadores y luego le ponemos un entrenamiento de 20 épocas el tamaño de imagen de entrada y batch 4 Bueno eso es lo mismo que yo hice en Y empezamos el entrenamiento observemos sí importante el optimizador y la información también de la tasa de aprendizaje bueno seguimos con el entrenamiento bueno termina el entrenamiento en 0 20 las horas están puestas en números decimales Sí nosotros son 20 minutos sí será la 20a parte de 60 minutos Luego las estadísticas al igual que las que vimos en tengo una precisión total de 65%, una precisión para cabeza sin casco 93%, una precisión para cabeza Conca con 96% y una precisión para la persona de 006 es parecido a lo que nos pasaba en ahora podemos probar ya que vimos que fue muy rápido más allá que el vídeo Está acelerado con un entrenamiento con el modelo x si ven el modelo x que tenemos aquí el vale con Nano con el modelo x con los mismos 20 de puigs pero con el modelo más cargado bueno largamos el entrenamiento bueno obviamente tardó media hora 05 horas es media hora significa media hora por la forma que manejas esa cifra bien aquí tenemos las estadísticas tenemos una precisión total mayor al caso anterior que lo hicimos con el Nano también en el caso del casco 94% en el caso de cabeza sin casco 94% el caso de casco 97% y en el caso de persona 000 lo que tenemos aquí no hemos visto es que todo este entrenamiento al igual que el caso de pfizer me genera la carpeta train dentro de detect y dentro de rams con Wake con vez y las y todo el resto de los archivos que ya ustedes conocen porque hemos hablado largamente y aquí vemos también en la carpeta que bueno obteníamos cuando entrenamos con los mismos archivos en el entorno de cola como el entorno desde mi computador por haberlo hecho en bien Vamos a descargarlos porque porque si no los descargo no se olviden que la sesión de cola termina y cuando termina estos archivos desaparecen Así que cada vez que yo hago un entrenamiento de este tipo Más allá de que lo necesito los archivos justamente para poder aplicarlos en mi aplicación Bueno tengo que bajarlos para resguardarlos lo vamos a bajar en nuestro disco en una carpeta que va a estar dentro de el marco de las otras carpetas que se generan automáticamente cuando el entrenamiento le acompaña bueno bajamos todos los archivos Bueno aquí los tengo en la zona de descarga donde lo acabo de descargar para la redundancia y voy a crear una carpeta dentro de tec junto con las que se generan automáticamente con Paisa como ustedes ya lo saben y le vamos a poner a esa carpeta traen cola 1 y ahí adentro vamos a pegar todos estos archivos y ya lo vamos a salvaguardar si se nos llega a cortar la sesión con cola me voy a llevar el best Pitt que es el que me interesa porque es el que genera el entrenamiento con cola a la carpeta principal del proyecto y así lo voy a pegar y lo voy a renombrar en este caso yo lo ve y yo lo veo 8 x custom a diferencia del anterior que había puesto n porque había utilizado el modelo Nano Sí vamos a poner custom cola diferenciarlo y un poquito más uno porque vamos a hacer otro entrenamiento a continuación con 50 eco Sí pero Primero lo primero que es esto bien Vamos a usar el mismo detección 8 que utilizamos antes con el modelo que entrenamos con paysan Pero ahora con el modelo que entrenábamos con cola Así que simplemente lo que tengo que hacer es cambiar este nombre aquí poniéndole el nombre de el archivo que generado con el entrenamiento en cola bueno podría poner otra carpeta Word pero voy a usar la misma ejecuto bueno así terminó la ejecución habiendo recorrido todo el vídeo el mismo vídeo que usamos en el caso anterior con Paisa y me voy a rams y a la carpeta que habíamos creado work y las situaciones Exactamente La misma que es lo que me pasó cuando usamos el modelo de python solamente que en este caso tenemos un modelo con más entrenamientos y con un modelo más pesado Por ende más preciso que el caso anterior donde teníamos solamente cinco airpods y con un modelo enano Sí pero ahí puedo ver como hice antes todas las detecciones en cada una de las capturas de los frames que detectó del video vamos a intentar hacer ahora una experiencia con el mismo modelo Yolo x Pero ahora con 50 entrenamientos en lugar de 20 como hicimos en el caso anterior esperando obviamente tener un mejor modelo que en el caso anterior bien ahí arrancamos el entrenamiento Entonces ahora con 50 Ecos el único cambio respecto de lo que hicimos este hace un rato bien ahí tenemos los 50 airpods acelerando el vídeo por supuesto y llegamos al 50 de 50 y vamos a ver las estadísticas ahora en principio la cantidad de tiempo fíjense que estamos más o menos en la misma línea del tiempo que tardó un poquito menos que lo que tardó solo por cinco airpods en patcher ya lo tengo 50 y con el modelo x antes era 5 con el modelo Sí hay una diferencia de tiempo más que apreciable por el uso de la gpu y las estadísticas son estas que están aquí donde tengo Bueno un mejor promedio 95 96 en el caso anterior vuelvo a tener la información pero ahora en un train 2 si la que fue producto del entrenamiento anterior quedó en la tren y ahora al igual que lo que hace con paisan al entorno de Windows Sí aquí también hace lo mismo en el entorno de cola crea otra carpeta traen dos producto de un segundo entrenamiento y toda la información está aquí y toda la información me la voy a volver a llevar al entorno de Windows descargándola como lo hice antes voy a crear una carpeta traen colap 2 para guardar también la información de estos archivos que me quiero guardar y tomo el ves y hago lo mismo que hice antes me voy a la carpeta principal lo pego y lo redondo tomando como referencia el nombre que use antes voy a usar algo muy parecido solamente que va a terminar en dos y ahora me voy de nuevo a detección 8 tengo los dos archivos ahora los dos colas y bueno lo que hago simplemente cambiar el 1 por el 2 y luego lo ejecutar ahora con un nuevo modelo con un mayor número de entrenamientos como ya vimos Bueno hasta aquí hemos visto todas las variantes que existen para poder entrenar entonces un modelo iolo con otro Data set que no es el original Coco con que viene pre entrenada está esta red pre entrenada valga la redundancia bien pero nos quedaba la otra alternativa Qué pasa si no quiero utilizar coco porque Ninguno de los objetos que detecta esa ese datasette con ese entrenamiento de hielo me sirven si entreno yo lo con otros tampoco me sirve porque ningún dato tiene las imágenes que yo necesito Necesito imágenes mías propias Bueno es un proceso muy complicado muy largo muy tedioso también pero justamente en el mundo del Deep learning en la tarea de etiquetado de las imágenes es una de las tareas más complejas que hay y más llevaderas por eso también vale mucho el poder utilizar digamos un datase que ya está hecho en algunos casos hasta porque no poder este sonar encelado son de pagos Mientras todos son gratuitos ese costo que yo estoy pagando porque el trabajo es muy complicado y el sacarme encima ese trabajo tiene un beneficio muy importante los que yo le voy a mostrar es Cómo es el proceso para poder generar a partir de una imagen que yo bajo del lugar que sea puedo bajarlo de Google como hicimos del pasado puedo sacarla donde quieras puedo generar las horas imágenes como decíamos recién cómo voy a hacer para hacer el etiquetado de cada una de esas imágenes obviamente nosotros nos vamos a entrenar una un modelo con esas imágenes Porque habría que tener por lo menos como vimos recién unas mil imágenes 900 imágenes para que tenga sentido y eso nos llevaría un trabajo muy largo Que obviamente escapa digamos al a los propósitos de este curso Sí pero vamos a ver el proceso y el proceso lo único que tiene que hacer es repetirlo Tantas veces como imágenes quieran o sea aquí no hay ningún secreto una vez que sabemos cómo hacerlo Bueno hay que hacerlo insisto repetidas veces hasta lograr la cantidad de imágenes que queremos mil veces supongamos sí así que lo vamos a hacer una sola vez sí pero bueno ya con eso van a tener elementos suficientes para darse cuenta de qué se trata esto y cómo hacerlo en el caso que ustedes quieran generar su propio lo primero que vamos a usar es este programa make sense en el cual nos va a ayudar a hacer esta tarea de etiquetado imágenes vamos a reducir esta imagen y vamos a tomar las imágenes que usamos hoy para validación como si fuesen bueno imágenes que estoy seleccionando para crear mi conjunto de datos recuerden estas imágenes de aquí que usamos hoy para poder este bueno hacerlas detecciones bien lo ponemos entonces aquí y lo primero que tenemos que hacer es iniciar un nuevo proyecto y una vez que cargó las imágenes voy a crear las etiquetas Sí que voy a crear en este caso solamente dos etiquetas voy a poner una para lo que sería este la cabeza sin casco Head y una segunda para el helmet es decir el casco obviamente aquí conforme yo vaya creando se van numerando será la primera vale cero y la segunda va a ser calificado como bien aquí lo que tengo que elegir es el tipo de forma de trazo con que voy a seleccionar la imagen voy a elegir el rectángulo y lo que hago es apretar el Mouse y dibujar un rectángulo cuadrado en la zona que quiero y le digo cuál de las dos etiquetas corresponde eso que marqué acá hago lo propio y digo esta no es una es un gel listo paso a la próxima imagen y acá solamente tengo una Head bueno la marco y cambio la etiqueta o la que corresponde no hay más cosas que marcar paso a la próxima imagen bien Qué pasa si me equivoco ahí hago algo por error Bueno lo elimino acá lo marco bien y le pongo chiquito vamos a marcar también y también es un gel bien me quedan dos imágenes más una Head y la última con dos helmets bueno Esto tengo que hacerlo para todas y cada una de las imágenes que yo quiero que formen parte de Mi gata supongamos Las mil que decíamos una vez que está hecho esto ya no hay más imágenes que marcar porque eran solamente cinco con la cual lo que voy a hacer es exportarlos anotaciones Son las etiquetaciones son las etiquetas En qué formato el formato zip para yolo lo descargue la zona de descargas y está es lo que lleva el escritorio así está al lado de la otra carpeta que tenía las imágenes bueno la voy a descomprimir de sipear aquí mismo bien y ahí tengo las famosas etiquetas vamos a poner una carpeta al lado de la otra borramos la y ahora tengo cinco archivos con las etiquetas de las detecciones de cada una de las cinco imágenes y voy a la carpeta que contiene las leyes que genere recién con el programa que Les acabo de mostrar y tengo fíjense una etiqueta igual igual igual a la que venía en el Data original tengo una cabeza sin casco es esa de ahí y las coordenadas de la cabeza con la segunda imagen me voy a la carpeta levanto la segunda etiqueta 1000 en 1 y tengo una sobre la cabeza sin casco con sus cuatro coordenadas una más construcción bien hay dos cabezas con casco bien y de esto se trata básicamente aquí lo que voy a tener que hacer es este ponerle a esta carpeta labels sí como como veníamos manejando íbamos con el Data set que bajamos sí público que bajamos Y a partir ya tengo mi carpeta de imágenes mi carpeta labels y la puedo volcar dentro de la carpeta de entrenamiento la train o la test tendría que distribuirlas con esta posibilidad de 80-20 Bueno ya tengo mi Data set con mis etiquetas y bueno puedo empezar a entrenarlo eso es toda la ciencia no es un tema complicado acá lo complicado es que hay que hacerlo muchas veces bien con esto terminamos esta clase número 21 Así que vamos a continuar la clase que viene con el tema de segmentación de objetos también con IVA hasta la clase aquí termina esta clase los espero en la próxima clase nos vemos  Titulo: Clase22 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/IrySbxp1lnA  \\n 2668 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 22 Hola a todos Esta es la clase número 22 del curso de Inteligencia artificial de ifes seguimos en el módulo de Deep learning seguimos en el módulo de redes neuronales convolucionales y hoy vamos a ver el tercer aspecto importante este tipo de redes nosotros vimos hace dos clases atrás el primer aspecto que es la clasificación de imágenes en la clase pasada vimos en las dos clases pasadas Perdón vimos la detección de objetos que es Otro aspecto importante de estas redes y hoy vamos a ver el tercer y muy importante aspecto que es la segmentación de objetos pero de qué hablamos cuando hablamos de segmentación de objetos bien recordemos lo que vimos en las últimas dos clases como recién lo mencionábamos vimos la detección de objetos allí no solamente identificábamos Qué tipo de objeto era el que estaba en una imagen en un video o en un streaming sino que además lo localizábamos en el marco de ese contexto cómo lo hacíamos con un bonding Box es un recuadro en el cual identificamos la posición de cada uno de esos objetos detectados con las referencias de las coordenadas del vértice superior izquierdo y del vértice inferior derecho bien pero ahí tenemos la imagen con un recuadro como decíamos recién lo que yo quiero ahora es avanzar un poquito más quiero tener la identificación concreta de la imagen como si la quisiese rebordear como si la quisiese contornear y quedarme específicamente con la imagen dentro de ese cuadrado y no con todo el cuadrado con la imagen adentro eso o de eso se trata la segmentación de de objetos como siempre para tratar de explicarlo Mejor vamos a recurrir a las imágenes Aquí vemos a algo de lo que vamos a llegar hoy con esta clase con esta primer parte de esta clase Perdón en donde bueno de esto se trata la segmentación de objetos fíjense que el objeto está enmarcado dentro del bonding box pero a su vez está sombreado como con una máscara concretamente la figura que importa que es la persona la cartera la mochila o el objeto del cual  no vamos a encontrar grandes diferencias respecto del código que vamos a escribir para tratar la segmentación de objetos respecto del proceso que hicimos para la detección de objetos con lo cual vamos a no solamente utilizar código que nos va a aparecer muy familiar sino además el proceso va a ser bastante similar porque Porque primero vamos a trabajar la segmentación Con el coco y lo que ya viene pre entrenado de la red Yolo para luego pasar a ver la misma red pero con un Data set que podemos bajar de un sitio público como lo hicimos la clase pasada y finalmente vamos a dejar un espacio para justamente tomar técnicas para generar nuestro propio Data set al igual que lo hicimos la clase pasada obteniendo si el etiquetado de cada una de las imágenes para detectar objetos Ahora les voy a enseñar al finalizar esta parte como etiquetar objetos para la segmentación de imágenes bien lo primero que vamos a hacer va a ser ir al campus virtual de ifes y vamos a encontrar todos estos archivos que están aquí y les voy a pedir que los bajen justamente allí y los pongan aquí en su computador en el lugar que ustedes consideren pertinente luego vamos a crear una carpeta que le vamos a poner segmentación Yo la quería en la raíz ustedes siempre la pueden crear donde les parezca mejor o más conveniente luego vamos a tomar estos archivos que estoy marcando aquí y me los voy a llevar justamente a esa carpeta que Acabo de crear recién aquí en segmentación pego esos archivos quiero destacar de manera muy especial este archivo que está aquí es el Yolo 8n es el modelo Nano ya no de detección sino de segmentación por eso se llama guión sg este archivo yo se los voy a pasar a través del campus virtual pero recuerden que siempre le sugiero que lo bajen ustedes directamente el sitio de hielo porque bueno obviamente Más allá de las facilidad de tenerlo a través del campo virtual es lo más ordenado lo más lógico que ustedes empiecen a bajar justamente todos estos pecetos estos archivos de estas redes para entrenadas desde el sitio donde los aloja ahora vamos a volver a la carpeta que tienen los archivos que bajamos del campo virtual y me voy a llevar este datos rar y lo voy a pegar dentro de mi carpeta segmentación una vez aquí voy a descomprimir y voy a alojar los archivos en esta misma carpeta y una vez que lo haya hecho me voy a sacar de encima la voy a borrar este errar porque ya no necesito dado que me ha creado esta carpeta datos que está allá arriba que tiene Bueno una serie de imágenes de videos y otras cosas que vamos a usar a lo largo de esta práctica ahora vamos a crear nuestro proyecto con file New project y lo vamos a hacer referenciando la carpeta que acabamos de crear segmentación sí como siempre cada vez que lo cree me va a preguntar porque detecta que la carpeta tiene archivos que son propios de un proyecto si puedo crear o si Quiero crear ese proyecto con esos archivos que están allí Quiero crear un proyecto nuevo bueno no todos esos archivos que están aquí ya me van a servir justamente para el proyecto por eso lo he hecho de este modo y respondo que sí que quiero crear un proyecto con la información existente y la pregunta de aquí que me dice siempre si quiero utilizar esta ventana o abrir una nueva bueno como tengo otro proyecto voy a abrir una nueva ventana y se crea el entorno virtual que esto siempre tarda un esta línea que está aquí me dice justamente que está creando el entorno virtual Bueno me sale un consejo como siempre esto lo leemos pero no hace falta que lo dejemos abierto y aquí ya tengo la estructura creada dentro del proyecto que Acabo de crear con todos los mismos archivos que yo tenía Ven aquí dentro de la carpeta del explorador de Windows sí acá están las carpetas que crean cuando se crea el entorno virtual la carpeta b y la carpeta punto idea bien obviamente estas no venían del campo virtual se crearon Cuando creé el proyecto volvemos al proyecto y vamos a empezar a trabajar con este archivo seguidolo 8 punto Pipe que lo que hace es justamente hacer una segmentación de imágenes de imágenes video y streaming y me encuentro que me falta la librería ultrality obviamente porque recién creó el proyecto y todavía no incorporé ninguna librería a este proyecto vamos a incorporarlo entonces yendo file settings como ya lo hemos hecho antes pero es exactamente y aquí con el signo más agregó la librería ultralitiks que es la que tiene como ya sabemos bien ahí ya la detecto y instalo el paquete termina la instalación le doy Ok y bueno como dijimos Entonces esta aplicación nos va a permitir segmentar imágenes desde presentar objetos Perdón desde una imagen desde un vídeo o desde la propia cámara que tenemos en nuestro computadores y un Stream para ello voy a crear un modelo justamente con el yo lo v8-6 que hablábamos hoy bien que está aquí porque lo trajimos del campo virtual más ya que yo le sugiero que lo bajen desde el sitio de iones y vamos a hacer una predicción sobre un vídeo el vídeo 1 punto mp4 que es uno de los recursos que ustedes bajaron del campus virtual dentro de la carpeta videos está este vídeo 1.94 luego le digo que quiero que grabe cada una de las segmentaciones con una confianza del 0 50% que me grabe también las etiquetas de los elementos que segmenta y que me vaya mostrando por pantalla cada una de estas segmentaciones Así que recuerden seleccionar ejecutó el programa y vamos a empezar a ver por pantalla el resultado de la ejecución del mismo justamente con las detecciones que van a ir apareciendo conforme vaya haciendo lo suyo Este modelo de ídolo están las detecciones está detectando personas en algunos casos sillas y carteras pero fíjense que como dijimos en la presentación no solamente ahora marca el recuadro del objeto que detecta sino que también los segmenta fíjense que todas las imágenes están sombreadas con un color parecido al del recuadro o sea el que tiene el recuadro verde lo colorea con un color verde y el que tiene el recuadro de color rojo lo colorea con el mismo color entonces aquí tengo una detección general con un cuadro y tengo una detección detallada y específica de la persona de la mochila de la Silla contorneando cada una de los bordes de el objeto detectado aquí aparece también este señor que trae una cartera Bueno creo que está claro Cuál es la idea Cuál es el proceso y cuál es el resultado que buscamos con este bien así terminó el programa Y como siempre guarda los resultados de la ejecución en rams segment en este caso no detecte porque obviamente la tarea es otra predica y también las etiquetas en runments esto lo veo como siempre yendo al explorador de Windows para buscar esa información que aquí me dice que ha guardado como producto de la ejecución de este programa Aquí tengo rans segmento t y predic igual que antes solamente que antes no estaba segmento sino detecte Aquí tengo el video con cada una de las segmentaciones detecciones Y segmentaciones sí obviamente vas bastante rápido el vídeo obviamente es cortito Lo que pasa que se ve lento en el proceso pero lo puedo manejar con el puntero del vídeo y verlo en detalle como aquí y luego el tema de las etiquetas recordemos Cómo era el etiquetado cuando hablábamos de detección de objetos dijimos que la detección de objetos se enmarca con un rectángulo con un cuadrado en donde tenía en principio la información de la clase detectada cero si era una persona o el número que fuere de acuerdo a los 80 objetos en principio de coco Data y luego tenía cuatro valores porque porque era la x y la y del vértice superior izquierdo y la x y la y del vértice inferior derecho fíjense en este caso la cantidad de números que tengo es totalmente diferente con lo cual uno dice Bueno pero cuántos números son bueno no hay una cantidad fija de números porque cada uno de estos números lo que me marca es un punto del contorneado de la imagen está bien con lo cual si yo tengo una persona Comparado con con una cartera la cantidad de puntos va a tener que ver con el objeto y va a tener que ver con el tamaño del objeto fíjense que la imagen hay objetos que están más más alejados y otros que son más cercanos con lo cual la cantidad de puntos es absolutamente relativa al objeto y no fija como antes que eran cuatro siempre Entonces es importante que visualicen esto aquí y fíjense si pueden ver en este primer caso lo primero que tengo es el cero porque lo que ha detectado es una persona y luego todo el conjunto de puntos que marcan el contorno en este caso de la persona pero fíjense algo muy importante la el contorneado de la este caso de la persona tiene que empezar a terminar en el mismo punto obviamente puede dar toda la vuelta circundando el objeto en este caso la persona por lo tanto si observan bien fíjense que este valor que está al final el último valor de todo este conjunto de valores es igual a este que está aquí y ese patrón se va a repetir en el resto de las imágenes que son etiquetadas en este acá podemos apreciar eso Por ejemplo esta señora bueno todos los puntos lo que va a hacer es circundar la figura de esa persona siempre teniendo obviamente la identificación del objeto y la el nivel de confianza de esa detección Pero bueno eso va a ser la etiqueta la información perdón de la etiqueta En referencia poder identificar con precisión el contorno de esa persona si vamos más abajo vemos otro conjunto de valores que tiene que ver con otra persona la misma imagen empieza con cero y el resto de los datos Aquí tengo otra fíjense que son personas en todos los casos pero fíjense que la cantidad de puntos es diferente Aquí tengo otro con más puntos que el anterior Aquí tengo bueno el de abajo de todo también tenía menos puntos Y como siempre la referencia entre el valor que empieza Sí el contorno y el que lo termina que tiene que ser en todos los casos coincidentes 0 4 5 5 6 y aquí este que está acá al final lo busco está aquí arriba otro caso 04667 y aquí termina dar toda la vuelta bien Esto hemos hecho con la predicción de un vídeo ahora lo vamos a hacer con una imagen imagen unos puntos jpg que está dentro del conjunto de elementos que yo les pasé a ustedes a través del Campus virtuales esto que está aquí no es esta imagen de un teclado una tasa y un cuaderno bueno esto ya está preparado por eso comento la línea anterior y activo esta otra que está aquí y lo pongo a ejecutar el programa Ahí está me detecta el cuaderno y el teclado obviamente esto aparece y desaparece rápido porque simplemente una imagen pero si voy a rams segment ahora tengo un predic 2 con la imagen que la puedo ver y mirarla bien hay un índice de confianza mayor en el teclado que en el libro si se ve bien Vamos a dejar la imagen al alcance de la mano para poder ponerlo al lado la etiqueta en este caso es una sola etiqueta porque no es un vídeo es decir aquí es una sola imagen el video son tantas como frames Y ustedes ya lo saben esto que va tomando como capturas de pantalla del vídeo en este caso es una sola pero detecta a los dos objetos Sí sí que tengo todo un conjunto de puntos que tienen que ver con la primer detección el objeto 66 todo el contorno de ese objeto y luego lo mismo pero que con el otro objeto detectado ahí está el 73 con todos los puntos que circundan y permiten segmentar esa imagen el proceso es el mismo insisto solamente que en el caso del vídeo tengo muchas capturas de pantalla en este caso es simplemente una imagen bien Ahora comentamos esta línea ejecutamos la de abajo que es la que hace el streaming si tomando mi propia imagen a través de la computadora bueno fíjense que lo que voy cambiando es el sur el resto Exactamente igual Sí el surfs pasó de vídeo a imagen y ahora pasa cero que acuérdense que Cell no quiere decir cámara sino que es el número de cámara que por lo general cuando tengo una sola cámara se llama Potter numerado reconocida como cero bien ahí me ha hecho a mí una segmentación luego una pelotita una bolsa Sport y ahora hace lo propio con una botella Recuerden que estamos en el modelo Nano no es el modelo de mayor precisión si es el modelo más rápido pero fíjense la máscara que me hace a mi persona este bueno y todos los objetos Es realmente muy detallada y ha hecho una detección perfecta bueno y los recursos como siempre los podemos ver dentro de rams segment en este caso aparece un predic 3 y aparecen todos los objetos que detectó y segmento por supuesto no hay una mayor seguridad hacia mi persona otro hacia la botella y una menor hacia la botella bueno Y aquí tienen todas las etiquetas aquí volvemos al caso de una imagen de tipo vídeo con lo cual no tengo una etiqueta sino tantas como capturas de pantalla se han producido a lo largo de este streaming y tengo de nuevo aquí la segmentación de la persona que lo primero que apareció no había otro objeto a la vista en la primer captura de pantalla por lo tanto Ahí está y bueno el resto de los archivos tendrán que ver justamente con el resto de Las capturas que han sido 154 en este caso Sí el tiempo que duró o lo que dio en relación a lo que duró ese streaming bien Obviamente que puedo seguir haciendo esto con todas estas imágenes que están aquí sí lo mismo con los vídeos puedo cambiar de videos video uno y vídeo 2 los otros todavía no lo vamos a usar lo que llaman dice mariposa tiene que ver con lo que sigue como práctica en este en esta clase pero vamos a usar imagen una imagen dos y video uno y video 2 y perdón imagen 1 2 y 3 y video uno y videos bien y dejo como pendiente el cambio de modelo el lugar de usar el hilo Nano para segmentación puedo usar el hielo x Pero ese no lo tengo sí aquí y enano si lo tengo en el proyecto sí no lo tengo fíjense Aquí está Solamente el Nano no está el X debería bajarlo debería bajarlo pero fíjense que curioso que si yo ejecuto ahora Este programa donde no está el Yolo x guión sg qué va a pasar bueno ejecutémoslo y vamos a ver inmediatamente qué pasa vamos a tomar el primer caso si El del video bueno ya vamos ahí tenemos la respuesta si el modelo no está lo baja va al gilcap de ultralitics y hace lo que nosotros tenemos que haber hecho sí no es lo ideal obviamente pero bueno es importante que lo sepan porque es algo práctico supongamos que yo me haya olvidado Bueno lo hace suerte Obviamente que esto lo hace la primera vez después ya lo van a ver que forma parte de la PT que están como el otro como el Nano que está dentro de la carpeta cementación y ya no lo voy a tener que instalar más después va a ejecutar justamente el vídeo 1 como lo hizo antes con el modelo Nano solamente Que obviamente en este caso van a ver una notable diferencia de velocidad va a ir mucho más lento más preciso también pero mucho más lento bueno primer objetivo cumplido segmentar imágenes con el modelo iolo natural que viene pre entrenado con los tipos de objetos de propios de El Coco lo que vamos a hacer ahora es entrenar aiolo con otro datase público yo les he pasado a través de del archivo digamos que bajamos del campus virtual que justamente lo vimos al principio de la clase una carpeta de un dataset público que tiene imágenes de mariposas y ardillas para segmentar cada uno de esos tipos de objetos bien lo que vamos a hacer también al igual que la clase pasada es entrenar aiolo con ese dataset Pero dentro del entorno de cola para poder hacerlo con un mayor nivel de epoch y mucho más rápido al igual que la clase pasada insisto una vez que tenemos eso volvemos con nuestro archivo al explorador de Windows lo ponemos dentro de la carpeta de nuestro proyecto y lo ejecutamos dentro del entorno de Paisa Bueno lo primero que vamos a hacer vamos a investigar los datos que ustedes tienen aquí porque han bajado y que son propios de te da hasta set público por el cual vamos a entrenar a nuestro ídolo están las imágenes dentro de esta carpeta de naturaleza vamos a entrar a ella y tenemos la carpeta de train para entrenamiento y la carpeta de test para evaluar o valorizar ese entrenamiento Bien dentro de train tengo imágenes y etiquetas entre imágenes y tengo bueno las imágenes de mariposas y luego de ardillas vemos la primera de ellas sería esa Y si voy ahora a labels tengo que tener un archivo que se llama Exactamente igual recuerdan esto que logramos la clase pasada solamente que en este caso la extensión no va a ser jpg porque no es una imagen sino txt porque se trata de el contenido de cada uno de los puntos que bordean a esa imagen pero recuerden que los nombres tienen que ser exactamente los mismos para que el entrenamiento funcione bien Vamos a la mariposa 10 bien y voy ahora Perdón aquí alebrijes y también tengo butter 10 puntos txt con cada uno de los puntos que bordean en este caso a la mariposa bien Este es el primer tema Obviamente que si yo voy a test también tengo la carpeta imágenes 122 imágenes y sus etiquetas 122 etiquetas no miramos la cantidad de imágenes que tengo en el conjunto de entrenamiento son 439 imágenes y obviamente 439 etiquetas bien qué es lo que voy a hacer me voy a llevar este conjunto de información con imagen y etiquetas esta carpeta naturaleza a mi entorno de Google Drive sí a mi propio Google Drive vamos con eso como ya les comenté en la clase pasada he creado una carpeta archivos ustedes pueden crear la que quieran dentro de su drive y lo que hago es arrastrar toda esta carpeta naturaleza dentro de la carpeta archivos vuelvo a la carpeta segmentador de nuestro proyecto y me tengo que traer un archivo más de los que baje de el campus virtual por lo tanto voy a la carpeta donde baje todos esos archivos y el archivo en concreto es dataset cola.yam Es que vengo aquí lo copio y lo pego aquí recordemos que era este archivo este archivo es el que le indica al entrenamiento donde están las carpetas con las imágenes y las etiquetas tanto del conjunto de entrenamiento como el conjunto de Test lo abrimos para recordarlo esto ya lo vimos la clase pasada esta Va a ser la ruta principal que termina en archivos porque Recuerden que mi carpeta se llama archivos todo lo previo digamos es lo que pone como ruta cuando yo voy a montar que ya lo vamos a ver este esta unidad de Drive con mi sesión de cola y luego bueno fantástico una vez que estoy dentro de archivos donde están las imágenes de entrenamiento dentro naturaleza train amazies y lo mismo para el entorno de validación naturaleza test y nation lo podemos corroborar aquí entro archivos naturaleza dijimos primero trains y después tengo naturaleza test que es lo que voy a usar para validar emails bien finalmente es importante que tengamos presente que este conjunto de datos tiene dos clases por eso aquí le digo NC dos puntos dos y los nombres de cada una de las clases sí ardilla y mariposa obviamente esto viene con este nombre lo puedo poner en castellano no hay ningún problema aquí lo que es importante que se llaman iguales que también se podrían llamar en castellano es las etiquetas con sus imágenes tienen que tener el mismo nombre con la extensión que corresponde a cada caso bien Así que terminado todo esto lo que tengo que hacer con este archivo que me traje a este entorno es llevármelo a la carpeta naturaleza voy a arrastrar esto aquí a la carpeta naturaleza bien aquí vemos el archivo esto de ponerlo dentro de la carpeta de naturaleza tiene que ver un poco con una cuestión de orden que estoy tomando como criterio en realidad puede estar en cualquier parte siempre y cuando sepa Cuál es la ruta para poder ubicarlo lo pongo dentro de la carpeta de naturaleza porque entendemos que en todas estas prácticas vamos a tener muchas carpetas que tienen que ver con distintos Data sets y lo ideal sería que yang de ese Data set esté dentro de la carpeta pero es una cuestión de orden no tiene nada de parte de un proceso que tiene que ser sí o sí de ese modo si lo anterior el tema de tener obviamente este conjunto siempre dividido entre 30 y 3 y dentro de cada uno de esos de esas carpetas subcarpetas emails a continuación lo que tengo que hacer es empezar el entrenamiento ya tengo todos los elementos para poder este manejar el entrenamiento y poder justamente generar un modelo desde colap y después usarlo en python el tema es que no tengo el programa para eso es uno de los archivos que yo le paso a través del campo virtual es este que está aquí clase 22 398 más naturaleza y pongan a esta este archivo dentro de la unidad de Drive que mejor les parezca o que mejor convenga al proyecto que está llevando adelante bien una vez que lo hicieron lo pueden abrir yo aquí ya lo tengo abierto sí tema principalísimo el tema de ver cerciorarse que esta esta sesión la vamos a llevar adelante con un entorno de tipo gpu de 4 gpu o gpu no con el modo cpu que van a tener siempre de manera predeterminada cada vez que hagan una sesión con cola Sí entonces sabiendo que tengo eso ya tengo yo ya seleccionado Esa esa modalidad y este Bueno ya está activa la sesión de trabajo vamos a empezar con lo que tiene el código ya cargado en este archivo que Les acabo de pasar bien lo primero es instalar yolo ultralities Mejor dicho que la librería que tiene yolo una vez que hago esto importo y oro ultralitics y luego lo que ya sabemos tenemos que conectar esta sesión con todo el entorno de datos que está nuestro Drive que hemos preparado justamente hace un rato lo hago montando la unidad de Drive a esta sesión de trabajo de Cola con este proceso que está aquí bien ya tengo entonces montado Sí mi Drive my Drive archivos se acuerdan lo que hablamos hoy cuando teníamos el archivo Ian Sí bueno toda esa ruta larga bueno tenía que ver con esto con content que siempre toda esta unidad se llama contet el contenido de los datos del entorno de datos que tiene cada sesión de cola luego Drive archivos y dentro de archivos sí tengo justamente todos los datos abro aquí y tengo mi conjunto de datos naturaleza con sus carpetas test y trae bien visto esto lo que tengo que hacer a continuación es crear mi modelo que lo voy a hacer con el modelo Nano de segmentación de Yolo bueno Y finalmente No tengo más que hacer que el entrenamiento es decir hago el entrenamiento donde le digo que el yang está movimos recién dentro de content Drive my Drive archivos naturaleza dataset cola.yam repasamos aquí content como todo Drive my Drive archivos naturaleza dentro naturaleza está aquí Data se cola cuando ya insisto es el que les dice dónde están las imágenes para entrenar es si el que le dice que venga a buscar estas imágenes a Drive mydrive archivos naturaleza test y train bien lo vamos a entrenar por vamos a cerrar aquí para ver un poquito mejor decía que lo vamos a entrenar por 20 épocas las imágenes tienen que ser 640 que es lo que siempre pide de manera predeterminada y olor y vamos a hacer con un bache size de cuatro bien ejecutamos bueno Y así terminó el entrenamiento el tiempo que tardó son más o menos en minutos acuérdense que esto lo expresa de manera decimal para ejecutar las 20 épocas ya tenemos nuestros modelos las Yves dentro de wave train segments y aquí tenemos bueno todas las métricas dentro de lo cual nosotros vamos a mirar como siempre prioritariamente la performance a través de map 50 donde vemos que para el total del modelo es 0983 muy bueno para el caso de la ardilla es un poco menor y para el caso de las mariposas es un poco superior bien no obstante son muy buenos indicadores y lo que vamos a hacer como dice aquí abajo ir a buscar los resultados de este entrenamiento a rams segment train 2 donde está dentro del entorno de cola vamos a buscarlo aquí rams segment train 2 y aquí tengo el ves y el Last dentro de Waze y algunas bueno de los archivos que se emiten siempre o que genera siempre ídolo después del entrenamiento con lo cual podemos pasarnos alguno de los que son más importantes o los que más nos interesan a nosotros que son resultang y obviamente el best pity que es el que me trae el modelo Así que voy a empezar por descargar este bien Aquí los tengo sí en principio tengo result que tiene todos los datos de cada una de las epoch y veo cómo va avanzando cada una de las métricas que vimos recién y después que el más fácil de visualizarlo y más importante las curvas las curvas de pérdida de precisión vemos Que todas las curvas de pérdida van hacia abajo como es lo que se desea y la de precisión van hacia arriba por eso ha llegado como esta que me está mostrando aquí a un valor importante fíjense que en este caso el Last tiene un poquito menor que el best o sea las un poquito más abajo y el best es un poquito más arriba por eso tomé el ves y no bien vamos ahora entonces a renombrar este archivo justamente ves con el nombre apropiado para el ejemplo que estamos llevando adelante vamos a renombrarlo parecido al original el original se llama iolo de 8 n y 11 y castón cola le vamos a poner por si algún momento alguien quiere hacer un custom desde el mismo país que lo pueden hacer nosotros en esta clase No lo hicimos el anterior Sí porque ya sabemos concretamente que haciéndolo en cola es más rápido bien Ahora me voy justamente el entorno de ya tengo aquí mi modelo y ese modelo aparece aquí arriba con lo cual ahora voy a ejecutar Este programa seguido los 8 custom Pay con mi modelo que Acabo de crear que se llama a ver si estamos bien Sí yo lo veo 8 n- se custom collage se casan cola punto y lo hacemos con la primera el primer vídeo de mariposa sí acuérdense que aquí en datos teníamos 12 vídeos de mariposas uno y dos vamos a usar el vídeo número 1 seguimos buscando que muestre todas las este las inferencias que va haciendo que las guarde también y con una confianza de 05 y bueno savecrop no lo vamos a usar que sería el recorte de la de la cementación hace como una máscara cual yo puedo llegar a recortarla este y no vamos a guardar en este caso las etiquetas Aunque lo podemos hacer Entonces vamos a ejecutar esto que está aquí siempre teniendo la idea de verificar que diga en file para que ejecute el archivo que yo quiero y allá vamos bien y ahí tenemos vamos a ampliar un poco esta imagen Bueno ya de por sí en el vídeo la imagen de la mariposa está muy de costado Bueno ahí lo vemos desde que es muy buena la segmentación con el modelo que acabamos de crear vamos a detenerlo y vamos a ejecutar otro que yo les dejo aquí todos los las líneas ya escritas pero les dejo comentado para que no se ejecuten todas juntas que la puedan ustedes y ejecutando una por una ahora vamos a hacerlo propio pero con un vídeo de una ardilla ejecutamos bueno Y aquí tenemos fíjese también es muy buena la segmentación de la ardilla bien qué más tenemos una imagen de una mariposa ya no videos imágenes obviamente la imagen desaparece rápidamente voy a hacer lo propio con la imagen de una ardilla y después vamos a ir a verlo donde están los resultados como ustedes saben en la carpeta de explorador de Windows y finalmente que no lo vamos a ejecutar usar una cámara cuando usen una cámara lo que le conviene obviamente porque usted no soñó la mariposa en ardilla Bueno lo más lo que se llama mucho en estos casos o poner una foto frente a la cámara o bien con el celular o una tablet mostrar una imagen para que vean justamente si la está segmentando bien si la está este marcando bien con la máscara propia de la segmentación o no Bueno y para cerrar esta primer parte de esta clase lo que vamos a ver es tal cual vimos la clase pasada como configurar o como preparar un Data set propio Sí en la clase pasada vimos que usamos un programa que nos permitía bueno recuadrar las imágenes nuestras para poder ir identificando las coordenadas del bonding box y del recuadro que marcaba el objeto que nosotros queríamos detectar y la etiquetación de cada uno de ellos y eso después hacía de que aparecía la imagen Con su respectiva etiqueta no solamente con la identificación del tipo de objeto sino también con las coordenadas las cuatro coordenadas que marcaban el bonding box sabemos que en este caso eso no es así hay otro tipo de técnica que bueno requiere más coordenadas que marquen o recorten la figura por eso vamos a usar un programa que se llama leibel me con Level me va a ser algo parecido lo que hicimos la clase pasada pero ahora no para detección de objetos sino para segmentación de objetos Sí así que vamos a empezar rápidamente con eso para justamente al igual que en la clase pasada ver cómo tengo mi propio Data set para detección ahora armo mi propio Data para segmentación con mis imágenes vamos con ello Bueno ahí Estamos instalando Level meam Level me es un programa que me va a permitir etiquetar las imágenes cuando justamente yo quiera tener mi propio dataset público sino mi propio ataque con mis propias imágenes Recuerden que esto lo vimos la clase pasada también para el caso de etiquetar imágenes para la detección de objetos bueno ustedes saben que ahora el etiquetado para la segmentación es muy diferente del del caso de la detección con lo cual ahora vamos a ver cómo es el proceso para etiquetar imágenes que voy a utilizar para entrenar una red que tiene como objetivo la segmentación de imágenes bien ahí ya instale entonces Label y ahora voy a abrir Este programa lo que hago escribiendo directamente lever me y dándole enter siempre desde la entorno de la terminal bien Aquí está el programa Vamos a darle opendir para que me abra directamente la carpeta esta que les he dejado aquí para para probar Esto bueno qué es lo que tengo que hacer aquí tengo que hacer un acercamiento pero no con un cuadrado como hicimos la clase pasada sino con polígonos Por eso voy aquí a crear polígono yo lo voy a hacer muy rápido no del todo prolijo pero para que se entienda la idea obviamente porque esto para hacerlo muy bien lleva mucho tiempo mucho más tiempo que lo que vemos la clase pasada marcar un objeto para la detección de justamente objetos bien marco aquí y lo que tengo que hacer es cercar o sea cada uno de estos puntos son las referencias numéricas que usted veían en el archivo de etiquetas Sí es decir que tengo que ir marcando todo el contorno lo que tengo que hacer ahora es continuar con la next image sí como dice aquí cuando voy a la próxima vez que me dice querés guardar ya este los datos de la imagen cercada Sí y la etiqueta sí quiero que lo guarde lo guardo en la carpeta y paso a la próxima imagen y hago Exactamente lo mismo bien si hubiese otro nombre porque estoy con más de un tipo de elemento que estoy tratando de cementar pondría el nombre que sea y ya me lo quedaría digamos dentro de distintos etiquetas en este caso la única etiqueta que he creado al momento es mariposa con lo cual es a la que me ofrece y esa es la que voy a tomar okay paso a la próxima de nuevo me pregunta si quiero guardar le digo que sí guardo Y esta es la última porque son solamente tres imágenes que estaban en esa carpeta que pusimos para que podamos probar esto bueno Y acá guardo porque es la última no hay más imágenes cierro Level me y me voy al explorador de Windows a la carpeta datos entonces aquí tengo las imágenes cercadas y tengo los archivos con la información relativa a cada uno de los puntos con que he marcado cada una de estas imágenes Sí el tema es que leibel me presenta estos datos en un formato Jason Sí para que el que no conoce es uno de los formatos estándares que sirven para manejar cierto tipo de información de tipo texto sí obviamente este formato que puede ser apropiado para otras redes no lo es para Yolo yo lo trabaja con archivo de tipo Jason para poder identificar las etiquetas de un objeto sino con el formato que ya conocemos y ya vimos de tipo este quiste primero la clase y después todo el resto de los datos pero siempre hay una solución para todo y es que justamente hay una aplicación que sirve justamente para convertir un archivo Jason en Tequila volvemos a la terminal de nuestro proyecto y vamos a escribir lo siguiente pip install Level me to iolo es decir el producto que saque el programa que instalamos recién para etiquetar Level me to- hacia yo lo convertiré instalamos Este programa y ahora lo vamos a utilizar para convertir las imágenes Jason que generó Level Meet en un formato propio de hielo escribo levemente tuyolo le digo que voy a convertir en Jason todo lo que está dentro del directorio y cuál es el directorio datos barra Level me imagins que es justamente datos donde están estas imágenes y dónde están las etiquetas pero en formato de Jason que quiero convertir en formato de Yolo bueno el resultado me lo pone como aquí dice el mensaje final dentro de esta carpeta yo lo Data set y tengo las imágenes que ya tenía de antes lo separan tres y entra en eso después ustedes lo pueden poner una sola carpeta o no depende lo que quieran hacer y las leyes también las separa en tren y vale Bueno pero concretamente vamos a tomar una de ellas y fíjense a convertido a lo que naturalmente estaba en formato Jason en un formato txt que entiende Yolo a partir de esto tengo mis imágenes tal cual las tenía cuando las baje o la saqué de donde sea y ahora tengo las etiquetas que estaban en Jason y eran también txt con lo cual al igual que como terminamos la clase pasada tenemos nuestra carpeta con nuestras imágenes con formato jpg y nuestra carpeta con notas etiquetas con formato txt resguardando que ambas se llaman iguales Más allá de la extensión cuando tengo eso tengo lo mismo que tenía cuando use el dataset naturaleza que usamos en esta clase Y a partir de ahí puedo entrenar mi ídolo o la red que fuere si en este caso yo la por supuesto por eso lo he probado con mi propiedad bien hasta acá llegamos con esta primera parte de la clase nos vemos en la siguiente parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte  Titulo: Clase22 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/Tbd-YAcInOI  \\n 2302 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella  Bueno aquí estamos en la segunda parte de la clase número 22 Vamos a abordar un tema bastante diferente del que veníamos trabajando porque ya trabajamos en tres aspectos centrales de lo que es la visión computacional es decir lo que es el Deep learning y las redes neuronales convolucionales para la clasificación de objetos para la detección de objetos y para la segmentación de objetos que los que vimos recién en la primer parte vamos a empezar a ver otro tema que vamos a iniciarlo en la segunda parte esta clase lo vamos a continuar en las clases siguientes básicamente tiene que ver con el uso de una librería que me permite a mí identificar determinados puntos del cuerpo en principio y en esta parte vamos a ver aquello que tiene que ver con la identificación de puntos de la cara luego Ver puntos de la mano en la clase siguiente y luego puntos del cuerpo completo bien la idea no es solamente usar esta librería y ver cómo funciona y ver cómo nos da la información de las detecciones sino también hacer alguna suerte de ejercicio o caso práctico con cada uno de ellos para ver bueno Cuál podría ser el sentido de uso de esta información es decir detectó y después bueno en qué puedo usar eso Obviamente que es un ejemplo y pueden haber muchos más nosotros vamos a tomar uno para que sea como una base Pero evidentemente si ustedes buscan información hay muchísimos ejemplos del uso de esta librería así que bueno sin más que decir vamos a introducirnos justamente al uso del medio Pipe que es la librería que vamos a usar para todos estos propósitos que recién mencionamos empecemos con ello como siempre vamos a recurrir a los archivos que yo les dejo a través del campus virtual que en este caso son estos tres punto Pay más una carpeta de datos como hacemos habitualmente para justamente tener imágenes de y videos o imágenes de fotos para poder probar justamente cada una de las prácticas que hacemos bueno estos serían los archivos que me tengo que llevar y vamos a crear como si hacemos habitualmente desde que empezamos con paichard una carpeta dentro de la cual vamos a poner estos archivos la carpeta Yo la he creado la unidad c y le he puesto de nombre Face mesh sí como está aquí feliz mesh entro allí y encuentro bueno los archivos que recién decíamos bajamos del campo virtual más otros que son producto de que yo ya he creado el proyecto de paicha con lo cual Bueno vamos a tratar de ir ganando algunos tiempos para no repetir cosas que ya enseñamos Y ustedes ya saben por lo tanto ya aquí ya tengo la carpeta creada con los archivos que me traje del campo virtual y con el proyecto nuevo de paichan creado justamente una vez que cree la carpeta puse los archivos adentro recuerdan como hacemos siempre Bueno hacemos primero eso y después vamos a paichar y le decimos que queremos un nuevo proyecto y le decimos que ese nuevo proyecto lo vamos a basar sobre una carpeta que ya está creada y ya tiene información dentro con lo cual siempre nos hace la pregunta si queremos crear un proyecto usando esa información que ya está en esa creada y le decimos que sí y ya tenemos el proyecto creado con lo cual vamos a adelantar esos pasos para bueno justamente tratar de aprovechar mejor la clase y ahora vamos directamente al entorno de paisan con el proyecto ya creado Y estos archivos dentro de él Bueno aquí estamos como dijimos recién ya dentro del proyecto con bueno el Face ya creado con todas las características que recién dijimos aquí tenemos los tres programas y bueno la carpeta datos con las imágenes y los vídeos con los cuales vamos a probar cada uno de los programas que vamos a hacer en principio la idea central va a ser hacer un Face mesh un reconocimiento de puntos del rostro a través de una imagen Sí una foto luego lo vamos a hacer dos variantes que tenemos justamente con el tema de los vídeos en principio solamente la detección y en el último programa es vídeo 2.y vamos a hacer un caso práctico justamente para ver en qué podría utilizar yo la información de la detección de los puntos de la cara pero ante todo lo que tenemos que saber es que obviamente vamos a trabajar con dos librerías una que ya conocemos y otra que vamos a empezar a trabajar como dijimos en la presentación de esta clase para poder hacer la identificación de distintos puntos del cuerpo en este caso del rostro que se llama media Pipe con lo cual como ya sabemos habiendo creado el proyecto tenemos que incorporar esas dos librerías yendo a file settings sí Y aquí obviamente yo ya las tengo incorporadas estas dos librerías ya sabemos que es opencv- python una de ellas sí la que identifica así y media Pipe la otra que incorporamos hoy por primera vez para los propósitos que vamos a ver de ahora en más bien estas dos librerías ya saben cómo incorporarlas yo le doy por hecho y vamos a continuar con la clase entrando justamente como base de el primer programa que es imagen punto Pay que el propósito Es este que está escrito aquí arriba que es identificar los puntos de la cara desde una imagen Sí de una foto antes que nada vamos a describir un poquito Cuál es la característica de seis meses Face mesh detecta Ni más ni menos que como dice aquí este comentario que les he dejado 468 puntos de la cara muchísimo no es la única librería hay otras librerías que detectan también puntos pero una muy menor cantidad sí es decir que estamos hablando de 64 puntos contra 468 sí bien Obviamente que en el momento que lo que estoy escribiendo Tiene que ver con el momento de esta clase probablemente en algún tiempo más haya otras niveles hoy existan otras de las cuales no estamos todavía informados porque la evolución de la guía Bueno nos lleva a esa realidad lo concreto es que lo que vamos a usar aquí el mediapite que es lo que he escrito aquí creo como siempre un alias en este caso para media y empiezo en principio voy a crear dos variables de objeto con instancias de legibilidad de mp por un lado mp6 mesh y otro MP drago Y por qué porque acá tenemos dos cuestiones primero la detección de los puntos de la cara y luego si yo quiero graficar esos puntos que estoy detectando sobre la misma imagen o directamente tomo la información y no la gráfico sobre la imagen y hago lo que tenga que hacer o lo que sea mi objetivo por eso es que tomo en principio la librería MP Solution 6 meses y luego MP Solution drawing útil sí bien lo primero que hay que hacer justamente con la primera de las variables creadas sí es configurar Qué tipo de características quiero en la detección en principio aquí tengo un primer parámetro que es static image Mode true o Falls Aquí les he dejado un comentario para que sea claro Cuál es el propósito de poner false o true en principio si es false sí ASUME que la imagen de entrada es un vídeo por eso no es estática static folk sí Entonces de esa manera lo que hace es buscar los puntos desde la primera aparición de la primera imagen del video y de allí empieza a hacer un tracking buscando que en las próximas secuencias de ese vídeo si va detectando si los próximos puntos de la cara entiendo que el video implica un movimiento de uno o de varios rostros que se presenten en ese video por el contrario si pongo true lo que hace es asumir que es una foto una imagen Entonces no hace tracking sino que lo que hace es buscar los puntos de sacar de esa imagen que está congelada digamos no como toda imagen que no tiene el patrón de un video bien luego lo que se especifica con maxnunfaces Bueno lo que ustedes imaginan Cuántas caras quiero detectar sí es decir que como máximo en una imagen puedo detectar una determinada cantidad de cara que le pongo que solamente una ese que si hay más de una cara en la imagen sea esta una imagen un vídeo solamente va a detectar una de ellas y luego el tema de diferencia decir bueno cuándo va a detectar cuando entienda que algo que ve en la imagen hay una confianza del 05 o superior de que eso entiende es un rostro sí acuérdense que lo primero tiene que hacer es detectar no es que va a marcar los puntos de la cara en cualquier lado primero tiene que ver que hay un rostro que lo identifica como un rostro que tiene una seguridad superior al 0,5% de que es un rostro y luego va a buscar los puntos de la cara bien lo que hago luego con civitud es cargar la imagen hemos puesto aquí model jpg si acá me paro Para que vean la imagen de una mujer acuérdense que tengo que poner siempre la ruta aquí tenemos datos imágenes y modelo jpg que lo que tengo aquí datos imágenes model jpg bien hay dos más para probar arrancamos con esto lo que voy a hacer es hacer un resouze para asegurarme que la imagen tenga 900 por ciento en este caso yo tomé tres imágenes sin ver cuál era la dimensión aquí lo que hago es cuidado con esto porque obviamente en este caso hay que ver si esta imagen original y fíjense que tiene 1920 por 1076 guarda más o menos esa relación si hubiese una imagen cuadrada por decir 900 por 900 o 1920 por 1920 y la la le hago un 900 por 600 la imagen se va a deformar un poquito sí así que eso hay que tenerlo presente a la hora de ver qué imagen tomo y que parámetros uso para el acuérdense que realizar siempre le pongo la imagen quiero reformatear si resaar por decirlo de alguna manera y cuáles son las las variantes de ancho y alto que quiero para la semana más luego lo que vamos a hacer aquí es pasar la imagen o transformar la imagen de un formato bgr a rgb Sí porque porque en civicú no trabaja Sí con el formato tradicional Sí entonces pasa yo debo pasar la imagen a través de civicú de dgr a rgb si son dos formatos que representan lo mismo la misma idea de los colores primarios sí de azul rojo y verde pero tienen bueno obviamente una estructura diferente con lo cual lo que estoy haciendo aquí acuérdense que el 2 implica como el tubo sigue básicamente cuando se usa mucho para decir que esto pasa tal cosa bien Esta es una es un método de civil tú que lo que hace justamente es cambiar la gama de colores de un formato bgr a rgb para que bueno pueda trabajarse con y lo colocó dentro de esta variable image barra o Perdón guión bajo rgb que cuesta una vez que hago eso lo que hago es aplicar Face mesh.prosses sí acuérdense que este lo primero que tengo que hacer es detectar las imágenes este las imágenes la cantidad de puntos de la cara primero el rostro después los puntos de la cara bien en este caso lo que voy a hacer es Face mesh punto process y le mandó adentro la imagen esta que tenía aquí sí que acabo de bueno cambiar estructura de color con lo cual que hace esto detectar rostros y puntos Sí y toda esa información la voy a poner dentro una variable que voy a poner results variables que ustedes le pueden poner como siempre decimos como les parezca mejor bien una vez que tenemos los resultados en results lo que voy a hacer es bueno hacemos una impresión aquí esto lo van a ver por pantalla como para que tengan justamente la información de que tiene adentro results lo que tiene adentro del sur son lands los landmarks son Ni más ni menos que los puntos de cada uno de las detecciones que ha hecho sobre ese rostro como vimos aquí arriba en este comentario hay o debería haber hasta 468 Land detectados obviamente esto tienen que tener presente de que la cara tiene que estar de frente yo doy vuelta a la cara obviamente no va a detectar los 468 esto tiene que estar en la posición frontal para que detecte todos esos puntos de la cara Sí si no va a detectar obviamente la cantidad de puntos que tenga que ver con la posición que yo tenga si tengo la cabeza girada bien lo primero que hago es bueno con un If If resulte multiface lands esto es como pregunta si eso es true qué quiero decir si hay puntos detectados si no hay puntos detectado todo lo que sería contenido no tiene sentido sí Obviamente que esta información que imprimía acá arriba título de información nada más para que lo podamos constatar qué tiene adentro la variable esto no no tiene otro objetivo más que eso con lo cual lo podría quitar Luego de eso me da información de que esto obviamente es posible porque hay información results tiene información si tiene información bien fantástico Entonces avancemos qué hago hago un Ford y lo que voy a poner aquí es por cada Face Land es una variable una variable iteradora Y qué cosa en results multiface Lamar y decir que qué hago con este Ford tomar todos y cada uno de los puntos detectados de la cara así que hago un Ford que va a dar Cuántos ciclos 468 porque 468 serán los puntos detectados de la cara y en cada uno de ellos lo que hago es dibujar información relativa al contorno del rostro sí o a todos los puntos de la cara y sus conexiones Acá hay como dos opciones vamos a ejecutar Este programa con las dos opciones en el principio y después lo vamos a hacer con solamente una de ellas para que vean Cuál es la idea en algunos casos vamos a ver todos los puntos en otro caso algunas destacando algunas partes importantes de la cara como la boca o las cejas Sí entonces bueno Esas son las diferencias entre lo que es el Face mesh y el Face de Zelaya sí bien Vamos a centrarnos en el primero sí entonces lo que hago fíjense con la librería drawing punto draw que es la que teníamos aquí arriba de Dragon útiles si se acuerda que de hecho le decía que eso tiene que ver con lo que yo quiero hacer con la información detectada en este caso lo que estoy buscando es la información que tiene result quiero ponerlas sobre la misma imagen para ver las detecciones de una manera gráfica entonces dijimos Face mesh marcalo contorno de la cara y lo que hago en cada caso es con Dragon speak si dibujar en principio lo que tiene que ver con los puntos que sería esta parte de acá arriba y con las conexiones esta parte de aquí abajo en ambos casos tengo que indicarle Cuál es el color con el cual yo quiero que salga Eso sí aquí tengo 0 255 0 que sería rgb bien este acuérdense que el 255 depende En qué lado lo ponga Y si los otros componentes son cero son o representan uno de los tres colores primarios sí bien con esto obviamente es el grosor Y en este caso como va a dibujar los puntos yo le pongo el tamaño del círculo porque cada punto va a aparecer con un círculo las conexiones aquí que tengo acá abajo no obviamente las conexiones va a ser una línea con lo cual no hay que poner información del círculo radio Sí aquí con el circo radio lo que voy a hacer es decir Bueno qué tamaño va a tener el circulito Sí con el cual yo identificar el punto obviamente no puede ser muy grande porque como son juntos si el círculo es muy grande se van a solapar uno con otro Sí bien y luego algo parecido como que esto que está aquí abajo no para los contornos sino para toda la cara completa Sí para todos los puntos una vez que con esto tanto con un objetivo como con otro ya tengo toda la imagen Sí remarcada con todos los puntos de contorno y el resto de los puntos con sus conexiones lo que hago es Ni más ni menos que un Ims de YouTube que hace la impresión muestra la imagen Sí qué imagen email acuérdense que siempre en mi show tiene que ir en principio esto que está aquí que es un texto que es lo que va a salir en la parte de arriba de la ventana si esta ventana que vamos a ver por pantalla tiene un título y bueno ese texto hace alusión a lo que quiero que salga es una referencia obviamente habitualmente se pone la misma frase que el nombre del objeto en este caso Pero puede ser Esta es la demostración de la imagen de la modelo sí un texto como te queda esto no aquí sí va la imagen Qué imagen la misma imagen que yo utilicé aquí para dibujarle las conexiones la misma imagen sobre la cual también dibuje los contornos Sí y la misma imagen que justamente incorporamos y reformateamos aquí al principio sí bien y luego con White lo que hace Es simplemente esperar que yo le dé enter para para cerrar la ventana dado que esto tiene que aparecer Y cerrar y con Distroller Windows lo que hace obviamente es dar por terminada la ventana digamos que esté gustan destruir la ventana que abrimos para verificar Esto bueno sin más explicaciones esto todo el programa lo ejecutamos para ver el resultado bien allí está se entiende Entonces todos los puntitos de cada una de las partes del rostro y las conexiones de cada uno de los puntos con este bueno obviamente las conexiones ustedes miran para qué sirven bueno sirven para marcar las distancias porque cada una de las cosas importantes que podemos saber justamente en la práctica es poder entender la distancia que existe entre un punto y otro de la cara para toda la información que se puede bueno presumirse puede sacar de de esta de esta ejecución de esta librería y del Face mesh habrán visto seguramente en alguna película que una de las cuestiones que pueden identificar a través de la seguridad es el patrón de cara que tenemos Nosotros sí es decir que han visto que inclusive sale dibujito de una malla parecida a esta en la pantalla donde se detecta la persona se pone frente a la a la cámara y lo que hace no es el reconocimiento de la cara sino el patrón que tiene esa cara con cada uno de los puntos de la cara y la distancia que existe entre ellos que es lo mismo que está aquí Bueno eso es un buen ejemplo que pueden seguramente muchos de ustedes lo pueden asociar entre lo que vieron en alguna película y esto que está aquí bueno dejo en ustedes poder utilizar a mail jpg y yo voy a usar people justamente para probar el tema de la variante esta de Max numfaces sí es decir que puse Aquí entonces cambie model por people sí people personas lo vamos a ejecutar y ven que detectó solamente un rostro Sí yo voy a cerrar esto y vamos a pasar a poner un faces 2 lo volvemos a ejecutar y ahora vamos a ver que vamos a detectar dos rostros está bien bueno esa sería la idea del manejo de ese parámetro Bueno vamos a pasar entonces ahora a Face makes video 1 Bueno aquí la Consigna es parecida solamente que con algunos cambios que son propios de lo que ya vimos antes pasar de una imagen a un vídeo bien la cuestión es que aquí Más allá de usar las mismas librerías y bueno crear los mismos las mismas variables de objeto que creamos antes ahora en lugar de abrir una imagen con image read de sibetchup lo que hago es CV tu video capture y voy a tomar uno de los vídeos que tengo aquí que se llama Woman mp4 y lo pongo como siempre en una variable K configuro mi Face mesh al igual que como lo hicimos antes solamente que aquí que voy a cambiar Donde antes yo tenía static image Mode en true ahora le voy a poner falso Esto no es estático es de tipo video Se acuerda lo que teníamos aquí aquí creo que lo teníamos como true porque era una imagen Era algo estático ahora no es algo estático por lo tanto este parámetro en falso configuro una un Magnum Face en dos sí podría ponerlo ustedes lo manejan como quieren y la confidencialidad igual que antes y acá viene el cambio que ya lo hemos visto antes en lo que tiene que ver el tratamiento de una imagen a un video donde tengo Wild porque justamente el vídeo me va a dar fe en cada uno de frames y cada una de las de Las capturas de pantalla propias de cada una de las secuencias de ese vídeo y como lo hemos hecho antes vamos a ver con ese Wild true cada una de las secuencias de ese video capturando cada una de ellas sí Por lo tanto justamente ejecutamos el cap read Recuerden que el caprit me devuelve dos parámetros de salida uno es el frame la captura permanente hecha y el otro es el Red que causa una variable tipo bandera donde me dice si la captura ha sido positiva o ha tenido algún tipo de inconveniente bien Obviamente que si red Sports Así que la tuvo algún tipo de inconveniente Esa esa variable de bandera me dice que hay algo anómalo bueno entonces el false Por ende a un break de El Walter y se termina toda la lógica de este caso contrario lo primero que hago es un flip que es un flip en realidad para este caso no va a ser tan necesario si después más adelante vamos a probar con un streaming Y ahí sí esto tiene sentido Pero lo podemos aplicar Aquí también lo que busca es evitar el efecto espejo es decir cuando yo me pongo frente a una cámara la imagen no se ve tal cual la que yo tengo sino que se va invertida porque justamente es el efecto espejo que hace una cámara bueno con el flip lo que hace es Buscar que mi imagen sea tal cual la que yo estoy poniendo frente a la cámara y no la que me devuelve la cámara bien eso insisto lo logra el método luego hago un resouze en este caso he elegido que mi imagen tenga 860 por 600 Acuérdese que esto ustedes pueden poner las dimensiones que les parezca mejor y voy a hacer una copia del frame en una variable ya vamos a ver para qué O sea que voy a tener como resultado no una ventana sino dos ventanas sí bien luego hago la transformación acuérdense que lo que hablamos también en el práctico de recién de bgr a rgb porque recuerdan esto porque justamente lo que viene a continuación el Face mesh necesita que la estructura de esta esta imagen sea con la gama de colores con el formato rgb y nosotros lo tenemos como vejer Entonces tenemos que hacer esta transformación de bgr a rgb solamente para que el Face mesh detecte el o los rostros y los puntos de el color roto bien Por eso eso lo pongo dentro como en el caso de oración dentro una variable frame rgb y es la que uso para que el Face mesh process me detecte toda la información y la ponga dentro de una variable results lo que sigue es Exactamente igual a lo que hicimos hace un rato con la imagen solamente que vamos a hacer un truquito aquí para que vean algo de lo que les comentaba recién que nos quedó pendiente es decir este tema del Face mesh que marca los contornos de algunas partes de la cara y el Facebook de selección que marca todos los puntos y todas sus conexiones Sí yo le dije que esto lo voy a tratar de ejecutar aparte vamos a hacer un truquito decir no lo vamos a ejecutar aparte lo que vamos a hacer es los contornos se lo vamos a poner al frame y las conexiones totales con todos los puntos lo vamos a poner en el frame 1 de este modo vamos a poder ver las dos este pantallas Sí las dos secuencias Sí en dos ventanas diferentes y poder justamente en la comparación entender mejor la diferencia entre el Facebook y el Face mess de selección luego lo que vamos a hacer es tratar de guardar cada uno de los frames si detectados con sus detecciones solamente del frame ese del primer caso no de frame 1 o sea que solamente va a ser la que detecta los contornos y lo vamos a ir haciendo con el sistema que ya manejamos antes sí con la librería que hemos abierto aquí sí DVD donde justamente eso recuerde que lo que me hace es generarme nuevos este nombres aleatorios para que cada uno de esos franceses guarde con un archivo con un nombre diferente al otro Sí bueno esto ya lo usamos antes con el civitud y m-rite y esta o esta mecánica de generar nombres aleatorios para que se guarde se van a guardar dentro de la carpeta vídeos que la carpeta que tengo aquí y con lo cual vamos a ver entonces terminó ejecutar Este programa Todo esos resultados luego hago un Ims como dijimos antes hago dos uno para frame y uno para frame 1 y aquí le pongo títulos diferentes para que vean lo que hablamos recién de que lo primero que pongo Es simplemente una referencia con lo cual a la ventana del frame le voy a poner como título contornos y a la ventana del frame 1 total lo detecciones totales lo que le parezca Y finalmente con esto que tengo aquí pregunto por el web key si es 27 o no Si es 27 que sería la tecla escape corto abruptamente la ejecución y si no dejo que siga hasta que termina el video finalmente Cierro la captura o la variable de captura de secuencias del vídeo y también las ventanas bueno explicado todo esto pasamos a ejecutar entonces Este programa para ver los resultados bien ahí tienen ven que aquí donde dice contornos claramente ven marca los puntos y fíjense que marca el contorno voy a ponerlo aquí central el contorno de la cara el contorno de la boca y el contorno de las cejas sí y también el de los alcanza a ver aquí porque tiene un color violeta Violeta intenso sí fíjense que se ve bien la comisura de la boca la parte media de la boca los contornos la boca los contornos de la cara la parte de arriba y abajo de la ceja de las cejas perdón y todo lo que es el borde del ojo más todo el resto de los puntos en este caso este caso acuérdense que es cuando uso el contorce cuando uso el teselation se ve la misma información pero con cada una de las conexiones que hay entre los puntos Bueno me pareció que era una buena forma de que ustedes pudieran este visualizarla la diferencia entre una opción fíjense que la detección a pesar de que el video va lento Sí sigue muy bien los movimientos de esta persona elegido esto a propósito justamente porque tiene bastante movimiento la secuencia Pero la va detectando Acuérdese que acá tenemos una máquina que no tiene gpu cual el procesamiento es aún más lento pero así todo hace una muy buena fíjense que cuando ella entorno un poco la cara se pierde la detección para la detección más que nada es con la cara frontal bien lo que nos quedaría ahora sería cambiar la opción de vídeo por la opción de streaming Sí por eso comento esta línea aquí arriba y habilitó la línea aquí abajo donde habilitó justamente la cámara 0 como siempre decimos lo vamos a probar y la cama me va a detectar a mí y ustedes pueden Probar con esto también bien lo que estamos viendo aquí es una imagen de todos los puntos que detecta facemesh Sí porque le muestro esto porque ahora para cerrar la clase o esta parte de la clase Perdón vamos a hacer una aplicación práctica de todo esto que hemos visto y vamos a tomar como ejemplo la posibilidad de poder ver si los ojos de una persona o la boca de una persona están cerradas o abiertos esto en realidad es una de las tantas aplicaciones no tanto lo de la boca Así lo de los ojos se puede utilizar mucho para el tema del seguimiento de los conductores de un sistema Público de transporte en este caso yo puedo poner una cámara apuntando al rostro del conductor y ver esa persona con su parpadeo de ojos puede estar de alguna manera dando una idea de que puede estar por quedarse dormido y con ello Obviamente el accidente que puede llegar a ocurrir a partir de eso bueno La idea es que con esto yo pueda detectar ese patrón de comportamiento y pueda bueno con algún tipo de sistema bueno tratar de avisarle a esa persona que está en ese estado y tratar de tomar alguna acción para no llevar algo que puede ser un accidente bien vamos a hacer entonces una aplicación para esto lo importante demostrarles Esto es para que ustedes puedan ver que bueno cuando quieran hacer algo con determinado punto de la cara hacer esta aplicación o cualquier otra tienen aquí los números porque a partir de este ejemplo que vamos a ver ahora vamos a ver que los lands me lo va a dar todos pero yo puedo tomar algunos y con ellos poder hacer lo que yo quiero hacer en este caso lo que voy a detectar son los Land 13 y 14 que me detectan si son los que me van a dar la la idea más concreta si la boca está abierta o cerrada y luego los 159 y 145 y 386 374 para ver el tema de los ojos Sí bien Esto es muy particular porque va a tener que haber bien con la calidad de la imagen con el tamaño de la detección si se ve una detección más grande porque lo que vamos a tomar son las distancias y la distancia tiene que ver justamente con la cantidad de de de la ubicación dentro de la altura Sí y el ancho de la imagen si la cuadratura de la de La coordenada alemana Más allá del punto este 3 86 va a tener un punto dentro de la imagen y este 374 va a tener otro punto entre la imagen con esos valores son los con los cuales yo voy a calcular la distancia y esa distancia insisto es relativa al contexto Pero bueno vamos a hacer una prueba un poco precaria rudimentaria con los elementos que tenemos pero seguramente nos va a dar una buena idea de cómo lo podemos hacer con elementos más avanzados vamos hacia ello entonces que sería el último programa de esta de este proyecto que hemos creado de Face mesh bueno acá estamos entonces con Face mesh vídeo 2 con los propósitos que recién comentábamos bien aquí lo primero que voy a hacer es crear un array con un Index que es el Index list son los puntos que acabo de mostrar en la imagen puntos que tienen que ver con en principio los contornos de la boca y de los dos ojos pero voy a tomar en particular a través de otra red los puntos principales que son también los que le comentaba recién y lo voy a poner para una cuestión más práctica Dentro de este obviamente voy a tomar una un streaming vamos a hacerlo digamos con el rostro nuestro y voy a definir una función que le voy a poner calcular a esa función le voy a pasar todas las posiciones si las posiciones que van a evaluar esto lo vamos a quitar el print de una referencia y con esas posiciones que van a ser las que se relacionen con los puntos principales voy a calcular las distancias es decir la posición 14 con la posición 13 las 145 con 159 y la 364 con la 386 tal cual yo se los dije recién cuando veíamos la Gráfica y luego voy a armar un mensaje que va a ser la salida de esta función y el mensaje va a decir boca abierta boca cerrada ojo izquierdo abierto no izquierdo cerrado ojo derecho abierto ojo derecho cerrado en virtud una distancia Esto es lo que digo que hay que trabajar un poquito porque hay que ver en cada caso el 2 puede ser poco o mucho el puede ser poco mucho son valores que aquí estamos poniendo como referencia y hay que ajustarlos en cada caso bien y devuelvo el mensaje ese mensaje lo voy a poner por pantalla conforme yo vaya teniendo actitudes ante la cámara y van haciendo evaluadas secuencia por secuencia bueno acá tengo configurado Entonces el Face mesh le digo que es un vídeo por eso false detectó una sola cara y le digo una confidencialidad del 0 50 por ciento luego Esto va a ser similar si solamente solamente que en este caso yo voy a circunscribir esto solamente a los Index del Index list es decir no voy a recorrer todo esto Tal cual lo recorría antes con todos los puntos y con todos los lámparas es decir lo voy a hacer solamente para estos puntos voy a detectar Solamente voy a recoger Mejor dicho la información solamente eso por eso dice for Index Sí y luego dentro de cada una de esas detecciones en particular voy a tomar dentro de este este diccionario que Acabo de crear acá las posiciones de los puntos principales los Main points y voy a poner sindex es decir si uno de los puntos detectados de todo este conjunto está dentro de este array lo que voy a hacer es poner el valor de y sí que sería el de la altura de la posición de altura dentro de la imagen y crear dentro del diccionario dos posiciones una que me dice el Index que sería el número de posición supongamos que en el primer caso detecte la 13 y luego la posición dentro de la imagen  y lo que voy a hacer obviamente voy a dibujar ese si no solamente dibujo el punto pero no recojo la información sí bien luego es información pues el diccionario se lo mando a la función calcular se entiende que ese diccionario tiene 13 y la posición de 13 14 y la posición de 14 159 ya cuando digo la posición me refiero a el valor de y dentro de las dimensiones de la imagen Sí bien y entro esta función que vimos recién entran seis valores el diccionario con sus seis valores Y dice Bueno restame la posición del 14 respecto de la del 3 y eso siempre tengo que poner primero en la resta para que no me dé negativo el valor que está más arriba sí de la coordenadas el punto 14 es el punto superior de la boca y el punto 13 el punto inferior y con eso armó mensaje y esta función me va a devolver el mensaje y lo que hago después es no imprimir el frame como venía haciendo hasta ahora sino imprimir el frame con un Pull text para Que aparezca el mensaje en la parte superior de ese frame si de esa pantalla streaming y que me vaya diciendo bueno tenés la boca cerrada o abierta tenés El ojo izquierdo abierto cerrado tenés el ojo derecho y eso lo haré hasta que decía no hacerlo más apretarla el botón escape la tecla escape y terminar el programa bien ejecutemos lo para ver qué pasa con todo este bueno Entonces hasta acá llegamos con esta práctica Espero que se hayan divertido con mis expresiones Pero y se diviertan con la ayuda también por supuesto pero bueno sí digamos siendo lo serio de este caso es importante que vean tuve que cambiar las distancias de la boca porque al ser una imagen tan grande que la hice grande a propósito justamente yo tenía dos y lo tuve que subir a 5 porque justamente con dos no daba la sensación que la boca estaba cerrada siempre la detectaba como abierta sí Entonces eso es lo que ustedes van a tener que ir trabajando en cada caso conforme el tamaño de la imagen este y bueno Y también la calidad y la calidad en realidad los puntos de los detecta muy bien el tema es ver que las distancias si van a estar en función del tamaño del de Lima eso también se puede hacer con algún cálculo matemático que se adapte sí que sea responsable responsivo digamos de alguna manera no como para que como Como sucede con con las Apps y con las web no para que de alguna manera el tamaño de la imagen si vaya a través de un cálculo matemático adaptándose no tiene que estar yo cambiando la mano pero bueno Este es un ejemplo muy sencillo muy básico así que por ahora lo hicimos tocando esos parámetros obviamente ustedes como programadores pueden tomar esta base y llevar esto a un ejemplo obviamente mucho más sofisticado Sí y más adaptable digamos a la circunstancias Bueno hasta aquí llegamos con la segunda parte esta clase hasta aquí llegamos con esta clase común todo vamos a seguir entonces en la clase que viene con esta misma línea de detecciones pero sobre la base de detecciones de los puntos de las manos y de el resto del cuerpo así que nos vemos en la próxima clase aquí termina esta clase los espero en la próxima clase nos vemos Titulo: Clase23 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/R7HAfYlQsNc  \\n 1540 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 23  Hola a todos Bienvenidos a la clase número 23 del curso de Inteligencia artificial de ifes seguimos en el módulo de Deep learning y seguimos en el ámbito de las redes neuronales convolucionales hoy Vamos a continuar Lo que empezamos en la segunda parte de la clase 22 por cuanto en aquella oportunidad vimos la librería media Pipe y el uso de la misma para la detección de partes del rostro como también lo dijimos justamente en la finalización de esa clase hoy Vamos a continuar en las dos partes de esta clase 23 viendo la misma librería pero para la aplicación de detección de partes de la mano y del cuerpo completo tal como hicimos en la clase pasada en ambas partes de esta clase vamos a seguir la misma mecánica en principio vamos a describir las características de cada una de estas librerías de cada una de las aplicaciones de esta librería sobre manos y sobre partes del cuerpo luego vamos a aplicar esto sobre la base de imágenes vídeo y streaming y luego vamos a pasar a una aplicación práctica que va a ser sentar base para que ustedes después puedan llevar esa aplicación práctica sencilla por supuesto a algo más complejo concretamente el práctico final que vamos a hacer va a ser esto que está aquí que es poder medir la distancia que existe entre la mano de la persona que está operando esta herramienta de corte y el lugar donde está haciendo el corte y ver si la distancia está dentro de lo que corresponde o no marcando con un cartel como aquí dice correcto o no acá estamos entonces en el contexto de paichar con ya el proyecto creado como decía recién Recuerden que ustedes lo que van a hacer como siempre es crear la carpeta poner los archivos que bajan del campo virtual allí dentro crear el proyecto y recuerden cuando queda el proyecto le va a aparecer una pregunta porque ustedes lo van a hacer basado en una carpeta que ya existe que es esta que pertenecíamos recién le va a preguntar si toma esa carpeta con todo su contenido para crear el proyecto con él o no y tenemos que responderle que sí bien dicho esto y recordado este tema lo que tenemos que empezar a trabajar aquí antes que nada antes de hacer uso de estos archivos que hemos puesto aquí dentro es la las librerías que vamos a utilizar yendo a settings y recuerden dos librerías muy importantes que son las que vamos a usar aquí que es media Pay por un lado Sí y Open que es justamente la de Así que no olviden Cuando el proyecto después de crear el proyecto y al Ingresar a python subir estas dos librerías al proyecto bien empezamos a recorrer el primero de los programas que tenemos en este proyecto han imagen punto Pipe y lo primero que hago como si estábamos recién importar las dos librerías a lo que hicimos el otro día vamos a crear un objeto para detectar el elemento en este caso la mano o las manos y otro objeto para dibujar los puntos de cada una de las manos o de una mano depende el tipo de detección que hagamos con lo cual creo un objeto Hans MP solución Hans y un objeto MP drowen MP Solution sí bien ahora con el with voy a configurar las características del objeto hands que es el que me va a servir para la detección de manos con lo cual lo primero que le digo es tatic imagemon true Recuerden que esto lo vimos también la clase pasada que si le digo que es true quiere decir que es una imagen estática Y si le digo Falls quiere decir que es un vídeo o un streaming es una imagen no está luego la máxima cantidad de manos que quiero detectar una o dos obviamente y luego el nivel de confianza de la detección sí es decir si es 05 0607 el número que ustedes quieran y lo nombran a este objeto como Hans luego lo que vamos a hacer es con image read de cb2 leer la imagen Recuerden que yo aquí les he dejado dentro de la carpeta datos una carpeta imgs donde hay varias imágenes en este caso vamos a tomar el de Woman jpg por lo tanto aquí pongo barra datos barra y mgs barra Woman punto jpg es decir esto que está aquí datos y mg Guzmán punto bien luego hago un resouze de la imagen porque es una imagen que la he bajado y no he prestado atención al size que tiene Así que la voy a reformatear sí o redimensionar como 860 por 640 y la vuelvo a poner dentro de la variable y finalmente tomo de allí estas bueno Esto lo podría haber puesto con una designación directamente 861 y 640 otro pero está bueno dejar esta forma porque cuando yo tenga la imagen de entrada y no sepa Cuáles son las dimensiones Esta es una forma de hacerlo este guión que está acá es porque el shape el método de shape del objeto imagen me devuelve no dos parámetros sino tres Qué pasa a mí el tercer parámetro no me importa con lo cual sí tengo que poner algo aquí porque si me devuelve tres parámetros tengo que ponerlo igualado a tres variables pero como no me importa ese tercer variable ese tercer elemento para los propósitos de este proyecto puedo poner un guión bajo de esa manera no rompo el esquema de que tengo que recibir tres salidas de ese método y no me gasto en ponerle un nombre una variable que no voy a usar bien luego se acuerdan que habíamos comentado que en el caso de la detección se necesitan las imágenes en formato rgb y la imagen viene en formato bgr con lo cual a través de cbt color recibir tu tomo la imagen y la paso de bgr a rgb creando una variable image- bajo rgb para diferenciarla del original y hago la detección de manos justamente sobre image rgb como con el método process parecido a lo que hicimos el otro día con la detección de partes de la cara o de la cara perdón y pongo los resultados de esa detección dentro de la variable como siempre hacemos un print aquí debajo yo se los dejo a ustedes porque es interesante poder ver el contenido que tiene justamente la variable results que es la que toma toda la salida de la detección que en este caso ha detectado una o dos manos bien este print insisto es un detalle después que lo que voy a hacer aquí lo puedo comentar y no me va a mostrar absolutamente ningún resultado paso siguiente empiezo por preguntar al igual que el otro día si hay multicam lands el otro día tenía que ver con la cara acá los lanmart tienen que ver con las manos es decir si hay detecciones Si esa variable results que es producto de haber buscado manos ha encontrado manos y que me dé la información no solamente eso sino también de los puntos de cada una de las manos vamos a hacer un paréntesis aquí y vamos a ver de qué se trata esto de los puntos de la mano bien aquí tenemos obviamente en una cantidad bastante inferior a la que vimos el otro día con los puntos de la cara tengo los puntos de cada uno de los dedos de la mano y otras partes de la mano como puede ser el inicio de la palma y el resto de la composición de la mano que tiene que ver justamente con la Palma y los extremos donde comienzan los dedos bien cada uno de los valores aquí está con esta gráfica con los puntos y aquí describe de alguna manera a través de un nombre cada uno de estos elementos se lo puede referenciar por un número o por el nombre asociado a esa parte con esta base de información es con la cual vamos a trabajar en todo lo que hagamos hoy en la clase aclarado esto continuamos con lo cual como dijimos recién pregunto hay detecciones bien supongamos que sí Entonces con un Ford voy a recorrer todas esas direcciones que ha hecho que supuestamente si puse una mano abierta tenía que ser todos los puntos que vimos recién en el gráfico y lo que voy a hacer es dibujar todos y cada uno de los puntos aquí es más sencillo el dibujo a diferencia del otro día que vimos que podía dibujar el contorno del rostro o determinadas partes del rostro o todos los puntos Aquí no hay contornos simplemente son los puntos y sus conexiones por eso lo hago con ham connections única opción y dibujo como siempre los círculos para los puntos y luego con la línea siguiente las conexiones hecho esto que lo hago sobre qué sobre el objeto image acuérdense que el objeto y mes es el que tomé de en este caso de la foto y después tengo image rgb que es el que me sirvió para detectar O sea que tengo dos imágenes una que separa detectar y otra que la use para todo el proceso completo de incorporación de la imagen y luego lo que voy a mostrar por pantalla como resultado final que voy a mostrar por pantalla como resultado final bueno justamente emails porque aquí Este drawing lo hice como bien dice aquí luego a un aquí y termina esto que ustedes saben que como es una imagen solamente no es un vídeo lo muestra y se termina Así que vamos a proceder a ejecutarlo bien y ahí tenemos una mujer que tiene una mano apuesta sobre la cara y fíjense que detecta todos los puntos que veíamos en el gráfico de recién cada uno de estos puntos recuerden tiene un valor x y un valor y Recuerden que ese valor x 6 no es el número de Pixel que tiene en esta imagen sí que habíamos dicho que era de 860 por 640 si estas dimensiones sino que lo que me muestra esta imagen a mí es Ni más ni menos que una valor proporcional respecto de toda esta dimensión si 0 bueno algo que tenga que ver con la proporción que tiene ese punto Dentro de este rectángulo Dentro de este cuadrado luego ese valor si yo lo quiero en Pixel tengo que convertirlo como vimos también la clase pasada multiplicándolo por el valor de la altura o del ancho de la imagen para obtener ahí sí ese valor que me tira landmark convertido a valor Pixel bien Eso es todo lo que tiene que ver con la detección de los puntos de una mano de una imagen Obviamente que en el caso de que fuese otra otra imagen acá le dejó de Father para que tenga más de una mano vamos a abrir Father Aquí sí ven que tengo dos manos entonces allí yo puedo detectar más de una mano Es decir vamos a probarlo aquí fíjense que me detecta una sola mano Sí porque porque yo le puse que ese era el parámetro Así que si yo detengo esto y cambio este Max de 1 a 2 y lo vuelvo a ejecutar van a ver que va a detectar las dos manos como se ve bien aquí claramente del padre donde justamente aparecen los puntos de las manos que ya referenciamos antes habiendo hecho esto con una imagen ahora al igual que la clase pasada pasamos al vídeo y el streaming empecemos por el vídeo bien el principio del programa Exactamente igual y luego hago como también hicimos la clase pasada con un vídeo para detectar las partes de la cara un Civic video capture referenciando un vídeo de todo lo que yo les he pasado aquí tenemos Hans datos vídeos y aquí tengo dos vídeos auto y fábrica bien en este caso yo voy a utilizar auto punto mp4 si referenciando la ruta como lo venimos haciendo habitualmente bien luego creo el objeto hands igual que como lo hicimos recién sí Solamente que en este caso estático image modelos por lo que decíamos hace un rato en el caso de las imágenes pongo una para detectar una sola mano y un nivel de confianza de 0,5 Luego hacemos Recuerden que como ya lo venimos trabajando un Wild para tomar cada uno de frames de en este caso el vídeo o después en el streaming Sí entonces Recuerden que en este caso va tomando de a 30 imágenes por segundo sí bien con lo cual hago red frame igual a caprit Recuerden que caprit me da cada uno de esos frames y pone el frame dentro de la segunda variable a la que le puse Franco le podría haber puesto cualquier cosa y la primera es una bandera que siempre me dice si funcionó bien la captura o funciona bien la cámara y si no justamente me da justamente falso Por eso pregunto si es Force hago un break porque evidentemente hay un problema sino avanzó y avanzo De qué modo bueno haciendo revisar como hicimos hoy de la imagen y bueno y un shape para conocer las coordenadas sí esto recuerden que no es necesario lo de esta manera yo podría poner directamente G igual a 860 y with = 640 Pero insisto lo hago esto porque ya queda planteado para que cuando no tengan esa información a través de esta metodología Perdón lo puedan hacer al igual que hoy hago la conversión de color así como hoy tenía image rgb ahora tengo frame rgb son nombres de variables referenciables Pero bueno para que sea claro el objetivo es decir que tengo el frame original y el frame convertido a formato rgb porque la entrada es vr y luego al igual que hoy a un proceso para detectar las manos no sobre una imagen sino sobre un vídeo a continuación igual que hoy Exactamente igual que hoy digo si hay detecciones listo la dibujamos y uso exactamente la misma línea que acabo de usar recién y finalmente hago un ime show del frame Y en este caso lo hago con un White King preguntando si quiero cortar antes que termine el video apretando la tecla escape o sigo hasta el final bien esta es la imagen de dos manos que están sobre un volante Así que la vamos a ejecutar ya mismo y Ver el resultado bien aquí está como verán detecta todos los puntos una mano está obviamente diferente a la de hoy que está la mano dada vuelta hacia adelante Aquí se ve la parte trasera digamos la mano no parte de arriba de la mano no bien Obviamente que si esto que terminó por sí mismo yo no apreté escape aquí lo paso a ejecutar con un Max 2 van a ver que van a detectar las dos manos y aquí sí lo vamos a cortar con un escape bien pero ahora yo voy a ir a la opción de streaming y ejecuto y Bueno aquí tengo la detección de las partes de una marca y de ambas manos sí está de más decir que si pusiera maxnu Hans uno vería solamente una más detección Perfecta de los lados va siguiendo bastante estoy muy cerca de la cámara Pero bueno este Obviamente si estuviese más lejos también los detectaría con la misma apreciación bueno Y finalmente vamos con el programa que nos queda que es el que hace una aplicación práctica de esto que vimos es hand video contra el punto Pipe en donde lo que vamos a hacer es esto que mostramos al principio de la clase lo vuelvo a ejecutar para que lo recordemos que es establecer Bueno una medida de riesgo entre la cercanía del dedo pulgar de este operario y la el lugar donde tiene acción la herramienta de corte que vemos aquí y poner un cartel que diga correcto incorrecto aquí arriba bien Esto lo empezamos en principio con la incorporación de esta este código que que no hace más que con la intención de guardarnos las imágenes de los frames de captura de cada una de estas secuencia establecer una ruta o una carpeta donde van a ir guardándose esas imágenes le vamos a poner imgs vídeo control dentro de datos sí lo primero que vamos a hacer es preguntar si existe o no la carpeta si no existe la creamos si existe vamos a recorrer su posible contenido por si tiene archivos adentro con este Ford y con Ritmo Vamos a borrar todos los archivos que puedan haber estado existiendo Antes de la ejecución de este programa paso seguido bueno Esto es muy parecido a lo que hemos Recién con hand video sí porque lo que hacemos Es crear los objetos mt drawings luego hacer el video capture y el width MP hands Exactamente lo mismo el resto también es igual porque dibuja todas las partes de la mano tal cual lo hicimos antes todos los puntos de la mano la diferencia en este caso es que solamente tengo que detectar un punto particular que es la punta del dedo pulgar bien Por eso aquí pregunto luego de haber recorrido y dibujado todos los puntos de la mano y todas las conexiones con este Ford nuevamente recorro todos los puntos y pregunto si es igual a 4 porque igual a 4 porque recordemos que si yo voy a esta imagen donde teníamos todos los puntos de la mano la vamos a recordar aquí podemos comprobar que 4 es justamente el valor que corresponde al extremo del dedo pulgar así que por eso en ese código yo pregunto por 4 bien continuamos una vez que detecto ese punto tengo la x y la y acuérdense que la xela y tienen valores decimales que tiene que ver con la proporcionalidad donde está ese punto respecto de las dimensiones del cuadrado pero no tiene la dimensión de El Pixel Donde está ubicado ese punto así que por eso lo tengo que multiplicar por el with la x y por el gel el y así obtengo la dimensión pero ahora ya no en proporciones de ese rectángulo de ese cuadrado sino de el valor Pixel que corresponde a la x y una vez que tengo esos valores utilizo justamente los mismos para dibujar justamente un punto rojo que va a estar ubicado sobre ese punto que me interesa a mí que es la punta del dedo pulgar y lo hago con 00255 que es el color luego una vez que tengo también ese valor x lo resto de 421 porque 421 porque 421 es la altura a partir de la cual está el punto recuerdan donde hace acción donde ejecuta la acción la herramienta de corte Obviamente que calcular con la imagen esto tiene que verlo con la dimensión si es un vídeo o si es un streaming Sí eso tiene que estar pre calculado con bueno obviamente un estudio de el espacio y la dimensión que corresponde exactamente a ese punto yo acá lo dibujo el punto para que se vea más claramente Pero bueno Esto es necesario dibujar el punto Porque si ustedes tienen el valor correcto bueno para hacer esta esta lógica de acá de ver si la distancia corresponde o no corresponde no es necesario dibujar ni siquiera el punto del pulgar por supuesto No pero lo hago para que quede bien claro el objetivo de lo que estamos buscando luego dibujo justamente el punto de la herramienta de corte con las coordenadas 4 21 320 que son un poco lo que le comentaba recién que tenía que averiguar Dónde estaba ese corte y luego también con color rojo y finalmente dibujo un rectángulo ese rectángulo es el que vieron que aparece en el vértice superior izquierdo con un color verde ese rectángulo verde es donde luego voy a poner el texto en color blanco que va a decir si la distancia es correcta o incorrecta que es lo que tengo aquí justamente con este If pregunto si la distancia menor a 17 que tiene que ver con este cálculo que insisto tiene que ver con el estudio de la situación pongo el texto incorrecto y si no el texto correcto Siempre computex sobre frente una vez que tengo todo eso volcado sobre frame lo que hago es Mostrar el frame justamente con Ims y guardo las imágenes los frames Sí redundancia Dentro de esta carpeta que referenciamos recién con video imgs Perdón vídeo control dentro de datos y Recuerden que el sistema de nombre es un sistema aleatorio que lo hago justamente gracias a la librería uuid que por eso es que la he importado aquí arriba al igual que o ese que no lo teníamos en el práctico anterior en el programa anterior porque no hacíamos esta lógica de ver si la carpeta existía o no estaba vacío bien con esto este termino todo esto así que lo ejecuto nuevamente para que lo ven en acción Pero bueno ya tienen claro Cuál es el objetivo ejecutamos de nuevo y vamos a dejar que llegue hasta el final Así ya podemos ver después la secuencia de las grabaciones Obviamente que en este caso La distancia es correcta No hay una acción digamos que vaya cambiando la posición del operario para que vaya cambiando ese ese valor Pero bueno por lo pronto en este caso La distancia es correcta y por eso sale ese texto ya vamos a hacer algo para para ver cómo funciona esto si la posición de la mano cambiara no bien ahí terminó si yo ahora voy a la carpeta y mgs vídeo control veo la secuencia grabadas abro una cualquiera está por ejemplo así como podría grabar también el vídeo completo no secuencia por secuencia sino el video completo esto ya lo hemos hecho antes así que bueno no representa ninguna novedad bueno para justamente salvaguardar esa situación que deseamos recién de que la mano no se mueve con lo cual no puedo ver si el cartel cambiaron vamos a hacer un streaming Sí y bueno Obviamente que el punto voy a aparecer yo obviamente y el punto no va a aparecer una herramienta de corte pero no importa Porque la situación es ver cuando yo mueva El pulgar respecto del punto si el cartel que dice correcto o incorrecto va cambiando Así que lo que he hecho aquí es comentar esta línea y he agregado esta otra línea donde es igual a YouTube video capture Que obviamente como ustedes ya saben detecta la cámara bien acá va a aparecer obviamente la imagen en cualquier punto aparece sobre mi cara yo me voy a correr un poquito más atrás y ven ahí tengo la distancia correcta y si me acerco es incorrecto me alejo correcto me acerco incorrecto bueno Esto obviamente no importa que ese punto esté sobre mi cara porque representaría el lugar de peligro con lo cual si me acerco ahí tengo incorrecto y obviamente que tendré que estar de una manera diferente para que justa Ahí está El pulgar Debería ser lo más cercano correcto bien con esto terminamos esta primer parte de esta clase con lo cual como les dije al principio de esta primer parte les voy a dejar dos programas para que ustedes vayan un poquito más y conozcan algunas cosas más Más allá de todo lo que vimos recién en la parte que compone la clase en sí misma sí estos programas son estos que están aquí contar de 12bz punto Pipe y distancia de 12 veces hasta punto el primero de ellos lo que hace es primero detectar si tengo levantada una o dos manos y luego me muestra a través de dos rectángulos como lo que vimos recién que muestra el texto Si es correcto o incorrecto la cantidad de dedos de esa mano que tengo levantadas pueden ser cero si tengo los puños cerrados y si no bueno la cantidad de dedos que corresponda luego el segundo de ellos distancia dedos cbz punto Pay lo que hace lo mismo detecta si tengo una o dos manos levantadas si tengo una calculo la distancia que existe entre mi pulgar y mi dedo índice y tengo dos calcula la distancia que existe entre ambos dedos eso lo hace automáticamente bien para usar estos programas vamos a tener que incorporar una librería más que no hemos usado hasta el momento que se llama que lo que hace es darme alguna facilidades para las Bueno lo que naturalmente trabaja media Pipe Bueno tiene métodos que de alguna manera hacen que algunas gestiones no tengan que detracearse tanto y con algunas instrucciones más sencillas poder tener resultados por ejemplo saber si un dedo está levantado o no de la mano está levantado bien es una nueva librería y viene bien que también sea parte de un aprendizaje complementario de esta clase que pueda manejar una librería más bueno ahora sí con esto terminamos esta primer parte y nos vemos en la siguiente parte donde vamos a seguir con media path y como dijimos en el principio recorriendo todos los puntos del cuerpo nos vemos en la segunda parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte Titulo: Clase23 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/YpgjXhaos9M  \\n 2170 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella bueno seguimos en la segunda parte de esta clase número 23 y tal como lo dijimos cuando terminamos la primera parte vamos a ver otra librería de mediapite que en este caso así como la usamos antes para reconocer parte de la mano parte del rostro esta librería reconoce todos los puntos del cuerpo al igual que como lo venimos haciendo en el uso de la librería de media Pipe vamos a empezar primero por hacer un trabajo sobre una imagen luego sobre un vídeo y luego sobre una aplicación práctica que justamente va a tener que ver con algo vinculado a la actividad física actividad práctica que vamos a hacer sobre un vídeo y después lo voy a dejar para que ustedes en su casa puedan visualizar la posibilidad también de hacerlo sobre un streaming con ustedes como protagonistas dicho Todo esto vamos a empezar por ir como siempre a el moodle para bajar los archivos con los cuales vamos a armar este proyecto el procedimiento va a ser el mismo que venimos usando en las últimas clases de hecho aquí tenemos los archivos que van a encontrar en el moodle la carpeta con datos Y estos cinco archivos punto Pipe lo que voy a hacer repasamos voy a crear una carpeta que se va a llamar pose dentro del disco c insisto lo pueden crear donde quieran y con el nombre que quieran yo ya lo he hecho aquí he volcado todos los archivos en esta carpeta y he creado ya un proyecto basado en esta carpeta igual que como lo hicimos en las últimas clases y al haber creado ese proyecto que yo ya lo he hecho obviamente me crea el entorno virtual a la carpeta bem y la carpeta punto ideal con lo cual habiendo explicado todo esto directamente vamos al entorno de Paisa a ver todo este proyecto que estamos viendo aquí desde el explorador de Windows bueno Y aquí estamos en el paycham con nuestro proyecto ya abierto nuestro proyecto pose y bueno este sería el primer programa que vamos a ver donde vamos a destacar las partes del cuerpo desde una imagen Aquí tengo las dos primeras librerías de más está decir que tengo que ir a file settings Sí y cargarlas bien aquí está media Pipe y acá está opencv python que es civitud obviamente yo ya las tengo cargadas por eso no me marca aquí como que faltan estas librerías lo que vamos a hacer muy similar lo que hicimos antes es primero crear un objeto Drop que es el que va a dibujar las partes del cuerpo sobre la imagen o sobre el video o sobre el streaming y luego crear una instancia del objeto pose de la librería pose que así como hicimos antes con Hans y como hicimos antes con Face mesh Bueno ahora es la que tiene que ver con lo que venimos diciendo detección de parte del cuerpo bien creo esa variable instanciando MP solutions.pose y creo con el with igual que antes aquí el objeto pose el cual voy a configurar con la siguiente característica que ya venimos trabajando Siempre que es diciéndole que estático image Mode es decir que es una imagen estática es decir que no es un video bien luego vamos con CV cargar la imagen con imrit la imagen es imagen 1 que está dentro de la carpeta que hemos bajado del móvil y que aquí vemos datos imgs imagen 1 Luego de eso Recuerden que hacemos la conversión de la escala de colores y el tipo de formato de colores bgr a rgb Recuerden que eso es necesario para justamente trabajar con la detección posterior y la reformateo a 600 por 600 esto Siempre hay que tener cuidado porque Depende como es la imagen original para que no se distorsione la imagen si esta imagen es cuadrada si tiene supongamos mil por mil la paso 600 por 600 no va a haber ningún problema si no tiene esa cuadratura debería tener presente que estas coordenadas deberían representar la misma lógica de proporción que la imagen original bien luego Busco establecer la detección en este caso de las partes del cuerpo reconocer aquí lo que tiene que reconocer primero es un cuerpo así como antes reconoció una cara y una mano aquí lo que tiene que reconocer es un cuerpo y luego de ello en results voy a tener habiendo reconocido que hay un cuerpo en la imagen todos los puntos de ese cuerpo Cuáles son todos los puntos de ese cuerpo Bueno vamos a ir a esta imagen que también les estoy pasando aquí a través del Campus para que ustedes lo puedan ver bien Si volvemos a ver los archivos que yo les dejo en el moodle está este archivo x que es el png que obviamente no les pedí que lo suban al proyecto porque no tiene nada que ver con el proyecto pero sí aporta una idea vamos a abrirlo para que ustedes entiendan Cuáles son los puntos que detecta esta nueva librería y son todos estos que están aquí Ven y aquí tiene bueno la referencia de cuál es cada uno de esos puntos Esto no es solamente descriptivo para que ustedes sepan que es un ojo izquierdo un ojo derecho este texto que está aquí también es una forma de referenciar El punto es decir que si yo por ejemplo estoy hablando de este hombro que está identificado con número 12 puedo identificarlo como el número 12 o identificarlo con este nombre de hombro derecho sí Entonces con esto que tienen Aquí tienen la guía no solamente de Qué significa cada uno de los puntos sino también una forma de llamarlo que no sea solamente numérica o por el Index referido del punto bien Ahora volvemos al proyecto entonces bien habiendo visto esa imagen Entonces yo tengo ahora que en results voy a tener la ubicación en la imagen de cada uno de esos puntos en tanto y en cuanto esté el cuerpo completo porque esto no tiene que estar el cuerpo completo quizás en este momento como estoy yo aquí de los hombres para arriba también pose me va a dar la información pero solamente de los puntos que detecta que están a la vista Es decir desde los hombros hacia arriba bien entonces lo que hago a continuación es muy parecido a lo que hicimos antes con las otras dos librerías y justamente recorro todos los lands y lo que hace Es dibujarlo sobre la imagen no solamente los puntos sino también las conexiones entre cada uno de esos puntos aquí lo que hago es destacar con estas dos líneas en principio los puntos como siempre con qué color lo voy a destacar Con qué grosor Y en este caso el radio del círculo con que va a ser referenciado ese punto Y en el otro caso todas las características que tiene que ver conexiones Recuerda que acá dibuja puntos y conexiones aquí dibujo si las conexiones con este color que especificó aquí y con un grosor de este tipo sí bien luego muy sencillo hago un Ims de la imagen resultante Sí con la detección y todos los puntos y conexiones marcas y con el wiki Espero que termine para darle enter y que termine la ejecución de este programa vamos a arrancarlo con esto y después vamos a ver cómo destacar algunos puntos y no todos Así que habiendo explicado Este programa no queda más que ejecutarlo y ver los resultados con la imagen 1 jpg ese está aquí está aquí como verán bueno destaca todos los puntos que dijimos recién o mostramos recién en imagen de los puntos que detecta y los detecta por completo porque se ve una imagen de un cuerpo completo sí bien aquí lo que ustedes podrían hacer si quieren después es Probar con atleta 1 y atleta 2 es decir cambiando la imagen pero antes de eso lo que vamos a hacer es como dijimos recién ver cuál sería el código para destacar solo algunos puntos del cuerpo y no todos bien para eso he comentado todas las líneas de código que usé recién para tocar todos los puntos del cuerpo y me propongo como objetivo destacar algunos puntos y conexiones del brazo izquierdo desde el hombro hasta la muñeca bien para eso lo que tengo que primero saber cuáles son esos puntos con el número o con el nombre tal cual el gráfico que le mostré recién volvemos a ese lugar y tengo el punto 11 el punto 13 y el punto 15 que corresponden al hombro al código de la muñeca es mi propósito podría destacar todos los que quisiera Está bien lo hago como un ejemplo bien volvemos aquí entonces lo que tengo que hacer o lo que voy a hacer es tratar de encontrar la coordenada x y la coordenada y de cada uno de esos tres puntos que la información que me da justamente resulta entonces tomo Land 11 sí que es el hombro vamos a la imagen acá sí el hombro bien y lo multiplicó sí por 600 porque Recuerden que yo ya les dije que el landmark a mí me da una ubicación proporcional a la imagen más no me da el punto Sí donde tiene que ver con la coordenada la dimensión Sí el píxel donde está ese punto me da una coordenada de 0 tanto que tiene que ver en la proporción de la imagen en dónde está ubicado eso para obtener el número del píxel concretamente lo que tengo que hacer es multiplicarlo por la coordenada correspondiente como esta imagen tiene 600 por 600 la el ancho digamos en este caso sería 600 bueno multiplico por 600 si fuera otro número multiplicaré por otro número bien esta coincidencia de que las dos coordenadas son iguales las dos tamaños digamos las dos referencias del tamaño son iguales bueno hace que siempre 600 sino como dije recién en este caso veo el ancho el número que sea y el Alto En el caso de la línea que sigue el número que sea bueno repito el proceso creando la y aquí tengo una variable que he creado pensando que es la coordenada x del primer punto que me interesa que es el hombro y repito el mismo proceso pero ahora creando la coordenada ahí Entonces ahora tengo el punto x el punto y transformado de ese punto referencial proporcional a la imagen en número constante de píxel en valor Sí bueno Esto Tan sencillo como lo hizo recién lo repito para los otros dos puntos es decir las dos líneas de las dos coordenadas del punto que sigue y es el codo y las dos líneas de las otras coordenadas que es la muñeca Entonces ya tengo en formato como dije recién de Pixel las referencias de las coordenadas del hombro del codo y de la muñeca luego de ello que tengo que hacer imaginen vuelvo a la imagen aquí lo que debería hacer sería dibujar las dos conexiones la que va de 11 a 13 y la que va de 3 a 15 bien hacia eso vamos entonces con estas dos líneas que uso el método online de cv2 dibujo dos líneas la primera va desde la coordenada x1 y 1 a x2 y 2 sí es decir del hombro a la muñeca sí tenemos aquí sería esta de 11 a 13 y luego bueno obviamente poniéndole el color y el grosor de la línea Entonces ustedes ya lo saben ya lo manejamos y luego lo correspondiente para la otra línea la que va del codo a la muñeca bien ya tengo una parte de El objetivo desarrollado que me estaría faltando es decir ya dibujado esta línea negra y esta línea negra que me haría falta este punto rojo este punto rojo y este punto rojo sí Bueno entonces lo que voy a dibujar a continuación es justamente eso y como lo dibujo con las tres coordenadas es decir dibujo un círculo en la coordenada x1 lo mismo en la x2 y 2 y lo mismo en la X3 y 3 siempre destacando el grosor y el radio del círculo sí Y obviamente el color bueno como habrán visto es mucho código pero es muy concreto en un lugar recojo la información de las coordenadas y nuestro dibujo las líneas y los círculos no están complejos a pesar de que es mucha la información Eso fue lo que cambié por el código que tenía antes las otras dos líneas son Exactamente La misma que es Mostrar el resultado de esto así que vamos con eso bueno y el resultado es este ven que aparecen los tres puntos y las dos conexiones así que ya tenemos todos los elementos que hacen a lo que sería bueno como tomar las partes del cuerpo no al completo sino solamente algunas las que nosotros nos interesa destacar bueno habiendo visto cómo usamos pose sobre una imagen ahora vamos a hacerlo sobre un vídeo Bueno si ustedes digamos se atraen un poquito de esta cuestión de lo que estamos viendo si caras sin manos o si cuerpos van a ver que este ciclo es Exactamente igual lo que tengo que hacer ahora es tomar un vídeo y lo que voy a hacer ahora es tomar cada uno de los frames de ese video por lo tanto la primer parte exactamente igual sí creo el objeto m Pedro que era el objeto MP pose y después creó el objeto pose acá vamos a incorporar un tema que están viendo que tengo una libertad bien de qué se trata eso bien acá cambiamos State image Mode porque ya no es estático es un vídeo y luego Empiezo con el Wild que ustedes ya conocen y que lo que hacemos Es justamente tomar la idea de que voy a tomar cada uno de los frames de la imagen de un vídeo o un streaming sí Entonces en este caso es un vídeo bien siempre hago el cap luego obviamente pregunto si hay algún problema ROM podíamos la estructura y terminó el programa sino tomó las coordenadas del frame sí que estoy tomando en este caso como referencia danza mp4 de la librería datos ideas aquí tenemos danza mp4 bien Nada nuevo hasta allí Sí luego igual que recién convertimos si la estructura de colores y buscamos identificar en ese video poses de cuerpos una vez que hago esto lo que tendría que hacer simplemente para los fines de mostrarlo es un size de esa frame de ese frame perdón para poder mostrarlo bueno por pantalla en algo que no se me vaya del tamaño de la pantalla en este caso he puesto 640 por 480 que es lo que hablábamos recién en este caso la imagen no es cuadrada con lo cual el reformateo de la Imagen tiene que guardar una proporción similar a la imagen original por eso aquí es lo que estoy poniendo En referencia a eso y luego bueno marco con el mismo código que usamos hace un rato con la imagen si los puntos del cuerpo y con Ims los muestro Y como siempre pongo el White 27 para que detenga el vídeo con escape si quiero detenerlo antes que termine si no terminara cuando termine qué le hemos agregado aquí algo que es un dato importante para más que nada entender un concepto importante que es Cuántos frames estoy capturando en la imagen o en el vídeo que estoy viendo Bueno lo que hago simplemente es medir la cantidad de ciclos que da este sí este Wild la cantidad de veces que da la cantidad de ciclo queda y la cantidad de capturas de pantalla que va a tener del video para eso utilizo la librería Time y arrancó con una variable puesta en cero y lo que voy a hacer Sencillamente hacer un cálculo midiendo si cada una de los horarios digamos en los cuales estoy emitiendo o capturando ese frame y haciendo un cálculo matemático para tomar la hora anterior y la hora la que se metió ese frame Y a partir de eso calcular una variable fps que es frame por segundo y bueno mostrarla con un boutex en este caso no he puesto un rectángulo como es mucho antes para que quede un fondo simplemente lo pongo sobre la en un lugar donde no moleste si esto insisto no es obligatorio es una cuestión es más seguramente en una aplicación que ustedes hagan no lo van a poner pero es un dato importante porque tiene que ver justamente con cuántas capturas hago por segundo y eso me habla de la velocidad de la cámara y es un dato importante para tener en cuenta para todo lo que yo vaya a hacer así que bueno es un agregado que ponemos aquí en el marco de esta práctica para que ustedes lo tengan presente y lo usen cuando tengan ganas de hacerlo así que ejecutamos este programa y vamos a ver entonces este vídeo que se llama danza de un chico bailando Así lo tienen y ahí tienen el fps ven va apareciendo gracias a Dios la imagen es clave se ve muy bien obviamente va variando no siempre está la misma cantidad de frames por segundo porque obviamente tiene que ver con la velocidad del vídeo y también con la capacidad de Lima bien Esto lo podemos cambiar podemos poner en lugar de ese video perdón lanza lo comentamos y buscamos otro vídeo en este caso vídeo 2 mp4 en este caso yo lo que voy a hacer fíjense lo voy a ejecutar para que vean el efecto de lo que les comentaba Hoy hace un rato respecto del reformateo de la imagen sí ven que esta imagen se ve distorsionada claramente está como como comprimida por qué Porque yo lo que hice aquí es re formatear Al igual o tomando como base el vídeo que vimos hace un rato el video que teníamos un rato tiene otro formato con lo cual este reformateo le viene bien pero la nueva imagen no fíjense que antes tenía 640 por 480 y ahora voy a necesitar que sea al revés porque justamente esta imagen es más rectangular que la anterior era rectangular pero de manera horizontal estas rectangular pero de manera vertical ahí lo digo correctamente con lo cual fíjense que acá yo le he dejado un comentario que hago un reformateo para danza mp4 que es la imagen hace un rato y otro reformateo para vídeo 2.94 que es otra vez Perdón les comenté esto tengo que descontar esto que es como dije recién en lugar de hacer 40 por 480 es 480 por 640 fíjense que ahora lo voy a ejecutar y la imagen se va de un modo más lógico se entiende o sea más parecido a lo que le macronía bueno fíjense que aquí tenemos una persona haciendo sentadillas y detecta todos los puntos del cuerpo por completo y lo va siguiendo al igual que la imagen anterior al movimiento de la persona y aquí aparece la cantidad de frames por segundo un poquito más largo lo vamos a cortar antes y lo que nos va a quedar ahora como tarea es utilizar el streaming es decir nosotros usamos aquí los dos vídeos vamos a comentar este Primero este segundo Sí perdón comento el primero comento el segundo y dejo vídeo captur 0 que ya sabemos que es el streaming Entonces yo lo voy a ejecutar ahora y va a detectar partes obviamente como dije antes de lo que ve la cámara de mi cuerpo es decir del hombro para arriba bueno y ahí está el resultado como detecta lo que ve de mi cuerpo no obviamente que yo me parara Sí detectaría bastantes cosas más sí se lo dejo para que lo peguen ustedes fíjense que acá está deformada la imagen está reformateo porque evidentemente he tomado algo más vertical una estructura rectangular más vertical que horizontal vamos a volver a la que usamos antes que es más parecida digamos a lo que tengo en el formato natural de la cámara Bueno entonces ya se pueden dar cuenta e irlo manejando ven ahí ya tiene una estructura un formativo más lógico a lo que es el formato original de la cámara Bueno lo que nos va a quedar ahora para terminar la clase como siempre es un ejemplo de una aplicación práctica en este caso algo vinculado a la actividad física rápidamente vamos a ver el objetivo ya terminado y después paso a paso cómo lograr bien aquí tenemos una aplicación que es una un contador de sentadillas si tenemos la persona que está haciendo sentadillas y aquí arriba sale justamente el conteo de las sentadillas para eso estoy detectando tres puntos del cuerpo la cadera la rodilla y el tobillo y estoy jugando con esta cuestión de detectar el ángulo que es lo que me va a permitir a mí poder darme cuenta de si vamos a ejecutarlo de nuevo para que se siga viendo decía el ángulo me va a dar la idea de si la sentadilla fue completada el ciclo de las sentadillas fue completada o no sí con lo cual acá tenemos muchas cuestiones que van a tener que ver más con la matemática y con la lógica que con la aplicación en sí la aplicación en Sí en cuanto al uso de la librería es muy sencilla no hay mucha diferencia lo que hicimos hasta ahora detecto tres partes diferentes únicas no todo el cuerpo de lo que venía haciendo si cadera rodilla y tobillo y después toda la lógica es empezar a ver como dibujo todo eso que dibujé allí esa sombra esa figura y cómo voy trabajando para reconocer la variable del ángulo que me va a permitir a mí saber si las sentadilla fue completado no Y luego el conteo que es lo que puse en el vértice superior izquierdo Así que son varios objetivos lo vamos a ir haciendo en etapa vamos a hacerlo en tres etapas con lo cual aquí por eso ustedes tienen tres programas que son para lo mismo pero he decidido separarlo para que puedan verlo justamente en cada una de esas tres etapas Así que vamos al primero de ellos que es contador sentadillas 1 Vamos a cerrar el objetivo final y Bueno aquí lo que tienen es un vídeo de entrada que ya lo usamos recién video 12 mp4 creo el objeto pose como lo venía haciendo el Wild true para cada uno de los frames del movimiento de esta persona y así como en el caso anterior detecte todos los puntos de esta persona Ahora como dije recién solamente me interesan tres Cuáles son los tres el 24 el 26 y el 28 De dónde saco esta información bueno como dijimos antes en el gráfico ese que yo les pasé con todos los puntos del cuerpo lo saco de así y al igual que como lo hicimos antes Tengo que reconocer no el punto que me tira el landmark que decir esta información que está aquí sino Cuál es el píxel y para eso como vimos antes Tengo que multiplicarlo por el valor del ancho de la imagen o el valor del Alto de la imagen entonces lo que hago al igual que como hicimos hace un rato con el con la imagen este del primer ejercicio bueno tengo que ver el x1 y 1 x 2 y 2 x 3 y 3 para obtenerlos lo multiplicó por el ancho y por el alto de donde sale Y dónde sale de aquí arriba sí que creo estas dos variables que salen de el método shape sí de frame shape no me da solamente Esa información me da como salida tres valores bueno como el tercer valor a mí no me importa esto creo que lo hemos visto antes lo que yo hago es ponerle un guión bajo cuando los datos que me arroja en este caso este método de Jade o cualquier otro si no me interesan todos yo lo que puedo hacer es en lugar de crear una variable que no voy a usar directamente le pongo un guión en aquella salida que no me interesa si no pongo Esto me daría error porque me dice se esperaban tres variables y solamente puse dos Sí bueno detalle propio más de python que otra cosa bien una vez que tengo Esto entonces como dije recién voy a este deducir Cuál es el píxel de cada una de esas coordenadas recuerden cadera rodilla y tobillo y hago lo mismo que hicimos en la práctica número 1 sí la abro aquí por pose img si se acuerdan esto que había hecho aquí estas líneas donde averiguaba las coordenadas y después dibujaba las líneas y los círculos es exactamente lo mismo que tengo acá sí Solamente que en este caso lo hago para un vídeo y en aquella oportunidad lo hice para la imagen Bueno luego lo que hago es Mostrar el Franco ni myo y con un wiki propongo la posibilidad de cortar el vídeo antes de tiempo y si no seguirá hasta el final bien con esta primer parte lo que yo tengo la vamos a ejecutar es lo que aquí vamos a ver Sí bueno la persona con los tres puntos detectados lo que vamos a hacer a continuación en contador sentadillas Dos punto Pipe es ver cómo dibujamos si las líneas entre esos puntos la línea que va o que me conforma ese triángulo porque yo voy a manejar las líneas de dibujar Perdón las líneas que van sobre la pierna de la persona pero también hay una tercer línea que es la que completa El Triángulo es decir esas dos que marcan las piernas de la persona perdón y la otra que une los puntos que irían desde la cadera hasta el tobillo bien Esto es importante para poder obtener el dato del ángulo bueno resulta que ese triángulo yo puedo dibujarlo sobre el frame y no hay ningún problema pero quiero recurrir a un detalle tipo efecto para que ustedes siempre aprendan algo nuevo no voy a dibujar el Triángulo de esa forma con el relleno que ustedes vieron en la imagen original porque la idea mía es que ese relleno aparezca atenuado es decir que no aparezca sólido porque si no taparía completamente la imagen La idea es hacer una un detalle en este caso el dibujo del triángulo pero en una manera atenuada permitiendo ver el fondo original de la imagen para eso en lugar de ponerlo con Bueno siempre fíjense que acá dibuje el círculo pero no dibuje las líneas porque justamente tiene que ver con esto que diversión vamos a hacerlo sobre otro espacio que voy a generar automáticamente voy a crear un fondo que se va a llamar aux no existe lo voy a crear acá cómo lo creo con dos ceros sí es decir hago como si fuera un fondo se acuerdan como hicimos el rectángulo Pero bueno un fondo sí que yo lo voy a utilizar solamente para este truco de poner una imagen encima de la otra de manera atenuada ahora sobre ese fondo yo voy a dibujar estas tres líneas Sí ese fondo de ella va a tener la misma estructura y la misma dimensión de la imagen de la cámara sí bien Por eso utilizo las mismas coordenadas que tenía antes si x1 y 1 x 2 y 2 x 3 y 3 para dibujar en este caso las líneas de conexión sí bien Aquí tengo la primer línea que va desde la cintura hasta la rodilla Aquí tengo la segunda línea que va desde la rodilla hasta el tobillo y aquí tengo la tercer línea que va de x1 X3 es decir una línea que pasa por atrás de la persona que completa El triángulo que va desde la cadera y hasta el tobillo sí se entiende o sea de la cadera al tobillo de la cadera de la rodilla de la rodilla el tobillo dibujando ese triángulo luego creo un array que le voy a llamar contours sí que básicamente tiene que ver con los contornos que dibujan ese triángulo para qué Para rellenar ese triángulo y para rellenar ese triángulo uso el método film quiere ser llenar y poli de polígono sí de cívico vamos a detener la explicación un segundito aquí para que no pierdan de vista lo siguiente todo lo que estamos haciendo como el rellenado del triángulo y el dibujo de las partes del triángulo Recuerden que lo estoy haciendo sobre un espacio diferente al frame sobre aux y por lo tanto ahora vamos a crear una variable output en el cual voy a fusionar el frame con image esto lo hago a través de el método add sí que permite justamente eso solapar una imagen sobre Otra ahora qué voy a solapar frame y más pero me permite con este parámetro que está acá y con este parámetro que está acá especificar qué nivel de tonalidad quiero que tengas imagen en el caso del frame sí uno le estoy diciendo que vaya a la imagen al 100% y en el caso de aux y más con 07 le estoy diciendo que vaya a un 70 ciento de su digamos contraste natural Así que se vaya desatenuando para generar un efecto de transparencia que permita ver la imagen del fondo que del frente Sí y luego Obviamente el ime show ya no lo voy a hacer sobre frame sino sobre output que es el que tiene las dos imágenes solapadas vamos a ver el resultado de lo que programamos bien Entonces tenemos ahora los tres puntos que teníamos en el caso anterior en la versión 1 de este programa Y tenemos las líneas que dibujamos de cadera a rodilla de rodilla tobillo y la imagen de la línea que va por atrás de la cadera al tobillo vemos el triángulo que como está con un 70% fíjense que deja ver Sí bueno la parte de la pierna y el pasaje de atrás Bueno lo que sea digamos no tapa la imagen es un detalle pero es importante que entienda ahora pueden ver cómo justamente poner una imagen encima de otra y hasta manejar el tono con que quieren que salga esa imagen antes de ver el trabajo final de la parte final de este programa la versión 3 vamos a entender lo siguiente lo que vamos a asumir acá es que cada vez que esta persona tenga una un ángulo que supere los 160 grados va a querer decir que está en la parte superior de la sentadilla y cada vez que el ángulo sea menor a 70 grados ejecutó de nuevo para que lo veamos bien Voy a considerar que la sentadillas está terminada obviamente la persona nunca va a estar a 180 grados para ver que terminó la sentadilla y tampoco hasta las 0 grados si en una inclinación que no sería físicamente imposible para ver que las sentadillas terminó Así que estamos esos valores que son valores aproximados todos nosotros podemos ir trabajando lo de acuerdo a lo que consideremos más ajustado nuestra realidad bien para eso lo primero que voy a hacer es luego de aquí donde tenía el dibujo sobre el frame a generar tres variables p1 p2 y p3 en formato de array con la combinación de cada una de las coordenadas ese con p1 voy a generar un array con las dos primeras coordenadas con p2 lo mismo con lo mismo para que luego con esto pueda calcular las distancias euclidianas sí es decir p2p3 p1 - p3 y p1 más las Tres formas Esto me va a llevar a la siguiente imagen que estamos viendo aquí que me permite a mí a través de un algoritmo matemático poder reconocer el a través del arcoseno el ángulo o el valor del ángulo que se va formando en cada frame esto lo voy calculando conforme vaya avanzando cada frente sí la fórmula es esta que está aquí y bueno la lógica es esta que se muestra Aquí sí es decir la fórmula con los tres lados y justamente con esta expresión matemática que está aquí me devuelve el ángulo ahora volviendo al código de python le aplicó a ese resultado a la función de Gris para obtener ese resultado en grados y lo pongo dentro de la variable bien con esa información ahora lo que tengo que hacer es voy a generar una variable tipo bandera que me diga si la persona completó la sentadilla o no Para eso voy a crear una variable que he declarado aquí arriba que se llama abajo Falls y lo que voy a porque voy a considerar que va a arrancar de arriba con lo cual la variable abajo O sea que está abajo es falso quiere decir que este justamente La idea es que si el ángulo es mayor a 160 grados es decir que está en la parte superior y abajo es quiere decir que completó una sentadilla por lo tanto cambio esa ese estado de esa variable lo paso a false Sí porque no está abajo Sí y empiezo a contar con el contador que también he declarado aquí arriba la cantidad de sentadilla sí es decir que va a arrancar de arriba Cuando haya llegado abajo y haya vuelto a llegar arriba va a ser cuando haya completado una sentadilla por lo tanto el ángulo va a ser 160 grados abajo va a ser tú que sé que alguna vez llegó abajo es el último lugar donde estuvo fue abajo entonces quiere decir que paso abajo a Fast y cuento uno por el contrario si llego al final de la central a la parte más baja de la sentadilla Si no al ciclo completo al medio ciclo lo que voy a preguntar es si el ángulo es menor a 70 que es lo que voy a tomar como referencia para eso y voy a poner la variable abajo en true que se llegue abajo y el ciclo se va a volver a repetir es decir va a ser en algún momento a superar el ángulo de 160 grados porque se va la persona hacia arriba y bueno cuando vuelve a superar ese ángulo vuelvo a preguntar si ya viene de abajo si llegó abajo si llego abajo cambio esa variable a false y cuenta una nueva sentadilla cada valor que cuento lo que hago es ponerlo sobre la imagen como lo está haciendo sí es decir que en este caso voy a poner un civil rectángulo para dibujar ese cuadrado de color verde intenso que vimos recién y luego el count en color blanco sobre qué sobre frame no sobre output que es la imagen final con la que vengo trabajando sí y bueno con eso está terminado el ejercicio lo ejecutamos y vemos la que ya vimos antes sí el efecto de el dato del ángulo Perdón también lo puse esto estaba sí tengo dos botes Perdón Tengo este pudtex que es el con que es la cantidad de sentadillas y también es un podtex del ángulo que sale adelante de la rodilla de la persona sí ejecutamos de nuevo para que también se vaya viendo Ven aquí para que también se vaya viendo le podría haber puesto un fondo Bueno lo pueden hacer Ustedes sí este bueno para que se vaya Sabiendo esta lógica de cómo van cambiando los ángulos y cómo va cambiando las sentadillas sí es decir ahí llega a 4 llega menos de 70 llega más de 160 5 sí es decir cada vez que llega a 1970 perdón a menos de 70 y sea a más de 160 bueno se completó la sentadilla y Por ende cambia ese com y es lo que parece hacia arriba hasta aquí llegamos con esta clase número 23 ha sido un poquito más larga esta segunda parte les dejo algunos desafíos para que puedan pensar en virtud de esta misma este mismo tipo de actividad lo pueden trasladar a cualquier tipo de actividad pero bueno Obviamente el estilo dos ejemplos uno puede ser poder ver flexiones de brazo sí que en realidad si está la imagen de frente de la persona no haría falta trabajar con el ángulo había que ver la altura de los hombros y calcular En qué altura se puede considerar que la persona está arriba y que altura se puede conseguir que las personas trabajo Sí si estuviese de perfil ahí sí podríamos ver justamente el mismo ejemplo del ángulo con el hombro el codo y la muñeca y también podemos ver justamente con esos tres elementos por ejemplo una persona que está haciendo un contador de bíceps sí siempre puesto de perfil por supuesto No y tener la misma lógica del ángulo que aplicamos recién obviamente todas estas cuestiones este llevadas al campo de la actividad física pero pueden hacerlo y aplicarlo para cualquier otro ejemplo que no tenga que ver con esto bueno hasta aquí esta clase nos vemos en la próxima clase aquí termina esta clase los espero en la próxima clase nos vemos  Titulo: Clase24 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/Eyo9M8LH01s  \\n 2146 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número  24 Hola a todos Bienvenidos a la clase número 24 del curso de Inteligencia artificial de El ifes eh Esta es la última clase de redes neuronales convolucionales donde vamos a ver dos temas muy importantes como todos los que hemos visto antes en este caso en la primer parte de esta clase vamos a ver el tema de tracking y en la segunda parte de esta clase vamos a ver Cómo identificar texto desde una imagen así que bueno vamos a empezar con el primer tema de ellos que es el tracking y vamos a recurrir a lo que vimos algunas clases atrás cuando vimos el tema de la detección de objetos y recordarán esta imagen donde estaban en un centro comercial y detectamos a las personas donde ponía digamos el dato de de la persona en este caso de la clase que identificaba que eran personas y en otros casos detectaba también en mochilas y o carteras Bueno aquí uno puede presumir si entiende que el concepto de tracking es el seguimiento de un objeto que eso que estamos viendo aquí es un tracking en realidad no es un tracking porque en realidad el tracking implica no solamente el estar detectando el objeto sino que ese objeto se ha identificado de manera única y diferenciándose claramente del resto de los elementos que aparecen en la imaje vamos a poner un poquito más de Claridad sobre el concepto de tracking concretamente lo que tenemos aquí y siguiendo Este ejemplo que tomamos recién es la detección de un grupo de personas vamos a tomar esta persona que está aquí y en la siguiente imagen vemos Que seguimos identificando a la misma persona esto a la vista parece un tracking pero no es un tracking desde el punto de vista de la información que genera este algoritmo en esta instancia Yo tengo un grupo de detecciones en la cual tengo la detección de esta persona en la siguiente instancia siguiente frame tengo detección de todo un grupo de personas entre las cuales veo o identifico la misma persona bien Esto es así pero no hay ninguna relación entre la persona del frame anterior y el frame siguiente Por más que nosotros visualmente veamos que se trata de la misma persona si yo quisiese identificar a esa persona con un número supongamos número 15 no está identificado En el frame anterior con 15 y en el siguiente frame con 15 esto de esto se trata del tracking y sobre esto es sobre lo cual vamos a trabajar en la primer parte de esta clase el tracking lo vamos a manejar con Yolo es uno de los modelos o el tipo de actividades que se lleva a cabo Yolo como vimos cementación como vimos clasificación como vimos detección bien aquí tenemos el espacio donde se explica el sitio oficial de ultralytics por Yolo el multi tracking voy a bajar un poquito hasta aquí y voy a ver que acá tengo un ejemplo de un código que si ustedes lo lo llevan a su proyecto lo van a poder ejecutar sin guardar demasiada diferencia respecto de lo que nosotros hicimos antes en la detección de hecho el modelo que se usa es el mismo que la detección solamente que en este caso el método sería Track y luego utilizo el anotate frame con el método plot para justamente detectar y dibujar el recuadro en cada objeto nosotros vamos a hacer algo un poquito diferente en este caso porque vamos a buscar por primera vez generar ese recuadro por nuestra cuenta y vamos a aplicar algunas cuestiones que después van a concluir en el proyecto cuando cerremos el proceso de tracking completo para hacer como empezamos Siempre vamos a crear un proyecto nuevo y vamos a tomar todos los archivos que yo les dejo a través del campus virtual que son todos estos que estoy marcando aquí y como hacemos habitualmente creo una carpeta que le voy a poner tracking y voy a copiar dentro de esa carpeta todos estos archivos bien ahí están copiados todos los archivos entre ellos hay archivos vinculados a datos sí como estos 2 mp4 en este caso no he hecho la carpeta la famosa carpeta datos Pero bueno este vamos a dejarlo así por ahora y luego los archivos los tres pun p que van a ser parte de este proyecto Así que vamos a crear el proyecto vamos con todos estos archivos que están dentro como siempre diciéndole que quiero crear un proyecto sobre una carpeta que ya tiene archivos y que quiero usar Su contenido para que forme parte de proyecto Bien acá estamos con el proyecto creado ya y vamos a empezar a trabajar con el primer programa que es tracking punp bueno aquí empezamos por importar las librerías que venimos usando en los últimos proyectos cb2 y ultr litic para Yolo Y también vamos a usar math y Random que no hace falta incorporarlas desde lo que venimos habitualmente utilizando file settings sí Ya están incorporadas dentro de El p bien lo que a continuación hacemos Es tomar un video people mp4 que es un grupo de personas que están en un centro comercial tomadas desde arriba y Eh bueno creamos el modelo con el modelo Nano de Yolo v8 sí obviamente esto si ustedes quieren Lo pueden cambiar al modelo X o el modelo que ustedes quieran como ya dijimos antes no vamos a dibujar los bonding Box en este caso nosotros los recuadros y no tomando lo que hace naturalmente iolo para ese caso también vamos a agregar un pequeño detalle important an un detalle de color justamente que es que vamos a tratar de que cada bonding Box sea de un color diferente ustedes recordarán inclusive en la imagen que misen que todos los bonding Box son los mismos en tanto y en cuanto representan la misma clase Ahora vamos a dibujar bonding box de diferentes colores sin importar que sea la misma clase u otra para ello voy a poner este código que está acá que es crear una matriz de colores con lo cual voy a generar a través de un Run int valores que van de 0 a 55 para el primer color para el segundo y para el tercero y con un for digo que lo quiero hacer 100 veces así que voy a tener 100 combinaciones de colores diferentes obviamente puede hacer 100 o el número que ustedes quieran en este caso hemos elegido 100 para tener una idea aproximada luego dado que e bueno con esto que vamos a hacer podemos detectar cualquier tipo de objeto y queremos rescatar ar eh su su etiqueta vamos a crear justamente una Ray con los nombres de los 80 objetos de coco y hice esto que he puesto aquí a continuación empezamos con el wtr que venimos usando regularmente donde hago un Red ox Access y frame de El C R luego voy a tomar todas las detecciones pero con el método Track sí en donde le voy a poner que la tiene que tomar the frame decir de la captura de pantalla y con una confianza de El 50% una vez que tengo eso como siempre tomo todos los resultados de las detecciones y empiezo a recorrerlas y dentro de esa recorrida voy a tomar los datos de result boxis Data Y eso con el método tulis lo voy a convertir en una lista y lo voy a poner dentro de esta variable r es decir por cada detección de este frame Sr va a devolverme a mí siete valores y Por ende en la siguiente línea igualo justamente esa variable r a siete otras variables que recogen esos valores en principio las cuatro coordenadas el ID el score el nivel de confianza y la clase detectada todos ellos salvo el score son valores de tipo texto Por ende lo voy a tener que convertir a partir de int a un valor de tipo entero a continuación Quiero obtener el nombre de la clase Por ende lo que voy a hacer es justamente con el valor del Class ID voy a tomar desde la matriz Class names ese su índice y Por ende voy a obtener el nombre de la clase es decir que si el ID detectado es cero voy a hacer el elemento cero de la matriz Class names y voy a obtener el nombre si veo la matriz Class name justamente el elemento cero es person y Por ende esta variable Class name va a tener el texto person a continuación voy a mostrar el nivel de score es el nivel de confianza en la predicción y a través de matell voy a redondear y mostrarlo a través de una variable conf con ello preparo un texto en el cual lo voy a poner en una variable Label poniendo por un lado el nombre de la clase detectada y el nivel de confianza expresado del modo que calculamos recién y con ello voy a crear toda la estructura para empezar a hacer lo que dijimos recién el rectángulo con un color diferente en cada caso y el texto arriba personalizado a como lo queremos nosotros a continuación voy a dibujar el rectángulo como dijimos anteriormente con un color diferente de acuerdo a cada detección sin importar que sea el mismo objeto o no la misma Elmo tipo de clase o no y lo voy a hacer con rectangle sobre el frame con las coordenadas detectadas Sí en cada uno de los results en este caso x1 y1 marca como siempre el vértice superior izquierdo y x2 y 2 el vértice inferior derecho luego en el caso de los colores siempre le poníamos un color fijo en este caso lo voy a armar con la siguiente lógica en principio voy a tomar el valor ID para que eh vea que cada elemento es un elemento diferente y voy a operar justamente en virtud de la cantidad de colores que tiene la la matriz colors sí que dijimos que tiene 100 y voy a ir tratando de identificar de esta manera una construcción que haga que cada Ed va a estar identificado con un color diferente y una vez que haga eso justamente voy a poner esto en el lugar donde antes Yo ponía un valor fijo y con tres como siempre específico bueno el nivel de de de ancho digamos de ese rectángulo m bien luego tenemos otra cuestión nueva que creo que es importante que es calcular el largo del texto para qué Porque después yo voy a poner un rectángulo que va a ser el fondo en el cual va a descansar todo este texto que texto el el nombre de la clase y el nivel de confianza eso tiene un largo lo calculo aquí con la función de cb2 get tch size Sí y luego pongo ese valor en una variable t size Por ende en el próximo paso voy a poner dentro de es2 que es otra variable que creo justamente las coordenadas que debería tener ese futuro rectángulo que va a ser el fondo del texto en virtud del largo del texto a colocar bien y en la siguiente línea dibujo justamente ese rectángulo en donde digo que sobre el frame voy a poner x1 y1 pero la segunda coordenada de ese rectángulo va a ser C2 C2 es esto que he calculado aquí a partir de el largo detectado en esta otra línea de aquí luego le pongo como siempre men1 para decir que es un rectángulo que tiene que estar relleno y Pongo aquí el tipo de línea que quiero que tenga y finalmente el texto es decir pongo el put text con el frame sobre el frame perdón el Label que es el texto que tengo que Mostrar que es esto que está aquí la clase y el nivel de confianza y las coordenadas x1 y 1 sí -2 le pongo justamente para que no quede muy encimado en el principio del rectángulo y el resto de la información que venimos usando habitualmente el color bueno el ancho el tipo de de texto digamos etcétera etca el tipo de fuente una vez que tengo armado todo esto bueno no me queda más que hacer el show que hacemos siempre y el White Ke con 27 para si yo quiero cortar el video antes de tiempo así que bueno he expresado toda la lógica de este programa no nos queda más que usarlo ejecutarlo y Ver el resultado ven que todas las personas tienen un color diferente y aparece arriba del recuadro en el mismo color que el recuadro el tipo de clase y el nivel de confianza fíjense dat muy importante que acá No hay tracking y lo veo del color fíjense que el color o los colores van cambiando conforme van pasando los frames sí es decir vamos a hacer una persona acá esta persona tiene un color luego otro ven luego otro luego otro eso me da la Pauta de que AC hay una detección pero no hay un tracking es decir cada frame es una instancia totalmente nueva y esa persona que voy siguiendo para el siguiente de frame es una persona que no tiene nada que ver con la anterior Por más que nosotros visualmente pensemos que es laa persona bien habiendo terminado esto ustedes tendrán la sensación de decir Bueno pero estamos teniendo el tracking aquí en realidad todavía no eh o mejor dicho tenemos una parte pero nos falta otra parte muy importante lo que hemos logrado aquí es utilizar la función tracking de iolo que tiene el mismo modelo que la que usamos para detectar pero me va a tirar otra información para que yo con otro programa pueda lograr el tracking de la forma que yo se lo expliqué a ustedes recién si hemos logrado que no lo habíamos hecho antes hacer el recuadro el bounding box por nuestra cuenta con la información que me da la detección y también poder lograr este efecto de poder este poner de un color diferente que ya lo vamos a usar también en el tracking a las diferentes personas solamente que el color va cambiando como decía recién y e finalmente Bueno este truco de poder calcular la longitud del rectángulo en virtud de la longitud del texto Bueno qué es lo que vamos a hacer ahora vamos a buscar un programa que es público que se llama sort Pi y que me va justamente a dar lo que me está faltando hasta ahora para junto con la función Tracker de Yolo poder hacer definitivamente el tracking que nos propusimos sobre la imagen que vimos recién de esta gente que está caminando sobre el centro comercial Así que primero vamos a ir a el github que comparte este programa sort pii bien aquí estamos en el github de sort pii con lo cual fíjense que aquí está todo el código de sorp y lo puedo bajar directamente desde aquí o bien copiar y pegar lo concretos que no hace falta nada de eso porque yo se los puse dentro de los archivos que iban en el campo virtual Y ustedes ya tienen ese archivo dentro del proyecto ven aquí está dentro del proyecto Sí bueno Esto lo vamos a usar para poder justamente hacer la parte que nos está faltando no vamos a meternos en el código de sorpi quien lo quiera consultar por supuesto puede ir al github Pero bueno es algo que está para ser utilizado como si fuese una librería La diferencia es que est librería yo puedo ver el código y si quisiese en algún momento podría eh corregir algo o cambiar algo en principio no es el propósito de esta clase por ahora sino usar eso Como si fuese una librería cerrada sí o una caja negra Más allá de que lo tenemos que traer a nuestro proyecto porque si no no lo podemos usar bien con lo cual ahora vamos a ir a el programa tracking coating people y le he puesto entre paréntesis hundir Por qué Porque ustedes ven que las personas van en ambas direcciones lo que vamos a hacer aquí es trazar una línea que la cual va a ser la línea que nos va a permitir o a tomamos como referencia para contar si cuánta gente está pasando esa línea pero lo vamos a hacer en una dirección Sí es decir como gente que va en un sentido y lo cuenta pero si va en el otro sentido no lo va a contar con lo cual parecería con por ejemplo un lugar donde yo quiero contar cuánta gente entra y no cuánta gente sale Bueno se puede contar en nas direcciones por supuesto no hay ningún problema pero bueno le vamos a Bucar una lógica un poquito más compleja para que ustedes Tengan algo más parecido a una realidad que puede tener que ver justamente con la o detectar de una manera diferente la cantidad de gente que entra que sale por lo general uno no cuenta ambas cosas salvo ir a ver cuánta gente concurrió pero por lo general se toma un indicador en salida y en entrada y no la suma de ambos Sí bueno con este ejemplo un poquito más complejo ustedes van a poder armar en las en las dos direcciones s si la detección justamente complementando la parte que hacemos una dirección en la otra dirección contraria bien lo primero que vamos a hacer aquí en este caso es determinar en la imagen Cuáles serían los límites para trazar Esa esa recta esa línea que la vamos a dibujar para que sea visible después no hace falta que sea visible pero en principio para poder chequear esto que estamos haciendo lo vamos a hacer visible bien los límites para eso son estos que he puesto aquí es decir que es una coordenada de inicio de una línea y la coordenada de la línea anterior lo vamos a ejecutar antes de terminar de hacerlo para poder justamente entender este concepto de la línea y la línea anterior esta es la línea y esta es la línea anterior entonces fíjense que acá va en cero y yo voy a contar a esta persona porque acá atraviesa la línea anterior y luego va a atravesar la línea definitiva Ahí está pasando y cuando pase este señor va a contar uno ven ahí pasó a uno por qué hago esto de la línea anterior y la línea principal porque es la manera o una de las maneras por lo menos que se me ocurra a mí de poder detectar si la persona va en esta dirección o en la otra cuando la persona vaya en otra dirección no habrá una línea verde del otro lado que me quede registrada para saber que esa persona que está cruzando aquí antes pasó por la línea Verde La idea es que cuando llega la línea roja si pasó por la línea verde la cuenta y si no pasó por la línea verde no la cuenta Cuáles son las personas que no van a pasar en la línea verde las que vienen Como por ejemplo este 53 que viene acá de arriba hacia abajo los que vienen de abajo hacia arriba son los que van a pasar por la línea verde y luego por la línea roja Esa es la idea de esas dos líneas volvemos aquí las coordenadas de esas dos líneas son estas las coordenadas por ahí eh una receta que les puedo dar justamente para trabajar esto es que tomen una captura de un video lo lleven a un editor de de imágenes y puedan justamente teniendo en cuenta las dimensiones sí de del video establecer Cuáles serían las coordenadas del la x y la i de un lado y la x y la i del otro para la línea limits que sería la roja y limit a lim and que sería la verde bien seguimos con lo siguiente que no reviste mucha diferencia respecto de lo que veníamos trabajando porque tenemos el wth la captura del frame de las imágenes el tracking esto que está aquí puntos ofensivos Ya lo vamos a dejar para más adelante y luego Bueno lo mismo que veníamos haciendo hasta este momento que era justamente capturar todos los datos de las detecciones los bonding Box sí de cada una de las detecciones la información de la clase y todo el resto de lo que venía trabajando en el programa que vimos recién Sí tracking punp este de aquí y lo que vamos a hacer a continuación es en principio preguntar si la Class name es persona o sea in una matriz que solamente tiene un elemento person si fuese si hubiese Perdón otros elementos más pondría allí person coma Track coma car coma en este caso me interesa contar personas con lo cual lo que tengo que detectar son personas bien lo que voy a crear aquí es una variable carrent array que justamente la voy a crear con np aray y le voy a pasar cinco valores las cuatro coordenadas y el nivel de confianza en la detección y luego ese ese vector de cinco posiciones lo voy a ir apilando formando de esta manera una matriz en la dete variable que recién dije salteamos si esta línea que está acá y tenía que ver con esto que justamente estoy creando acá un nmt si una matriz vacía que la voy a ir llenando como la voy a ir llenando con cada una de estas líneas Es que voy a tener una matriz Por qué Porque voy a tener varios vectores de cinco posiciones y en cada vector es una detección cuatro coorden y el nivel de confianza sí y así el resto por qué Porque ese es el formato que necesita como entrada s. pii sí Entonces cómo lo hago con npb stack de vertical una pila vertical b stack y lo hago con detections la matriz que ya tengo creada más current array Y dónde lo pongo en detections O sea que tomo la matriz le voy poniendo una fila nueva y la pongo la sobreescribas voy haciendo cada vez sí formando esa pila esa pila va a terminar siendo una matriz de cinco columnas y tantas filas como elementos se haya detectado esta esta captura del video teniendo en claro eso lo que voy a hacer es empezar a usar sorp sorp para utilizarlo en principio lo tengo que hacer es justamente declararlo aquí arriba y decirle que from sort import asterisco o sea que traiga todo el contenido de sort sí lo abrimos el lo recordo recordamos qué es esto que está aquí sí O sea voy a acceder a todas las funciones que tiene soft sí Recuerden que para hacer esto aquí lo hacemos muy fácil porque s está en la misma la misma estructura digamos de dependencia Sí en la misma jerarquía que el resto de los programas si estuviese dentro de una carpeta o algo por el estilo obviamente tenía que ponerlo aquí en el la llamada digamos al programa sí acuérdense que aquí sort no es como ultralytics no es como cb2 que son librerías que yo he incorporado al proyecto sort es un programa que tranquilamente podría ser un programa que lo hubiera programado yo mismo bien entonces aclarado esto lo que voy a hacer justamente con sort es crear una variable Tracker Y esa Tracker la voy a crear a partir de crear una instancia de sort del del método que tiene adentro esa librería sort y le voy a pasar tres parámetros que que tien que ver este Max age es cuánto tiempo va a durar un tracking si ese tracking o ese objeto desapareció de pantalla vamos a suponer yo detecto una persona y en algún momento esa persona sale esa persona sale Y puede volver a entrar a la imagen al recuadro de la imagen Bueno si eso no ocurre antes de los 20 segundos ese número de ID se perdió o sea esa persona era cinco el número cinco quedó liberado para que otra persona nueva que entre a la imagen sea etiquetada como cinco eh min hits es la cantidad de intentos que hace para que esa detección sea eh tomada como verdader decir yo capturo el número cinco en la siguiente imagen supóngase que yo eh No detecto a esa persona en la siguiente secuencia bueno va a ser tres intentos para que siga viva ese siga vivo ese tracking sí es decir Esa persona fue detectada en esta imagen le puso cinco en la siguiente imagen esa persona no importa Por qué Porque se crusa alguien alfrente por lo que fuere porque no no es tan legible eh No es detectada nuevamente con lo cual se pierde hace cinco no hace tres intentos y luego esos tres intentos si efectivamente tres veces no lo pudo detectar ahí sí se libera es cinco como en el caso anterior dijimos que se hubiese salido de pantalla y treshold iou threshold tiene que ver justamente con la veracidad de la del recuadro en virtud de la detección bueno en este caso todos estos parámetros se configuran con el método Sword y creo una var Tracker con esas características aclarado esto voy entonces a Tracker update es decir voy a darle voy a actualizar los datos de sort justamente Con qué con la matriz detections que hablamos recién que dijimos tenía apiladas todas las detecciones de una determinada secuencia bien entonces una vez que tengo esto lo que hago es justamente Llamar update Tracker y esos resultados lo pongo en una variable que se llama results Tracker esa result Tracker va a ser justamente el insumo mío que me va a permitir empezar a trabajar con cada una de esas detecciones y un número de identificación un número de ID bien lo primero que tengo que hacer antes que nada es dibujar la línea para el límite del conteo la línea Roja se acuerdan que vimos recién y luego la línea para establecer la dirección que era la línea verde esto civ Line c l no tiene mucho secreto porque es lo que hablamos recién y lo hace con las coordenadas limits s 0 1 2 3 y lim 0 1 2 3 que es lo que habíamos visto insisto aquí arriba sí Bueno ahora como dijimos recién Perdón dijimos que en Tracker estaban tod los track de todas las detecciones sí Entonces ahora qué hago lo voy a recorrer como antes recorrí las detecciones ahora voy a recorrer los tracking los tracking son las mismas detecciones solamente que ahora tiene un dato más que es el id bien Por eso el result me arroja cinco valores de salida de nuevo las cuatro coordenadas que son exactamente iguales a la que yo le pasé pero le agrega ahora el ID que se creó dentro de Z o sea ahora cada uno de estos recuadros tiene un número y la idea es que ese número conforme ese recuadro vaya avanzando permanezca vinculado a ese recuadro y a esa persona bien como hicimos hoy paso todos los valores que son devueltos por este result de manera String a texto sí o a entero H bien y vuelvo a hacer el mismo jueguito que hice hoy con el tema de los colores porque quiero que justamente Más allá de que cada imagen tenga un número de D diferente tenga también un color diferente entonces calculo justamente en base al ID con la misma lógica que usamos recién el programa tracking un color bien entonces para este ID el color va a ser este uno de los 100 Recuerden que el tema de que sean 100 es una cuestión arbitraria ustedes la pueden determinar como que sean más colores o no bien Entonces luego lo que voy a hacer Voy a dibujar el rectángulo del bonding box s sobre frame con las dos coordenadas x1 y 1 x2 y 2 Con qué color con el color color que es el que acabo de determinar recién o sea el color que está identificado con Sid y un grosor de tres luego voy a armar un Label con un texto que diga ID dos puntos espacios y el número de ID s número de ID que lo tengo que transformar una incoherencia recién lo transformé a entero texto Pero bueno es para justamente formar parte de este Stream y el strip lo que hago es que bueno le borre los este los espacios que pueda tener a la izquierda o a la derecha Luego de eso al igual que hoy calculo el largo de ese texto que en este caso va a ser simplemente un número más este texto ID perdón y bueno juego con eso para establecer justamente el largo del próximo rectángulo que va a ser el rectángulo de fondo donde va a salir el ID y finalmente pongo el ID propiamente dicho Sí con puttext pongo el ID es decir este String perdón de ID bueno el resto de los datos ya lo manejan que son las dimensiones los colores etcétera etcétera Sí qué pasa en este caso Cómo determino cuando la persona cruza la imagen vamos a ejecutar de nuevo el programa para ver eso quizás ese detalle pasó de largo pero para que lo puedan entender bien el recuadro es un recuadro muy grande Cómo determino cuando la persona cruza bueno a cada persona fíjense que le he puesto en el centro del bonding box un puntito Entonces ese punto veamos acá el caso cinco que es la primera persona que cruza el rojo fíjense que acá está cruzando el borde Superior y la contabilidad sigue en uno cuando cruce este punto a la línea ven Perdón estaba en cero recién va a pasar a un Entonces el elemento central aquí a tener en cuenta es si pasa ese punto por la línea o no Porque Obviamente el rectángulo es muy largo y bueno tomar una postura otro no tenía mucho sentido tomo el centro de la imagen bien eso en realidad lo tenemos que armar el siguiente modo Cómo calculo el centro bueno fácil tomo las dos coordenadas x las dos coordenadas y con eso calculo el ancho y el alto y una vez que tengo eso divido por dos una cosa divido por dos la otra y obtengo el centro con lo cual en CX y C tengo el centro de ese rectángulo una cuenta matemática muy sencilla y una vez que tengo ese dato escribo o defino el círculo con las coordenadas CX y C es decir el centro de ese Bond s bueno el resto insisto son colores relleno el ancho et el grosor Perdón etcétera etcétera ahora viene la parte final y para eso vamos a ejecutar Este programa nuevamente para que se entienda la lógica de lo que vamos a hacer Voy a generar un vector en donde acumule todos los ID que pasaron por la línea verde es decir este vector tiene en este momento TR 5 ahora va a tener 33 pasa esta persona etcétera etcétera y voy a tener otro vector en el cual voy a acumular todas las personas que pasaron por la línea roja o sea ya tengo el tres al 5 y ahora se va a sumar el 33 ahora el 45 se suma al vector verde por de alguna manera y ahora el 45 se suma al vector rojo para qué hago esto porque la idea sería cuando un ID pase la línea roja preguntar si antes pasó por la verde Si antes pasó por la verde quiere decir que va en la dirección de abajo hacia arriba que que es la que quiero contar si Por ende pasó la línea roja como lo que vienen de arriba hacia abajo pero no pasó por la verde porque no está en el vector de los que cuentan los ID que pasaron por la verde no va a contar cómo lo hago Esto bueno en principio pregunto con esta pregunta Sí si el centro x y el centro I de la persona que está pasando por una línea verde en este caso está pasando por los límites de la línea verde sí los límites de la línea verde son el ancho y el alto voy a tomar 20 valores antes y 20 valores después porque Bueno me toma la licenci es difícil por ahí detectar que justo está el rojo el punto Perdón sobre la línea verde entonces bueno para tomarme esa licencia tomo un poquito antes un poquito después lo que hago es si ese punto pasó por la línea verde pregunto si ese ID está en el vector vector a lo he creado junto con el acá arriba bien volvemos si es ID no está en el vector count a Lo agrego con lo cual la próxima vez si es que pasara una secuencia porque quedan dos secuencias muy pegadas y en ambas secuencias ese punto está 10 20 arriba o 20 abajo de la línea verde no lo volvería a contar porque ese ID ya está en el vector con a está bien luego hago lo mismo con lo que pasa en la línea roja pregunto por el centro x y el centro y ahora con los límites que corresponden a la línea roja recuerden vuelvo acá arriba que tengo límites para la línea verde l an y limits que son los límites de la línea roja bien con eso ahora voy a preguntar igual que hoy Cid ahora no está en count que es el vector que cuenta Lo que pasa por la roja pero si ese ID sí está en el vector de lo que pasaron por la línea verde Entonces si no está en el vector rojo porlo deuna manera pero sí está en el vector verde quiere decir que es una persona que está atravesando la línea roja y va en la dirección correcta con lo cual agrego ese ID en el vector de los que pasan por la línea roja en la dirección que yo quiero con esta información luego ya tengo todo resuelto Por qué hago un rectángulo para poner arriba el conteo y pongo en el conteo el length de count Qué quiere decir leng de count el largo de count si count tiene todos los ID que pasaron por la línea roja producto ha pasado por la verde quiere decir que esas cantidades son las cantidades de personas que pasaron por la línea roja habiendo pasado por la verde H Con eso tengo todo resuelto ya que lo que me queda simplemente es hacer el Im show y el wiik H lo probamos una vez más para poder verlo definitivamente este cómo es este proceso Pero habiendo entendido la lógica de El array eh que cuenta los que pasan por la verde aquí el tres ya pasó por la verde ahora va a pasar por la roja y vemos que ya cuenta uno H bien Ahora va a ser lo propio el cinco acá agrega el el id5 al vector de los verdes y ahora cuando pasa la línea roja dice Bueno este cco que pasa por la roja pasó por la verde Sí entonces cuento dos diferente el caso de la señora 32 que ahora está pasando por la línea roja pero como no está en el vector de los que pasaran por la línea verde no lo contó si va a contar el 33 si va a contar el 45 aquí no tengo ningún caso Pero bueno este podría constatarlo ustedes corriendo la línea de una persona que pasa en la dirección que yo quiero pero por fuera de los anchos de los límites de la línea roja sí como si fuera el 12 pero en la dirección contraria bueno con esto terminamos eh esta práctica la ejecución de este programa y lo que les dejo aquí es otro programa que se llama tracking + Counting Traffic piie que usa este video Traffic 1. mp4 para hacer algo parecido pero en lugar de contar personas cuentan autos en una ruta bien eh Insisto que pueden hacerlo en las dos elecciones Quizás lo de la ruta no la imagen que yo les mando el video pero pueden buscar otro video donde se vean claramente las dos manos de una ruta y bueno poder contar en una dirección o en la otra haciendo lo mismo que hicimos aquí Pero obviamente poniendo en un caso la línea verde en un sentido y en otro caso la línea verde en otro sentido bien les dejo esto para que como siempre indaguen Un poquito más larga esta primera parte Perdón por lo largo Pero bueno era necesario para ver toda la la riqueza que tiene este tema del tracking terminamos esta primera parte nos vemos en la segunda parte para ver cómo detectar texto desde una imagen aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte Titulo: Clase24 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/XjDrboUA5Oc  \\n 1516 segundos de duracion \\n Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella  Hola a todos nuevamente estamos en la segunda parte de la clase número 24 para abordar el tema que adelantamos en la primer parte de esta clase respecto de la interpretación de texto desde una imagen para eso vamos a usar una librería una de las tantas que hay para esto nosotros vamos a usar una que hemos elegido especialmente se llama easy ocr bien Y como siempre vamos a tomar los archivos que yo les dejo en el campo virtual y lo van a colocar en una carpeta yo he pensado en crear la unidad c y le voy a poner como nombre text extraction ustedes pueden ubicarla y poner el nombre que quieran Como siempre digo como dije recién Aquí he creado la carpeta text extraction en la unidad c dentro de ella los cinco archivos que yo les estoy dejando en el campus virtual y donde vemos Que tres de ellos son archivos jpg de imágenes y dos punto Pipe que son programas que nos van a permitir distintos tipos de niveles de extracción de texto desde una imagen como siempre solemos hacer vamos a crear un proyecto nuevo con basado en esta carpeta con archivos existentes con lo cual el proceso sería igual a lo que hemos hecho en las clases anteriores pero en este caso particular lo vamos a ir haciendo paso a paso porque nos vamos a topar con un problema y la idea es que ustedes puedan ver cómo lo resuelvo vamos con ello bien acá estoy en parche y le digo que quiero un nuevo proyecto con lo cual voy a buscar como siempre la carpeta donde ya tengo los archivos es esta que está aquí la selecciono le doy Ok y le digo que voy a crear un nuevo proyecto basado en una carpeta existente con recursos existentes Así que le doy clic y procedo a crear el proyecto de este modo insisto como lo venimos haciendo de costumbre lo que vamos a hacer a continuación es configurar las librerías que vamos a usar en este proyecto en principio la más importante y la que es el centro justamente de esta segunda parte de esta clase y sierre así que vengo aquí y también interpretar más y escribo el nombre de esta librería selecciono install y una vez terminada vamos a instalar también la librería pandas para ya vamos a ver en detalle guardar la información de las detecciones que vamos a hacer así que la selecciono pandas y también la instalo bueno con esto tenemos por ahora terminado la configuración de todas las librerías que vamos a usar en este proyecto por ahora porque como les adelante vamos a tener un problema y resolver algunas cuestiones si ustedes miran acá y volvemos a entrar a settings vamos a poder ver que al haber instalado y sierre tengo automáticamente instalada algunas librerías sí como por ejemplo opencv python headless esa librería justamente va a ser la que nos va a traer algún inconveniente y por eso es que prefería hacer este esta incorporación Esta preparación del proyecto paso a paso para que podamos ver cómo se nos plantea este problema y cómo lo podemos resolver es Por ese motivo que vamos a ejecutar Este primer programa Y si o sea R1 punto Pay sin explicar obviamente de qué se trata Este programa como lo hacemos siempre porque quiero priorizar esta cuestión de que ustedes vean cuál es el problema que aparece Y qué configuración o Qué cambios tenemos que hacer en la configuración para ya no tener más problemas a lo largo del resto del proyecto al ejecutar Este programa me aparece este mensaje de error justamente luego de la línea que es Civic 2 punto imshow donde muestro el email esto ya lo hemos hecho no no hay ninguna ningún misterio con este Comando no funcionó muy bien en todos los programas anteriores que lo usamos pero en este caso si evidentemente hay algún problema con esta librería y la descripción del error es esta que está aquí y en primera instancia no es una descripción que me ayude rápidamente a entender cuál es el problema ni mucho menos como resolverlo que se hacen estos casos bueno obviamente se recurre a buscar en internet Bueno si hay gente que ha tenido este problema y como lo ha resuelto caso típico de lo que es el ámbito de la programación con lo cual aquí me describe cuál es el problema si yo estoy trabajando en el ámbito de Windows o en el ámbito de Linux yo lo que hice fue buscarlo en internet y encontrará solución que justamente está en desinstalar esta librería que les mostraba recién que se instaló automáticamente junto con izzr esta opencv python en realidad no tiene problema con inicios de recogería incoherente que cuando está lord instale esto y justamente Esto está en contra dice CR Lo que pasa que el problema es que yo lo que intento hacer como siempre lo hacemos Es Mostrar el resultado de las detecciones por pantalla es allí Por eso en esta línea es donde me manifiesta el problema es allí donde aparece el problema por eso lo que voy a hacer es desinstalar esa librería desinstalar opencv y volverlo a instalar todo esto desde la terminal a través de por ello voy a marcar aquí este icono que al posicionarme el mouse encima me dice terminal donde puedo ejecutar cualquier tipo de comando sí como si fuese una ventana de terminal de el sistema operativo de obs Entonces yo aquí voy a escribir lo siguiente pip and install porque lo que voy a hacer es desinstalar y el nombre de la librería opencv python le doy enter me va a preguntar si procedo le digo que sí bien y ya está desinstalado lo que voy a hacer ahora es algo similar pero en lugar de Open pyt healers directamente con Open python opencv python Perdón bien y ahora lo que voy a hacer va a ser lo contrario es decir no desinstalar sino instalar opencv le doy enter me lo instala y tema solucionado por eso si ejecuto el programa ahora Estamos adelantando Pero tranquilo que tiene que ver con entender esta cuestión de la configuración no debería tener ningún problema para Mostrar la imagen que quiero mostrar y la detección de texto que quiero mostrar bien aquí el programa anduvo nos detenemos aquí simplemente la idea era probar que el programa Anduvo con este cambio de configuración y ahora sí vamos concretamente al contenido del programa para entender cómo logramos esto que estamos viendo bueno aclarado todo esta dificultad o Cómo resolver esta dificultad vamos concretamente al contenido del primer programa que mencionamos recién y siocr1.pay lo primero que vamos a hacer va a ser incorporar las librerías si vi tu y siocr y pandas y luego lo que tengo que empezar a hacer es crear el objeto de izzr que me va a permitir bueno inferir el texto desde una imagen que contiene un texto por supuesto con lo cual voy a crear una variable que he pensado o He decidido nombrar a reader y justamente lo que voy a hacer es llamar al método de reader de iOS r y le voy a poner dos temas o dos parámetros importantes para justamente configurar y empezar a utilizar ese identificador de texto en imagen el primero es Qué tipo de lenguaje voy a usar esto es fundamental obviamente no va a entender Si un texto en un lenguaje si lo tengo configurado en otro lenguaje como voy a tomar una imagen que tiene texto en español le digo que justamente el lenguaje en este caso va a ser español con esta abreviatura obviamente esto lo pueden buscar en el sitio de scr Donde están todas las las denominaciones para cada uno de los lenguajes existentes luego voy a leer la imagen con imry de Civic como lo hemos hecho muchas veces y la imagen que voy a tomar en principio es cartel es punto jpg este que está aquí con la idea de justamente poder identificar cada una de las palabras de ese cartel por lo tanto voy a crear una variable resultol donde voy a recoger todos los resultados de las detecciones que justamente va a llamar al método ritex de el objeto reader sobre qué sobre image email es la imagen que acabo de mostrarle y configurando muy importante este parámetro párrafo en false de esta manera le digo que de ese cartel de esa imagen no importa que sea no quiero tomar un párrafo completo sino palabra por palabra bien luego voy a crear un Data frame para guardar todos los resultados de las detecciones Sí bueno resultado que ya vamos a ver en qué detalle lo vamos a ver o lo vamos a encontrar pero ya les adelanto que en principio vamos a detectar el bonding box es el recuadro que va a marcar a ese texto el texto propiamente dicho y el nivel de confianza en que eso que encontró allí es esa palabra que justamente está dentro de texto bueno como yo ya sé que esos tres parámetros los voy a tener a pesar de que todavía no llegamos a esta parte pero yo sé de entrada que eso va a estar Es que aquí ya voy creando un Data frame que va a tener tres columnas con esos tres tipos de datos Y eso lo va a tomar desde resumen que es es lo que está aquí y acaba de tomar de esa imagen que vimos recién justamente a través de la configuración de tomar texto por palabra y poniendo esto en results justamente creo este Data frame con los resultados de resumen bien una vez que tengo eso ya tengo creado si ese Data frame lo que voy ahora es lo que venimos haciendo siempre es Mostrar la imagen y mostrar las detecciones con la información del resultado de las detecciones Por eso voy a recorrer a este array results uno por uno a través de una variable de red hago un print de res para que podamos ver también por pantalla de manera adelantada Porque después lo vamos a ver en el Data frame Cómo es la información de cada una de las detecciones y la idea va a ser que vayamos tomando cada uno de los puntos de las detecciones y la vayamos poniendo en distintas variables y en cada una de estas cuatro variables Por qué digo esto porque lo que yo voy a encontrar va a ser algo que va a estar re cuadrado ese recuadro como siempre en los bonding Box me va a marcar una coordenada X e Y pero no como pasaban las detecciones que era vértice superior izquierdo de artis inferior Derecho sino que va a darme la información de los cuatro vértices y por eso tengo cuatro parejas de valores que serían las x y las y para cada uno de los cuatro vértices enmarcado ese texto luego lo que voy a hacer es con otra parte de la información de res que no es cero que es donde están las coordenadas sino es el valor 1 que justamente es lo que me da en el valor 1 el texto sí fíjense que lo que yo aquí puse como segundo elemento en el Data frame es justamente el texto y luego el nivel de confianza en esa detección luego la idea va a ser en principio crear un rectángulo para recuadrar ese texto como hicimos siempre con Los bonding Box como detectamos un objeto y además voy a poner por sobre encima de ese recuadro un pequeño recuadro como si fuese una tira con un recuadro de fondo o sea con sólido digamos con fondo porque lo voy a usar para poner encima de eso Como si fuese un Level una etiqueta que va a tener que información bueno en principio el detectado y el nivel de confianza es lo que hablábamos recién como elementos 1 y 2 que son los que estoy mostrando aquí y estoy volcando en esta variable Label y que justamente es lo que voy a mostrar en este texto aquí por lo tanto con este rectángulo lo que hago es marcar el recuadro en el cual va a aparecer la etiqueta qué etiqueta esta que muestra a continuación con la información lo que dije recién el texto encontrado y el nivel de confianza y luego voy a hacer otro rectángulo como recién adelantamos para enmarcar toda esa ese texto detectado en el contexto de la imagen bien finalmente a un email y un White porque vamos a ir deteniendo esto por cada texto por cada palabra detectada en el marco de la imagen finalmente vamos a recurrir a este Comando que ya usamos en la parte de análisis de datos que es como llevar un Data frame a un archivo de tipo ccd Data punto tu ccv y el nombre del archivo que en este caso he decidido poner resultados punto csv finalmente bueno destruye todas las ventanas y se termina el programa lo ejecutamos y vamos a ir viendo paso a paso la información de esto para que se entienda mejor todo esto que hemos hablado aquí Así que lo ejecutamos Este primer mensaje me dice que esta librería funciona mucho mejor o mucho más rápido con gpu es una advertencia nada más no es ningún error bien aquí tenemos la primera detección menos que le decía Recién con un rectángulo voy a marcar el rectángulo del texto y con el otro rectángulo es este que está aquí arriba el que me sirve como una especie de fondo para el texto y el texto que es el texto detectado es mi propiedad y el nivel de confianza en la detección voy a correr esta imagen un poquito para que podamos ver esto que está aquí voy a minimizar mejor así se ve mejor fíjense Aquí tengo lo que les mostraba recién ven las cuatro parejas voy a subir aquí al código es esto que está aquí arriba las cuatro parejas de detecciones con la x y la y de cada uno de los bordes bien esta es la información que está ubicado como el elemento cero de la variable red que tienen los resultados luego el siguiente elemento dijimos que era el texto y aquí está propiedad y el final el nivel de confianza nivel de confianza y texto que es lo que si voy aquí al código estoy usando aquí para armar como una suerte de etiqueta y luego las tres líneas que dijimos recién son estas que están aquí con esta hago el fondo para mostrar la etiqueta vamos a la imagen me refiero a este fondo ese rectángulo dibuja este fondo luego viene el puntex que es lo que hace bueno el texto que vimos también en este recuadro y finalmente el último rectángulo lo que hace es dibujar el contorno de el texto detectado en la imagen todo eso a partir de las coordenadas PT 0 pt1 pt2 pt3 que tome desde la detección de los cuatro extremos de la imagen detectada bien Esto se paró aquí porque yo le puse un huequito para que para poder ir haciendo esto paso a paso yo ahora le doy enter a esto y me va a aparecer a la siguiente detección siguiente detección que se minimizó esto nuevamente Ven aquí tengo la segunda detección con sus cuatro coordenadas con cuatro parejas de coordenadas el texto detectado y el nivel de confianza fíjense que en este caso el nivel de confianza es mayor en el caso del texto protegida respecto del texto apropiado sigo con con alta confianza sigo con alarma alta Así que una hay que ver porque la confianza no fue tan alta en el primer caso en el resto de los casos Sí fue mucho más alta bien y así se termina la detección de texto en esta simple imagen bien entendido esto vamos a pasar ahora a probar esto que está comentado aquí es decir vamos a probar un texto en inglés por lo tanto vamos a comentar esto y vamos a desconectar lo otro el cartel en inglés es este que está aquí hay solamente dos palabras bueno con lo cual ahora paso a la configuración de inglés y paso a una imagen que tiene texto en inglés bien habiendo hecho esto lo ejecutó nuevamente y vamos a ver lo mismo que hay pero con otra imagen y con solamente dos detecciones obviamente acá tenemos la detección de lo primero y la detección de los segundos con una muy buena confianza en ambos casos para cerrar todo lo relativo a este programa vamos al final y recordemos que dijimos que íbamos a guardar toda la información de estos tres elementos importantes el recuadro de el texto el texto propiamente dicho y el nivel de confianza en un Data frame ese Data frame lo exportábamos a un archivo resultado csv archivo que aquí vemos y le hacemos doble clic y ahí vemos las dos detecciones de El último caso que dimos justamente el texto en inglés y aquí vemos como dije recién las cuatro parejas de coordenadas el texto futuro y adelante y el nivel de confianza de ambas detecciones bien con el otro programa de este proyecto Y si o sea r2 vamos a hacer algo similar lo de recién pero ya buscando un archivo que tenga una imagen con mucho más texto desde el cual yo pueda extraer no palabras sino párrafos la imagen es esta que está aquí y es una descripción de la red social Facebook La idea es que yo pueda tomar el título el primer párrafo y el segundo párrafo quien quiera avanzar hacia algo más que esto obviamente hay otras herramientas que justamente vamos a empezar a ver en la próxima parte este curso con el procesamiento del lenguaje natural por lo tanto digamos por ahora justamente esto es bueno porque es una forma de poder tomar información de un documento que no es de tipo texto y después con las herramientas que vamos a ver en la otra parte del curso poder hacer una extracción aún mayor o de más calidad o poder hacer alguna interpretación también del contenido de este texto que está en este caso en esta imagen bien aclarado esto vamos al programa propiamente dicho y aquí no vamos a ver demasiados cambios en principio voy a seguir usando las mismas librerías el mismo objeto si en este caso voy a tomar como español voy a tomar la imagen de recién texto jpg te voy a hacer un resoplar de la dimensión que puedo ver en la pantalla y si el cambio más grande por llamarlo de alguna manera está aquí donde Ahora sí pongo párrafo Perdón en verdadero cuando Antes había puesto sí párrafo en falso en este caso sí quiero párrafos Por ende pongo párrafo en verdadero lo que sigue a continuación es la creación del Data frame que es parecido lo que hicimos antes pero fíjense que ahora solamente con dos valores el bonding box y el text no el nivel de confianza porque porque en el caso de la palabra yo puedo pensar que puede haber un nivel de confianza en que ese texto se corresponde con esa palabra es fácil de tratar de comparar y poder comparar y determinar un nivel de confianza en esa predicción en este caso detecta todo un texto porque es un párrafo me ofrece la detección me va a dar el texto que ha detectado pero no puede haber un nivel de confianza a nivel de párrafo es muy difícil decir con algún nivel de precisión que todo ese conjunto de texto corresponde con lo que ha inferido con lo cual en este caso hay solamente dos elementos detectados dentro de results Y con esos dos elementos en este caso voy a crear el Data frame bien luego el resto es Exactamente igual solamente que en este caso Obviamente no voy a poner una etiqueta arriba de cada detección porque como es todo un párrafo no tiene sentido con lo cual simplemente lo que voy a hacer va a ser recuadrar cada una de las detecciones y finalmente la voy a poner dentro un archivo que en este caso le puse resultados uno para que no se pise con el anterior y eso es el gran cambio que tiene Este programa respecto del anterior vamos a ejecutarlo y aquí tienen en el primer caso la detección de el primer párrafo es el título Sí bueno ya aquí fíjense que yo le estoy poniendo un print Siempre sobre la terminal Que para más que nada para que vayamos viendo la información tengo las coordenadas y tengo el texto nada más acá No hay nivel de confianza le doy enter para continuar con esto me detecta el segundo párrafo y fíjense que me aparece aquí en la terminal las cuatro coordenadas y todo el texto completamente que ha encontrado en ese párrafo Sí se ve aquí abajo Facebook es una red social que ya disfruta miles de millones etcétera etcétera que es Ni más ni menos que lo que ha encontrado en este párrafo aquí doy enter y me toma el último párrafo donde Bueno si voy a la terminal veo aquí también aparece todo ese largo párrafo en una detección de formato de texto detección Que obviamente después deben entrar más para que se termine el programa Va a ir a parar a este archivo resultados unos puntos csv que aquí lo tengo y como lo tengo lo voy a abrir con doble click para ver Su contenido Ven aquí que tengo las tres líneas los tres párrafos que ha detectado el primero es un título pero es un párrafo bueno con sus coordenadas y Su contenido el segundo con sus coordenadas de Su contenido lo vamos a tratar de ver despacito aquí todo el texto de el segundo párrafo y luego las cuatro coordenadas y todo el texto del tercer párrafo obviamente esto que está aquí ahora en un archivo de tipo csv va a ser como ya les adelante un insumo para lo próximo que vamos a ver que justamente la interpretación de el contenido de este texto con herramientas de nlp bien y con esto llegamos al final de la segunda parte de esta clase al final de esta clase y al final de el módulo de redes neuronales convolucionales desde la próxima clase número 25 empezamos a incorporarnos al mundo del nlp o procesamiento natural del lenguaje nos vemos en la próxima clase aquí termina esta clase los espero en la próxima clase nos vemos Titulo: Clase 25 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/XOBv5U1nHws  \\n 1418 segundos de duracion \\n  Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número  25  Hola a todos Bienvenidos a la clase número 25 del curso de Inteligencia artificial de ifes seguimos en el módulo de Deep learning pero ahora pasamos a otro campo de Deep learning o otro tipo de aplicaciones importantes del Deep learning venimos del mundo de la visión computacional trabajada con redes neuronales convolucionales ahora desde hoy vamos vamos a empezar con otra rama importante del Deep learning como dije recién el procesamiento de lenguaje natural o  nlp qué es procesamiento de lenguaje natural o nlp bien para dar una definición vamos a usar nuestro propio canal de Inteligencia artificial donde Catalina nos va a explicar claramente Qué es nlp te has preguntado cómo las máquinas entienden y responden a nuestro idioma bienvenidos al fascinante mundo del procesamiento del lenguaje Es una rama de la Inteligencia artificial que permite a las máquinas entender interpretar y responder al lenguaje humano traducción automática análisis de sentimientos generación de texto reconocimiento de voz y chatbots inteligentes son solo algunas de las emocionantes aplicaciones que puedes explorar con nlp ahora que tenemos Clara Cuál es la definición de nlp vamos a ver algunas aplicaciones del nlp y aquí podemos tener por ejemplo estas aproximaciones a lo que hoy son los usos más importantes o más significativos del nlp en principio la detección de spam esto ya lo vimos también en algoritmo de Machine learning pero es muy importante también saber que desde nlp también se puede contribuir a poder detectar si un email tiene o no spam el texto preditivo esto es al lo que vemos todo el tiempo cuando escribimos correo Por ejemplo o cualquier mensaje en cualquier red social vemos que nos aparece si nosotros lo configuramos alguna sugerencia de texto de lo que supone la inteligencia que es lo que deberíamos escribir en virtud de lo que venimos haciendo esto tiene que ver con un conocimiento de nuestras costumbres o el tipo de texto que estamos acostumbrados a escribir y hace un texto preditivo el reconocimiento de voz también es algo muy importante recién hablaba Catalina del tema de algunas inteligencias que reconocen justamente una orden y no responden en consecuencias Como por ejemplo Alexa o Google análisis de sentimiento es una técnica muy muy utilizada porque me permite saber por ejemplo si tengo un canal en el cual puedo recoger los comentarios de mis clientes de mi empresa ver si son comentarios positivos o negativos esto se usa mucho también en la red Twitter donde justamente hay muchas personas que son famosas y que recogen a partir de los comentarios de las personas bueno ver cuántos comentarios positivos hay y cuántos no son  positivos finalmente la en llp que lo vamos a abrir en el siguiente detalle para que ustedes lo puedan ver  mejor en términos de avance reciente en esta disciplina destaca el uso de la Inteligencia artificial en la traducción de idiomas como el caso de Google Translate que emplea cada vez más esta tecnología para mejorar cada una de sus traducciones en los diferentes niveles de idioma que tiene disponibilidad luego la generación de texto como la que utilizan en los chatbot ha experimentado notables mejoras en los últimos tiempos tiempos lo que ha permitido una comunicación más fluida y natural entre los clientes y las empresas Eh bueno un ejemplo emblemático de los avances del nlp es gpt3 ahora gpt 4 Próximamente seguramente gpt 5 desarrollados por Open Ai estos modelos han demostrado una comprensión excepcional del lenguaje humano y tien la capacidad de generar texto que a menudo es difícil de distinguir de un texto escrito o no por un humano finalmente eh decir que empresas como Google Facebook y Microsoft por citar los tres grandes líderes de lo que es informática están empleando estas tecnologías para mejorar sus servicios ya sea en motores de búsqueda en redes sociales o asistentes virtuales en general el procesamiento del lenguaje natural se ha convertido en un campo esencial en la revolución de la tecnología tal cual es hoy en día bien habiendo hecho una introducción hacia Qué es el nlp y hablar de un poco sus aplicaciones más conocidas hoy en el mundo de la informática vamos a empezar a abordar todos los conceptos básicos y esenciales del mundo del nlp cada uno por separado y con una explicación pormenorizada que nos dé una idea bien completa de Qué significa cada uno de ellos y por qué son importantes o por qué son conceptos esenciales en el campo del nlp el primero de ellos es vectores este concepto es muy importante porque lo que hace es dar la posibilidad de que un texto pueda ser procesado por un sistema informático a través de la transformación de ese texto en una expresión numérica Esto no es propio del nlp Nosotros sabemos que venimos del mundo de las redes neuronales convolucionales que trabajan con imágenes en el marco de las cuales también pasaba lo mismo la red no podía recibir una imagen en su formato natural sino que hace una una conversión de los píxeles de es imagen a sistemas distintos de representación como puede ser con tonos de grises o con rgb o bgr en representaciones numéricas y de ese modo un sistema o una red inteligente podía procesar una imagen el caso del nlp es Exactamente igual desde el problema obviamente la resolución es diferente en este caso lo que hace como dije recién es transformar un texto en un vector esta imagen que está aquí es bastante representativa vemos que hay una persona que está enviándole una orden sí que puede ser en voz o en texto escrito a un sistema de computación que tiene dentro un sistema de nlp y justamente lo que muestra el gráfico es que hay una conversión de esos valores en representaciones numéricas en este caso de unos y ceros bien la forma en realidad en que traducimos esos textos en vectores es el input para el entrenamiento y aprendizaje de un algoritmo de aprendizaje automático aquí volvemos a la cuestión de que esta inteligencia que ahora trabaja con textos que provienen de un audio o de algo incorporado desde el teclado de una computadora sí o de cualquier sistema como puede ser un celular también hay que justamente pasárselo a su inteligencia para que esa inteligencia tome eso como insumo entrene en base al conocimiento que le da ese input y puede ser una red que pueda predecir o trabajar o cualquiera de las acciones que nosotros hemos descrito recién que hacen con los textos para poder llevarlo a cabo de manera eficiente Pero concretamente qué es un vector y tenemos que empezar a ver por qué es la estructura elegida para poder representar un texto en números bueno en principio un vector es lo que vemos aquí en la imagen Es algo que tiene una cantidad una magnitud y una dirección Sí es decir que el vector tiene una cantidad que se representa a través de la magnitud del vector Pero además tiene una dirección ese vector es una matriz de escalares y aquí vemos justamente que esa matriz de escalares representada por este vector puede tener una asociación con otros vectores y justamente a partir de el ángulo que se describa entre ellos tener una noción de distancia es decir cuando vemos que hay una dirección vemos aquí el vector marrón tiene una distancia respecto de el que está de color negro porque tienen una magnitud y direcciones diferentes y respecto también de Que aquí tiene este color celeste verdoso Por decirlo de alguna manera que también tienen una magnitud y una dirección diferente bueno Esto es esencial para que entendamos o vamos a empezar a ver más adelante que esto es vital para poder interpretar justamente los textos y Cómo compararlos y ver cuáles son más parecidos a otros o no Cuáles son las ventajas de trabajar con vectores Bueno evidentemente yo tengo que trabajar con una representación numérica dado que no puedo hacerlo con el texto Por ende he elegido el vector como justamente la representación más apropiada para ello con esta este tipo de de idea de trabajo de eh Mis texto en vectores yo puedo hacer tareas como estas que están signadas aquí que son dos de las tantas que puedo hacer la detección de correos para saber si es spam o no o bien el eh una de las tareas también importantes que es la Organización de documentos mediante el uso de vectores es decir Yo puedo diferenciar un grupo de documentos de otro por el contenido que tiene todo esto desde la Inteligencia artificial por supuesto es decir que lo que puedo hacer es una separación de dos grupos de vectores como los que están aquí en esta imagen representados supongamos que estos puntos verdes son algunos vectores y los cuadrados rojos son otros vectores con este plano est estoy separando unos de otros por ejemplo estoy separando los mail que tienen spam de los mail que no tienen spam yo puedo presumir a través de la guía que tienen spam y también puedo hacer algo parecido a lo que dije acá abajo que puedo suponer que los puntos verdes son un tipo de documento supongamos que son documentos que hablan de geografía sí Y los otros que son representados aquí con cuadrados de color rojo supongamos que son documentos que hablan de otra ciencia como puede ser física Está bien entonces esto amente lo que me propone a través de este gráfico es la idea de que cada uno de estos círculos verdes o puntos rojos son vectores y esos vectores son traducciones de un texto en una representación numérica es decir que cada uno de estos puntos son textos pero están convertidos a vectores y justamente mi inteligencia que estoy haciendo con nlp me da la posibilidad de separar spam de no spam documentos de un tipo de documentos de otro o lo que fuer habiendo entendido el concepto de vectores vamos a otro concepto muy importante del campo del nlp que es el concepto de bolsa de palabras Qué es una bolsa de palabra o Back of Word es una técnica muy importante en el campo del nlp que trata un documento de texto como una colección desordenada de palabras sin tener en cuenta su orden o estructura gramatical A ver vamos a desmenuzar este concepto yo aquí tengo un texto sí I love this movie etcétera etcétera ese texto lo vuelco en una bolsa de palabras fíjense que esta bolsa tiene el mismo texto que está aquí es decir el mismo conjunto de palabras todas las palabras que están en este texto están en esta bolsa Pero como dice la definición aquí están desordenadas no desordenadas de manera a propósito digamos están porque lo que importa aquí es ver que esta bolsa tenga exactamente el contenido de todas las palabras que están en este texto pero no me importa el orden es decir están desordenadas de manera aleatoria pero sí también tengo una información muy importante de Cuántas veces está cada una de esas palabras en esta bolsa concretamente en este texto por eso aquí ven una lista que va de mayor a menor de Cuántas veces está cada una de las palabras y que es la palabra que más aparece seis veces hay 5 d 4 y así con el resto vamos a analizar algunas cuestiones para entender mejor este concepto de bolsa de palabras pero para ello vamos a hablar de la característica del lenguaje y aquí básicamente vamos a hablar de un concepto que no escapa a lo que ustedes puedan entender o comprender respecto de la característica de un texto el lenguaje y el texto son secuenciales ustedes saben que hay un conjunto de palabras que puestas en secuencia le dan sentido a una frase como esta que está aquí donde dice para mañana hay pronóstico de yuvia pero si me voy al concepto de bolsa de palabras Recuerden que lo que teníamos era una bolsa que tenía un conjunto de palabras Por ende esta frase si la coloco en una bolsa de palabras estoy colocando en una bolsa para mañana hay pronóstico de lluvia es decir que tengo una bolsa con 1 2 3 4 5 se palabras Por ende aquí estoy escribiendo otra frase para lluvia de mañana hay pronóstico una frase que no tiene ningún sentido desde la interpretación nuestra del texto pero para la bolsa de palabra ambas frases son exactamente iguales Por qué Porque ambas frases tienen la misma cantidad de palabras y el mismo conjunto de palabras para seguir hablando de este concepto vamos a ver la pérdida de información que me da el uso de la bolsa de palabras porque por ejemplo aquí tenemos materiales de construcción y construcción de materiales lavado de máquina o máquina de lavado las dos frases tienen sentido a diferencia de lo que teníamos en esto que teníamos recién aquí es decir cuando hablamos de para yuda de mañana de pronóstico que es una frase que no tiene sentido Aunque para la bolsa de palabra ambas frases son iguales en este caso ambas frases materiales de construcción y construcción de materiales como lavado de máquina o máquina de lavado son frases iguales para la bolsa de palabras más allá de que en este caso sí ambas frases tienen sentido Y seguramente una de las dos frases es la que supuestamente la persona que está escribiendo es la que quiere representar para la bolsa Las dos son iguales pero para la elección del usuario no da lo mismo porque una de ellas es la opción  elegida visto todos estos problemas ustedes me van a decir por qué es importante la bolsa de palabras para el nlp si tiene todos estos problemas Bueno La idea es que la bolsa de palabras no pone su foco En aquellos procesos que necesitan del seguimiento en secuencia de un texto sino solamente para aquellos procesos que tienen que hacer foco en si un conjunto de palabras está o no en un texto por ejemplo el análisis de sentimientos yo puedo tener una bolsa de palabras en donde tengo todas las palabras que representan un sentimiento positivo y otra bolsa de palabras o otro conjunto identificado con palabras que detentan un sentimiento negativo con lo cual yo puedo tener como input un texto y ver si en ese texto hay un conjunto de palabras que coinciden con las que están en la bolsa de palabras representando un sentimiento positivo o lo contrario y Por ende a partir de eso poder calificar un texto como positivo o como negativo lo mismo con la detección de spam yo puedo tomar como input el texto de un correo y ver si sus palabras están referidas a lo que tengo en una bolsa de palabras como palabras que o frases que pueden representar una conducta negativa o algo que puede ser calificado como como spam o lo contrario un texto que puede verse como un correo normal y lo mismo también en los modelos probabilísticos y de aprendizaje profundo y el la representación más importante de este ámbito es el chat gpt donde justamente utiliza la bolsa de palabras para esta comparación similar a la que estoy diciendo recién en cuanto análisis de sentimiento o detección de spam pero aplicada Obviamente con una base enorme de información a procesos más complejos y profundos bien el siguiente tema a abordar es el de conteo de palabras y justamente a través del conteo de palabras vamos a poder empezar a entender esto de transformar un texto en vectores de qué se trata el conteo de palabras justamente es una de las técnicas que existen para llevar a cabo esta tarea que mencioné recién convertir un texto en una representación numérica concretamente en un vector Cuál sería la técnica para hacer esto bueno en principio yo voy a tomar un texto y y supongamos que tengo un texto de 100 palabras Por ende si tengo un texto de 100 palabras voy a tener representaciones de texto en vectores de Dimensión 100 y luego voy a contar Cuántas veces está esa palabra en ese texto y Por ende esa cantidad se va a tomar como referencia para cada uno de los valores de esa dimensión ahora vamos a pasar un ejemplo práctico se va a entender bien pero este sería el concepto fundacional de eso sí además de eso vamos a definir Qué es o Qué entendemos por documento el documento es una cadena de texto que vamos a usar en llp y que puede ser de cualquier longitud sí Y puede ser un paper un tweet un post de Instagram un comentario sobre un producto un Script de YouTube o cualquiera de estas cuestiones con las que cotidianamente nos enfrentamos Aquí vamos a tener por ejemplo una situación en la cual vamos a empezar a dar un ejemplo práctico para poder entender esta cuestión del método de conteo de palabras el método de conteo palabras es un proceso para determinar el tamaño de un vocabulario como dije recién si tengo un texto de 100 palabras pues entonces mi vocabulario restringido pero vocabulario al fin va a ser un vocabulario de 100 palabras y va a estar conformado por cada una de esas 100 palabras que están en ese texto el proceso de creación de un vector para cada documento basado en el cono de palabras es al lo que justamente vamos a ejemplificar Empezando por tomar como idea muy básica de documentos estas tres líneas que están aquí Tengo estas tres líneas y cada una representan un pequeño documento el primero dice me gusta hacer deportes el deporte genera vínculos humanos primer documento el segundo dice el deporte que más me gusta es el tenis creo que soy un buen jugador de tenis segundo documento y el tercero dice no me gusta el Ski porque el esqui se practica en un ambiente frío y yo odio el frío bien a partir de esto Cómo se implementa Esta técnica del método del conteo de palabras bueno basado en este ejemplo de la siguiente manera sigo manteniendo tres frases y lo que voy a hacer en principio es armar mi vocabulario en virtud de estos tres documentos sí Entonces fíjense que yo arranco por ejemplo tomando el primer documento y digo me y acá tengo la primer palabra gusta el deporte luego cómo sigue él pero qué pasa él ya lo tengo Entonces ya no lo voy a agregar al vocabulario porque justamente es una palabra que está repetida con lo cual lo que voy a poner aquí es que en el documento uno la palabra é está dos veces luego viene deporte que también ya está entonces no la voy a agregar al vocabulario sino que voy a cambiar la cantidad de veces que aparece esa palabra en es documento pasando de uno a dos y finalmente genera vínculos ahí termino la primer frase diamos el primer documento el primer texto de este mini documento y paso al segundo documento que está en esta segunda columna y empiezo mi no está Lo agrego el vocabulario deporte sí está entonces directamente pongo uno luego favorito no está Lo agrego es no está Lo agrego é sí está por de la palabra é digo Bueno ya está en el diccionario lo que tengo que decir es que esa palabra del diccionario que ya existe en el diccionario en este documento dos está una vez y luego así sigo con el resto de bueno la la frase del segundo documento y del Tercer documento completando de esta manera todo el vocabulario completo El vocabulario completo empieza en me gusta é y llega hasta frío es decir todas estas palabras están formando estos tres documentos con lo cual mi mundo de vocabularios es esta columna de palabras y luego lo que hago es poner Cuántas veces está esa palabra en el contexto de ese documento con lo cual lo que yo estoy haciendo aquí es creando vectores de qué dimensión de la dimensión que yo tengo aquí es decir 1 2 3 4 5 6 7 8 10 11 12 13 14 15 16 17 18 dimensiones tienen estos vectores porque mi vocabulario Tiene 18 palabras y cómo o qué valor le doy a cada cada una de esas dimensiones Perdón la el valor que tiene que ver en cada documento la cantidad de apariciones que existe en ese documento con lo cual justamente la palabra me va a ser un elemento que va a tener una representación de uno en el documento uno de uno en el documento dos y en total dos veces sí la palabra el es la que más aparece a lo largo de los tres documentos y así con el resto de los casos sí donde puedo ver no solamente Cuántas veces aparece esa representación en el contexto de un documento sino en el contexto de este conjunto de eh documentos que va a formar lo que se va a llamar un Corpus el Corpus es el conjunto de documentos que yo voy a tomar para analizar en este caso una técnica de control de palabras o la que fuera en el marco de El nlp para terminar de cerrar este ejemplo vamos a volver una diapositiva hacia atrás y recoger esta información respecto del método del conteo de palabras es un proceso de creación de un vector para cada documento basado en el conteo de palabras es decir el documento uno Cómo está representado por un vector de 18 posiciones y Qué valores va a tener cada uno de esos elementos cada una de esas dimensiones Bueno en ese vector va a tener en la primer posición un número uno porque tiene una vez la palabra que está identificada con esa posición un uno en la posición número dos porque está una vez la palabra identificada con esa posición y así el resto Pero obviamente en el resto de los casos que van de mí hasta llegar a la última palabra va a tener un valor cero porque cada una de estas palabras en el documento uno está cero veces en el documento dos y el documento 3 la dinámica es la misma en el documento dos no existe la palabra me con lo cual en la primera posición del vector que representa el documento do el valor es cer en la segunda lo mismo en el tercero está una vez esa palabra con lo cual en el contexto de las 18 posiciones que tiene el vector del documento dos en la tercer posición el valor es se entiende la lógica es muy sencilla y justamente es la base de cómo transforma un texto en un vector el método de conteo de palabras hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte Titulo: Clase 25 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/MANTtPfb1G0  \\n 1687 segundos de duracion \\n  Esta es la segunda parte de la clase número 25 te invito a empezar con  ella  el siguiente tema que tenemos que ver es la tokenización y vamos rápidamente a tratar de entender Qué es la tokenización la tokenización es dividir un texto en tokens los tokens son las unidades individuales de un texto por lo general eh van a ser palabras pero en realidad la tokenización puede dividir el texto en otras unidades que no sean palabras aquí tengo un ejemplo basado en python donde tengo un texto Hola mundo coma Buenos días dentro de una variable que se llama texto y luego lo que hago justamente es aplicar el método split que ustedes ya lo conocen por python que lo que hace es dividir justamente un texto en palabras y lo coloca dentro de una variable que le he dado llamar tokens y luego hago un PR de tokens con lo cual lo que veo es que me sale la palabra que tiene perdón la frase que tiene tiene cuatro palabras con cada una de sus palabras en una Ray individualmente identificadas fíjense que hay algo muy importante aquí que en el caso de olola y en el caso de mundo hay una diferencia mundo tiene una coma y eso lo toma como parte de la palabra y Díaz que es la frase final tiene dos signos de exclamación que también los toma como parte de la palabra Bueno eso es parte de lo que vamos a tener que empezar a ver más adelante de si estas estas estos separadores o estas expresiones que no conforman una palabra del abecedario las voy a querer juntar con la palabra las voy a poder tratarlas como un token aparte o simplemente voy a querer prescindir de ellas que ya vamos a ver más adelante que en realidad la interpretación que se hace a través del nlp muchas veces puede prescindir de estos signos sin perder calidad de  interpretación como dijimos recién en la evolución de la toca haciendo un análisis mucho más fino del proceso tenemos formas diferentes de tokenizar en principio basado en palabras lo más tradicional lo más común auto moto bueno el elemento que sea representativo de una palabra de un texto basada en caracteres puedo tomar letras o números o expresiones como la roba o lo que debos recién una coma o un signo de admiración o basado en su palabras Es decir por aquí tengo una palabra que está conformada por otras dos o que se puede subdividir por el sentido que tienen otras dos contraataque en contra o ataque bien consideraciones a tener presente un poco lo que repasamos recién diferentes casos de letra es decir tenemos letras que tienen caracteres complicados como puede ser un tilde una diéresis depende del lenguaje que se trate y justamente lo que tengo que tomar como criterio es si voy a utilizar eso o no en el proceso de aproximación que haga hacia interpretar el texto que viene a mí que yo transformo en un en un vector o en una estructura de números por eso también es importante tener en cuenta lo que dijimos recién la forma de tocan ia para ver si tomo los elementos o no de manera individual o como parte colectiva o parte de la expresión en cada palabra donde está eh vinculado ese ese carácter especial y justamente la puntuación ver si la voy a tener en consideración o no de acuerdo al sentido que yo busque de el tipo de inteligencia que aplique para la interpretación que haga en algunos casos ustedes ya van a ver más adelante que se puede prescindir de una coma se puede prescindir de un punto o no y en algunos casos el tomar la opción de hacerlo no nos va a hacer peligrar la la eficiencia como decía hace un rato de la interpretación del texto Obviamente que todo esto que estamos hablando no hace otra cosa que como dijimos hace un rato generar información transformada de texto en vectores pero para que la inteligencia empiece a entender el sentido de ese texto y empiece a serer distintos tipos de tarea justamente con textos que empiecen a aparecer como input Por ende este tipo de información representa un conjunto de datos que está a disponibilidad de esa inteligencia para ser entrenado Por ende ese volumen de datos para el aprendizaje obviamente es un factor muy importante y como cuando vimos el resto de las inteligencias que vimos en el curso o antes en el curso de que venimos llevando adelante justamente Cuanto más datos tengamos obviamente más preciso y efectivo va a ser el modelo sí sabemos que siempre hay un límite no quiere decir que siempre tenga que ser el número máximo Pero obviamente Cuanto más tengamos en principio de Debería ser ese algoritmo mucho más eficiente Por ende o por ello Mejor dicho el el éxito de los grandes este las grandes aplicaciones que hay hoy como GP t4 que han sido entrenadas con cantidades enormes de información y de texto y por eso el alto nivel de eferencia que tienen un tema final a tener presente en este proceso asociado a la tokenización es el manejo de las letras minúsculas y mayúsculas en realidad para la máquina un hola donde la H está en mayúscula o la misma palabra donde la H está en minúscula son dos palabras totalmente diferentes yo tenemos que tenerlo en cuenta porque justamente si nosotros ponemos esta o dejamos libre esta diferenciación entre una palabra y otra simplemente por tener la primera letra mayúscula estamos creando un vector con mayor dimensión ustedes imagínense que todas las palabras tengan esta esta situación de dos palabras diferentes por tener la primera letra en mayúscula o no estaría duplicando Prácticamente todo el vocabulario literalmente todo el vocabulario Por lo cual por ejemplo recién el vector que teníamos 18 posiciones va a pasar a ser un vector de 36 posiciones siendo que la palabra es exactamente la misma con lo cual lo que me conviene hacer como proceso de tokenización o parte del proceso de tokenización además de dividirlas con el método split que vimos hace un rato es aplicar lower para poner toda la expresión en minúscula de esa manera mi diccionario va a estar conformado por cuatro palabras de otro modo modo yo tendría que tener un diccionario donde por ejemplo Hola mundo y buenos que tiene la posibilidad de tener una expresión en minúscula y mayúscula tendría que tener contemplados los dos casos con lo cual tenía un diccionario de siete palabras en lugar de cuatro como hay aquí el siguiente concepto importante son las stop words sí o palabras de parada bien tenemos por ejemplo aquí una frase de ejemplo que la he puesto en una var de texto en algunos países los días son muy largos y las noches muy cortas bien si yo toqu eniso esta frase que tiene 13 palabras tendría una toca de 12 palabras por qué Porque está la palabra muy que se repite dos veces como vimos recién el diccionario sería de 12 palabras a partir de este texto está bien Ahora en esta frase existen lo que se llaman stop Wars sí las spw Por lo general son palabras que forman parte de las frases pero representan algo que no cambia el sentido de la frase y Por ende se puede llegar a prescindir de ellas en este caso por ejemplo las palabras en los las muy e son palabras del lenguaje español que están en todas partes Es decir no va a ser algo que voy a encontrar en esta frase y no voy a encontrar en muchas otras frases porque son justamente elementos clásicos de nuestro lenguaje y Por ende justamente no aportan mucho a la frase ni tampoco cambian mucho el sentido de la frase si esas stop Wars no están en este caso si yo me circunscribieron las work que mencionábamos recién Entonces esto puede verse que a la hora de leerlo desde la que es nuestro conocimiento del lenguaje podía representar una frase que se puede interpretar más o menos Hacia dónde va pero no es muy clara bueno Esto es muy importante en el mundo en LP porque la máquina sí lo va a poder interpretar de manera correcta y lo que estoy haciendo aquí es prescindiendo de una cantidad de dimensiones en este supuesto vector si contemplara las stop Word dentro que justamente al no tenerlos se reduce el vector yo gano como ustedes ya imaginan por supuesto en velocidad en rapidez y en eficiencia justamente teniendo menos dimensiones que teniendo una cantidad de dimensiones mayor y quizás innecesaria a los fines del nlp Bueno pero se terminaron las palabras se terminaron las definiciones y los conceptos es hora de aplicar nuestro primer código de nlp para empezar a trabajar en este caso particular con stop Wars y tokenización para ello tenemos este la pnl 1 que es un archivo de colap que es el primero con el cual vamos a empezar a trabajar y es uno de los archivos que ustedes tienen en el campo virtual vamos hacia ello aquí estamos entonces en el ámbito de Cola con el la pnl 1 bien tenemos la variable texto a la cual le ponemos la misma frase que hablamos hace un rato en algunos países los días son muy largos y las noches muy cortas lo que vamos a hacer como primera medida es importar la librería nltk nltk significa librería natural Language tool kit Entonces tenemos que importar esa librería y tenemos que bajar dos archivos que son importantes el primero para la tokenización y el segundo para trabajar con las stop World Así que vamos entonces con la primer celda que no lo habíamos ejecutado que es poner dentro de texto esa frase y luego importar estas librerías paso seguido que lo ponemos en una celda aparte pero lo podríamos Haber puesto junto con las dos las tres líneas anteriores Perdón tenemos que también incorporar eh d nltk Corpus stop Wars y d nltk tokenize Word tokenize bien importamos esas librerías también y ahora empezamos a ver el tema de las stop Wars en principio voy a crear una variable que le voy a poner stop bajo Wars en la cual voy a volcar todo el dicionario de palabras de stop Wars que existen en el ámbito del Castellano sí palabras que tienen que ver justamente con lo que voy marcando aquí arriba con esta librería stop Wars que pertenece al natural Language toolkit y de hago una impresión para que se vea justamente Cuáles son todas las stop Wars en español así que ejecuto esta instrucción y aquí tengo una lista muy larga que bueno la podrían tirar a una Ray si quisieran podrían convertir esa variable como sabemos hacer de costumbre stws taray para poder verla de una manera más más amigable est una al lado de otra y no una bajo otra pero a los fines prácticos si aquí tenemos entonces todas las palabras que forman parte de las stop words o lo que está definido dentro del natural Language tool kit en español como stop Word obviamente es una lista muy muy larga si como siempre es complicado recorrerla de este modo Pero bueno lo dejo en ustedes Poder recorrerla De punta a punta bien una vez que tenemos esto lo siguiente sería tokenizar la frase texto si la que teníamos arriba de todo que volvemos ha arriba nuevamente en algunos países los días son muy largos y las noches muy cortas bien Voy a tokenizar esa frase de De qué modo con Word tokenize de texto de la variable texto y voy a poner ese resultado dentro de la variable tokens es decir que en tokens van a estar Qué cosa todas las palabras de esa frase luego lo que voy a hacer va a ser recorrer cada una de esas palabras por eso pongo Word Forward in tokens por cada palabra de cada una de las palabras que están en la colección de token se acuérdense que la colección de token fíjense que ahí me paro y me aparece la ayuda está conformada por tres items bien y luego le pongo como condición If not Word la palabra concretamente in stop Wars con esto resumo por cada palabra que está en tokens pero que no está en la hop Word con lo cual me quedo con las palabras que están fuera de las stop Word y luego imprimo el contenido de esta variable que le he puesto texto ssw como siendo bueno stop texto sin stop Wars Perdón entonces ejecuto esto y fíjense lo que me muestra que es lo que habíamos visto hoy en el ejemplo en la parte teórica en países días largos noches cortas pero yo veo aquí si me voy a las stop Wars y subo un poquito aquí rápidamente veo que n está dentro de las stop Wars Qué pasó que me muestra quién siendo que yo le puse como condición que lo que tenía que estar dentro de texto ssb debían ser aquellas palabras que no están dentro de War Bueno lo que pasa es que como ustedes pueden apreciar aquí en está con la e en mayúscula mientras que en las stop Wars volvemos a aquí en está como todas las stop Wars en minúscula entonces lo que tenemos que hacer es reconfigurar lo que hicimos Recién con una instrucción previa texto igual a texto pun l también lo vimos en la teoría esto y también es una instrucción de python un método de python que ya lo conocen bastante con lo cual con esto voy a convertir toda la frase De punta a punta en minúscula y voy a volver a ejecutar las mismas tres líneas que ejecutamos antes y voy a ver que ahora van a aparecer la misma toen ación es decir la misma eh el mismo arreglo el mismo aray que tiene aquellas palabras tokenizadas pero que no están en el Word en este caso sin la palabra en que había salido antes bien con esto Terminamos el primer lab de este  curso los conceptos que tenemos que ver a continuación son estos dos stemin y lematización en realidad tienen propósitos similares pero tienen técnicas diferentes de qué se trata esto en realidad vamos a tener en cuenta al algunas cuestiones antes de entrar en las características de cada uno de ellos las palabras similares se tratan como entidades separadas es decir corre corriendo o corre son palabras muy similares pero habitualmente se tratan por ser palabras distintas con entidades separadas las mismas en realidad van a ser representadas por vectores que van a estar muy cerca a unos de otros por qué porque las tres palabras tienen un sentido similar o un sentido que asociativamente es parecido pero van a estar representadas por vectores diferentes esto lo teníamos en la imagen que vimos hace un rato vamos a ver un poco ella para poder recuperar este concepto que está aquí recuerdan este gráfico bueno supongamos que esto de correr corriendo corre fueran estos vectores que probablemente justamente como son palabras que tienen un sentido muy similar estén cerca estos vectores eso va a ser justamente una representación vectorial que representa esa realidad de palabras similares pero concretamente yo voy a estar teniendo un espacio con tres dimensiones Y eso es lo que trato de evitar con el steaming y la lematización por eso como dice aquí lo que vimos recién genera una gran dimensionalidad la cual se podría reducir si utilizáramos una misma raíz de esas palabras para representarlas todas del mismo modo es decir correr corriendo y corre tienen una palabra raíz que puede ser asociada a las tres y de esa manera representar las tres con una palabra que sea representativa y no con las tres por separada para eso existen dos técnicas que son stemin y lematización y vamos a ver justamente cada una de ellas de qué se trata el stemin es una técnica que elimina los sufijos de una palabra por ejemplo tengo caminar y caminando ambas palabras se van a transformar en camín es decir toma lo que tienen en común ambas palabras y a la primera le quita el ar y a la segunda le quita el and con lo cual de dos palabras paso a tener una de dos dimensiones paso a una la lematización busca un objetivo similar pero con una una técnica o una modalidad diferente la lematización lo que hace es Buscar la palabra raíz u origen de esas palabras que tienen un sentido similar por ejemplo caminar caminando Caminaré camino se transforman en que su palabra raíz es caminar aquí no quita una parte de la palabra para llegar a lo que sería una palabra resumida quitando lo que decíamos recién los sufijos como hace el steaming sino que busca la palabra raíz de esas cuatro palabras esto obviamente es un proceso que es más lento porque lo que hace el St Es simplemente recortar parte de una palabra aquí lo que tiene que hacer es tomar todas esas palabras tod Ese Conjunto de palabras en este caso de cuatro y buscar cuál es la palabra raíz que representa esas cuatro Pero nuevamente el objetivo es el mismo porque paso de tener en este caso cuatro dimensiones a solamente una  dimensión al igual que hace un rato esto lo vamos a ver en la práctica por eso existe otro archivo de colab que tienen en el campus virtual que se llama lab pnl 2 y justamente me da un ejemplo de streaming y de lematización vamos hacia ello bien est vamos ahora nuevamente en colap pero ahora en el la pnl 2 en este caso el tema es stemin como decíamos recién y lo primero que vamos a hacer es importar bueno natural Language toolkit y vamos a hacer un Download parecido a lo que hicimos En el lab anterior pero ahora de wordnet Por qué wordnet cuando Antes había elegido Punk y stop Wars bueno en el caso de punkt tenía que ver con la tokenización y en el caso que nos ocupaba en el primer lab yo quería sola ente poder tokenizar esta frase es decir Tomar las palabras separadas de esta frase y las stopwords eran con el propósito que dimos recién que tenía que ver con la posibilidad de quitar de esa frase las stws en este caso la el stemming y la lematización se trata de poder inducir a que la inteligencia me permita en algunos casos quitar el sufijo y en otros casos encontrar la palabra raíz y para eso necesita como referencia una librería que contenga el vocabulario luego el vocabulario yo lo puedo configurar para que esté en un determinado idioma Pero por eso en este caso particular que se diferencia claramente de los propósitos del lab anterior tengo que carrar la librería wordnet Así que lo primero que hago es eso y una vez terminado esto lo que voy a llamar d nltk stem importar snowball steamer esta librería con la cual voy a crear una instancia una variable que le voy a poner steamer instancia a través justamente snowball steamer configurando esa instancia de snowball es decir básicamente lo que va a representar la acción del steamer en español con lo cual creo esta variable Y a partir de eso y justamente con esta variable de objeto lo que voy a hacer va a ser tomar tres palabras como las que tomamos recién como ejemplo y ver el steamer Cuál es la reducción que hace al quitar los sufijos de cada una de esas palabras por eso hago tres prints Cuando pruebo esto veo que comiendo comer y comió en los tres casos el steaming lo que hizo fue quitarle los sufijos y reducirlos a las palabras que tienen en común la c la o y la m bien es el turno ahora de la lematización en el caso de la lematización no vamos a usar la librería nltk que usamos antes para el steaming y que también usamos en el lav anterior dado que si bien tiene la herramienta para hacer la lematización esa herramienta no está disponible en lengua castellana por eso vamos a recurrir a la librería spacy que sí tiene esa opción de lematización en lengua castellana y de paso me sirve esto para decirles de que estamos sumando una librería más para ese tipo de actividades u otras y que eso no implica que sean las dos únicas que existen para este tipo de tareas en el mundo del nlp pero sí es importante que tengan presente que son las dos o dos de las más conocidas el spacy no viene naturalmente instalado en python como el nl tk por lo tanto tenemos que instalarlo para eso usamos el argumento que ya conocemos el método que ya conocemos la herramienta de python que es pip pip install spacy y el menq básicamente es para que no muestre una serie de información que tiene que ver con la instalación y luego lo que tengo que instalar es el diccionario o el conjunto de palabras justamente que viene para usar el spacy en castellano con esta frase que está aquí Bueno esto tarda mucho yo lo tengo instalado para no per tiempo lo vamos a considerar como que ya está pero esto es simplemente hacerle clic al al Play de cada de cada una de estas celdas y está instalado por un lado del spacey y por el otro lado del dicionario bien Vamos al código concretamente y lo primero que hago es importar spacey luego lo que voy a hacer a continuación va a ser Perdón me qu un error aquí volvemos a la línea lo que vamos a hacer a continuación es lobar de spacy es decir cargar el dicionario de Castellano y lo voy a cargar dentro de una variable que se le voy a poner nlp la cual obviamente va a contener todo este conjunto de palabras que me ofrece spacy para la lematización en castellano paso Seguido lo que voy a hacer es usar justamente esa variable nlp para hacer una tokenización de una frase una frase que en realidad es un poco ficticia pero tiene que ver con rápidamente demostrar de qué se trata la lematización y Cómo podemos visualizar los resultados de ella Por ende esa frase dice comiendo comer comió evidentemente no tiene un sentido en la lengua castellana Pero insisto a los fines de esta mini práctica viene bien con lo cual en tokens gracias a nlp lo que voy a tener justamente es esta frase separada entre tokens comiendo comer y comendo y Por ende con este for voy a recorrer for token in tokens es decir por cada uno de esos elementos que están dentro de tokens por cada uno lo que voy a hacer es arme un String digamos para que se vea por pantalla claramente texto dos puntos token pun text es decir el token ese mismo comiendo por ejemplo en el caso y luego lema Qué sería Cuál es la lematización o cuál es la palabra resultante de aplicar la lematización sobre comiendo luego sobre comer y luego sobrecomo acuérdense que la lización no quita el sufijo como el steaming sino que busca una palabra raíz por eso cuando ejecuto esto me muestra que para comiendo para comer y para comió existe una palabra raíz única que seama se llama comer con lo cual estoy reduciendo esta posible dimensionalidad de vectores de tres palabras a una sola algunas cuestiones que hay que tener en cuenta con la lematización que recién referenciamos Es que la lematización puede ser más efectiva que el stemin pero su gasto computacional es mucho mayor por qué Porque el uso de la lematización requiere de una etiquetado previo ya que hay que encontrar una palabra que represente a un grupo tal cual veíamos en el ejemplo de reciente Por ende hay que hacer un trabajo extra que no existe en el estamiento simplemente recorto la palabra si veo aquí estos ejemplos donde este cuadro que está a la izquierda representa una lematización comiendo comer y comó se sintetizan en una palabra comer que es una etiqueta con la cual está etiquetando estas tres palabras comiendo comer y comió con una etiqueta comer en los tres casos por el contrario en el caso del stemin con este ejemplo que está aquí yo lo que estoy haciendo es simplemente sacar el sufijo es decir comiendo comer y comió tienen en común las tres primeras letras con lo cual aquí el gasto computacional al no tener que buscar esta palabra raíz como el caso de comer y etiquetar cada una de esas palabras con esa etiqueta es obviamente en el caso del stem mucho menor el gasto computacional y Por ende el algoritmo es más rápido para cerrar el tema St y lematización vamos a poner foco sobre algunas aplicaciones en donde se pueden aplicar valga la redundancia estas dos técnicas en principio hay empresas que usan asistentes virtuales o chatbox por ejemplo Amazon con Alexa o empresas que proporcionan servicios al cliente que utilizan la lematización y el stemming para comprender las consultas de los usuarios con mayor precisión a ver por ejemplo si un usuario pregunta al chatbox de una empresa que tenga atención al cliente Dónde está su paquete el chatbox Debería ser capaz de entender la pregunta así como también otra pregunta parecida a esa en el análisis de sentimiento por ejemplo si vamos a monitorear Twitter con las redes sociales y alguien twitea por ejemplo estoy enfadado con la empresa Nno o XX se podría entender que enfadado y enojado o irritado son sentimientos similares y por lo tanto deben ser tratados de la misma manera para análisis entonces una lematización en este caso podría ser que cada vez que diga enfadado enojado o irritado todas tengan la misma base raíz que sería un sentimiento de enojo también con los motores de búsqueda Ocurre algo parecido a lo que decíamos en el caso de los chatbox dado que una consulta expresada de varias formas diferentes debe dar el mismo resultado de la búsqueda zapatillas para correr en la montaña zapatillas de treking zapatillas de en senderismo etcétera otro caso son las empresas que tienen sistema de recomendación Como por ejemplo Netflix Spotify Amazon u otras en donde utilizan la lematización y el stemming para mejorar la precisión de la recomendación a ver por ejemplo si un usuario Busca películas de acción debería Netflix ser capaz de entregar películas de guerra o policiales es decir que tengan que ver con películas de acción finalmente también se usa mucho en la publicidad online por ejemplo para hacer etiquetado en redes sociales a ver si queremos hacer una publicidad y queremos vender celulares por ejemplo o algún accesorio para gente que le gusta esa tecnología o convive con esa tecnología es posible que esta empresa me ponga anuncios no solo con la palabra celulares sino también con un disminutivo popular como celus o una denominación más formal como smartphones o cualquier denominación con palabras similares que básicamente representen lo mismo Hemos llegado al final de esta clase nos vemos en la próxima  clase Titulo: Clase 26 (parte  1) Curso de inteligencia Artificial \\n URL https://youtu.be/s-Yg52wSNKc  \\n 1540 segundos de duracion \\n  Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número  26 Hola a todos Bienvenidos a la clase número 26 del curso de Inteligencia artificial de ifes continuamos con el módulo de procesamiento del lenguaje natural y esta clase que vamos a dividir en dos partes como la mayoría de nuestras clases va a tener una primer parte destinada a hacer una práctica integral que refuerce todos los conceptos que vimos en la clase pasada y en la segunda parte de esta clase vamos a ver nuevos conceptos y también algunas prácticas adicionales para también reforzar esos nuevos conceptos así que sin más empecemos con la primer parte de esta clase bien es momento de avanzar a otra práctica a otro lab con lo cual vamos a ver un ejemplo completo de tokenización ese ejemplo completo lo vamos a llevar a cabo utilizando un dataset de noticias que vamos a bajar del sitio kag este lab se va a llamar la pnl 3 y como todos los que vimos anteriormente los pueden bajar desde el campo virtual de ifes bien en este lap vamos a desarrollar un ejemplo completo de tokenización como bien dice aquí el título y vamos a hacerlo tomando un dataset del sitio que ya conocemos kaggle con Eh bueno dat que tiene información de noticias y cada una de esas noticias está caratulada como un tipo de noticias Sí policiales deportivas etcétera Ya lo vamos a ver bien y todas las características como está rotuladas cada una de estas noticias cuando hablo de rotuladas ya van teniendo idea de que justamente me enfrento a un dataset que tiene etiquetas y este algoritmo que vamos a tratar de construir desde nlp tiene algunos contactos con lo que ya hemos hecho en Deep learning y en Machine learning porque se trata justamente de leyendo el contenido de una noticia tratar de inferir qué tipo de noticia es esa Está bien como dijimos recién deportiva política polical etcétera etcétera La idea es que justamente yo usé este dataset para qué Para lo que hemos hecho un montón de veces entrenar una inteligencia Y a partir ir de entrenar es inteligencia puede inteligencia empezar a deducir o a inferir qué tipo como dije recién de noticia es un determinado texto vamos a incorporar librerías que ya algunas las conocemos muy bien pandas por un lado py it learn con nuestro ya conocido trest split pero no tan conocido con vectorizer esta es una aplicación que tiene que ver con lo que vimos recién la teoría trata justamente de transformar textos en vectores a través de la contabilización de la la cantidad de veces que aparece en determinado texto en determinado documento eh una palabra y finalmente vamos a usar multinomial nb de night bice que es un tipo de algoritmo que no vimos en Machine learning pero es un algoritmo muy liviano muy efectivo y muy aplicable para casos de nlp Así que empezamos por cargar todas esas librerías y luego vamos a montar el Drive como ya lo hemos hecho muchas veces para Acceder al dataset que les mencionaba recién y habiendo hecho esto lo que voy a hacer es hacer nuestro ya muy conocido rit csb voy a tomar esta información de este dataset desde un dataset que tiene formato csb con el encoding utf8 y lo voy a poner dentro de una variable que le voy a llamar DF y luego hago un Head como lo hemos hecho infinitas veces y tengo en realidad un dataset con tres características nada más la URL donde está la noticia el texto de la noticia y el tipo de noticia que es cada uno por lo tanto tengo un eh si hago un shape pued darme cuenta que tengo un dataframe de 1217 noticias y como dije recién tres características si miramos eh el primer la primer característica de la primera observación por eso DF news porque la news es el nombre como llama esa característica el subcero indicando que es el primer valor bueno ejecuto eso y voy a ver todo Sí esto que aparece aquí muy recortado Sí todo el texto completo y así todo continúa sí bien luego lo que voy a hacer como insisto tantas veces hicimos decir bueno Cuál es mi característica x y cuál es mi característica i o Target bien la x va a ser el news y el target va a ser type que t justamente lo que vemos aquí es el tipo de noticia insisto lo que queremos aquí es a partir de un texto poder hacer una inteligencia que luego me permita predecir Qué texto puede tener que ver con cuál de los types con cuál de los tipos así que bueno creamos la x creamos la i y luego vamos a hacer una contabilización de eh los los tipos y además vamos ver cuáles son todas las variantes de tipo que existen ver dos propósitos en uno en principio puedo ver que los tipos que son macroeconomía alianzas Innovación regulaciones sostenibilidad otra y reputación y que bueno la cantidad de noticias que hay dentro de cada rubro o sea nada que ver lo que yo le había dicho política deporte Pero no importa era la esencia era transmitirle cuá es la la característica de lo que se quería pretender con este dataset Pero bueno tenemos 340 noticias de macroeconomía 247 de alanzas etcétera etcétera como siempre decimos no está muy balanceado porque ser ideal que hubiese la misma cantidad de noticias de cada uno de estos tipos Pero bueno tampoco es algo que nos vaya a complicar demasiado bien visto esto hago como Insisto fíjense que este este proceso es muy parecido a lo que hemos hecho muchas veces en Machine learning lo que hago es separar el conjunto de prueba del conjunto de Test tomando la x tomando la i y considerando un test size Es decir para el conjunto de test de El 20% quedando el 80 en el conjunto de entrenamiento bien para vericar como qued como quedaron esas cantidades hago un print de x trin y de X test viendo que a partir de ahora de Las 1217 observaciones tengo una división de 973 observaciones En en el conjunto de entrenamiento y 244 en el conjunto de Test bien lo que viene ahora es la vectorización recuerden vamos a ir justamente a la parte de la teoría donde decimos un ejempl muy sencillo Pero efectivo que era el de estos tres documentos que eran virtualmente tres líneas pero lo que vamos con el espíritu de tres documentos para hacer un ejemplo chico que se entienda bien lo que hacía era generar todo un vocabulario conformado por 18 palabras porque justamente la suma de las palabras que había entre estos tres documentos sin repetir la que aparecía más de una vez representaban 18 palabras bueno Esto lo que vamos a hacer ahora justamente con este vectorizer con lo cual hago una instancia que le pongo vectorizer de con vectorizer con vectorizer lo tenemos aquí arriba como una de las librerías de psych learn que acabamos de incorporar al principio de este lap y luego lo que voy a hacer es algo insisto parecido a lo que hicimos muchas veces con Machine learning un fit transform de los datos que van a usar para entrenamiento y un transform de los datos que van a usarse para Test en este caso por qué el transform Bueno por lo que venimos hablando porque tengo que transformar cada cada uno de los contenidos de los documentos en vectores entonces una vez que hago Esto me interesa sa ver así como en aquella oportunidad yo supe que tenía vectores de cuánto de 18 posiciones o sea documento uno tenía un vector de 18 posiciones documento dos lo mismo y documento 3 lo mismo lo que variaba es que elemento Qué valor tenía en cada una de esas posiciones bien en este caso es lo mismo y lo que hago es un shape de x30f para ve eso puedo descubrir o empiezo a poder descubrir que sigo teniendo mis 973 observaciones pero esas 973 observaciones tienen 26807 posiciones es decir que cada una de esas observaciones son vectores de 26807 posiciones Por qué Porque evidentemente a los a lo largo de todas esas 973 news noticias o descripciones de noticias se ha podido determinar o llegar a la conclusión que todo ese conjunto de palabras conformó un vocabulario no repetitivo recuérdense este este tema importante que no no repite no es la cantidad literal de palabras sino la cantidad de palabras diferentes por decirlo de alguna manera de 260807 palabras bien Es decir insisto Este ejemplo sencillo que ten tenos acá de 18 de vectores 18 posiciones ahora se transforma en este ejemplo de este lap en vectores de 2687 Cuántas 973 tan igual que la cantidad de observaciones Obviamente que esta transformación también tiene lugar en las 2 44 que representan el conjunto de bien aclarado esto vamos a crear el modelo bien el modelo lo creo con model multinal nave byes como si hubiera creado modelo de regresión logística de árbol de decisión de bosque aleatorio bueno el que fuere de soport vector Machine que fuera en este caso otro modelo diferente hago un fit igual que como lo hacía antes y mido si a través del predict la posibilidad de tener la el score vinculado al modelo de train y al modelo de Test entonces creo el modelo lo entreno y establezco la predicción y luego hago la medición de la precis lo que vamos a hacer a continuación es aplicar una técnica de stemming o de lematización para reducir la cantidad de dimensiones que hemos tenido como resultado de esta primera aproximación de cada uno de los vectores que representa cada una de estas noticias Pero antes de pasar a eso quiero rever un poquito este concepto de la precisión del modelo de train y test esto nosotros lo venimos trabajando desde Machine learning y en un primer análisis podremos pensar de que es muy bueno el score de modelo de TR y el score de modelo de Test eh No es bajo pero tiene esta cuestión de que está un poquito alejado del modelo de entrenamiento y eso consideramos en el mundo de machin que no es bueno aquí en el caso de los textos esto hay que revero muy bien y hay técnicas para poder justamente superar hasta el límite de lo posible esto por qué Porque los textos en realidad muchas veces tienen una diversidad que cuando separa un conjunto de entrenamiento un conjunto de Test puede ser que en algún caso esa separación sea muy diferente y si bien uno pretende que sea un algoritmo que generalice lo mejor posible esto depende mucho en el mundo en nlp de cómo estén distribuidos los datos entre el conjunto de Test y entrenamiento Así que a tomarlo como algo no tan malo Este score Y si vamos por supuesto que se puede mejorar desde ya por supuesto hay otros modelos que son más avanzados Queen scores más altos pero si vamos a centrar nuestra idea en cuanto a este práctico con lo que dije Recién con el propósito de reducir el tamaño de los vectores para eso tenemos el stemming la climatización que vimos en el lab anterior empezamos por el stemming en donde lo primero que vamos hacer es como hicimos en el lab anterior importar las librerías siempre usando nltk en el caso de temin Recuerden que en el caso de la matización usamos space bien cargo también la información de las librerías que sirven para tokenizar y para quitar las Wars y cargo eh o Creo mejor dicho una variable que es una instancia no steaming destacando que voy a usar un lenguaje en castellano y luego voy a definir una función que lo que va a hacer es recibir un texto en este caso el de cada noticia y tokenizar Y estimarlo sí En realidad va a hacer varias cosas en principio lo va a tokenizar lo va a estimar Y además le vamos a quitar toda aquella palabra que no presente parte del vocabulario alfabético sí eh como lo son por ejemplo los signos de puntuación H Entonces vamos a hacer todo esto en esta función que tenemos aquí abajo pero Primero lo primero que es eh crear el steamer y ahora repasamos la función que estamos creando Aquí bien el criterio de esta función que es esta que está aquí sería vamos a la línea siguiente vamos a crear una nueva columna que vamos a poner New stem como diciendo bueno es la news pero habiéndole aplicado el proceso de steaming y vamos a hacerla justamente tomando la news original y aplicándole con el método punto Apple el la función token steam la función token steam va a ser lo que dijimos recién va a hacer en principio una tokenización y luego va a serer el stemming aplicando dos acciones más en principio reducir todo el texto a minúscula y además quitar cualquier elemento que no sea de tipo alfabético todo eso lo tengo en esta función Ahora sí vamos a la función justamente la función se llama token steam tal cual yo lo puse aquí en este appli y luego lo que recibe es un text el text en realidad va a ser cada una de las news es decir yo le voy a ir pasando a través de método Apple justamente cada una de las news cada una de las news va a ser tokenizado como con Word token i pero previamente esa news va a ser transformada toda a minúscula entonces token hizo lo que previamente fue transformado a minúscula Con eso tengo en tokens todos los tokens de esa newo y lo que voy a hacer va a ser recorrer cada uno de esos token estimar losos sí que tal el sufijo se acuerdan de esto que vimos hace un rato lava anterior perdón y lo voy a hacer por cada token de tokens o sea por cada palabra de esa news diciéndole que esa palabra tiene que estar dentro de token Alpha tiene que ser un carácter alfabético finalmente voy a devolver esto Esta combinación que es típica de este tipo de funciones que es un espacio en blanco punj Sims que lo que hace es como voy a recibir un montón de palabras de tokens que están estimados voy a ponerlo uno al lado de otro justamente se hace con esta instrucción con el este espacio en blanco punto join que juntar justamente los steams que son los steam la salida de toda esta lógica que acabo de mostrar recién bien ejecutamos esta función cuando se ejecuta la función Es simplemente para que quede cargada la función dec la función no hace nada sino hasta el momento que es usada y luego la voy a usar una vez definida una vez que es reconocida para este este Notebook en esta línea que está acá que como decía recién va a crear una nueva columna que es la news pero ahora estimada tokenizado y habiéndole quitado justamente las letras en mayúscula y los caracteres que no son alfabéticos bien ahí terminó ejecutamos en Entonces esta línea y para ver cuál es el resultado voy a tomar la primer noticia la primer New habiéndole aplicado todas estas cuatro cosas que dijimos recién Está bien entonces la ejecuto y veo que el resultado es esto para ver bien ya se lee Sí cuando lo estamos leyendo a primera vista vemos que ha recortado muchas palabras justamente con este criterio de quitarle el sufijo si voy hacia arriba y la comparo con la frase original durante el foro la banca articuladora empresarial para el desarrollo sostenible etcétera etcétera se transformó en duran el for la Ban articul empresarial par el bueno Esto insisto no lo tomen como una cuestión de decir bueno Esto destruye el lenguaje transforma en algo ilegible estamos hablando de cómo maneja internamente el np con esta cuestión de los vectores y esta reducción a pesar de que a la vista humana creo que se interpreta bastante claramente per obviamente no forma parte del Castellano puro puede nuestro inteligencia interpretarlo de manera correcta bien a partir de ahora qué vamos a hacer ya no vamos a entrenar un algoritmo con news Sí y con type sino con news steam y con type por eso redefin la x y la i y bueno separo conjunto de Test y conjunto de entrenamiento y hago la vectorización O sea todo lo que hice antes en TR lo hago en una sola y lo que voy a ver ahora lo más importante fíjense ahora tengo 973 observaciones igual que antes pero el vector no tiene la misma dimensión igual que antes por qué Porque justamente esta cuestión de haber quitado el sufijo de las palabras hizo que mi vocabulario ahora sea mucho menor y Por ende la dimensión de mis vectores es mucho menor si me fijo Cómo era la dimensión anterior era 26 6807 Y gracias al stemming se reduce a 11928 menos de la mitad lo importante y lo significativo de esto bien creo el modelo y voy a medir la precisión y veo que la precisión pasó a ser 092 y 078 bueno a primera vista uno puede decir pero estamos peor que antes a ver siempre seamos medidos con esto por qué Porque justamente si es un poquito inferior antes tenía 093 y 080 ahora tengo 092 y 078 Pero tengo una pequeña diferencia de precisión con un conjunto de vectores que tiene menos de la mitad de la dimensionalidad  bien y finalmente para cerrar este lap vamos a la climatización La idea es seguir un proceso parecido al stening para verificar que justamente a través de estas técnicas se reduce considerablemente la dimensión de los vectores pero Pero obviamente tomando el texto original no el último que está estimado Sí entonces lo que tengo que hacer es instalar Eh bueno que es import spacy sí habiéndolo instalado antes y instalando también el el diccionario de lenguaje en castellano bien Digo como lo vimos antes a continuación creo una variable nlp como hicimos hoy space load y cargo justamente ese dicionario en castellano Y a partir de esto lo que tengo que hacer es trabajar como hicimos hoy con una nueva función esa función la idea avanzo Está aquí es lo mismo que hicimos hoy así como teníamos hoy el news steaming creo que lo habamos puesto a ver vamos aquí arriba el news stem Sí ahora vamos a crear un news lema que es la news con Apple aplicándole el método Apple redundancia solamente que en este caso no vamos a usar la función que usamos hoy que le habíamos puesto aquí arriba token steam sino que le vamos a poner aa función token lema esta función es parecida a lo que tuvimos hoy en este caso lo primero que hace es tokenizar no obviamente con el mismo método que antes porque ahora Estamos usando la librería spacy O sea que lo hace con nlp sí nlp acuérdense que es la instancia que hice de spacy load y lo que hace es justamente tokenizar en virtud de un texto que es pasado a lo mismo que hoy con otro método con otra librería pero lo mismo que hoy sí con lo cual ahora tengo tokens y creo un for parecido a lo que estuvimos hoy donde digo justamente token lema O sea aplico la lematización a cada token por cada token de tokens sí tokens es esta variable que insisto tiene todos los el contenido de una determinada news en un determinado momento hecho en tokes dividido en palabras sí bien entonces por cada palabra del conjunto de palabras siendo que el conjunto de palabras y toda la news siendo que ese token esté dentro de Alpha sea un elemento de is Alfa que responde a este método is Alfa que es propio de python no que también lo usamos hace un rato para ver que solamente tenga en cuenta los elementos de orden alfabético entonces una vez que tomo los tokens de la New y selecciono solamente aquellos que son y Alfa hago la ligmatizacion soy aquí arriba recuerdan un sh in the steams decía que de esta manera arma como una cadena donde vuelve a juntar todas las palabras que antes separó en la toqua S como un proceso inverso s no saco o separo las palabras le aplico el stemning o le aplico la lematización y luego las vuelvo a juntar para que me devuelva toda la news completa pero habiéndole pasado antes el stemin y ahora la lematización Bueno eso es lo que aplica esta línea lo que vamos a hacer con esto es cargar sí la la función si no no me la va a reconocer para que ahora allí yo cree esta esta nueva columna bien habiendo terminado este proceso al igual que lo que hicimos hoy vamos a imprimir la primer New en el caso de la lematización y fíjense que justamente aplica la lematización en este resultado y es diferente Pero también diferente al este me tornaba esto en una en un conjunto de palabras mucho más ilegible porque quitaba el sufijo en este caso lo que hace es tratar de reemplazar eh A esa palabra por su palabra raíz con lo cual en muchos casos una palabra puede estar más de una vez sí eh o reiteradas veces en este texto y no tener otra similar a la cual que haya que recurrir para buscar una palabra raí se acuerdan que hoy decíamos correr corriendo y quizás aquí por ejemplo la palabra durante fíjense que está está al 100% Por decirlo deguna manera porque quizás no haya otra palabra en todo este abecedario de todas estas news con la cual pueda confrontarse y poder lograr una una palabra raíz que represente a durante y otra palabra que esté muy cercana a durante sí Entonces esto obviamente transforma este texto en algo más legible pero aún así ha hecho una reducción importante de la dimensionalidad de los vectores Y eso lo vamos a comprobar ahora para lo cual lo que vamos a hacer es en principio como hicimos hoy con el caso del stemin redefinir la x y la i separar conjunto de tes y entrenamiento y vectorizar todo este conjunto ya está y ahora voy a averiguar Cuál es el tamaño del vector fíjense que el tamaño del vector es de 16736 es decir es una reducción inferior Sí en resultado digamos tiene una dimensión mayor Sí pero bueno en en inferior en en el resultado porque esta ha sido la mejor reducción porque redujo a 11,000 y esta redujo a 16,000 pero si la comparo con la dimensión original que era de 26,800 aún así es una muy buena reducción que está un poco por encima del 50% del vocabulario que se obtuvo al principio cuando no había aplicado ni el stemi ni la lematización bien en cuanto a eh crear el modelo lem matizado y eh medir la precisión veo que tengo buenos scores y que es guarda bastante relación con el caso de El el stemin tenía 92 y 78 Aquí tengo 91 y 79 sí es decir que la conclusión de esto es que la reducción de dimensiones que es muy importante no ha afectado el nivel de precisión del modelo tanto en el caso del stemin como en el caso de la climatización con lo cual tengo modelos casi una precisión prácticamente idéntica si una pequeñísima diferencia pero con reducciones de tamaños de El 50% y en el caso de la alción casi un 50% Así que esto es lo más importante tener en cuenta de los propósitos de este práctico y para todo lo que venga ahora más hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte Titulo: Clase 26 (parte  2) Curso de inteligencia Artificial \\n URL https://youtu.be/zMF966bkKvY  \\n 1070 segundos de duracion \\n Esta es la segunda parte de la clase número 26 te invito a empezar con  ella  otro concepto del nlp es la similitud entre vectores en realidad esto de alguna manera Ya lo hablamos cuando decíamos de que cada palabra o cada documento depende el caso estaba representado por un vector y la cercanía entre esos vectores me daba la idea de que eso podría ser similar si Estaban cerca o no tan similar si no Estaban cerca por lo tanto esto que tenemos aquí este gráfico lo vamos a recordar justamente porque tengo la idea de tres vectores en este caso A B y C y justamente puedo ver si A B y C son o no similares en virtud de su cercanía la cercanía en realidad se calcula en base al coseno del ángulo entre esos vectores es decir por ejemplo en este caso entre a y b la cercanía se me diría calculando el coseno de este ángulo que está aquí dibujado con esta línea roja o de a con c con esta línea verde det trazos bien en el caso que yo calcule el coseno de 0 y me de un es cuando literalmente un vector está encima del otro es virtualmente la misma palabra con lo cual la similitud sería máxima en La Virtud que eso no pasa hae bueno Obviamente que describirá un ángulo que en la medida que sea más chico va a describirme una situación de un texto o una palabra más similar y lo contrario sucede justamente que el ángulo fuera mucho más lejano o más grande tenemos que recordar como siempre venimos diciendo que justamente el vector en nlp representa una tokenización de los textos los textos están representados o las palabras de los textos ya vamos a ver más adelante depende que tomemos como unidad de tokenización están representados por vectores Y nuevamente la cercanía o la lejanía de ellos me habla de la similitud vamos a ejemplos de la aplicación de la similitud dees una de las aplicaciones prácticas es la similitud de documentos Sí ya que se usa mucho por ejemplo para si tenemos una biblioteca de datos muy grande que queremos clasificar podemos usar la similitud de vectores para juntar los documentos que son más parecidos y clasificarlos otra aplicación es el spinning de artículos y seo seo es el search engine optimization optimización en motores de búsqueda esto es muy importante porque muchas veces por ejemplo Google hace un seo de la página web que es cómo va a posicionar la página concretamente en base a la riqueza del documento determina esa ese posicionamiento perdón y hay muchas personas que piensan que copiando un documento y pegándolo en su página web van a lograr posicionarse mejor en Google pero Google tiene sistema de similitud de vectores justamente que hacen que descubran cuando hay muchas copias de los textos para evitar eso justamente también la aa nos propone una solución a través del spinning de artículos que se trata de tomar un artículo y variarlo tal que no diga Exactamente lo mismo pero conceptualmente el artículo diga o se trate de lo mismo otra aplicación son las recomendaciones por ejemplo de Netflix esto ya lo vemos aplicado en varias de las cosas que hemos visto hasta ahora Quién trabaja también usando el tema de la vectorización para ver en base a la review de las películas y buscar coincidencias con las películas que ya hayamos visto antes finalmente otra aplicación son los chatbot que también usan mucho la similitud de vectores ya que cuando hacemos una pregunta al Bot lo que hace este es vectorizar dicha pregunta y buscar en su base de datos vectorizada a cuál vector se parece más la pregunta que estamos haciendo Y nos manda la respuesta en base a ese concepto de similitud de vectores el siguiente concepto que vamos a ver aquí es el concepto de tf idf a ver de qué se trata este concepto básicamente tf significa frecuencia del término e idf significa frecuencia inversa del documento En qué se basa este método o Esta técnica Esta técnica es utilizada para reflejar Cómo o cuán importante es una palabra en el contexto de un conjunto de documentos en este caso no se toma en cuenta la relevancia o la importancia o el peso de las palabras sino por el contrario reduce ese peso y aumenta el peso de las palabras menos frecuentes dicho de otra manera las palabras que aparecen con más frecuencia en un documento pero muy poco o nada en otros documentos son consideradas las más importantes para este método Cómo se aplica este método tfidf se aplica aplicando valga la redundancia esta fórmula que tenemos aquí tf de TD por idf de TD para entender bien esta fórmula vamos a ir viéndola paso a paso primero el primer término Qué es tf de TD es el número de veces que un término aparece en el documento d dividido el total de términos en el documento d por ejemplo supongamos que el término stemen que vimos la clase pasada aparece 10 veces en un documento que tiene 1000 palabras Entonces el tf de ese término sería 0,01 porque sería literalmente 10 di 1000 paso Seguido el idf de TD significa el logaritmo base e del total de documentos del grupo denominado d mayúscula en este caso dividido el número de documentos en donde aparece el término t volvamos al ejemplo de recién supongamos que el total de documentos que tenemos es 10 Sí donde uno de los documentos el que referenciamos recién y solamente en un documento aparece el término steaming es el primero el que vimos recién en el caso anterior Entonces digamos que si tenemos 10 documentos y en un solo documento aparece el término stemin entonces la fórmula sería logaritmo e de 10 di 1 lo cual nos daría 2.3 Finalmente y habiendo logrado el cálculo de los dos términos de esta fórmula como lo vimos recién en el ejemplo tendríamos que tdf DF serí igual a 0 01 que es la primer parte de la Fórmula por 2.3 que es la segunda parte de la Fórmula lo cual nos daría como resultado 0.023 pensemos en lo primero que vimos si stemin aparece 10 veces en un documento de 1000 quiere decir que su pie sería 0.01 Pero aplicando este concepto de tf y DF donde le da más importancia a las palabras que menos aparecen el peso de esa palabra sería 0.023 es decir mayor a que hubiera tenido de manera natural como dijimos al principio de esta clase vamos a profundizar este concepto poniéndolo en práctica justamente a través del lab pnl 4 que es uno de los lab que ustedes tienen a través del campus virtual un lab de colab y vamos a aplicarlo a un ejemplo de un sistema de recomendación de películas vamos con ello bien estamos ahora entonces en el apn l4 dentro del entorno de cola donde el Ob objetivo como ya lo dijimos antes es crear un sistema de recomendación de películas aplicando el método tfidf es importante que tengamos en claro que para hacer un sistema de este tipo lo que tenemos que ver es la similitud entre una película y otra es decir para recomendar una película tengo que ver películas parecidas a una determinada cuando hablo de esto hablo de justamente en este tipo de técnicas transformar cada una de estas películas en un vector y para saber si una película es parecida a otra o no lo que tengo que hacer es justamente ver si esos vectores están cerca o están lejos para eso justamente lo que tengo que hacer es utilizar el cálculo del coseno para que me dé justamente el ángulo que existe entre esos vectores Y si están cerca estaré hablando de películas similares y si están lejos estaré hablando de lo contrario bien teniendo en claro esto vamos a pasar a ver las librerías que vamos a incorporar para este lab que en principio es pandas tf DF vectorizer que justamente el que va a ha ser la tarea de vectorización que referenciamos recién y también la librería para el cálculo del coseno y el ángulo justamente para ver lo que también referenciamos recién bien incorporamos estas librerías y luego vamos a ir justamente a montar el Drive donde tenemos este archivo de películas que como dijimos este recién también bajamos del sitio kagel vamos a ver justamente En qué sitio cag o qué página del sitio kagel bajo esta recomendación Aquí está justamente el link donde está este archivo movi metadata.csv que es el que vamos a utilizar para este lado bien habiendo aclarado eso justamente estamos con la idea de que ese archivo yo lo voy a poner obviamente en el campus virtual ustedes lo van a bajar y lo van a colocar en un Drive Drive que al montarlo como yo he hecho aquí en este caso lo vamos a usar para crear justamente un dataframe con esa información como estoy haciendo aquí luego vamos a ver el dataframe no con un Head vamos a ver todo el dataframe para ver que justamente tengo el título de la película aquí y tengo algunas características que me van a servir para poder comparar una con otra el género y un plot kw que son algunas referencias adicionales al género que son importantes para elocidad si una película es o no similar a otra lo que vamos a hacer aquí son dos cosas en principio tratar de buscar una característica que aune a estas dos pero antes que eso tengo que limpiar los datos de ambas características porque en ambos casos justamente está separado un concepto de otro a través de una barra vertical con lo cual voy a tener que reemplazar esa barra vertical por un espacio en blanco antes de ello vamos a estudiar Si este dataframe tiene valores nulos con lo cual ejecuto esto y veo que justamente género no tiene valores nulos pero plot keyw sí es más si yo miro Aquí rápidamente el dataframe con esta visualización rápida que hicimos ya veo que aquí en el cuarto elemento tengo en plot kw un valor nulo bien lo que tengo que hacer con ello como siempre es limpiar esos valores nulos y en realidad voy a reemplazar todos los valores nulos con espacios en blanco sea de esa característica pl divos u otra con lo cual si si Ahora vuelvo a ejecutar El Comando de Recién veo que en este momento ya no tengo más ningún carácter nulo paso Seguido lo que voy a hacer es reemplazar como dijimos recién estas barras verticales que veíamos Si volvemos aquí estas barras verticales que aparecen en género y en plot keywords por espacios en blanco justamente a través de esto género y con replace voy a reemplazar la barra por espacios en blanco y lo mismo para pl ejecutamos y hacemos una verificación de Data frame por las dudas y vemos que realmente ha hecho el cambio como corresponde ahora lo que tengo que hacer como dije antes es lograr una característica juntando estas dos Bueno vamos para ello con lo cual esta línea responde a esa idea texto igual a género más pl keyw con un espacio en el medio lo ejecutamos y vamos a ver rápidamente las cinco primeras observaciones para hacer un chequeo a ver si lo hizo bien efectivamente podemos ver justamente mostrando género y pl keywords y y la resultante de la Unión de ambos que le hemos puesto texto bien luego lo que tenemos que hacer es empezar a vectorizar Pero antes de vectorizar tengo que decirle esos vectores de qué dimensión van a ser y lo hago justamente especificando Max feature en 2000 con lo cual cada película va a ser representada por un vector de 2000 posiciones ahora teniendo claro esto lo que voy a hacer justamente es la transformación con lo cual voy a tomar toda la todo el Data frame completo sobre la característica texto y justamente transformar Esa esa nueva característica texto que recuerden la logramos uniendo género y plot keywords a vectores bien estamos aquí lo ejecutamos y ahora voy a ver qué tengo en x tengo una estructura de 55043 observaciones como el Data FR original pero no como el dataframe original 29 características sino que tengo ahora 2000 características porque justamente es lo que yo le dije que era el tamaño del vector que yo quería bien teniendo claro esto lo que vamos a hacer es tomar un caso testigo para hacer este este ejercicio de la comparación de una película con otra y vamos a tomar eh el vector de Piratas del Caribe Piratas del Caribe Recuerden que es la segunda película sí tenemos la primera sabat la segunda es O sea la que está indicada como número uno porque sabemos que el número ordinal empieza siempre de cero es la segunda película es piratas del carbe bien Entonces vamos a tomar aquí la segunda película con x1 tu array es decir voy a convertir Piratas del Caribe a una estructura de tipo array y aquí veo justamente cómo está conformado el vector de 2000 posiciones perdón de Piratas del Caribe lo que voy a hacer ahora justamente es con esta librería de similitud a través del coseno poniéndole como parámetro en principio la película que quio tomar como testigo la película a comparar a buscarle su par y luego todo el dataframe completo toda la x del dataframe completo para ver bueno compararme esta película con todas y decirme a Cuáles es parecida o qué Mejor dicho no va a decir a cuál es parecida va a dar el número o el porcentaje de similitud con cada una de ellas con todas las 543 en este caso con las 5042 otras películas que están en el dataframe Así que ejecutamos eso y ya tenemos dentro de similitud todas las comparaciones con cada una de las películas de El dataframe y Si vemos el contenido de similitud vamos a observar por ejemplo que tiene una similitud de 0.10 con la primer película que era Avatar como recordarán y tiene una similitud de uno es dec del 100% con la segunda película Por qué Porque se está comparando consigo misma Sí así del mismo modo veo que con la siguiente película tiene una similitud inferior a con Avatar fíjense que aquí tengo 0.05 y con Avatar era 010 si me fijo en la segunda película voy a ver que la segunda película es espectro no conozco la película pero sí vemos aquí que en caso de Avatar y pirata del Caribe tienen la Fantasía como un elemento en común y aquí esto es más un Thriller con lo cual está claro de Por qué es más parecida a Avatar que esta otra película bien Esto es para que tengan una aproximación numérica Y en este ejemplo de cómo maneja esta cuestión de la similitud en este caso si yo quiero ver la similitud la primer similitud por separado bueno pongo similitud 0,0 Por qué Porque si bien Esto está como una Ray digamos desde lo que vemos si esto que vemos aquí es una Ray es una Ray bidimensional Más allá de que a los fines de la cantidad de números es unidimensional con lo cual si yo quiero ver solamente el primer elemento podría transformar ese vector bidimensional en un vector unidimensional Cómo a través del método flatten que es lo que vamos a hacer a continuación y ahora fíjense que yo voy a Acceder al primer elemento pero no como 0 cer sino directamente como cero lo ejecuto y ven el mismo resultado pero lo tengo aplastado acuérdate que flat es un concepto de cuando quiero una matriz que está conformada como de dos dimensiones pero tiene en realidad una sola dimensión aplastarla en una única dimensión con la cual la transforma literalmente en un vector bien lo que voy a hacer ahora con esta instrucción es justamente ver Cómo ordenar las películas de más parecida a menos parecida justamente voy a crear una variable similitud ord y con el menos similitud le voy a decir que la quiero en orden inverso y con ord justamente las voy a ordenar bien lo ejecutamos y ahora con similitud gu B voy a tomar las 10 películas más parecidas de la 1 a la 11 como sabemos que se numera esto las 10 películas más parecidas en este caso a Piratas del Caribe ejecuto y veo los números de las películas más parecidas pero lo que voy a hacer es tratar de traerme el nombre de esas películas para que sea más claro entonces lo que hago justamente Es sobre el título de la película y a través del método ilock justamente con similitud gu or que es esto que tenemos aquí traer los valores 1 a 11 para que me traiga los nombres de cada uno de estos números de películas ejecuto y veo que las 10 películas más parecidas a piratas de Caribe son estas que están tituladas aquí justamente la última es otra versión de Piratas del Caribe bueno con esto ustedes pueden justamente cambiar en lugar de tomar Piratas del Caribe pueden tomar otras y justamente ver qué resultados les da y poder bueno entretenerse con este tema de la comparación a ver cuá reales a la percepción que ustedes tienen bueno con esto terminamos esta práctica y con esto terminamos la clase de hoy Hemos llegado al final de esta clase nos vemos en la próxima  clase Titulo: Clase 27 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/fVKkpb5q1Oc  \\n 1469 segundos de duracion \\n  Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número  27  Hola a todos Bienvenidos a la clase número 27 del curso de Inteligencia artificial de ifes hoy vamos a ver uno de los temas más importantes del mundo del nlp el tema  embeddings Qué es un embedding un embedding es una forma de vectorización similar a la que vimos pero que plantea una evolución respecto a esas formas para recorrer esas formas vamos a recurrir a esta imagen que graficab612 y de poner un ejemplo bastante sencillo habíamos tomado como que cada uno de estos renglones era un eh documento y cada una de estas columnas que tenos tenemos aquí dibujadas era el vector que representaba a cada uno de sus documentos y todo lo que tenemos aquí a la izquierda el abecedario me gusta el deporte hasta llegar a que odio y sí eran 18 palabras con lo cual tenía una representación de cada uno de los documentos con vectores de 18 posiciones y cada una de las posiciones tenía un número que representaba Cuántas veces estaba Esa palabra que estaba a la izquierda estaba en el contexto de ese documento Pero cuál es el problema aquí aquí tenemos vectores de 18 posiciones pero fíjense cuántas posiciones están realmente ocupadas en el primer caso tenemos ocupadas seis posiciones con lo cual tenemos 12 posiciones que no tienen ningún tipo de información en el caso del documento dos está un poquito digamos más relleno tenemos 1 2 3 4 5 6 7 8 pero tenemos igual más del 50% de las posiciones Sí están siendo ocupadas por espacios en blanco por ceros en el caso del documento 13 es lo mismo bueno Esto pensemos que es un ejemplo muy muy sencillo pero justamente si lo tratáramos de llevar a un ejemplo más complejo más real donde puedo tener un vocabulario de 2000 3000 y hasta por qué no 20,000 palabras ustedes piensen si en ese contexto yo dejara de utilizar O estaría sub utilizando de alguna manera Por decirlo este las posiciones de los vectores en esta misma proporción en una proporción del 60 por por decir algo estaríamos hablando de vectores de 20,000 posiciones de las cuales estaríamos usando solamente el 40% dec 8,000 es decir sería un despropósito y estaríamos Obviamente con una sobrecarga de información que no ayudaría nada a nuestro algoritmo y nuestro proyecto los enedis vienen a proponer una forma diferente de vectorizar un concepto diferente de vectorizar justamente tratando de proponer una solución que no eh bueno que proponga justamente una postura diferente que evite desperdiciar la cantidad de espacio que estamos desperdiciando en vectores que no tienen ningún tipo de información por eso este concepto de embeddings me lleva a una idea que sería la siguiente supongamos que en el caso de recién yo pensara en tener vectores en lugar de 18 posiciones que tengan con un vocabulario de 18 palabras supongamos ocho posiciones sí es decir un número significativamente menor con el método del contol de palabra Eso no se puede hacer pero sí lo podría hacer pensando en tener vectores que tengan valores que no tengan que ver como en el caso anterior que tenemos aquí la cantidad de veces que aparece una palabra sino otro tipo de valor que tuviera que ver con justamente ahorrar la cantidad de dimensiones pero que cada una de las posiciones del vector tenga un valor es decir que no haya posiciones que tengan valores cero o que estén subutilizadas en este caso fíjense que yo tengo por ejemplo Un ejemplo muy sencillo en el cual supongamos que yo pudiera estar representando un conjunto de palabras solamente con dos dimensiones y aquí tengo por ejemplo la palabra auto que está representada por un vector 0109 o sea de dos posiciones que tengan los valores 0.1 y 0.9 y la palabra camioneta eh representada por otro vector de dos posiciones pero que tiene valores 015 y 085 veamos que más allá de esta cuestión de que las palabras no están representadas por una gran cantidad de dimensiones sino solamente por dos los valores que tiene auto y camioneta son bastante parecidos esto es importante y lo tenemos visto ya desde la clase pasada donde hablamos justamente de la similitud de vectores Porque estos valores de camioneta y de auto representan la similitud que tienen sem iamente esas dos palabras entonces obviamente los vectores tienen que actuar en consecuencia tienen que tener valores en consecuencia del mismo modo tengo el ejemplo de casa y departamento donde veo que los valores 075 027 o 07 y 02 son muy similares y muy distintos a su vez de auto y camioneta es decir casa es similar a departamento auto es similar a camioneta pero ni auto ni camioneta son similares a casa y departamento y viceversa bien importante aquí es que el concepto de embedding me lleva a utilizar vectores que no tienen la misma cantidad de Dimensión que la cantidad de de palabras que tiene el vocabulario tiene una dimensión mucho menor y los valores de cada una de esas dimensiones representan un valor en un espacio vectorial de la cantidad de posiciones que sea pero que no tiene que ver con la ya con la cantidad de palabras que o la cantidad de veces perdón que aparece esa palabra en el documento sino con un número que en el espacio me permite Sí con un valor determinado establecer una similitud entre una palabra y otra o diferenciarla con otra palabra diferente los modelos para en bedim utilizan eh conceptos vinculados al Deep learning al mundo del Deep learning que nosotros Ya estuvimos incursionando en él justamente con lo que tenemos aquí como primer opción de tipo de de red para eh realizar o llevar a cabo el embedding que son las redes neuronales convolucionales bueno es una de las opciones no es la opción más utilizada la opción más utilizada es las redes neuronales recurrentes que es lo que aparece aquí arriba de la derecha y dentro de ese contexto Y con esa ampliación que plantea esta rnr hay un concepto que es concretamente el que se usa hoy en día que es el de Transformers que es el concepto de mayor evolución y de mayor uso en la actualidad vinculado a los procesos de embedding técnicas para eh llevar a cabo los procesos de embeddings hay muchas nosotros vamos a empezar por ver dos de las que hoy son muy importantes en el mercado Pero después vamos a ver otras que plantea justamente la evolución de las últimas tendencias esta primera que está aquí arriba es Word to Back es decir básicamente un juego de palabras que es o implica la transformación de una palabra en un vector y justamente es un algoritmo como dice aquí que utiliza un modelo ronal para aprender asociaciones de palabras a partir de un gran Corpus de texto o sea cuando hablamos de Corpus hablamos de un conjunto de palabras que puede estar representado por uno y hasta por varios documentos bueno el Corpus de texto es todo ese gran conjunto de texto a partir del cual yo voy a entrenar una red como bien dice aquí una vez que entreno el modelo puedo detectar palabras sinónimas o sugerir palabras adicionales para una frase sin terminar es tal cual lo que vimos lo que venimos hablando perdón de lo que nos pasa cuando estamos en gmail y escribimos un texto y nos aparecen palabras sugeridas a continuación de lo que estamos escribiendo Word tob representa cada palabra distinta con una lista particular de números llamada vector es decir lo que vimos recién en el ejemplo de esta relación aut camioneta o casa departamento Bueno eso es Ni más ni menos lo que hace Word o lo que hacen los enedis en general Sí y eh A partir de eso puedo justamente ver la similitud que existe entre vectores y en consecuencia ver la similitud que existe entre los textos a través de la teoría o el teorema del coseno que vimos la clase pasada que justamente midiendo el ángulo que existe entre los vectores puedo ver el nivel de similitud entre las palabras que representan esos vectores otra técnica para embedding es glow glow es un desarrollo de la Universidad de Stanford y significa vectores globales es una opción similar a Word desarrollada por otro proveedor como muchas veces pasa en el mundo de la informática y en el mundo de la ia y eh si bien tiene propósitos muy similares tiene una particularidad tiene en cuenta las estadísticas de coocurrencia del Corpus es decir se fija o va registrando estadísticamente los conjuntos de palabras o los pares de palabras Cuántas veces aparecen juntas en el Corpus para de esa manera cuando una palabra determinada esté siendo redactada por una persona por ejemplo en gmail como referenciamos recién la palabra que le sugiera a continuación sea la que más frecuentemente aparece cuando esa persona escribe al lado de la palabra anterior el próximo tema que tenemos que ver es el concepto de analogía de palabras en el ámbito de los embeddings y aquí tengo un ejemplo en un espacio bidimensional donde tengo países Argentina y Uruguay y sus respectivas capitales pero básicamente son ciudades Buenos Aires y Montevideo la idea aquí es que Argentina y Uruguay están cerca entre ellas porque en realidad identifica que se trata de dos países y del mismo modo Buenos Aires está cerca Dentro de este contexto de esta estructura bidimensional de Montevideo porque entiende que son dos ciudades pero más allá de eso también se puede plantear un concepto de analogía de qué se trata la analogía que yo pueda decir que si hago una relación Argentina Buenos Aires pueda descubrir Cuál es la potencial relación que puede tener Uruguay o Montevideo es decir que si yo digo que Buenos Aires es a Argentina como Montevideo es a Uruguay en realidad cuando yo digo estoy diciendo esto yo pretendo que la ia sea quien me lo diga por ejemplo yo puedo decir Buenos Aires es Argentina con Montevideo es a y ahí la ia me va a responder Uruguay Ese es el planteamiento de analogías y es parte de lo que vamos a ver a continuación en el lap número cinco donde vamos a ver justamente similitud de vectores con embedding y el concepto de analogías utilizando un dataset cagle de palabras en castellano nosotros vamos a hacer un primer lab en esta primera parte de la clase en el cual Vamos a abordar el tema de utilizar un embedding o un archivo que tiene los embedding ya hechos y en la segunda parte de esta clase Vamos a abordar el tema de eh hacer nuestros propios embedding con nuestro propio conjunto de palabras o nuestro propio Corpus bien aquí estamos en el apn l5 en el ámbito de colab y vamos a empezar con el propósito que aquí tenemos similitud de vectores con embedding utilizando una de kaggle con palabras en castellano antes de empezar a configurar todo este trabajo vamos a hablar un poquito de este eh dataset que vamos a bajar este Corpus hablando más en términos de nlp que bajamos de kaggle eh este dataset se llama pretrained Word vector for spanish el el título de El dataset como se lo identifica en kaggle es un dat que contiene 1,653 Word embeddings de Dimensión 300 o sea 1,653 palabras cada una de esas palabras está representada por un vector de 300 dimensiones fíjense eh lo importante de lo que hablamos hace un rato Es decir yo quizás con el conteo de palabras hubiera tenido un vocabulario de 1,653 palabras pero a su vez vectores para cada una de esas palabras de 1,653 en este caso fíjense la enorme diferencia que hay entre la dimensión del vocabulario y la dimensión de los vectores bien esto ha sido entrenado justamente con Word tob que es una de las herramientas que vimos recién en la teoría y obviamente ha sido entrenado con Corpus de billones de palabras en español esto es muy común hay muchos archivos que ya tienen los vectores armados con un vocabulario muy grande de palabras y me sirven mucho para esta fase de aprendizaje para probar cosas y no tener que formar yo mi propio Corpus pero como dijimos hace un rato en la segunda parte de la clase eso es lo que vamos a hacer obviamente no vamos a llevar a un Corpus tan grande como esto pero es importante que sepamos usar lo que ya está hecho bienvenido Que esté hecho y también empezar a usarlo nuestro dicho esto vamos entonces al principio y vamos a importar pandas y luego de ello vamos a montar el Drive como siempre hacemos dado que allí tiene que estar el archivo que hablábamos recién sbw vectors 3001 5.txt obviamente Este es un archivo que ustedes van a tener en el campus virtual Y ustedes lo van a tener que poner como siempre animos haciendo en el Drive que ustedes quieran utilizar Bueno ya tenemos montado el Drive lo que vamos a instalar ahora es una librería que se llama hensim que Eh no la tenemos seguramente instalada con lo cual vamos a hacer primero un pip install genim Recuerden que siempre que les digo que eh cada vez que no tengan alguna librería en la actividad que estén llevando adelante bueno pueden instalarla con pip install enim el signo de admiración refiere a que este Comando no se ejecuta dentro de colap sino fuera de colap y el menos q es para que no me muestre mucha de la información que muestra habitualmente un proceso de instalación es un detalle Simplemente no pasa nada si no pongo el menú q bien luego de importar el genim eh Perdón luego de instalar el genim lo importamos y como lo hicimos tantas veces si creamos un dataframe con pandas con rsb en base a este archivo que referenciamos recién y lo ponemos dentro de una refame que le ponemos como nombre DF bien Ahora vamos a ver el contenido de DF y podemos ver rápidamente queé tenemos en principio la palabra d dígito la enl O sea toda la palabra que tiene este gran voc formulario de 1,653 palabras y cada una de esas palabras está representada como dijimos antes por un vector de 300 posiciones en el cual aquí estamos viendo las primeras cuatro y un poquito de la quinta posición en algunos casos aquí solamente las cuatro Pero podemos observar las cuatro primeras posiciones de cada una de estas palabras con valores que son los que tiene justamente cada uno de esos vectores en el contexto de un espacio dimensional de 300 dimensiones sí bien eh aquí tenemos entonces toda la la información en una sola columna o sea está junto la dimensión del vector con el texto de la palabra y como dijimos antes las eh 1,653 palabras bien una vez que vemos esto vamos a empezar a crear los vectores o traernos los vectores a nuestra estructura a través de una variable que le voy a poner vectores y la voy a eh llenar justamente a través de la biblioteca gensin que acabamos de instalar recién y con lad Word tob format le voy a decir que vaya a buscar esa información de los vectores si a el archivo que recién acabamos de observar a través de este Data frame entonces a partir de esto todos estos vectores que estamos viendo aquí s todos estos vectores que están aquí al lado de cada una de las palabras van a ir o van a ubicarse dentro de esta variable de vectores en este práctico como dijimos antes vamos a ver dos cuestiones propias de los embedding la similitud y las analogías aquí por ejemplo Argentina y Uruguay son palabras similares Buenos Aires y Montevideo son palabras similares ahora Argentina y Buenos Aires son palabras análogas y Uruguay y Montevideo lo mismo eso lo vamos a ver justamente ahora en la codificación de El colap aquí estamos entonces luego de haber haber eh colocado todos los vectores en esta variable de vectores lo cual nos llevó un buen tiempo pero ya tenemos esta cantidad enorme de eh vectores dentro de esta variable con lo cual vamos a definir en principio una función analogía para ir poniendo distintos valores y no tener que estar reescribiendo el código tantas veces esta función le voy a poner ver analogía le vamos a dar como input tres valores La idea es como dijimos hoy en el ejemplo de la de la clase teórica que yo le pase por ejemplo Argentina y Buenos Aires y luego le pase Uruguay y automáticamente la inteligencia me dé la palabra Montevideo con lo cual siempre le voy a tener que dar tres valores y va a aparecer automáticamente el cuarto que va a ser la analogía de uno de esos valores para ello vamos a usar de eh la variable que creamos aquí vectores el método m similar en el cual le voy a pasar la el valor de entrada p1 y p3 que se dirían las dos palabras similares pero no análogas y luego con el argumento negativo le voy a pasar la palabra análoga de la primer palabra que puse Aquí concretamente vamos un ejemplo supongamos que en p1 yo pongo Argentina en p2 podría Buenos Aires en p3 pondría Uruguay y estaría la la representación de la palabra faltante como algo pendiente a resolver por parte de justamente m similar de vectores eso lo voy a poner dentro de esta variable similitud y luego voy a hacer un print donde voy a poner que p1 es ap2 como similitud 00 que es justamente el valor que me va a tirar la ia es ap3 bien con esta lógica que vamos a cargar porque esto acuérdense que las funciones la tenemos que declarar y ejecutarla no como que vaya a hacer una acción sino que esté identificada y dispuesta eh disponible Perdón esta función vamos a usar en principio con rey y hombre y luego la palabra mujer lo cual nos tiene que dar la posibilidad de que la palabra análoga que nos tiene que brindar esta función y Por ende la ia es la palabra reina lo ejecutamos y justamente el resultado es rey esa hombre como reina esa mujer lo probamos con Argentina argentino y español y dice que Argentina es argentino como español es a español como española Perdón es a español bien en el caso anterior la palabra que me brindó la ia es reina y en el caso este es española en este caso fíjense que toma Argentina como una mujer es decir no como la la patria como el país porque justamente la primer letra está puesta en minúscula si yo ahora pruebo poner la primer palabra en mayúscula va a tomar Argentina como país y o como una persona de sexo femenino propia de nuestro país sí Entonces ahora la respuesta va a ser Argentina es argentino como España es a español bien y así puedo probar aquí con taller mecánico y médico talleres a mecánico como consultorio ex a médico y con algunas ciudades Madrid es España como montevo es Uruguay bien con eso tenemos varios ejemplos de analogías ahora vamos con el tema de similitud con lo cual vamos a hacer otra función que le vamos a llamar ver cercanía donde aquí no entran eh un conjunto de palabras como en el caso de la función anterior sino simplemente una palabra a la cual le vamos a buscar cuál es el vector más similar o que representa una mayor similitud con esa palabra con lo cual en este caso vamos a usar el Moss similar pero ahora solamente con un argumento positivo antes teníamos el positivo los positivos y el negativo por esta cuestión de la analogía ahora simplemente Busco algo que es similar al argumento que entra con lo cual Busco algo que sea similar a lo que yo tengo aquí como argumento y luego hago un print de eh Cuáles son los elementos cercanos a p Por qué digo los porque justamente lo que vamos a ver aquí es tratar de determinar una cantidad de valores similares o sea no voy a tomar la sugerencia de solamente el vector que tiene mayor similitud sino voy a tomar una serie de candidatos eh de similitud con lo cual me va a dar un conjunto de palabras similares a la que yo le doy por eso hago un for con el cual recorro Sí justamente la variable vecinos que es la que aloja a todo el conjunto de vectores similares a la palabra ejemplo que yo le estoy dando y Forward Score In vecinos voy a mostrar justamente cada una de esas palabras y en este caso no estoy mostrando el score lo podría hacer también con lo cual puedo estar mostrando la palabra y el nivel de factibilidad que me da la la inteligencia respecto de cuán similar entiende es esa palabra a la que yo he candidatado bueno habiendo cargado esta función vamos a probarla con lo cual voy a poner la palabra fútbol y aquí tengo balonpie fútbol soccer béisbol basketbol o sea palabras similares recuerde que similares no quiere decir que sea concretamente algo que sea Exactamente igual sino palabras que tengan un alto nivel nivel de asociatividad pongo Argentina en minúscula representando como hoy sí una una persona de exceso femenino que pertenece a nuestro país no el nombre de la patria bueno y aquí me aparecen las recomendaciones de similitud ahora Argentina mayúscula y me aparecen países ven que en este caso aparecen países y en este caso aparecen otras cuestiones que tienen que ver justamente con Eh bueno las bolivianas las mendosinas las paraguayas las brasilenas tomando el femenino de una persona que está representada por esa parte bien con esto Terminamos el lab y em les dejo para ustedes a ver si quieren ponerles aquí Este esta variable score Sí este parámetro score para ver justamente Eh bueno qué nivel de score le da a cada una de estas palabras que consider las similares obviamente esto es importante porque después pu en base a eso pueden determinar con cuál de todas estas palabras ustedes se van a quedar a la hora de hacer una una inteligencia o programar un un algoritmo de nl bien con esto terminamos decía este lab con esto terminamos también esta primer parte de esta clase lo que nos queda para la segunda parte Entonces es algo similar de un proceso de embedding pero con nuestro propio Corpus hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte i Titulo: Clase 27 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/XLAdvjbQe6M  \\n 2313 segundos de duracion \\n Esta es la segunda parte de la clase número 27 te invito a empezar con  ella  bueno como dijimos en la primer parte de esta clase vamos a trabajar ahora con embeddings pero con un Corpus nuestro y no A diferencia de como hicimos en la primera parte de esta clase que trabajamos con un Corpus que habíamos bajado desde el sitio cer en principio vamos a usar tres archivos que tienen diferente nivel de volumen sí son diferentes niveles de tamaño yo se lo voy a estar pasando como siempre sipr a través del campus virtual y vamos a verlo rápidamente de qué se trata en cada uno de ellos bueno el primer archivo se llama chat gpt txt y es Ni más ni menos un archivo que me generó justamente esa herramienta en tanto y en cuanto yo le puse como prom que me dijera Cuál era la historia del ch gpt así que bueno me dio todo un texto que yo lo he volcado a un archivo de tipo txt que es el que le estoy pasando aquí el segundo archivo es eh el libro 20,000 leguas de viaje submarino que lo bajé en formato pdf que es Obviamente el famoso libro de julio berne y eh lo tercero es una carpeta recursos humanos que he bajado Bueno también de un sitio que se puede bajar documentos gratis 10 libros relativos a recursos humanos y este es el Corpus más grande que vamos a tener por lo tanto como dije al principio tenemos tres niveles diferentes de tamaños de Corpus volviendo a la pnl 6 vamos en principio a instalar una librería que se llama pip df2 que sirve para leer desde un documento de tipo PDF y transformarlo digamos en un formato txt o más propio del tipo de librerías que usamos nosotros para el tratamiento de los textos es decir que instalamos esa librería y luego vamos a importar la librería String hensim models del cual vamos a sacar el Word to que es el que nos va a permitir bueno hacer la transferencia de palabras en vectores y finalmente vamos a importar la librería que recién acabamos de instalar vamos con ello y luego como siempre montamos El colap Bueno allí tenemos ya activado el Google Drive donde vamos a acceder como dijimos antes al archivo de chap PT al archivo de 20,000 leguas de viaje submarino y a la carpeta con los 10 archivos de recursos humanos nuestro primer trabajo va a ser con el archivo más chico y vamos a ir gradualmente pasando a los de mayor tamaño con lo cual vamos a hacer aquí un Open del archivo chat gpt que yo lo tengo dentro de una carpeta nlp dentro de archivos dentro de mi Drive ustedes siempre le pueden poner aquí la ruta que corresponda al lugar del Drive donde ustedes hayan montado este archivo luego le decimos que lo queremos sair solamente para lectura y este encoding es uno de los encoding que tenemos como opciones para poder bueno evitar algunas cuestiones que tiene que ver con el formato en que nosotros traemos el archivo txt Bueno no es el único hay varios después les puedo dejar todas las opciones que hay no siempre este va a funcionar para todos los archivos de tipo txt con lo cual esta tarea de ubicar el encoding inapropiado es una de las tareas que siempre tenemos que ver más que nada cuando abrimos archivos perdón de tipo txt Bueno cuando leemos el contenido de este archivo chat gpt txt lo vamos a volcar a través del F read en un elemento un objeto que se va a llamar documento lo ejecutamos e inmediatamente vamos a ver qué tiene ese documento documento que no va a tener otra cosa más que todo el documento chat gpt.com eh en documento está todo el documento completo Si recordamos lo que vimos hace un rato se trata de este documento que está acá sí obviamente como dijimos recién solamente por eh el límite que tiene de visualización El cola me muestra una parte Pero dentro de la variable documento está todo este texto este Corpus que está dentro de este archivo que hemos puesto o le hemos dado a llamar chat gpt txt bien una vez que hicimos esto vamos a medir el largo del documento el largo del documento no es Ni más ni menos que ver Cuántas palabras tiene el documento es decir con len de la variable documento voy a ver Cuántas palabras tiene la variable documento hecho justamente en base a chat gpt txt Así que literalmente como que dijera Cuántas palabras tiene este archivo chat gpt txt lo ejecuto y me dice que tengo 31168 palabras Bueno lo que tenemos que hacer a continuación es el preprocesamiento de datos ya sabemos que los datos como vienen en un solo lote no podemos procesarlo sino que tenemos que hacerlo de al lotes Sí entonces lo que se hace primero en estos casos es separar todo el Corpus en oraciones ahora cuál es el criterio para separar o para darle a entender a esta inteligencia hasta dónde llega una oración y dónde empieza la otra bueno se puede hacer con diferentes este tipos de puntuación habitualmente con las comas y los puntos eh En este caso vamos a usar la coma porque en algunos casos de algunos Corpus el punto eh representa oraciones demasiado largas y quizás nos convenga que sea un poco más corto y la coma siempre representa una entidad que tiene un sentido también los Corpus se pueden dividir en oraciones que tengan una determinada cantidad de caracteres eso también es posible lo que pasa es que en ese caso le quitamos la entidad a la oración una oración si yo le pongo que lo corte cada 100 cada 100 palabras Eh quizás en esa eh palabra número 100 No termina de representar un sentido semántico simplemente es una cantidad de 100 palabras bueno eso hay que tenerlo en cuenta porque el entrenamiento tiene que basarse en que entienda esa inteligencia el sentido de lo que está escrito y a partir de ello empieza a hacer todo el tipo de actividades que ya vamos a ver qué podemos hacer con ello Así que en este caso vamos a tomar como símbolo de split o como elemento para separar oraciones la coma lo hacemos ponemos ahora todas las oraciones del documento separadas por comas dentro de una Ray que le voy a llamar oraciones y eh imprimo e Perdón imprimo a través del l la cantidad de elementos que tiene ese Ray y veo que tiene 200 elementos con lo cual este Corpus que tiene 31168 palabras ha sido dividido en 200 oraciones si podemos ver eh cualquiera de ellas simplemente bueno como es una Ray colocando colocándole Perdón aquí el número de ítem que quiero ver sabiendo que puedo ir de 0 a 199 porque tiene 200 oraciones y voy a tomar el primero de ellos el cero y lo vamos a imprimir porque así puedo chequear justamente si funcionó bien el concepto de splite a través de la coma Por qué Porque me fijo que todo este texto termina en amente el desarrollo de chat gpt si voy al texto original veo que justamente en esa parte está la coma con lo cual la primera oración que ha identificado es toda la que precede a la primer coma y por supuesto en todo el resto de los elementos de la Ray con el mismo criterio está haciendo Exactamente lo mismo con lo cual veo que en principio con esta simple observación veo que está funcionando bien este este sistema de tokenización de splite a través de el uso de la coma la siguiente tarea que vamos a hacer va a ser la de limpiar las oraciones por qué miremos un poquito aquí la primera oración la cero que pusimos como ejemplo y ya estoy viendo que por ejemplo aparece un barra n que representa un enter Sí este igual es un documento muy sencillo De hecho ya viene de parte de skpt Que obviamente da un texto muy este muy accesible pero vamos a ver más adelante que los textos vienen muchas veces con un formato muy difícil de tomar Así tal cual está en algunos casos como en el caso que vamos a ver por ejemplo de documentos de tipo PDF bien hasta con imágenes y las imágenes obviamente es un elemento que no debe procesarse y debe quitarse cuando se transfiere de PDF a texto pero eso ya lo vamos a ver un poquito más adelante Lo importante es entender que siempre el texto del Corpus tiene elementos extraños que debemos limpiar Pero además también queremos quitar todos los elementos de puntuación es decir que los puntos las comas u otros elementos similares también van a ser quitados Por ende tengo que con el array oraciones crear un nuevo Ray que se llama ahora que se puede llamar como seaas yo le he puesto oraciones limpias Pero bueno Al fin y al cabo un nuevo Ray que tenga como su nombre lo indica aquí oraciones que estén limpias de todos esos tipos de elementos que acabo de mencionar recién por lo tanto creo nueva Ray y voy a con este for recorrer todos los elementos de la Ray oraciones como for oración y oraciones es decir tomando cada uno de los elementos de esta Ray Oraciones al tomar una oración cualquiera lo que voy a hacer es con el método Translate y con el método make trans d str voy a hacer una operación de división pero a la vez o de tokenización Mejor dicho pero a la vez quitando todos los símbolos de puntuación justamente esta este método mtr lo que me permite es reemplazar un elemento con otro elemento y en el tercer argumento de esa función le puedo decir Cuáles son los elementos que quiero eliminar en este caso particular yo no voy a usar ni el primer argumento ni el segundo porque no quiero reemplazar un elemento con otro sino solamente quiero la tercer parte de esta función o el tercer objetivo que puede cumplir esta función que es justamente quitarme todos los elementos de puntuación con esto Entonces quito todos los elementos de puntuación de cada una de las oraciones y con split simplemente lo que hago es dividir cada una de esas oraciones en palabras Es decir aplicando el concepto de tokenización que ya vimos antes una vez que tengo todos los tokens dentro de la r tokens voy a hacer el proceso de quitar de ese contenido todos los elementos que no sean de orden alfabético y además los paso todo a m esto ya lo hemos hecho en las las clases anteriores con lo cual no aporto Nada nuevo con lo cual repasamos aquí quito los símbolos de puntuación y splite para tokenizar la oración aquí entonces tengo cada una de las palabras luego con cada una de las palabras lo que hago es pasarlas a minúscula y sacarlas o no incorporarlas dentro de la Ray oraciones limpias Perdón que estoy creando si no son caracteres de tipo alfabéticos bien una vez que logro ese objetivo pregunto If tokens por qué porque no va a ser este caso pero ya vamos a ver más adelante que puede ser que luego de este proceso de limpieza de las oraciones una oración esté conformada enteramente por elementos que no quiero con lo cual es probable que yo tenga una oración menos del total de oraciones que tenía originalmente Por eso pregunto de la limpieza que hice En esta oración quedaron tokens Bueno si quedaron tokens quiere decir que tengo elementos para agregar a el array oraciones lipas bueno dada esta explicación lo ejecutamos en virtud de lo que decía recién voy a volver a hacer un len de la cantidad de oraciones por si llegase a dar el caso de que hubiese alguna oración que luego de ser eh tokenizado Y bueno con todos estos pasos que vimos anteriormente sacándole el símbolo de puntuación este y eh quitándole caracteres que no son alfabéticos pueda eliminarse de El la cantidad original que había de oraciones pero veo que tengo en oraciones limpias 200 oraciones como tenía en el array anterior que no estaba procesado no estaba limpiado y depurado con lo cual no he perdido ninguna oración esto se ve claramente porque insisto es un texto de charg PT no cabía otra posibilidad que fuera eso vamos a ver cómo quedó así como hicimos antes se acuerdan aquí arriba que miramos Cómo era oraciones subcero ahora voy a ver cómo es oraciones limpias su cero lo imprimimos y veo que tengo el mismo array de antes sí ven el contenido este que está acá sí el primer elemento pero ya no aparece como todo un texto conjunto como está aquí todo un texto conjunto sino que aparece tokenizado es decir palabra por palabra separada sí sin elementos que no sean de tipo alfabético y sin ningún tipo de símbolo de puntuación ni puntos ni comas sí bien ya tengo entonces eh este texto procesado con lo cual lo siguiente que tengo que hacer tal cual dice Este título que está aquí es tomar ese texto para entrenar un modelo con Word tob es decir tengo que pasar todos estas estas oraciones que han sido depuradas y limpiadas a formato de lector bien Ahora vamos a entrenar Entonces el modelo Word tob con lo cual vamos a crear una variable model como la hemos usado muchas veces para crear Este modelo y vamos a aplicar el método Word to Back para ponerle todos los parámetros con los cuales queremos que efectúe esta tarea de transformar palabras en vectores en principio de dónde tiene que sacar la información de las oraciones para hacer eso obviamente de la Ray que cargamos de crear oraciones limpias a través de el parámetro sentesis pero existen otros parámetros que en este caso lo vamos a ir eh Bueno aquí le he puesto una ayuda para que quede constancia de esto pero lo vamos a explicar rápidamente el primero esos es muy fácil de de entender le estoy diciendo que todos los vectores que me generé Word tob con este proceso de embedding que voy a hacer ahora todos los vectores tengan dimensión de 500 esto obviamente es un parámetro que ustedes pueden manejar en virtud de la dimensión del lenguaje que tienen luego tengo window que es para darle contexto acuérdense que en este caso es muy importante que la se entienda que la palabra no es un ent aislado que para poder entrenar esta inteligencia esa palabra tiene que estar dentro de un contexto con Windows le digo Cuántas palabras antes de la palabra que estoy procesando Y cuántas eh palabras después debe entender que forman parte del contexto de esas palabras luego con mincom le puedo decir cuántas veces tiene que aparecer una palabra para que pueda ser considerada si puede ser tenida en cuenta o no para entrenar el modelo con uno le digo que cualquier palabra Que aparezca obviamente va a aparecer una vez ya es lo mínimo que puede aparecer va a ser tenida en cuenta y finalmente workers 8 indica esto es una cuestión muy técnica si ustedes tienen la posibilidad de contar con un cpu que haga procesamiento en paralelo bueno con el ocho le pueden decir que están utilizando ocho núcleos para que trabajen en paralelo bueno Esta es una cuestión insisto muy técnica y hay que ver si Más allá de la de que sea una cuestión técnica ustedes tienen un equipo que tenga es esa posibilidad perdón de procesamiento dicho todo esto ejecutamos Esta instrucción y ya qué tenemos en model en model tenemos ahora todas las oraciones limpias transformadas a vectores y esto lo puedo ver justamente poniendo a través de este método wb corta una palabra cualquiera chat gpt que estoy seguro que está obviamente esa palabra Entonces le voy a decir que me muestre Cuál es el vector de la palabra charg PT H Entonces lo voy a poner dentro de esta variable vector para después poder imprimirlo Sí ahí me cargo el vector de esta palabra en la variable vector y luego voy a imprimir el vector vector h y voy a ver que en el caso concreto que estamos viendo aquí vamos aaro un poquito para verlo mejor s tengo que la palabra chat gpt está representada por todo este array que tiene cuántas tiene 500 dimensiones dado que cada uno de los vectores con que está representado el vocabulario de todo este Corpus en particular en este caso de la palabra chatp como una de las palabras tiene o está representada insisto por un vector de 500 posiciones esto es a título de curiosidad para que se entienda Cómo ha hecho esta transferencia este Word tob Sí este sistema o Esta técnica de Word to que recuerden como vimos en la teoría existen además de glob hay dos métodos muy conocidos para ello Nosotros hemos utilizado a los fines de este práctico Word luego una vez que ya tengo eh cargado todo este conjunto de vectores entiendan lo siguiente Estoy parado en la misma situación que la que estuve al principio del lab anterior del lab 5 por qué Porque el lab 5 yo lo que hice fue traerme todo un vocabulario que ya estaba vectorizado es decir que ya tenía todo este trabajo que hemos hecho aquí desde que empezamos este lap hasta ahora yo ya lo tenía resuelto cuando En el laba anterior me traje ese archivo en este caso el archivo lo generé yo entonces ahora estoy en condiciones de hacer lo mismo que hicimos En el lab anterior es decir poner una palabra y que me di Cuáles son las similares a esa palabra entonces justamente Aquí está el método que usamos recién m similar y le voy a poner chat gpt y le voy a decir que me muestre las 20 palabras más parecidas sí lo pongo dentro de una variable de palabras cercanas e imprimo el contenido de palabras cercanas vamos con ello y Aquí tengo las 20 palabras más cercanas de hgpt siempre van a ver que van a aparecer entre las eh primeras seguramente fundamentalmente cuando es un cuerp muy chico esto ya se va a ir depurando y a lo largo de este práctico que vamos a ver que hay Corpus más este más grandes que las palabras que van a aparecer al principio son las palabras comunes que no es que tengan una implicancia directa con ch gpt Pero obviamente son las palabras más utilizadas que tienen más relación no con ch gpt sino con cualquier palabra que esté en el Corpus y al final aparecen palabras Un poco más propias lenguaje medida y a aplicaciones y respuesta hago lo mismo con red y Bueno aquí hay una excepción apareció la primera variedad HM pero después aparecen palabras más comunes pero sí después a diferencia de la anterior aparecen ya eh más palabras que no son dan de uso genérico como máquinas uso aplicaciones luego ia luego avances lenguajes garantizar siempre fíjense que aparece un índice de score que lo vimos también en el lab anterior que es la palabra que tiene más eh concurrencia con eh Esa palabra que yo he puesto aquí más asociatividad más similitud bien y finalmente lo hacemos También con otra palabra máquinas y vemos que tenemos Bueno aquí la experiencia es similar a el primer caso me parecen siempre palabras muy comunes hasta que luego aparece medida chat gpt respuestas Y a y lenguaje ahora volvamos a lo que hablamos ha un rato respecto de que yo les decía que en el lab anterior habíamos tomado un archivo que ya estaba vectorizado Y en este caso el archivo lo tuvimos que hacer nosotros resulta que cuando yo hago mi propio archivo no es que lo tengo que procesar permanentemente ente yo puedo hacer ese archivo y guardarlo para que después si lo quiero volver a usar no tenga que volver a hacer todo lo que hicimos hasta ahora y cuando yo cargo ese archivo que lo hice yo va a ser lo mismo que lo hicimos Perdón que lo que hicimos En el lab anterior cuando cargamos un archivo que no era nuestro Pero no importa en realidad yo estoy cargando un archivo que ya está vectorizado el origen es lo de menos obviamente en este caso va a ser el origen un archivo que hice yo y en el caso del lado anterior un archivo que lo tomé desde C para hacer eso es importante que veamos aquí que tengo un método save que me permite guardar mi archivo de vectores con extensión punto model y luego voy a hacer lo inverso que es bueno volver a traerlo como si nunca lo hubiese generado en este propio momento sino que ya hubiese estado generado de antes y vuelvo a recurrir a él como con Word to lad lo vuelvo a cargar y lo cargo En una variable que se va a llamar modelo cargado bien Ahora yo no voy a trabajar ya con model voy a trabajando antes sino que voy a trabajar con modelo cargado usando el mismo método mod similar es decir voy a hacer Voy a simular la situación como dije recién que es un modelo que yo no hice ahora sino que lo tenía de antes y ahora recurro y lo cargo insisto igual que como hicimos con ese archivo que no nos pertenecía en el Lama anterior bien a partir de que tengo Esto bueno ejecuto y veo que el resultado que tengo por ejemplo para para chat gpt exactamente el mismo que el que tuve con el modelo que fue el que generamos ahora mismo sí de este modo se pueden dar cuenta que el modelo va a actuar de igual manera si yo lo proco el mismo momento o si me lo traigo luego de un tiempo para utilizar bien vamos a hacer ahora el mismo recorrido pero con un libro en este caso 200000 leguas de viaje submarino libro de Julio berner es decir este libro que muestro Aquí bueno que es un PDF y que tiene Bueno una cantidad bastante integr Grand de 327 páginas Sí bueno no tiene muchas imágenes Sí al principio lo que les comentaba hoy pero seguramente no va a ser un texto tan depurado como en el caso de que utilizamos en el perdón en la práctica reciente este documento tengo que preprocesal como siempre pero en el caso particular insisto no puedo tomarlo como el el caso del ejercicio anterior porque no es un archivo de tipo texto yo tengo que transformarlo ahora en un archivo de tipo texto Sí para ello voy a crear una función que le voy a poner extraer texto desde PDF al cual le voy a pasar la ruta del archivo y el nombre del archivo por supuesto y voy a retornar el texto completamente eh diferente al formato original más propio de el formato que vimos Recién con el archivo gpt txt bien el hecho de hacer una función siempre tiene que ver con una cuestión de practicidad lo pueden poner como un código completo sin que esté en el formato de una función desde ya bien lo que voy a hacer en principio con el Wi Open es abrir el archivo y una vez que abro el archivo voy a ejecutar el método PDF reader de la librería que incorporamos para leer justamente contenidos de archivos tipo PDF de el archivo que Acabo de abrir y voy a poner todo ese contenido dentro de una array lector con lo cual Dentro de este lector dentro de esta variable que va a ser el tipo array lector voy a tener Ni más ni menos que todas las páginas de este archivo es decir del libro que acabamos de mencionar recién y luego vamos a crear un String donde voy a tener todo ese contenido de este array pero en un formato de texto parecido insisto al del ejercicio anterior para ello voy a hacer un for y voy a recorrer justamente a través de lector pages sí toda la cantidad de páginas que tiene lector la cantidad de páginas que tiene El lector es la cantidad de páginas que pude traerme desde el archivo de 20,000 leguas de viaje en sumaría bien cada vez que tome cada una de esas páginas por eso por página in Range Sí o sea tant tantos recorridos tanto forer como páginas tenga ese documento lo que voy a hacer Voy a aplicar el método stract text de lector pages su página es decir voy a tomar la página cero y voy a extraer el texto y lo voy a poner en la variable texto en el siguiente Ford voy a extraer la página número uno voy a extraer el texto y lo voy a ir acumulando en esta variable de texto con lo que vengo trayendo desde la páginas anteriores a fin de que ejecute esta función pues entonces en texto tendré todas las páginas una al lado de la otra dentro de una variable con un formato similar insisto al tipo txt que usamos antes bien para ejecutar esta función que la cargo con esta con esta con este rank que le voy a poner ahora voy a colocarle aquí justamente el uso de esa función y lo voy a poner dentro de una variable documento y le voy a mandar a esa función Ni más ni menos que toda la ruta y el nombre del archivo que voy a usar bien una vez que cargué todo voy a ver qué tiene la variable de documento y veo no me voy a sorprender tengo todo el contenido del libro Pero obviamente como siempre me muestra los primeros la primera parte del documento fíjense que aparecen ya muchos más eh mayor cantidad de caracteres extraños que los que tenía en el caso del documento anterior obviamente era mucho más sencillo y mucho más chico bueno Esto es importante que lo tengamos presente porque ahora vamos a ver primero Cuántas palabras tiene este documento y veo que tiene 86949 palabras y a continuación lo que voy a hacer es dividir ese documento como hicimos antes en oraciones también con el mismo símbolo de coma lo ejecuto y voy a poder ver que tengo en este caso en base a esa cantidad de palabras 10729 oraciones recuerdan el caso anterior que tenía 31,18 palabras y tenía 200 oraciones Bien voy a ver también como lo hice antes el contenido de la primera oración Bueno aquí está el contenido de la primera oración y voy a ver otra por ejemplo la 221 y ahí tengo ejemplos de distintas oraciones obviamente la medida que yo le ponga un un número aquí Perdón que esté digamos dentro del Rango de este de esta cifra Bueno me va a mostrar el contenido de esa oración lo que vamos a hacer ahora es el mismo proceso de oraciones limpias que hicimos en el ejercio anterior Así que no hay mucho que explicar aquí simplemente lo ejecutamos y voy a ver ahora que la cantidad de oraciones limpias que tengo es 10720 Qué pasó Aquí ya no es lo mismo que en el caso anterior se acuerdan que le dije que en este proceso de oraciones limpias podía darse la situación de que tenga oraciones que estén íntegramente conformadas por caracteres no deseados con lo cual esa oración deja de existir y es lo que ha pasado aquí fíjense que yo tenía originalmente 10,700 estas 29 oraciones y ahora oraciones limpias tengo exactamente nueve menos si ahora miro oraciones limpias con el índice 221 que es el que miré antes sí este que está aquí lo voy a ver pero en un formato de oraciones limpias justamente donde están las palabras bien tokenizadas y sin ningún carácter extraño luego de esto lo que voy a hacer es entrenar un nuevo modelo al igual que como lo dice antes con los mismos valores de los parámetros Sí con vectores de 500 buen Exactamente igual y empiezo a buscar palabras similares como eh en el contenido de este libro refiere a cuestiones de un viaje submarino la primer palabra que voy a usar va a ser mar y fíjense que ya no aparecen palabras de uso general como antes salvo del h es muy excepcional la mayoría de las palabras tienen que ver específicamente con el tema en sí que es mar nautilus que era el nombre de submarino fondo su rojo bueno Asia también es una palabra uso general salón interior Horizonte océano agua cielo golfen pruebo con nautilus que insisto es el nombre Cómo se llamaba el submarino del cuento sí bien mar dirigió Sur Horizonte Polo salón rojo bueno etcétera etcétera uso una más Nemo que era el nombre del capitán del submarino bueno fíjense que casi no existen palabras que sean o representen uso general si aparecen algunas palabras que hay que ver después bien cómo ha sido el splite más que nada con los enters muchas veces puedo tener eh palabras que están por ejemplo este hijo que está acá o este guo evidentemente es parte de un enter que en el cual yo eh está dividido una palabra que no es lo usual sí supone que el enter tiene que ser que la palabra quedó en la línea anterior o pasó a la próxima bueno son parte de las cosas que hay que hay que depurar pero evidentemente ya tenemos una evolución en cuanto a la similitud de palabras respecto de el caso del ejercicio anterior bien y la última parte de este lap sería con el caso más complejo donde tengo 10 libros de recursos humanos en formato pdf lo primero que voy a hacer va a ser instalar la librería tqdm la cual me va a dar la posibilidad de este proceso que va a ser un poquito largo que voy a llevar a continuación me muestre una barra de Progreso y no solamente eso sino que me pueda dar información respecto de En qué nivel de tiempo lleva ese proceso y cuánto me falta para poder llegar al final luego voy a importar Eh bueno estas librerías algunas ya las importado antes esto importa Porque evidentemente queremos contextualizar cada ejercicio a sí mismo por más que a lo largo de este colab ya lo hemos hecho obviamente cada uno de ustedes sabe perfectamente qué tiene que volver a cargar y queé no con lo cual cargo os el librería que me sirve para leer los PDF y esta librería que acabamos de instalar lo que vamos a hacer a continuación es si una función de extracción del texto desde PDF que es Exactamente igual a lo anterior de nuevo la vuelvo a cargar a los fines de contextualizar esta esta parte del práctico pero no sería necesario hacerlo y luego lo que voy a hacer ahora sí que es diferente es poder ir Armando todo este gran Corpus ya no leyendo un documento como en los dos casos anteriores porque Más allá de que uno era txt y otro era PDF igual era un documento sino que tengo que recorrer un conjunto de documentos que está en una carpeta e irlos incorporando uno por uno por eso en principio voy a crear una matriz que le voy a poner todos los textos y voy a recorrer for archivo en tqdm de os listdir Comando que ya lo conocemos muy bien porque me da todo el contenido de una carpeta de ruta carpeta Qué es ruta carpeta esta ruta que les estoy mostrando aquí arriba sí acuérdense que esta carpeta rr hh Perdón es la que yo le pasé por el campus y el resto del pas tiene que ver con lo que yo establecí dentro de mi Drive Y ustedes tienen que contextualizar de acuerdo al a lo que ustedes tengan en su propio Drive Bueno luego lo que hace es pregun ar si el archivo que encuentra dentro de esa carpeta termina en PDF Entonces lo tiene en cuenta y arma la ruta completa y tra Acuérdese que tra es intento intenta tomar todo el contenido de ese archivo que está en PDF y ponerlo dentro de estab de documento es igual a lo que hacíamos antes solamente que ahora lo estoy recorriendo en el contexto de un Ford por eso en este caso necesito este aray todos los textos para juntar cada uno de los documentos cada uno de los contenidos de los 10 documentos que es lo que estoy haciendo aquí con el apen es decir tomo cada uno de los documentos y los agrego a este array todos los textos luego le pongo un excep por el caso de que la apertura de algún documento pueda generarme algún tipo de error como usamos el tqdm voy a ver la barra de Progreso de este proceso que va a ser como dije antes un poquito largo bien una vez terminado este proceso voy a ver el largo que tiene este Ray Que obviamente tiene que ser 10 porque había 10 documentos constato que eso es así y bueno ahora también tengo que hacer un proceso diferente respecto de lo que hacía antes antes ejecutaba solamente esta instrucción Qué pasa ahora obviamente tengo como dije antes una Ray Por qué Porque tengo 10 documentos entonces voy a crear una aray oraciones totales y voy a crear una variable tamaño más que nada a título de curiosidad porque tengo que determinar como lo dice antes la cantidad de caracteres totales que tengo o sea en este gran Corpus conformado por 10 documentos con lo cual voy a hacer un for voy a tomar for documento en todos los textos Sí todos los textos acuérdense que es lo que tengo aquí el contenido de cada uno de los 10 documentos y lo que voy a hacer en principio es tamaño igual a tamaño más len de documento es de cada uno de los documentos contabilizando Sí el total del Corpus tomando los len de cada uno de los documentos luego hago el splite del documento en concreto que estoy tomando sí es del documento 1 2 3 cu Sí cada uno de ellos lo pongo pongo oraciones y ese oraciones lo voy a colocar dentro de El array oraciones totales en este caso como bien dice aquí la explicación No puedo usar apen porque oraciones es una ar rray entonces voy a agregar una rray a otro ar rray se usa extend y no apen bien ejecuto esa  instrucción y ahora vemos Cuántos caracteres tiene este gran Corpus tiene 5,1666 palabras las cuales están separadas en 7049 oraciones a continuación hago el mismo proceso de limpieza de operaciones con el mismo aray que le vuelvo poner el mismo nombre de oraciones limpias y voy a ver cuál es el largo de oraciones limpias que tengo ahora y veo que tengo 37904 oraciones vamos a medad del original tenía 7049 y ahora tengo poco más de la la mitad en este caso el proceso de operaciones limpias ha sido muy eh significativo Porque ha eliminado un número muy importante de oraciones que como dijimos en otras oportunidades porque tienen solo símbolos de puntuación o solo elementos que no son alfabéticos eh Son eliminadas digamos de esta de este proceso de limpieza de oraciones es decir que del total del Corpus que tenía 7049 oraciones tenía bueno unas 33,000 y un poquito más oraciones que en realidad no me servían y no me aportaba nada al Corpus esto es muy bueno porque lo que voy a hacer ahora es una vectorización con las oraciones que realmente sirven al igual que como lo hicimos antes vamos a mirar un determinado elemento de ese radio de oraciones limpias vamos a tomar el elemento 36571 Pero obviamente puede ser cualquiera lo ejecutamos y vemos aquí bueno todo un conjunto de eh caracteres de palabras que forman parte de la Ray número 36571 bueno Esto pues ustedes pueden poner el nomo que quiera y pueden ver justamente el resultado de las oraciones limpias esto es tal cual lo que hemos hecho antes por eso tal cual lo que hemos hecho antes también es la creación del modelo y ejecutamos la misma línea de código que venimos usando en los dos casos anteriores Y ya tengo mi modelo Que obviamente va a tardar un poco más ahora pero lo voy a volver a probar como lo hicimos anteriormente bien terminado este proceso vuelvo a hacer lo mismo que en los casos anteriores en cuanto a ver palabras similares y elijo empleado al principio ahora voy a elegir top 10 porque ya este Corpus es más grande y fíjense que no hay ninguna palabra que sea de tipo de uso general artículos el la d que veíamos más que nada en el primer caso y un poco menos en el segundo caso en este caso no aparece y justamente esto eh de que aparezcan primero las palabras Eh más generales hacía que yo pidiese que el top n fuera 20 para poder ver Bueno más adelante para ver que sí tuviesen más que ver con la que yo utilizaba aquí como referencia en este caso puedo reducir justamente a 10 porque ya insisto las palabras que son de uso general no aparecen hago lo mismo con la palabra trabajo y bueno vean que aparece puesto bueno acá aparece de nuevo una palabra pegada que son cosas que hay que depurar puesto lugar empleo laboral valor relativo cargo equipo comportamiento eh F aporte supervisor currículum desastre hay que ver porque desastre instructor movimiento bueno Esto lo dejo para que ustedes aquí con humano también puedan este bueno poner la palabra que ustedes quieran y puedan practicar y ver qué sale competencias bueno y insisto ya ustedes pueden poner la palabra que quieren Bueno un último dato que les dejo aquí al final de esta clase que ha sido bastante larga es este resumen interesante para poder comparar justamente los tres Corpus que hemos usado en este lab en principio en el caso de chat gpt txt la cantidad de caracteres que generamos fue 31168 200 oraciones y 200 oraciones limpias ya cuando pasamos a un documento que tenía 327 hojas recuerden pasamos a 86949 caracteres 10729 operaciones y una diferencia solamente de nueve cuando hacía el proceso de limpieza de operaciones de oraciones perdón y luego en el caso de los 10 documentos de recursos humanos ya tengo bueno una cantidad de caracteres mucho mayor prácticamente Eh sí casi seis veces un poquito más de seis veces respecto del libro de 20,000 leguas de viaje submarino 70.000 oraciones pero una reducción como ya hablamos antes casi de poco menos de la mitad de oraciones limpias con lo cual me quedaron 37904 en todos los casos Recuerden que tomamos la postura de cuando creamos el modelo usar vectores de 500 posiciones si bueno Esto también es un tema a ver porque en algunos casos puede ser que sea exagerado y en otros casos puede ser que sea prudente y no sirve en realidad esto es un parámetro que nosotros tenemos que manejar y podemos ver también a la hora de analizar el modelo que tenemos Si es el valor ideal bien con esto terminamos esta clase nos vemos en la próxima clase Hemos llegado al final de esta clase nos vemos en la próxima clase  foreign Titulo: Clase 28 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/S9PHei3dbsA  \\n 1522 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número    28  Hola a todos Esta es la clase número 28 del curso de Inteligencia artificial de ifes vamos a empezar en esta clase a ver las redes de tipo Deep learning para el mundo del nlp y con ello la red por excelencia del mundo del nlp que son las redes neuronales recurrentes y vamos a ver ver no solamente el modelo básico de este tipo de redes sino una evolución de este tipo de redes que son las redes de tipo lsm normalmente una neurona recurrente se dibuja de este modo donde hay una función que obtiene un valor de entrada y que produce un valor de salida además de la salida también produce otro valor de retroalimentación que pasa a la siguiente de neurona si desplegamos esto que estamos viendo aquí en esta neurona recurrente para poder ver lo que sería una red de tipo neuronal recurrente el resultado sería este leyendo de izquierda a derecha podemos introducir X en la primer neurona y calcular un resultado y así como un valor que pasa a la siguiente neurona en la segunda neurona ingresa un nuevo valor x el cual junto con el que viene de la neurona anterior Calcula a su vez un nuevo valor I en la tercer neurona ingresa un nuevo valor x el cual junto con el valor que viene de la neurona anterior Calcula otro nuevo valor I Y así sucesivamente esa secuencia paso a paso que va a continuar con la misma lógica va a dar lugar a lo que llamamos redes neuronales recurrentes Pero qué son estos valores x que entran en secuencia en cada una de las etapas de una red neuronal recurrente es Ni más ni menos que las palabras que pueden formar parte de una frase como esto que tenemos aquí Hola Cómo te va hoy lo cual describe lo que ya sabemos es un texto Ni más ni menos que una secuencia de palabras la arquitectura y el funcionamiento de tipo secuencial que tienen las redes neuronales recurrentes justamente hacen a que sean tan importantes en la filosofía que tienen los textos justamente de tipo secuencial y por ello la importancia de este tipo de redes en el mundo del nlp como vimos las redes neuronales recurrentes trabajan con dos entradas por cada una de las neuronas primero con cada una de estas palabras que hacemos entrar en secuencia y luego con la inform ación que viene de las neuronas anteriores con lo cual tiene la información de la nueva palabra que entra y del contexto que ofrecen las palabras precedentes pero lamentablemente ese contexto no es indefinido no es infinito Qué pasa si esta frase que vengo trabajando en la primera parte de esta animación Hola Cómo te va hoy yo le agrego más frases como la que vemos aquí en este día de otoño voy a tener un problema que se denomina falta de memoria por qué Porque el contexto de las palabras que están más cerca de las que están entrando últimamente van a tener más peso más prioridad y más relevancia que las palabras que ya quedaron muy atrás en el tiempo porque forman parte del principio del texto para solucionar este problema de falta de memoria es que se crearon unas redes especiales de tipo redes neurales recurrentes que se llaman redes de tipo lst m Pero por qué hablamos tanto de contexto porque es tan importante este concepto en el mundo del nlp bueno porque una de las tareas más importantes del nlp es la generación de texto y la generación de texto se trata de una palabra que aparece a continuación de otras precedentes y tiene que tener sentido con esas palabras precedentes justamente porque sio el algoritmo va a poner una nueva palabra sin tener en cuenta lo anterior y lo que va a ar es una secuencia de palabras que no tengan ningún sentido ni ninguna relevancia uno de los ejemplos más claros de esto es justamente el chat gpt un generador de texto por excelencia Y ustedes tienen seguramente más que presente que si ustedes le ponen un contexto a chat gpt le dicen Bueno quiero que escribas este texto como si fueses un experto en supongamos marketing el el chat gpt va a generar justamente un texto más apropiado y más relevante con lo que ustedes buscan si ustedes no le marcan en el prom Cuál es el contexto obviamente no lo va a hacer tan bien ni tan ajustado a lo que ustedes buan bien Por eso en el mundo de las redes neuronales recurrentes Obviamente el contexto es muy importante y para ello vamos al siguiente ejemplo vamos a suponer que tenemos esta frase hoy tenemos un hermoso cielo evidentemente Esta es una frase muy corta y Por ende la posibilidad de conocer el contexto completo de todas las palabras que llevan hasta este momento esta frase que solamente tiene cinco palabras va a ser muy factible y Por ende es muy probable que elija una palabra acertada y que tenga sentido con el contexto Como por ejemplo la palabra azul y la frase termina siendo hoy tenemos un hermoso cielo azul con lo cual es una frase que tiene un sentido contextual Pero pensemos Cómo sería la situación ante un ejemplo como este yo vivía en Alemania por ello cuando iba a la escuela los maestros me enseñaron cómo hablar en obviamente la palabra sería alemán dado que la cuarta palabra de esta frase es Alemania Pero esto implicaría que a esa altura después del n tiene que estar recordando la cuarta palabra y la cuarta palabra por la característica de una red neuronal recurrente simple sabemos que por el problema de la pérdida de memoria muy probablemente Ya esa palabra Alemania tenga poco peso y pueda llegar a inferir otra palabra que no sea alemán por lo tanto existen las redes de tipo lstm que son justamente como vimos antes las que me van a dar la solución para que yo pueda tener justamente el problema de memoria solucionado y que allí aparezca la palabra que sugiere este contexto que es alemán en concreto las redes neuronales que hemos estado observando van pasando la secuencia de palabras y aprendiendo el contexto pero ese contexto a largo plazo puede ser engañoso y es posible que no podamos ver como los significados de las palabras lej ganas van perdiendo relevancia y puedo confundir el contexto las redes lstm son la solución para este problema puesto que introducen algo que se llama estado de Zelda que es un contexto que se puede mantener durante la secuencia a largas secuencias de tiempo y que puede aportar contexto desde el principio mismo de la oración lo fascinante es que también puede proceder de manera bidireccional y en ese caso podría ser que las palabras posterior de la oración también puedan aportar contexto a las anteriores para que la red pueda aprender la semántica de la oración con mayor precisión bueno para terminar esta primer parte de esta clase vamos a ir al lab pnl 7 donde vamos a usar por primera vez una red neuronal pero aún no de tipo recurrente ni mucho menos de tipo lstm lo vamos a hacer para un problema de detección de frases sarcásticas es decir vamos a tener un dataset que nos va a dar justamente una serie de frases y van a estar rotuladas para que después yo pueda ponerle una frase nueva y ver justamente con esa red entrenada si puedo detectar si esa nueva frase es o no sarcástica lo primero que hago como siempre es importar las librerías primero Jason porque justamente el dataset que voy a usar es o está hecho Perdón en formato Jason tensor Flow que es lo que ya conocemos librería que ya hemos usado antes y que nos va a permitir justamente empezar a a volver a trabajar mejor dicho con redes de tipo de learning pandas maplit wordcloud que es una herramienta que les voy a mostrar justamente para ver cómo hacemos esa famosa nube de palabras y obviamente luego la librería tokenizer y pat sequences para justamente lo que es el armado de la secuencia de tokens de cada una de las palabras de este Jason que voy a trabajar así que voy a importar esas librerías que acabo de mencionar y luego voy a montar como siempre el Drive desde el cual ustedes como siempre les digo van a tomar el archivo que yo les voy a pasar a través del Campus bien Voy a leer el Jason y voy a crear un dataframe justamente en este caso con r Jason y el nombre del archivo que es sarsan Jason bien y luego voy a hacer un Head para mirar rápidamente el contenido de este dataframe bien Aquí tengo el dataframe donde veo justamente que tengo una headline que es como un titular el la URL donde está ese artículo de ese titular y una identificación con uno o cero que es lo que rotula o etiqueta esta headline como de tipo sarcástico o no es decir en este caso y sarcastic 1 es sarcástico y cero es no paso seguido voy a hacer un shap para reconocer la estructura de este dataframe y puedo ver que tengo 28619 observaciones y tres características que son las que ya vimos antes aquí arriba bien lo que vamos a hacer ahora es hacer una nube de palabras en realidad esto no es obligatorio no es exigible para los propósitos de lo que vamos a ver con el algoritmo que queremos hacer y el objetivo de ese algoritmo pero sí es una herramienta muy importante para el análisis de datos con lo cual acá voy a crear primero a través de plt la configuración del tamaño en que quiero Que aparezca ese cuadrante que me va a mostrar nu de palabras y luego voy a crear con wc una instancia de wordcloud que es justamente la librería que me permite eso con lo cual voy a definir algunos parámetros la máxima cantidad de palabras que quiero el ancho el alto y voy a con generate tomar Qué tipo de eh características Pero qué tipo de observaciones son las que quiero para este trabajo que voy a hacer y justamente lo voy a trabajar con esta condición de que headline que es lo que voy a tomar en consideración que es esta columna sí lo voy a circunscribir a solamente aquellas eh aquellas observaciones cuyo is sarcastic sea igual a cero qué quiere decir con esto que justamente voy a elegir los casos como dice el título aquí arriba no sarcásticos y luego con plt y MS lo que hago justamente es imprimir este wc que es esta númera de puntos Así que lo ejecutamos y vamos a ver como siempre sabemos que es esta mecánica de este tipo de gráficos donde muestra en con mayor tamaño Sí este las palabras que más frecuentes salen y lo inverso para las palabras que menos frecuencia tienen en este caso no se trata de frecuencia no frecuencia sino las palabras que tienen o que aparecen más identificadas con titulares no sarcásticos y más chicas las que representan lo contrario sí bien ahí tenemos entonces Trump new se Donald TR nuevamente Woman people way bien Vamos a hacer lo mismo ahora pero para lo contrario para las frases sarcásticas bien entonces en este caso la lógica es exactamente lo mismo lo que voy a cambiar simplemente aquí ese condicionamiento donde voy a poner que el campo o en este caso Perdón la característica y sarcastic le voy a poner que sea igual a uno así como antes puse que fuese igual a cer Bueno ejecutamos este gráfico y y esperamos el resultado a ver que nos muestra ahora para el caso de los titulares de tipo sarcástico y cuáles son las palabras más relevantes entre los titulares de tipo sarcástico bien acá tenemos Man New Como las palabras más importantes y luego nation report make Bueno lo que ustedes Ven aquí y pueden con tranquilidad analizar y ver por qué Por ejemplo las palabras Money New representan algo muy típico en los titulares de sarcástico bueno habiendo podido conocer esta nueva herramienta que les va a permitir incorporar una opción más a la hora del análisis de datos vamos a como hacemos siempre crear la x y la y en este caso la x va a ser el titular el headline y la i justamente la etiqueta de si algo es sarcástico o no bien creamos nuestra x y nuestra I y voy a hacer una visualización rápida del x simplemente para ver que justamente tengo en las x los cantidad de observaciones totales Las 28618 observaciones que son headlines que corresponden o yo he colocado dentro de la x para qué Para justamente empezar por hacer la división como hacemos siempre de los conjuntos de Test y los conjunto de entrenamiento en este caso no voy a usar al TR spr sino que lo voy a hacer numéricamente yo a dedo como quien dice entonces voy a poner las training synthesis y las testing synesis o sea las dos X en el primer caso de la cer a la 20000 y el último caso Perdón hasta la anterior a la 20000 y en el último caso es decir las de testing desde la 20000 hasta la última que como dijimos antes es la 28618 lo mismo voy a hacer para las training labels y las testing labels con los elementos que voy a sacar en este caso de la variable I bien ejecutamos esto y ahora vamos a proceder a la tokenización bien como estamos acostum hacerlo ya vamos a crear un tokenizador justamente utilizando la librería tokenizer en este caso la tomamos no de nltk sino de tensor flow bien Aquí le voy a indicar que el tamaño máximo de vocabulario que quiero va a ser 10,000 Ya vimos en la clase pasada que yo puedo asumir que el vocabulario es la representación total de todas las palabras de todo lo que yo tengo como conjunto de dasos o simplemente puedo decir solamente quiero esta cantidad y obviamente va a cortar a esa ese número la lo que es el vocabulario completo con lo cual puede ser que sea justo o no es un número obviamente elegido con un criterio pero no es Exacto con lo cual puede ser que el vocabulario sea más grande o más chico que eso qué pasa si es más chico el vocabulario Bueno aquí entra en juego justamente este parámetro que estoy poniendo aquí el o token donde le indico que si como el vocabolo es más chico que la cantidad total de palabras que tienen todo este conjunto de datos cuando me toque una palabra que no esté en el vocabulario no voy a saber qué número ponerle porque justamente esa palabra no existe en el vocabulario Entonces en ese caso le voy a poner este indicador que está aquí que es out of vocabulary que justamente Esto me permite hacer en el caso de que la palabra no exista qué oob token voy a elegir se puede poner esta referencia que es la clásica que se pone en estos casos u otra que ustedes pueden elegir bien una vez que creo el tokenizador lo que hago es entrenar mi tokenizador justamente a partir de la Fuente de datos que yo tengo que son los training synthesis que son las primeras 19 20000 porque empieza de cero hasta la anterior a 200000 las primeras 20.000 los primeros 20000 elementos de training sentesis sí bien entonces una vez que entreno al tokenizador lo que hago es empezar a crear un índice para cada una de las palabras Sí entonces lo que hago es con Word Index sobre tokenizer bueno bueno colocar ese resultado en una variable que he dado llamarle al igual que el método Word Index bien Ahora convertimos los titulares los headlines del conjunto de train en secuencias de números con qué con el método Tex to sequences que ya lo habemos usado con la otra librería ahora con tokenizer pero de transfer Flow sobre las oraciones de entrenamiento y esas oraciones de entrenamiento pasadas a secuencias la voy a poner en una variable que le voy a llamar secuencias de entrenamiento sí bien una vez que tengo eso voy a padar esas secuencias con piding post Recuerden que el tema de padar es si el la cantidad de números sí que representa esa oración es inferior al tamaño del vector lo que tengo que hacer es rellenar todos es espacios vacíos con elementos que habitualmente son ceros sí lo que tengo que indicar es si lo que voy a hacer es ponerlos al principio esos ceros o al final en este caso con post lo que hago es que ponga el texto al principio y ese relleno con ceros al final bien Luego hacemos Exactamente lo mismo o sea las secuencias y el padeo así como lo hice antes para el conjunto entrenamiento ahora también para el conjunto de Test bien ejecuto toda esta cadena de instrucciones y voy a pasar ahora a ver cómo poder identificar si un una secuencia de números que representa un texto sí es o no un texto de tipo sarcástico para ello vamos a construir una red de tipo de-learning para poner en en uso a nuestros embedding Sí en principio voy a crear como ya lo hemos hecho otras veces con queras de tesor flow sequential cada una de las capas de mi eh red neuronal y la voy a llamar a este modelo model lo primero que pongo es una capa de de tipo embedding donde le voy a decir con el primer parámetro que el vocabulario que ya como ya vimos antes es de 10,000 palabras la el tamaño de los embedding es el vector los vectores de cada una de estas palabras van a ser de 16 posiciones y el tamaño máximo de input es decir los textos que vayan entrando sí deberían tener una dimensión de hasta 100 caracteres luego voy a crear una capa de pulling de una dimensión sí esto no confundir con lo que usamos en las rales convolucionales más allá que que también había una capa de pulling y luego voy a hacer dos capas densas una de 24 neuronas con la función de activación relu que ya hemos usado antes y finalmente una capa de salida de tipo densa también con eh una una este una función de activación de tipo sigmoide dado que tengo una sola neurona Por qué tengo una sola neurona porque este es un un problema de tipo binario tengo que decir si un texto es o no sarcástico con lo cual tengo dos salidas posibles es sarcástico o no es sarcástico bien y finalmente la compilación que lo voy a comparar lo voy a compilar perdón con la función de pérdida vinary Cross entropy Recuerden que esta la uso cuando es justamente una salida de tipo binario así que tengo una red que me va a dar solamente dos opciones no es de tipo categórica el Cross entropy que lo usaba cuando tenía más de dos opciones de salida voy a usar el optimizador Adam y le voy a decir que la métrica para ir midiendo la precisión de esta red sea la de acur ejecuto esta celda y luego con el sumari vamos a ver como muchas otras veces s Bueno un pantallazo de cómo quedó conformada nuestra red neuronal habiendo hecho esto voy a pasar a entrenar a mi modelo y voy a guardar la la historia digamos de cada uno de estos entrenamientos para después graficarlos dentro de esta variable history voy a crear una variable para poner la cantidad de epoch que voy a definir en TR y Bueno luego lo que tengo que hacer como siempre model fit pongo justamente las los elementos de entrenamiento que en este caso son training pad que son los vectores que representan a las headlines luego las labs que me dicen si esa cada una de esas headlines es o no Eh sarcástico bueno la cantidad de epoch y los conjuntos de validación el det test para justamente las noticias y el D test para las etiquetas bien ejecutamos esto y va a tardar un tiempito porque son 30 eh epoch los que hemos aplicado Aquí bien aquí Terminamos el entrenamiento tenemos un muy buen accuracy 99% eh para el conjunto de entrenamiento y un muy buen accuracy también para el conjunto de validación que está en el orden del 80% bien Ahora vamos a probar esta red que hemos creado esta redal que hemos creado poniendo en este caso eh dentro bueno creando una variable sentences que que le voy a poner dos oraciones donde la primera bueno está escrita en inglés la pueden Traducir si quieren yo busqué funciones este dado que el dataset está puesto en inglés este he buscado oraciones típicas de habla inglesa que representan Sarcasmo Y en este caso la primera representa Sarcasmo y la segunda simplemente anuncia cuándo va a empezar a eh la temporada final de Game of thrones es una un titular no sarcástico en este caso la las dos oraciones que están dentro esta var sentences voy a aplicarles el mismo criterio que utilicé para con los datos que usé para entrenar es decir lo que tengo que hacerlos pasar por el método text to sequences y el resultado de ello lo voy a poner en una variable que le he puesto a llamar sequences luego insisto al igual que lo que hice con los datos que entrené los voy a padar Por lo cual Bueno sigo exactamente la misma lógica que en aquel caso una vez que tengo estas dos oraciones que han sido eh inicializadas y padas voy a hacer la predicción Entonces el resultado final que está en esta variable padded voy a aplicarlo a predict de model y voy a imprimir el resultado bien ejecuto eso y veo que en el primer caso me da 9.06 al elevado a la -10 lo cual me daría en realidad un 90.66 por con lo cual quiere decir que la primer frase tiene un 90 por eh está más cerca de digamos de eh de ser de tipo sarcástica en un casi 100% que no sarcástica por el contrario la otra es 5.9 elevado a la 8 a 10 a la -8 perdón con lo cual obviamente es un 0,0005 de eh posibilidades de ser sarcástica Por ende es una frase no sarcástica y justamente como dijimos antes lo que anuncia aquí obviamente es un titular serio y totalmente alejado de ser algo sarcástico aquí teng tengo en este caso tres oraciones donde las dos primeras son sarcásticas y la última no el resto de de de lo que yo aplico aquí en este caso del código es exactamente el mismo Solamente he cambiado el contenido de la variable sentence y lo ejecuto y veo el resultado donde justamente las dos primeras eh oraciones tienen en el primer caso un 96 y en la segunda un 99% de posibilidad de ser sarcásticas Sí están más cerca del uno Por ende justamente son sarcásticas y eh la otra es 2.1 elevado a la 10 a la-4 con lo cual 0.0002 y justamente está mucho más cerca del cero Por ende es una frase caratulada o rotulada por este modelo que hemos creado de tipo no sarcástico bien hasta acá llegamos entonces con la teoría de esta clase y con la práctica de esta primer parte de la clase en la segunda parte de la clase vamos a hacer una práctica donde Ahí sí vamos a aplicar justamente la creación de Nuestra primer red neuronal recurrente con el stm hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte Titulo: Clase 28 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/DCoVqxi-EjY  \\n 2298 segundos de duracion \\n  Esta es la segunda parte de la clase número 28 te invito a empezar con   ella  bien aquí estamos entonces en la segunda parte de esta clase donde como dijimos en la primer parte de esta clase vamos a trabajar ahora sí con una red neuronal recurrente con lstm y con la opción bidireccional que vimos en la teoría que abordamos en la primera parte de esta clase lo primero que vamos a hacer es instalar el la aplicación que utilizamos para leer archivos de tipo eh PDF Así que arrancamos con eso vamos a instalar luego una serie de librerías que ya vamos a ver bien dónde las vamos a aplicar vamos a montar el Drive para Acceder al archivo que vamos a usar para esta clase y luego vamos a extraer con esta función extraer texto de PDF que la venimos usando en las clases anteriores el contenido de ese archivo y lo vamos a llevar dentro de una variable texto es es exactamente la misma función que usamos en las clases eh anteriores Así que esto ya seguramente no hace falta que lo expliquemos pero sí vamos a abordar Cuál es el contenido del archivo que vamos a usar en este caso para esta práctica el archivo se llama relatos.pdf y lo he armado en base a pedirle a ch gpt que me haga o me simule o me genere relatos de relatores de fútbol argentinos sobre lo que sería una jugada previa y el gol sí es decir que la idea sería que con varios relatos de este tipo generemos un Corpus la inteligencia pueda entrenarse con ellos y a partir de eso yo haga lo que tiene que ver con el tema de esta clase que es generar texto a partir de una frase es decir la generación de texto puede ser algo que tiene que ver con un prom como nosotros vemos muchas veces en chat gpt donde le digo que me cree una poesía que me cree un una definición o un relato que yo quiera hacer una exposición que yo quiera hacer desde la nada simplemente dándole un prom o alguna serie de características pero también existe o en lado dentro de lo que es la generación de texto en cuadrado dentro de lo que es la generación de texto está también la situación parecida a lo que nos pasa a nosotros cuando por ejemplo en Google escribimos un mail y nos aparecen palabras sugeridas a la derecha bueno eso también es generación de texto y algo de eso es lo que vamos a hacer generar un texto completamente como decía recién en ch PT que pimos por ejemplo que nos escribe una poesía eh requeriría un Corpus muy grande o una inteligencia entrenado con un Corpus muy grande para generar todo un texto y que tenga sentido y que sea lo que nosotros esperamos no genere en este caso lo que vamos a hacer va a ser Buscar una inteligencia a la cual yo le puedo dar una primer frase supongamos de tres o cuatro palabras parecido a lo que insiste lo queos hacemos cuando escribimos un mail y que nos genere las 10 o 20 palabras próximas a esa primera frase y que podamos ver que tenga conexión con el inicio de la frase Sí bueno Esto un poco lo que vimos en la teoría cuando ejemplificamos el caso de bueno que cuando estudié en Alemania me enseñaban a estudiar alemán me enseñaban hablar en alemán bueno o que es un día azul Sí bueno esa cuestión es cómo generar la próxima palabra de acuerdo al contexto que traigo aquí el contexto va a ser muy cortito un grupo de palabras tres cuat cinco no más que ellas y después lo vamos a pedir Insisto que nos escriba 10 o 20 palabras que seguirían a ese texto según lo que entiende la inteligencia Según Lo Que Fue entrenada bueno y el resultado Obviamente el resultado en sisto está muy acotado este archivo está lejos de ser un archivo un muy grande pero bueno es una primer prueba y justamente La idea es que ustedes puedan después con esto poder buscar otros archivos más grandes justamente en la finalización de esta clase Voy a hacer eh una una muestra de Cómo entrenar se acuerdan con lo que usamos la clase pasada los 10 libros de recursos humanos a ver si nos genera algo mejor dado que en ese caso se trata de un Corpus mucho más grande volviendo al punto en lo relativo a esta función exalar texto desde PDF lo que vamos a abrir va a ser este archivo que lo vamos a señalar Primero aquí y después lo vamos a ver aquí en en el código sería Drive my Drive archivos nlp y dentro de nlp está relatos PDF Sí bueno cerramos esta ventana y esa ruta es la que tenemos aquí sí bien con lo cual ya tenemos definida la función Insisto que la venimos usando en las últimas prácticas y la apertura del documento que insisto es lo mismo porque usamos la misma función Así que lo ejecutamos sin más palabras y ya tengo entonces dentro de documento justamente ese contenido de relatos si ahora yo veo el contenido de documento Bueno aquí está sí obviamente es un poquito más largo que esto pero bueno me muestra como siempre la primer parte s Buenas tardes amigos nos encontramos en estea emocionante partida bueno el rato no es 100% tal cual como relato en argentino pero se aproxima bastante ha hecho un buen trabajo de chat gpt igual obviamente la idea que yo podría eh haberle puesto más proms para pedirle como ustedes saben chat gpt que no estoy conforme con lo que me generó y me gener algo mejor en base a algunas cuestiones algunas consideraciones para que vaya generando mejores l pero como base para esto es más que interesante eh obviamente como dijimos lo vamos a hacer después con los los Corpus de recursos humanos Y ustedes lo pueden hacer con el material que ustedes les parezca mejor generado por char gpt o libros o documentos del tipo que ustedes consideran Bueno una vez aclarado esto lo que voy a hacer como tantas otras veces es eh colocar todo este texto en minúscula Y splitear en este caso lo voy a splitear por enter es decir fíjense por este carácter barra n que creo que ya saben todos ustedes que esto significa el la marca de enter sí bien o el corte de línea para decirlo de otro modo con lo cual ahora en Corpus tengo bueno piteado este documento en párrafos que están separados eh en cada caso en Que aparezca un un indicador de corte de línea o que ve da de línea o ent bien para ver esto ahora veo el Corpus y veo lo mismo que vi anteriormente aquí arriba pero obviamente ahora separado porque Corpus es una array que tiene párrafos del texto original obviamente en ese párrafo ya no existen estos caracteres barra n porque han sido considerados solamente para tomar como referencia para separar en en oraciones todo el texto y obviamente los quita del Corpus bien ya tengo el Corpus Entonces explito por lo tanto lo que vamos a hacer ahora es simplemente como hacemos siempre para que se vea más así Más allá de que Perdón aquí visualmente ya es bastante claro como se ve este la separación en oraciones bueno imprimo la oración sub uno y veo Buenas tardes amigos nos encontramos qué es lo que veo aquí al principio de este texto sí ven La cero es un es un espacio vacío sí la uno es esta frase que ven aquí y que reproduzco aquí cuando le digo que el número uno bien hecho esto como siempre ahora voy a tokenizar con lo cual creo el tokenizador y lo que hago justamente es entrenar el tokenizador con el cor acuérdense que para que pueda tokenizar bien ese texto yo tengo que to entrenar Perdón ese token con el Corpus que yo tengo Sí y luego justamente empiezo a a partir de eso este tomar la referencia de Cuántas palabras tiene ese tokenizador con el Word index Acuérdese que el Word inish lo que hace es darle a cada palabra un valor numérico sí no Como tantas palabras haya sino por tantas palabras diferentes que haya sí bien Entonces ejecutamos esto y ya tengo obviamente en el total words la cantidad de de de palabras que tiene mi vocabulario en base al Corpus y lo imprimimos y podemos ver el tokenizador donde está el número que le asignó a cada una de las palabras el uno la dos en tres bueno esto ya lo hemos visto un montón de veces y Cuántas palabras en total forman parte de mi vocabulario generado a partir del Corpus que son 442 palabras bien antes de continuar con la clase práctica de código duro Vamos a abordar algunos conceptos teóricos que son necesarios ir viéndolos este en detalle y paso a paso para entender todo lo que va a venir después en principio lo que tenemos que diferenciar Aquí Cuál es el objetivo que perseguimos en la primera parte de esta clase y el objetivo que estn buscando ahora el objetivo que perseguimos en la primera parte de esta clase fue resolver un problema de clasificación clasificación de texto como clasificación hemos hecho otras veces problemas de clasificación hemos hecho otras veces con redes neuronales convolucionales con redes neuronales y con Machine learning sí es un problema clásico solamente que en este caso está contextualizado en lo que estamos haciendo justamente que es el abordaje de los textos bien ahora eh lo que vamos a hacer es otro tipo de problema como ya lo adelantamos hace un rato que es de generación de texto en el caso de la clasificación de texto y todos los problemas de clasificación en general sabemos que necesitamos un conjunto de datos de validación Sí por qué Porque justamente para ver si ese algoritmo está aprendiendo de que ese texto por ejemplo como veíamos recién era sarcástico no era sarcástico yo tenía que tener textos etiquetados como sarcásticos y como no sarcásticos para que el algoritmo pudiese entrenar pudiese aprender y luego con el conjunto de validación Yo podría ver si el algoritmo está resolviendo bien o mal en base Al conjunto de tex Bueno estoy explicando lo que ya hemos visto un montón de veces creo que está no hace falta explicarlo mucho más que esto Pero cuál es la cuestión aquí cuando estoy llevando adelante un problema de generación de texto No necesitamos como bien dice aquí un conjunto de validación Sí por qué Porque es una cuestión predictiva sobre lo cual está creando algo y no hay algo que ya ha sido creado antes para ver si esa creación está bien o está mal esto impera el criterio nuestro que somos los que vemos el resultado de la generación que hemos pedido Sí entonces aquí hay algunas cuestiones muy particulares a través de las cuales el algoritmo aprende obviamente no tiene una etiqueta con lo cual aprender Pero va a tener un sistema que va a tener como una especie de etiquetado parcial que va a ir construyendo a partir del cual va a aprender Bueno cómo es esto vamos a verlo paso a paso Vamos a tomar como ejemplo una línea cualquiera del Corpus que dice que la semana tiene más de 7 días Sí es parte del relato donde el relator usa esta frase como ya tenemos este Word Index creado donde ls un la s2 etcétera etcétera sí lo que vamos a hacer es transformar Sí cada una de esta esta frase digamos esta frase en concreto Sería para todas las frases pero en el caso del ejemplo de esta frase en concreto pasarla a una secuencia de números sí entonces 2 8 3 10 31 13 3 3 12 3 13 sería como el número que representa esto es un ejemplo por supuesto cada una de estas palabras sí supongamos que le correspondía a eso S Bueno una vez que yo convierto esta secuencia de palabras que forman el texto en una secuencia de números a partir del Word Index lo que se hace es armar subsecuencia que tienen que ver con ir Armando parejas y que eh esas parejas vayan creciendo en tríos en cuartetos en quintetos o sea arranco de un par y después le voy sumando sí En las siguientes secuencias un número hasta llegar al final con Este ejemplo se ve claramente aquí primero tengo 2 o porque son los dos primeros valores la siguiente secuencia es 28 31 la siguiente 2 8 31 31 se ve claramente es muy fácil de ver no entonces la última secuencia Cuál va a ser la que va a coincidir con la expresión original 2 8 3 10 3 11 16 3 3 12 3 13 Es decir genera sobre una secuencia de números Sí una cantidad de secuencias n secuencias que es igual a la cantidad resultante entre conformar una primer pareja y después cada uno de los elementos que se vayan osando Sí por eso en este caso tenemos 1 2 3 4 5 6 7 cuando justamente el el la Ray inicial era 1 2 3 4 5 5 6 7 8 ese siempre va a ser uno menos porque arranco de un par bien hasta ahí vamos con la primer parte de esta historia y lo vamos a ir haciendo en código para que se vaya viendo justamente en el transcurso de lo que vamos haciendo y podamos visualizarlo por eso esto que tenemos aquí es lo que vamos a codificar aquí creo una una Ray input sequences Sí y lo que voy a hacer es forline in Corpus es decir voy a recorrer cada uno de los párrafos de mi Corpus y lo que voy a hacer es convertirlos en secuencia text to sequences de cada Line y a partir de tokenizer lo voy a poner dentro de token list es decir que lo que voy a hacer es cada línea s voy a hacer algo como esto cada línea nuevo aquí la voy a convertir en est O sea que ese for está haciendo eso que señalé recién por cada línea una vez que hago esto lo que tengo que hacer es la segunda parte de lo que vimos aquí es decir ir Armando este conjunto de secuencias a partir de una secuencia de números bien cómo hago for in Range Range que va de uno a token list Qué es token list bueno es la cantidad de elementos que tiene justamente mi token es decir en este caso dijimos se acuerdan que teníamos ocho entonces va de un a o eso que quiere decir que va a ser uno menos que la cantidad total porque acuérdense que empieza desde cero la numeración Entonces qué va a hacer en cada caso va a armar token list si token list va a ir desde el principio hasta y + 1 Sí en el primer caso el principio Cuál va a ser c y + 1 es 2 y acuérdense que cuando pone hasta dos quiere decir que siempre es la anterior O se va 0 y 1 y así en cada vuelta I va a valer un valor más y Por ende el valor hasta de lo que yo vaya a poner en N Grand sequen va a ir siendo un valor más cuando termine este for el resultado va a ser algo similar a lo que vimos aquí en este sencillo ejemplo bien lo ejecutamos y ya tenemos el resultado bien en este caso por qué armo esta estrategia porque justamente la forma de aprender que va a tener este algoritmo es la siguiente aquí justamente lo he puesto en un párrafo para que se entienda bien la idea es decir yo voy a armar parejas después tríos después cor más secuencias y para cada caso es decir cuando haya una palabra va a aprender de que existe como sugerida una próxima cuando haya dos palabras también va a existir una próxima cuando haya tres palabras Existirá una próxima Y así sucesivamente Qué quiere decir que la inteligencia tiene que entender que que después de dos y 8 hay un 310 después de dos y 8 310 hay un 31 y así sucesivamente Esa es la forma en que se entrena esta Esta generación de texto este algoritmo de generación de texto porque no hay una etiqueta más allá que la etiqueta la vamos a fabricar no hay un etiquetado previo va a haber una suerte de etiqueta que justamente se va a generar a partir de esta lógica que estamos Armando por lo tanto a continuación lo que vamos a hacer en principio es eh medir eh Cuál es el largo de la máxima oración por qué porque de aquí ya tenemos un problema que ya lo vemos justamente cuando hamos esta cadena de secuencias de que todas las cadenas no son iguales la primera es de dos y la última de ocho y eso no lo podemos dejar así tenemos que tener vectores iguales sabemos que no puede haber vectores irregulares bien Por lo tanto lo primero que vamos a averiguar es cuál es el vector más largo Sí cómo bueno con esto que hemos puesto Aquí vamos a buscar justamente Max de lende x cuando x Sí es lo que está en las input sequences Es decir de todas las input sequences que acuérdense que lo generamos aquí que tiene todas las secuencias de todos los párrafos es decir insisto todo esto de todos estos imagínense todo lo que hay Sí vamos a buscar cuál es el más largo entonces for x in imp de cada uno le vamos a medir el len y del len o sea de los largos de cada uno Dame el máximo Y eso lo voy a poner dentro de una variable sequen len que después la vamos a imprimir para conocer Cuál es el valor de eso y vemos que es 29 Cuál va a ser el trabajo que vamos a llevar a cabo Ahora va a ser un trabajo que se llama padeo es un proceso Perdón que se llama padeo que consiste en tomando esta cuestión de la dimensión máxima rellenar con ceros cada una de las eh posiciones que no tienen número entonces supongamos que para el ejemplo que venimos trayendo que no es este 29 que tenemos acá sino el ejemplo teórico Sí vamos a suponer que la cantidad máxima es 10 Entonces qué tengo que hacer se acuerdan la primer secuencia era 2 o bien a esa 28 qué hago le pongo och ceros sí formando un vector de 10 posiciones y lo que le voy a poder o la opción que tengo para hacer es si quiero que los ceros estén adelante o estén atrás obviamente como yo estoy haciendo algo que me va a buscar siempre el próximo valor me conviene que los ceros estén adelante y no atrás sí bien Entonces esto es lo que le decía la clase pasada de que si bien los en beding busca eh en general que no hayan posiciones con valores ceros sino que estén ocupadas todas las posiciones A diferencia de los problemas anteriores que había por ejemplo el sistema de conteo de palabras que generaba muchos espacios vacíos para este problema particular de generación de texto no queda otra que que todos los vectores sean iguales y como tengo que ir Armando estas secuencias y estas subsecuencia no queda otra que rellenar esos espacios con cero bien se entiende Este ejemplo gráfico se enti Este ejemplo gráfico lo que vamos a hacer ahora es justamente hacerlo en el código que venimos trayendo es dec tengo que generar ahora todos vectores de 29 posiciones para ello lo que hago es decirle que a input sequences que tiene toda esa cadena que hablamos recién quiero que su largo máximo sea Max sequen Lane que es este 29 que tenemos aquí y con padding le digo como dije recién dónde Quiero los ceros pre antes post después los quiero antes Entonces le pongo pre con np justamente transformo todo este resultado en una array y lo vuelvo a poner en el mismo input sequences que tenía antes s ejecuto esto lo puedo poner otro nombre variable aparte bueno es para reutilizar la misma nom el mismo nombre de variable Perdón podría haber sido la misma Ahora viene una cuestión muy interesante que es Cómo se arman las etiquetas de una lógica que vengo trayendo de una resolución de un objetivo que no tiene etiquetado bien muy fácil yo tengo que tratar de tomar en esta secuencia que tenía acá arriba que tomamos como ejemplo Sí el criterio de establecer de que el último elemento es la etiqueta Por qué Porque como dijimos hoy sí si yo tengo que el primer elemento es un dos tiene que haber algo que me diga sabes qué después del dos lo habitual es que siga un ocho cuando tengo un dos y un O tiene que ver algo que diga sabes qué después del dos y el 8 lo mejor o lo más habitual no lo mejor lo más habitual es que haya un 310 eso es lo que tiene que ir aprendiendo la inteligencia Y cómo lo va a ir aprendiendo bueno justamente lo que yo voy a hacer es que cada elemento final de cada vector va a ser la etiqueta va a ser el I se acuerdan este concepto de i Sí y el resto va a ser la x es decir que estos vectores si tengo vectores de 10 posiciones para este ejemplo no de los 29 decir Tratamos de separar El ejemplo teórico que vengo trayendo del ejemplo práctico de lo que estamos llevando adelante en el código no bueno si estoy parado en esta cuestión de que todos los vectores son de 10 posiciones Entonces todos los vectores van a tener nueve posiciones para definir a la x y una posición para definir a la y s si vamos al código nuestro serán 28 para una cosa y la número 29 para la otra sí bien entonces Este ejemplo que está aquí gráficamente Sí bueno lo que voy ahora a hacerlo es en la práctica con lo cual lo que voy a definir ahora es xs y y lo que digo que de input sequences me tomé todas las filas y solamente las columnas hasta la men1 Qué quiere decir tomo todas menos la última y en el caso de labes que sería la i va a ser exactamente lo contrario tomo todas las filas pero la columna solamente la última bueno ejecuto esto y finalmente vamos a generar las is yo aquí le puse labes pero todavía en labes no tengo la forma de arma de las I Por qué Porque en el caso de las labes no vamos a trabajar con el número si que corresponde a cada etiqueta 8 31 31 etcétera etcétera sino que vamos a tener que formar un vector que tenga tantas posiciones como el vocabulario completo Acuérdese que vocabulario completo tenía Tenemos aquí arriba 442 si este elementos y para representar el o que sería por ejemplo la primera etiqueta vamos a tener que hacer algo como esto sí poner 0 1 2 3 4 5 6 7 Se en la séptima posición poner Perdón en la octava posición que corresponde al subíndice número siete sí voy a tener que poner justamente un uno representando que ese elemento que está allí es el número ocho Pero por qué Porque está o tiene un uno en la octava posición bien una vez logrado esto hago lo que acabo de mostrar justamente para crear mi S A partir de lab sí diciendo que la cantidad de elementos totales es igual a la cantidad de palabras totales que tiene mi Corpus y lo que voy a hacer es pasarlo a categórico Sí el número categórico sí es en una representación de ceros y unos poniendo el uno en la posición que le corresponde al número natural bien haciendo esto Entonces ya tengo mi x tengo mi es decir tengo mi x y mi etiqueta que en realidad no vino desde el principio como siempre nos pasa cuando hacemos este o entrenamos algoritmos sino que la tuve que yo ir generando con todo este proceso que descubrimos en todos estos pasos precedentes bien insisto tengo mi x tengo mi ahora lo que tengo que hacer Ni más ni menos como dice Este título aquí es crear nuestro modelo de red neuronal en este caso red neuronal recurrente vamos a recurrir a sequential que ustedes saben que es este el método que usamos la cas que usamos de tensor flow para ir poniendo o definiendo cada una de las eh capas de la red que vamos a crear de manera secuencial bien la primer capa es una capa de eding Sí qué es lo que va a hacer Bueno lo que ya sabemos es transformar todo este conjunto de vectores de 29 posiciones en embeddings por lo tanto lo que tengo que darle como información es cuál es el tamaño de mi vocabulario las 400 y pico de palabras y con 200 le digo el tamaño de los vectores en los que yo quiero que haga los embeddings Recuerden que los vectores que tengo son de 29 posiciones pero con 29 posiciones no vamos a hacer un embedding porque eso obviamente no tiene que ver con el sistema de similitud que vimos en las clases pasadas Por ende defino que quiero eh vectores de 200 posiciones que igual fíjense recuerden siempre que 200 es mucho menor que la cantidad de palabras o sea es la mitad o sea que eso Si volvemos a a sistemas más viejo como el conteo de palabras obviamente tenemos vectores de 400 y pico de posiciones en este caso son 200 las que elijo y luego le indico de todo lo que tiene que ver lo que va a entrar De qué tamaño es o el input lens sí los textos que van a entrar estas secuencias que están numéricas que representan los textos que van a entrar De cuántos son bueno de un largo igual al máximo 29 men1 porque acuérdense que la etiqueta no va a entrar en este caso sí bien Entonces luego lo que hago es agregar una capa de tipo lstm y bidireccional dos conceptos que vimos en la primera parte de esta clase lo defino con 150 neuronas y finalmente voy a hacer una capa de salida de tipo softmax por qué porque acá no tengo que hacer un problema de clasificación de cer o uno si es es sarcástico o no sarcástico acá lo que tengo que decir es cuál de las 400 y pico de palabras voy a dejar de ser y pico y voy a decir el número que corresponde para ser lo más preciso posible 442 palabras Cuál de las 442 palabras sí es la que debería seguirle a la que bueno forma parte del texto que vengo escribiendo por ello la función de activación de la capa de salida tiene que ser sof Max porque tengo 442 posibles resultados de salida y luego hacemos el sumary para ver justamente el modelo completo como lo vemos habitualmente y bueno dicho Todo esto lo ejecutamos bien allí tenemos el sumar o el resumen de lo que es la composición de la red que acabamos de crear y lo que vamos a hacer ahora es como siempre hacer la compilación y el entrenamiento bien allí terminó el entrenamiento 200 os bastante para que a pesar de que es un texto muy cortito trate de tener un proceso de aprendizaje importante y lo que vamos a hacer es graficar vos aquí que la curac es de 097 es muy bueno vamos a graficar la curva justamente de aprendizaje que es que está aquí donde vemos justamente como ha crecido y Que obviamente en los entrenamientos ya prácticamente de el 25 en adelante si ya la evolución fue muy poquita es decir que ya la mayor parte del aprendizaje la obtenido justamente hasta epoch 25 vemos cuando lo recorremos aquí que ya es así O sea inclusive en algunos casos como en el 146 ha sido superior al final bueno Esto nos puede llegar a entender que Obviamente el número de epos ha sido más que suficiente desde antes de los 200 que yo le puse bien lo que vamos a hacer ahora finalmente es generar el texto con nuestro modelo con lo cual la frase inicial va a ser lleva la pelota Sí y le vamos a pedir que eh haga 20 palabras luego de la palabra pelota que sugiera 20 palabras luego de la palabra pelota teniendo como referencia el entrenamiento hecho con el Corpus que yo le pasé bien Lo que vamos a hacer con esto es eh for una variable cuando usamos el el ión acuérdense que no voy a crear ninguna variable esto para iterar Tantas veces como nextwork o sea va a ir agregando 20 palabras y Esto va a iterar 20 veces en cada una de esas iteraciones lo que va a hacer es tomar la frase inicial Y tokenizar sí para que esté en la misma sintonía que eh toda la lo que hemos entrenado y lo ponemos dentro de tois Y qué vamos a hacer al igual que como hicimos con el Corpus original padar sí es decir agregarle con ceros completar con ceros para que tenga el mismo régimen sí que tienen los vectores del Corpus original es decir los 29 - 1 bien con los ceros puestos adelante Bueno lo que ya vimos anteriormente que hicimos con las secuencias del Corpus original lo mismo con este pequeño texto que lleva la pelota tiene que estar en la misma sintonía una vez que hago esto lo que hago es con predict de model acuérdense que ese modelo que creamos recién darle como como input el token list Sí y decirle Cuál de todas las predicciones es la mayor Sí con eso ar Max y lo pongo dentro de predict sí es decir cuál es la palabra más eh que tiene el mejor score o la que tiene la sugerencia más alta para poder justamente este ser la adecuada para continuar en este caso a la palabra pelota pero esto Va a continuar con lo cual va a ser la palabra la mejor palabra que continúa pelota y después si esa palabra fuera eh el jugador o el él puntualmente sí Cuál es la palabra ideal que va a seguir a él puede ser jugador Cuál es la palabra ideal que va a seguir a jugador Y así sucesivamente pero acá hay un tema muy importante que tenemos que ver bien Qué tiene predicted tiene la palabra sugerida por este algoritmo como que debería ser la más conveniente para seguir la frase desde la palabra pelota no en realidad tiene un número sí Recuerden que lo que me da Aquí vamos al modelo nuevamente la capa densa de salida es una softmax que tenía tantas posiciones como el vocabulario como el tamaño de vocabulario Está bien entonces solros recordemos que el tamaño vocabulario que teníamos era de 442 palabras Entonces qué Me da me da algo como lo que teníamos aquí cuando vimos el tema de las etiquetas Es decir me va a poner un número en una determinada posición si entonces lo que yo tengo que tomar a partir de allí es cuál es ese número y luego Recuerden que tengo un índice donde cada número está asociado a una palabra Por eso voy a crear una variable output Word que va a ser la palabra definitiva que me sugiere este algoritmo y voy a recorrer sí completamente el Word Index es decir el dice de palabras y justamente voy a estar comparando el número obtenido con cada uno de esos índices hasta encontrar cuando coincida el número de índice con el valor de la predicción entonces podré acceder a la palabra al texto de la palabra Y esa palabra será la que pondré en esta variable que Acabo de crear con lo cual finalmente voy a tener que la frase inicial es igual a frase inicial en este primer caso lleva la pelota más el output Word que es como el ejemplo que decíamos recién supongamos que la próxima palabra sea lleva de la pelota é Sí con lo cual la próxima frase inicial Qué va a ser lleva la pelota él y la predicción supongamos que sea jugador luego la frase inicial será lleva la pelota el jugador Y así sucesivamente entendido esto bueno ejecutamos el código y vamos a ver qué resultado nos da y vemos que dice aquí lleva la pelota están los pies del habilidoso mediocampista del Pampa Si miran el documento en alguna parte del relato le pone a un equipo lo pone lo pone como Pampa Sí realmente Endo que la Argentina tiene que ver con eso le puso ese nombre así que de allí viene Este no es algo desubicado que lo ponga porque justamente en el relato Pampa es uno de los equipos que usa el relator el medio capita del Pampa Diego el fútbol argentino que viva el fútbol argentino que viva bueno y ahí sigue bueno es bastante Bueno sí el resultado obtenido insisto porque tiene coherencia lo que dice va siguiendo una línea coherente de lo que está Armando aquí como relato a partir insisto de tres palabras y obviamente de un entrenamiento con un Corpus muy chico para hacer un ejercicio más importante yo aquí les dejo lo mismo que hicimos en el ejercicio de la clase pasada Sí donde tomamos 20 libros de recursos humanos con la idea de poder hacer el mismo circuito donde juntábamos todos los documentos sacábamos las salvaciones limpias y toda esta cuestión y lo que vamos a hacer es entrenar sí a nuestro algoritmo con eh ese con ese Corpus sí Pero cuál es el problema aquí en realidad eh hacer Esto va a llevar muchísimo muchísimo tiempo sí muchísimo tiempo para que tenga una idea yo he he pagado a cola para que me deje usar uno de sus de sus gpu sí que es este es el llamado B1 Sí una de las de las GP más poderosas y he tardado tres horas Sí aquí les muestro el entrenamiento hecho un entrenamiento de 50 sí bastante bueno eh conveniente porque fíjense que no es exagerado como el caso recién fíjense que llego a los 95 72 y todos los scors anteriores han sido inferiores a eso o sea que no he hecho entrenamientos de más Este y bueno obviamente obtengo un resultado pero para que ustedes lo puedan usar y no tengan que hacer todo esto lo que yo es he hecho es el guardado Este modelo y le he puesto model rrh h5 h5 es la extensión con que se guardan los modelos que se generan con t Flow y con model save guardo ese modelo esto lo hemos visto en algunas clases pasadas Sí con lo cual ahora lo que yo debería hacer sería poder justamente usar un modelo pero en lugar de crearlo entrenarlo como está aquí sí lo que voy a hacer va va a hac recurrir a ese modelo por eso acá al final dice utilizar el modelo guardado Sí y Bueno les muestro cómo sería todo el ciclo que tendría que hacer mucho más cortito que toda el anterior para usar ese modelo justamente lo levanto de aquí esto yo se los voy a dejar en el campo virtual por supuesto y acá les sugiero que lo que deberíamos hacer deberíamos hacer todos los pasos desde el principio hasta la creación del modelo o sea evitar el paso que es de crear el modelo de compilar y de entrenar y de guardar sí es decir vamos un poquito hacia atrás yo debería hacer todo hasta este punto Sí o sea este punto no lo voy a hacer este punto tampoco menos que menos porque es el de entrenamiento sí Y obviamente grabar no tiene sentido Porque yo lo estoy tomando de algo está grabado ya luego de esta recomendación vamos aquí abajo y ejecutamos sí esto y nos da este resultado es las eh Perdón la frase inicial es las habilidades intelectuales y acá está la frase que dice las habilidades intelectuales se refieren al saber hacer entre otras palabras son el conocimiento en acción laboral que puede ser desarrollado a través bueno Y esto ustedes también pueden jugar un poquito en ponerle este valor de next Wars en un valor más grande para que genere una frase obviamente mucho más Este mucho más desarrollada no tan cortita un detalle no menor que les dejo también este aquí para que intenten a ver si por ahí con su computadora que tienen sin comprar gpu de de Google de colap este pueden llegar a hacerlo es la posibilidad fíjense que aquí le pongo un break Sí en en esto que hemos hecho aquí de ir tomando los 10 libros de recursos humanos para que tome solamente el primero por allí tomando solamente el primer libro bueno pueda ser más factible lograr un entrenamiento con un tiempo prudencial al que ustedes puedan recurrir Obviamente que aquí ustedes pueden dejar esto andando mucho tiempo y si logran que luego del entrenamiento ustedes Bueno se desatienden de la computadora por decir de alguna manera Este pero se ejecute esta instrucción y esto lo colocan en el Drive Sí bueno si Por ende o por un motivo u otro la conexión se cae la la sesión de cola se cae no va a haber problema porque Ed ya habrán guardado este archivo dentro de su propio Drive Y bueno ya está tranquilo que el día de mañana este cuando ven vuelvan a reiniciar la sesión podrán recurrir a este modelo porque lo tienen en su propio Drive a pesar de que cuando terminó esto y cuando se terminó y se grabó y después se desconectó porque quedó la máquina sin uso como hace Cola habitualmente con Estos espacios este no van a perder el modelo y van a poder volver a levantarlo justamente con esta esta parte que yo les he dejado aquí de utilizar el modulo guardado Sí así que ya sea este modelo que yo les doy o bien otro que generen ustedes con un Corpus propio que puede ser este libro u otro no este pueden asegurarse que pueden darle la posibilidad de dar muchos entrenamientos que dure mucho pero que ustedes no estén pendientes de Cuándo se termina para que no pierdan el modelo Una vez que se les terminó la sesión de cola Bueno hasta aquí llegamos con esta clase muy larga nos vemos en la próxima clase Hemos llegado al final de esta clase nos vemos en la próxima  clase Titulo: Clase 29 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/geVYnK3p2CE  \\n 1532 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número    29  Hola a todos Bienvenidos a la clase número 29 del curso de Inteligencia artificial de ies continuamos con el módulo de procesamiento del lenguaje natural y el tema en cuestión hoy es l Chain con concretamente Qué es Lan Chain Por qué deberíamos usarlo y cómo funciona vamos a empezar a dar respuestas a cada una de estas preguntas Lan Chain es un framework de tipo Open source que les permite a los desarrolladores que trabajan con ia combinar grandes modelos de lenguaje como gpt 4 con fuentes externas de computación y de datos el marco Se ofrece actualmente como un paquete de python o javascript o typescript para ser específicos en este contexto en nuestro caso nos centraremos como venimos haciendolo al principio de este curso con python para comprender Qué necesidad satisface Line Chain echemos un vistazo a un ejemplo práctico a estas alturas todos sabemos que ya gpt o gpt 4 tiene un conocimiento general impresionante podemos preguntarle sobre casi cualquier cosa y vamos a obtener una respuesta bastante buena supongamos que queremos saber algo específicamente a partir de nuestros propios datos es decir a partir de de nuestro propio documento podría ser un libro un archivo PDF un Word etcétera etcétera plan change nos permite conectar un modelo de lenguaje grande como gpt 4 a nuestra propia fuente de datos no estamos hablando aquí de pegar un fragmento de un documento de texto en el mensaje de chat gpt estamos hablando de hacer referencia a una base de datos completa llena de nuestros propios datos y no solo eso una vez que obtengamos la información que necesitamos podemos hacer que l change nos ayude a tomar la acción que que debemos realizar por ejemplo enviando un correo electrónico con alguna información específica y la forma de hacerlo es muy similar a la que hemos venido aplicando hasta aquí esto es tomar el documento al que deseamos hacer referencia en nuestro modelo de lenguaje luego lo dividimos en fragmentos más pequeños y hacemos los embedding los cuales los vamos a terminar almacenando en una base de datos vectorial esto Ahora nos permite crear aplicaciones de modelo de lenguaje que siguen un proceso general un usuario hace una pregunta inicial luego esta pregunta se envía al modelo de lenguaje y se utiliza una representación vectorial de esa pregunta para realizar una búsqueda de similitud en la base de datos vectorial esto nos permite recuperar los fragmentos de información relevantes de la base de datos vectorial y enviarlos también al modelo de lenguaje ahora el modelo de lenguaje tiene tanto la pregunta inicial como la información relevante de la base de datos vectorial y por tanto es capaz de dar una respuesta o realizar una acción l change ayuda a crear aplicaciones que sigue un proceso como este y en base a las dos capacidades mencionadas nos permite pensar en una cantidad infinita de casos de uso práctico todo lo que implic asistencia personal será enorme puede tener un gran modelo de idioma para reservar vuelos transferir dinero y hasta para pagar impuestos ahora emos las implicaciones para estudiar y aprender cosas nuevas puede tener un modelo de lenguaje grande hacer referencia a un programa de estudios complejos y ayudarnos a aprender el material lo más rápido posible la codificación el análisis de datos las ciencias de datos todo se verá afectado Por esto una de las aplicaciones que se consideran más relevantes es el poder conectar grandes modelos de lenguaje con datos existentes de la empresa como datos de clientes datos de marketing u otros es muy probable que vayamos a ver un progreso exponencial en el análisis y ciencia de datos nuestra capacidad para conectar los grandes modelos de lenguaje con apis avanzadas como la de meta o la de Google realmente hará que las cosas despeguen significativamente la principal propuesta de valor de l Chain se puede dividir en tres conceptos principales Contamos con los contenedores llm que nos permiten conectarnos a modelos de lenguajes Grandes como gpt 4 o los de hing Face las plantillas de mensajes nos permiten evitar tener que codificar el texto que es la entrada a los llm luego tenemos índices que nos permitirán extraer información relevante para los llm las cadenas que nos permiten combinar varios componentes para resolver una tarea específica y crear una aplicación llm completa y finalmente tenemos y algo muy importante que vamos a ver más adelante los agentes que permiten al llm interactuar con apis  externas expuesto todo lo que es la introducción a unchain vamos a tomar este gráfico que está aquí como referencia dado que ahora en el lab vamos a identificar cada uno de estos componentes codificándolas con python y las librerías obviamente de l Chain antes de empezar con la aplicación concretamente en el ámbito de colap tenemos que ver algunas cuestiones de environment de contexto y concretamente tenemos que usar la Api de chat gpt para todo lo que tenemos que hacer y también tenemos que usar la Api de pinec que es una una de las tantas es la que vamos a elegir nosotros base para guardar los embedding de los vectores que vayamos generando en estos prácticos vamos a ver por qué el tema de python y cuál es la No solamente la utilidad sino la importancia de guardar estos vectores allí pero vamos a centrarnos primero en la la Api de openi y vamos aquí tenemos la la página open.com en la cual vamos a incorporarnos con login eh yo voy a entrar rápido porquees ya estoy logueado y yo ya he hecho la registración Pero obviamente lo que van a tener que hacer ustedes poner un uso de contraseña o bien entrar con la contraseña de Google que es lo más usual bien una vez que estamos aquí tengo chat gpt que entraría a usar el chat gpt como un corriente o la Api nosotros vamos a entrar a Api y aquí tenemos en principio sí esta esta apik esta opción apik hago clic aquí y haciendo clic en este botón puedo crear una nueva eh clave para usar mi apq una Secret key obviamente ya la he creado eh tiene para ponerle un nombre y obviamente la la apik me la sugiere en este caso Open y bueno obviamente ya la puedo cambiar eliminar Pero obviamente la voy a ver parcialmente pero puedo entrar a ver obviamente la la la la apik que corresponda está codificada aquí obviamente para que no la pueda ver suar cualquier persona por una cuestión de seguridad pero cada uno de ustedes la podrá tomar y vamos a ver que la vamos a tener que pegar ya vamos a ver en el co paso seguido vamos a ir a settings para bueno hacer lo más doloroso que es justamente el pago Sí en Billings sí tengo todas las cuestiones que tienen que ver con el aquí yo ven que tengo 8.85 pendientes porque he hecho una compra de $ todavía tengo ese saldo a favor y aquí tengo en principio el p metods que Obviamente le ponen lo que edes ya saben Sí el número de la la tarjeta de crédito o el nombre de ustedes o como si fueran hacer cualquier compra usual común y corriente y eh si quieren ver los precios pueden entrar aquí a pricing lo cual me va a llevar a esta web que tenemos aquí donde tengo todos los precios de todos los productos que tiene eh open hay y puntualmente este vamos encabezando los ya gpt pero no es el único ya vamos a ver aquí tengo gpt 4 turbo el cual depende del modelo que vas a usar es el costo en este caso Más allá de eso el costo es coincidente en otros casos no bien en principio tengo un valor para acciones de input como sería escribir una una pregunta o pedirle algo y otro para el output que sería la respuesta siempre es más caro el valor del output que el valor del input esto se cotiza esto es 0.01 cada 1000 ST Y en este caso lo mismo 0.03 cada 1000 tokens Aquí tengo inclusive una calculadora para poder estimarlo Igual vamos a poner un código dentro del cola para poder hacer con gpt 4 lo mismo los valores no cambian Perdón son un poquito más caros Sí aquí estoy viendo 003 versus 001 006 versus 003 Sí el caso en este caso el triple en este caso el doble si aquí fíjense que entre los modelos hay diferencia dentro gpt 4 gpt 3.5 Turbo es el mucho más barato muchísimo más barato fíjense que ya 0.001 cuando al principio tenía 0.01 y en este caso 0.03 bien obviamente en la medida que el modelo es menos avanzado Obviamente el costo es menor no bien luego tengo otros productos como asistentes Sí para bueno con retriable cor interpret perdón y retriable que ya vamos a ver son acciones que vamos a ver más adelante luego para para tuneo de modelos para los embeddings a b2 que también lo vamos a usar porque es parte de toda la práctica que vamos a hacer con el L change eh los modelos base d Vinci es uno de los que vamos a usar también y después tenemos otro tipo de productos que en este caso no vamos a usar Dali que son los productos que generan imágenes no es el caso lo que vamos a ver nosotros pero si los modelos de audio whisper y tts whisper lo vamos a usar para convertir un audio en texto y tts para lo contrario todos tienen Su costo obviamente bien aclarado esto esto lo que van a tener que hacer obviamente es hacer el pago como corresponde y van a tener habilitado a partir de allí el uso a través de la piki si ustedes usan la piqui pero no han pagado evidentemente les va a dar un error porque tien una piqui para Acceder al lpi pero la Api no tiene el pago para poder generar la acción que ustedes le están pidiendo y finalmente el pinec que aquí obviamente e lo puedo hacer clic en login para poder entrar y este servicio es totalmente gratuito Sí así que no no hay ningún tipo de inconveniente en poder poder este usar este producto a diferencia del de recién porque bueno es totalmente abierto y gratuito bueno acá estoy dentro de pyon hagan caso miso esto porque ya tengo un índice creado una base de datos de índices creados Así que eso en principio no lo encontrarían si cuando ustedes ingresen Aquí van a tener aquí a apik al igual que lo que hicimos recién en chpt Pero obviamente en Open perdón pero sin mediar cuestiones de de precio en este caso lo que voy a tener que hacer simplemente es crear mi piqui con este botón de aquí yo ya la tengo creada le he puesto primera y eh obviamente la la piqui es esta que aparece aquí este con este bueno estos símbolos que ocultan el valor sí estos numeral Perdón estos asteriscos eh Y con este botón la puedo mostrar Sí con esta la voo ocultar y con este botón la puedo copiar para pegarla en lo que vamos a hacer a continuación bien aclarado todo esto vamos a ver algo muy importante un pasito más antes de ir al colat que es cómo generar un archivo para guardar toda esta información de este environment lo próximo entonces que tenemos que hacer es crear un archivo con el bloc de nota cualquier editor de textos que se llame punto m como Ven aquí arriba pun enb de envir Y sí por favor muy importante no equivocarse en la representación de estos tres valores en el nombre de cada uno de estos parámetros Open apik pyon apik y Pon m porque si no obviamente van a tener un error qué datos van a poner allí los que tienen que ver con lo que vimos recién en principio la apik que la van a recoger de aquí cuando creen la picki de Open Ai es lo que van a poner en la parte de el primer parámetro de este archivo es decir esto que está aquí eso sería la información que ustedes tienen que poner entre comillas luego vamos a pycon y acá tengo dos datos en principio la apik que dijimos que la podemos visualizar con esto y luego el nombre del environment por eso cuando ustedes van el archivo de nuevo punto m ven que los datos que piden son la apik que tienen que ponerla aquí entre comillas y el nombre del envir también aquí entre comillas este archivo lo guardan Y luego sí lo vamos a tener que poner en nuestro Drive para poder levantarlo desde el cola Así que eso es todo lo pueden hacer en cualquier parte de su computador siempre después teniendo presente de moverlo hacia el espacio de Drive que es el que vamos a usar para poner todos nuestros archivos como venimos haciendo habitualmente con nuestros cols bien habiendo abordado toda la teoría introductoria y todas las cuestiones de configuración de entorno vamos a empezar con nuestro primer lab de cola la pln 9 en este caso donde vamos a ver como dice el título aquí los componentes principales de lunch de la primer parte después en la segunda parte de esta clase Vamos a continuar con el resto en principio lo que tenemos que hacer es importar las librerías pero más que importar primero tenemos que instalar todas estas aplicaciones open lch token que nos va a ayudar a poder calcular eh digamos todo lo que sera la tokenización que nos va a cobrar obviamente eh Open e kpt antes de hacerla para tener justamente la seguridad de que vamos a tener primero que tenemos saldo para hacerlo y después si estamos dispuestos a asumir ese costo luego pycon que es lo que vimos recién y python DM que es lo que nos va a permitir levantar el entorno esc el archivo que hicimos recién justamente para poder tener configuradas las apik tanto de Open Ai como la de pyon así que instalamos todo esto nos va a llevar un tiempito muy importante antes de empezar no olvidarse en este caso es muy importante conectarse con este Bueno un gpu de colap y no Simplemente con cpu porque muchas de estas cosas no van a funcionar Así que empezamos por Instalar todo esto que está aquí bien luego vamos a mostrar con show Line change la versión de Line change esto es muy importante porque es un software de Última Generación Que obviamente eh Está sacando versiones permanentemente Así que es muy importante en este momento Esta es la versión que vamos a usar que ustedes puedan constatar con cuál de todas las versiones de l change están trabajando bien paso seguido vamos a crear las variables de entorno esto les recuerdo aquí las este bueno las URL de cada una de las plataformas tanto la de la apik de openi como la de pycon Sí y bueno Y aquí si ustedes quieren dejar nota digamos de cada uno de los datos que tienen en el archivo que reci creamos con el punto m también lo pueden hacer aquí a título de dato para que justamente lo tengan siempre a mano bien dicho Todo esto lo que tengo que hacer ahora es montar el Drive para justamente ir a buscar entre otras cosas ese archivo punem que acabamos de colocar Así que montamos el drive y lo hacemos como habitualmente nos manejamos con esto bien allí terminamos de montar el drive y ahora vamos a importar os y DM para justamente hacer esta cuestión de levantar ar ese archivo que tiene el environment del cual habíamos hablado antes así que hacemos clic aquí y luego con loa DM donde voy a buscar en content drive mydrive y en el particular yo tengo este punto m dentro de archivos nlp así que de allí lo voy a levantar y luego con os en vinom and get Open ap key lo que voy a poder ver por pantalla a ver si me responde con la Api que yo sé que tengo entonces de esa manera me doy cuenta de que justamente estoy conectado como corresponde ahí La respuesta es positiva con lo cual quiere decir que yo ya estoy debidamente conectado y puedo empezar a utilizar Esta bueno esta este servicio de Open sí lo primero que vamos a ver aquí recurriendo a este imagen que teníamos en la teoría es el tema de los modelos Sí en principio vamos a ver el modelo llm y luego el modelo chat Sí en ese caso van a ver que son dos sistemas diferentes que tienen que ver con los tipos de servicios que nos da el tipo de modelos que nos brindan Open Así que empecemos por lo primero vamos a ver el Last model que saben ustedes que significa llm y vamos a usar el gpt 3.5 Turbo que es el que veíamos hace un rato sí bien para ello voy a frine change llms voy a importar Open a y allí voy a llamar justamente a la clase Open Ai le voy a dar el model name text inch 003 que es el que vimos recién cuando entramos a el detalle de los precios y voy a trabajar con la el Max tokens en 1024 que es lo que tiene configurado por defecto eh chat gpt y temperatura 0.7 temperatura es un parámetro que va de c a 2 y tiene que ver en principio Si queremos una respuesta que sea más precisa Más estructurada o más libre o más creativa Por decirlo de algún modo sí bien aquí tenemos eh e también la posibilidad de además de trabajar con text Vinci también con gpt 3.5 Turbo bueno Estos son elementos este opcionales y por eso aparecen justamente en la lista de productos que tienen en el detalle de precios eh Open Ai así que bueno en este caso vamos a usar el primero de ellos pero en realidad ustedes pueden usar el modelo que quieran Sabiendo de que en algunos casos puede que un modelo se ajuste mejor a una cosa u otra bien entonces creamos eh esta variable gpt3 que va a ser una instancia de Open con todas estas características que hemos mencionado recién bien y ahí tengo ya la instancia creada y ahora lo que voy a hacer es darle un prompt en este caso explíqueme que ch gpt a esta variable que Acabo de crear y voy a poner la respuesta en una variable que he dado en poner la  respuesta bien a partir de esto yo puedo hacer un print de la vara de respuesta para ver qué fue lo que me dio justamente como respuesta y aquí dice claramente chat gpt Es una herramienta de chatbot basada en procesamiento el lenguaje natural nlp que permite a los usuarios bueno bla bla bla bla bla bueno todo esto que está aquí es la respuesta que yo he logrado me de chat gpt a través de Eh bueno ahora python y toda esta este uso de esta librería lunch Sí es lo que habitualmente escribo a través de un prom el mismo entorno de hept ahora lo hago desde el entorno de colap mediante python y mediante la librería lch bien en este caso podemos ver que puedo pedirle que me dé la cantidad de tokens que corresponden a explíqueme que es un chat gpt y la cantidad de tokens que me da la respuesta entonces de este modo yo puedo darme cuenta que tengo ocho para uno y 149 para el otro y empezar a hacer este tema de los cálculos Sí porque en este caso le hice una pregunta de ocho tokens y una respuesta de 149 Y eso va a tener un costo bien aquí tenemos una referencia del costo de acuerdo al modelo que yo he utilizado Sí el d Vinci o el gpt 5 Turbo Pero obviamente ustedes tien que tener claro que esta referencia que está aquí no corresponde si uso gpt 4 por ejemplo sí bien en este caso yo he utilizado eh Tex davinci 003 y no gpt 3.5 turo porque hay una cosa muy importante que yo aquí se las he dejado Como comentario que en realidad la alternativa de gpt 3.5 Turbo hubiese funcionado bien con esta pregunta ahora si yo Quiero una no una pregunta sino un conjunto de preguntas un conjunto de pedidos digamos ag gpt El problema es que ese modelo No tolera eso y Por ende el que tolera tanto una pregunta o más o un prom o más es text vinch 003 bien Así que en este caso fíjense que lo que voy a poner es gpt3 generate sí no como aquí que directamente le ponía la pregunta sino gpt3 generate y le voy a mandar tres cosas que le voy a pedir a ch alunas pueden ser preguntas a otr pueden ser que me explique algo o me un detalle en el primer caso le pregunto Cuál es el equipo de fútbol más famoso del mundo en el segundo caso le digo que me explique brevemente la ley de la gravedad y en el tercer caso que me describa Cuáles son los tres pasos que yo debería seguir para hacer un dinamis no es son tres pasos más bueno eso le pedí que me lo haga tres sí en este caso insisto uso el método generate porque le estoy mandando tres proms y no uno y recuerden estoy usando el modelo text vin que es el que soporta eso lo ejecutamos y voy a tener ahora las respuestas en la variable de respuestas y la var de respuestas va a ser justamente una Ray con tres respuestas ya que le he hecho t entonces Si miro la primera respuesta 00 me va a decir que el equipo más famoso del mundo es el Real Madrid y bueno si quiero ver las tres voy a hacer un for y voy a recorrer en lugar de la 00 la i0 cambiándole el I de cer hasta el tercer punto que sería el número dos Sí así que hago el for y voy a ver en este caso la respuesta o las tres respuestas el equipo de fútbol más famoso del mundo Real Madrid la ley de la gravedad una ley física y me da los tres pasos de cómo hacer un tiramizu bien hasta aquí llegamos con lo que hemos usado como modelos de tipo llm ahora vamos a usar otro tipo de modelo que son los modelos de tipo chat por eso 1. modelos chat modelos de tipo chat perd Aquí vamos a usar lch esquema y L chat models y en el primer caso voy a importar tres elementos para el esquema que sería el Im human y el System Y luego el chat Open que va a ser la librea que voy instanciar para poder hacer esta tarea que me propesto hacer bien ejecutamos esta celda y luego vamos a hacer una instancia de chat Open Ai para colocarlo dentro de la variable chat chat Open Ai con el modelo gpt 3.5 Turbo y una temperatura de 05 y Max token el 1024 vamos a esperar que termine de cargar la librería ahora creamos la instancia y ahora en este antes de pasar al siguiente cela quiero volver a dejar en claro el tema de la el concepto de temperatura en este caso 05 quiere decir una propuesta más conservadora acuérdense que si es mayor a uno habíamos dicho que es una propuesta más creativa no tan ajustada a la mayor probabilidad que de lo que sería la respuesta correcta H estadísticamente correcta para lo que nosotros le pidamos en este caso sería algo más eh Le estoy pidiendo algo más cercano a la realidad a la mayor probabilidad de una respuesta ajustada y si es mayor a uno insisto es algo entre uno y dos que tenga que ver con una respuesta más creativa bien en este caso vamos a usar el el System message y el human message y vamos a ver cómo responde a lo que nosotros hacemos habitualmente con chat gpt si nosotros muchas veces le pedimos que nos de una respuesta marcándole un contexto y es lo que estamos haciendo aquí fíjense que le decimos con sistem mesis eres un experto en educación y luego con human mesis el pedido concreto bien Por ende esos dos elementos los vamos a poner dentro de una Ray messages y messages es lo que le voy a mandar a chat para que me dé la respuesta Así que lo ejecutamos y vamos a imprimir luego el resultado con RT chat. content que es obviamente donde va a estar alojada la respuesta esto que estoy pidiéndole Aquí bien allí terminó y ahora vamos a imprimir el contenido para ver justamente qué es lo que me generó y aquí me dice Cuáles son las cinco opciones que considera excelentes para incorporar la ia como una herramienta de enseñanza en el colegio y me pasa Bueno un un detalle de esa respuesta bien bien con esto tenemos entonces un introducción hacia lo que es el uso de los modelos de llm y de los modelos de chat que me ofrece Open en la segunda parte de esta clase vamos a ver los otros componentes de l change o que ofrece justamente l change hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda parte i Titulo: Clase 29 (parte 2) Curso Inteligencia Artificial \\n URL https://youtu.be/Ehw9F3Mi9QI  \\n 1930 segundos de duracion \\n Esta es la segunda parte de la clase número 29 te invito a empezar con    ella  bien aquí estamos entonces para continuar con lo que hicimos en la primera parte de esta clase en esta segunda parte de la clase componentes principales de lch Par para continuar con lo que estábamos haciendo dado que son dos colap diferentes vamos a tener que repetir algunos pasos de preconfigured esta clase Por eso tenemos que importar las librerías como lo hicimos antes Tenemos que montar el Drive como lo hicimos antes y tenemos que hacer todas las configuraciones de entorno hasta levantar la piqui como lo hicimos antes a partir de allí ya empezamos entonces con el primer tema de eh esta segunda parte y volvemos a este gráfico inicial para justamente entender en este contexto este gráfico que lo que vamos a abordar es otro concepto importante de lanch que son las plantillas de prom bien volvemos aquí el entorno de colap y Por ende vamos a empezar por importar prom template de l Chain que es justamente la librea que vamos a usar en este caso en el caso del template básicamente es una plantilla que me va a permitir darle un prom pero con algunas variables por eso se llama un template de proms eh voy a escribir un template poniéndole una frase que es lo que yo le quiero pedir a gpt pero poniéndole una dos tres la cantidad de variables que yo quiera poner en este caso quiero que chpt me escriba un mensaje de felicitación para alguien de acuerdo a un evento Pero considerando la posibilidad de que ese alguien sea un elemento variable y que el evento también u ocasión sea un elemento variable Por eso yo pongo en el template entre comillas escribe un mensaje de felicitación para Y con estas llaves la palabra nombre que sería el nombre de esa variable de el nombre de la variable nombre abundancia en su y entre llaves nuevamente el nombre de la otra variable que sería ocasión bien con lo cual este template va a tener dos variables Por ende lo ejecutamos para que quede cargada esta variable voy a crear otra variable prom en la que en la cual voy a poner prom template que es lo que yo aquí importé aquí arriba y ahí lo que voy a manejar son dos parámetros en principio input variables Cuáles son las variables de input que van a entrar dentro de ese prom nombre y ón obviamente tiene que llamarse igual a como yo las definí aquí arriba y luego decirle bueno fantástico Esas son las dos variables Pero cuál es el template el template es template Es decir es esta variable y creamos aquí lo que yo pongo aquí bien entonces con esto creo la variable prom Ya tengo la variable template y la variable prom que usó a esa variable template bien Ahora voy a Frontline Chain el lms importar open luego creo una instancia de Open Ai poniéndole al nombre de la variable gb3 se puede llamar como quieran ustedes s con un model name texta V 003 Una temperatura de 07 y la cantidad de tokens de que propio de gpt sí acá dejo dos opciones más para que ustedes puedan cambiar sí en este caso todos las tres opciones siguen el mismo formato respuesta igual a gpt3 y dentro de ello le pongo prom format y dentro de PR format le tengo que poner el nombre de la variable tal cual la definí arriba y un texto que tenga que ver como valor que yo quiero de input para esa variable en el primer caso le pongo nombre Juan ocasión casamiento en el segundo caso Laura ocasión bautismo de su hijo Lucas y en el tercer caso Perdón Laura ocasión ingreso a la empresa presa saa bien Vamos a arrancar con el primero y vamos a ver qué nos responde en la Val la respuesta chpt sí Y nos dice lo siguiente felicidades Laura estamos muy contentos de tenerte con nosotros empresa s estamos seguros de que tu trabajo aquí será un gran éxito bienvenida a la familia bueno si queremos seguir con esto vamos a descomentar la primera y vamos a ver la ocasión de Juan para el casamiento para el casamiento de Juan no bien generamos la respuesta la imprimimos y vemos que ahora dice Felicidades Juan y nombre de la pareja podríamos haberle puesto en este caso una var pero bueno est mu cito la opción de casamiento Que obviamente en losotros Casos no hay una segunda persona en este gran día estamos tan contentos por vosotros dos y deseamos que este sea el principio de una vida de amor bueno etcétera etcétera etcétera Sí y Bueno podemos ver el último caso que es el del bautismo pero digamos no escapa a lo que pasa habitualmente cuando nosotros escribimos algo solamente que en este caso esto lo podría poner en una función y directamente darle la las los n valores de las variables y que vaya generando automáticamente como yo lo estoy haciendo aquí pas paso pero en el contexto de una función bien en este caso imprimimos el resultado y me dice felicidades Laura que la vida de tu hijo Lucas est llena de alegría y bendiciones etcétera etcétera bien con esto Terminamos el concepto de planillas de prom volvemos al gráfico y vemos que tenemos otro concepto que tenemos que ver de lch cadenas justamente el Chain de l Chain es decir las cadenas bien volvemos aquí el cola y vamos a importar toda esta cantidad de librerías HM y Eh bueno lo que vamos a hacer es crear modelos que van a estar Encadenados por eso el concepto de cadena Sí bueno Qué quiere decir esto el modelo de encadenado esto muchas veces nos pasa que yo le pido algo a chat gpt y luego le pido una ampliación una aclaración o algo que tenga que ver con lo que me generó antes sí es esta conversación esta cuestión conversacional que tengo con chat PT por lo tanto vamos a crear un modelo uno en base a Open como lo venimos haciendo y el model name seguimos usando davin bueno Esto es Exactamente igual a lo que vimos hace un rato y voy a escribir un template como lo hicimos hace un rato también poniéndole como variable tema es decir Escríbame los puntos más importantes para un tema x que lo definiré yo a la hora de ejecutar esto luego como hicimos recién también hoy vamos a hacer el prom template le vamos a poner prom 1 donde le pongo que la variable el input variable es tema y el template es template un o sea nada diferente a lo que hicimos aquí arriba sí Solamente que ahora lo vamos a encadenar esa es la gran diferencia Sí con lo cual a cree el modelo uno y voy a crear a partir de esto la cadena uno como con llm change donde le digo Mira esta cadena la vas a armar con un modelo que llama modelo uno y un prom que se llama prom está bien Es decir no me lanzo a hacer esto directamente aquí porque esto terminaba en esta instancia acá es una cadena va a ser una parte de la cadena con lo cual simplemente lo defino con el mismo criterio luego hago lo propio para la segunda parte de la cadena el segundo eslabón de la cadena donde el proceso es exactamente el mismo solamente que tengo que tener en cuenta que el template que yo escriba aquí tiene que tener relación con el template de la cadena anterior Entonces le digo Necesito que me amplíes la explicación sobre el primero de los puntos de puntos esto d va a ser o va a recoger la respuesta de la primer cadena entonces este texto tiene que tener sentido con lo anterior que es exactamente lo mismo que nosotros hacemos en vivo digamos o in situ con chat gpt o sea si yo le pido una respuesta de algo lo que le escribo después le manifiesto de alguna manera que tiene que tener referencia con lo anterior si no va a tener no va a tener base para resolver eso Si Entonces le digo resuelvo de los puntos de lo que me mostró recién una de las consideraciones que o amplíe tal concepto que usted aquí me muestra de manera sintética A qué se refiere con tal cosa Bueno lo que fuere entonces la expresión de Esto va a tener que ver y puntos en este caso que es un nombre Val que voy poner cualquier cosa Va a tener que ver justamente con la respuesta de la cadena un y me va a dar una segunda respuesta en base a lo que yo le escriba acá que necesito que me digamos este Explore o aonde sobre lo que me dio en la cadena anterior obviamente Aquí tengo dos eslabones una cadena cortita le puedo poner todas las cadenas que yo quiera s es decir que esta cuestión de el modelo un el modelo dos puede ha hasta el modelo n para lo cual finalmente voy a con simple secuencia change ponerle con el parámetro change todas las cadenas que formen parte de esta lógica que yo armé en este caso son dos Entonces le pongo entre corchet cadena uno cadena dos y punto si con verbo true acuérdense que siempre lo que hago es tratar de que me muestre el resultado de cada una de las cosas que está haciendo si le pongo false no me va a mostrar esto es bueno para ir seguiendo un poco la lógica de lo que va haciendo bien todo esto lo voy a poner dentro de la variable que le voy a poner cadena completa variable que se puede llamar insisto como ustedes quieran y luego la respuesta la voy a obtener Llamando al método ram de la variable cadena completa y le voy a mandar adentro Qué cosa lo que va a ser el valor de la primera variable es en este caso de tema con lo cual en este caso le digo una campaña de marke de marketing perdón exitosa Cómo se lee Esto escribe los puntos más importantes para una campaña de marketing exitosa se entiende bien Entonces creamos el modelo un Perdón no había cargado las librerías Ahora sí Ahora creamos el modelo un ahora creamos el modelo dos el modelo y la cadena uno el modelo y la cadena dos creamos la cadena completa y ya teniendo esto Busco la respuesta fíjense que la primer parte de la respuesta me la dan un color y la segunda la voy a ver en otro color Aquí están los puntos de lo que le pedí una campaña de marketing exitosa y le pedí luego que me amplíe el primero de los puntos este que dice definir el objetivo de la campaña de marketing sobre eso tiene que versar esta respuesta que está buscando ahora en la segunda cadena bien aquí tien Entonces otro color Está bueno Esto justamente tiene que ver con el verbos que recién estábamos hablando si no solamente me mostraría esta parte de amarillo dice el primer punto que es definir el objetivo de la campaña de marketing es fundamental para que tenga éxito etcétera etcétera etcétera Finish Chain Sí ahí se terminó la cadena Entonces como yo le puse verb true me va a mostrar cada una de las partes si le hubiera puesto insisto verbor frast lo único que me va a mostrar es la última parte que es lo que queda alojado en respuesta bien hasta allí el tema de las cadenas ahora también hay otro concepto muy importante que volvemos al gráfico que tenemos aquí en donde me habla de almacenar los vectores los vectores ustedes saben que son proceso de hacer uno en vez son resultado de hacer un embedding que ya lo hemos hecho mucho en los eh labs anteriores Así que lo que vamos a hacer ahora es lo mismo que hicimos lo que vinimos haciendo en las clases anteriores o sea tomo un texto lo separo lo splite se acuerdan y luego lo que hago es con cada párrafo aún env lo que pasa que antes lo que hacíamos era esto mismo y lo usábamos de inmediato Ahora qué pasa si yo quiero recurrir mañana a eso bueno tenía que grabar el modelo Sí y después volver a a levantarlo no en este caso lo que vamos a hacer es grabar o guardar o almacenar los en vings Entonces de esa manera yo con un modelo que ya no es mío voy a usar el modelo de chat gpt sí voy a usar si los embed que son míos los vectores que son míos y con la vinculación de ambas cosas voy a volver a recuperar lo mismo que vene haciendo hasta ahora sin tener que hacer los pasos previos es decir vuelvo a traer los eddings sí de ese almacén cenamiento y vuelvo a utilizar el modelo de chask PT con mi base de datos representada por mis vectores sí Entonces lo primero que vamos a hacer después vamos a ver cómo recuperamos los vectores es generar estos vectores a partir de separar textos hacerlos en vez y guardarlo En dónde en la otra appi que creamos la otra aplicación de la cual hablamos que es pincon que justamente es una base de datos para vectores bien manos a la Entonces lo primero que tengo que hacer es separar textos para la cual voy a eh volver a configurar el entorno bueno Esto Obviamente si ya vengo de lo anterior no sería necesario lo volvemos a repetir para que quede claro que obviamente es como voy a usar pyon tengo que tener de nuevo conectado el tema de del entorno pero si lo hice antes como entiendo que ya lo hemos hecho aquí arriba si aquí arriba no sería necesario en este caso bien eh luego lo que vamos a hacer es Llamar a eh recursive chac text splitter de l Chain text splitter sí acuerdense que split es el tema de separar Sí ya vamos a ver de qué se A qué se refiere este concepto de recursive charter a esa librería le voy a poner rc porque obiamente es un nombre muy largo entonces me manejo mejor con un alias que va a ser rc lo que voy a hacer voy a abrir un archivo que ya lo usamos antes que es chat gpt txt que es el quea de la historia de chat gpt que lo usamos en clases anteriores para no inventar Nada nuevo con lo cual voy a eh luego de haber eh sumado esta librería voy a abrir este archivo chat gpt txt ya lo tengo abierto lo imprimimos para recordar un poquito de qué se trataba Esto bueno se acuerdan que yo se lo había pedido esto al propio chat gpt que me contara Cómo era la historia de chat gpt y lo pegué en un archivo de tipo txt para empezar a usar bien Ahora lo que voy a crear es una variable text splitter que es una instancia de rc acuérdense que rc es lo que vimos aquí arriba que es recurso char qué es lo que le voy a poner como información primero el chun size Qué quiere decir esto cuando vaya a splitear que lo haga de a 200 caracteres Sí pero también voy a manejar el concepto de chang overlap que le doy un valor 20 que es esta cuestión de que yo no voy a separar todo el texto en 200 de una manera totalmente excluyente los 200 primero van a estar totalmente parado de los 200 segundos sino que lo que va a hacer esta separación es cuando genere el segundo va a tomar los últimos 20 caracteres del primero cuando tome el tercero va a tomar los últimos 20 caracteres del segundo y así sucesivamente Por qué Porque cuando cortemos un texto por una cifra fija es muy probable que perdamos el contexto Qué pasa si ese corte eh ocurrió en el medio de una expresión sí que no había una coma ni un punto Sí hay una cifra fija acá no estamos hablando de una separación por punto ni por coma sino de un valor fijo entonces para que no pierda contexto el siguiente párrafo el siguiente chun lo voy a empezar con algo de lo que venía en el chun anterior Entonces de esa manera queda más enganchado con lo anterior y no queda como dos cosas totalmente separadas que en realidad puede ser la mitad de una frase bien entonces creo el text splitter teniendo en cuenta eso y luego lo que voy a hacer es el splite con Tex expit create documents sobre texto recuerdan que texto es todo este texto este txt que ha hablado de la historia de chas Bueno lo ejecutamos y veo que tengo 218 párrafos con print l de texto tengo 18 sí muestro alguno como suelo hacer habitualmente para que ustedes lo vean agarro el primero textos 1 page content chat gpt es un hito en la evolución etcétera etcétera etcétera textos dos lo mismo m fíjense aquí ven avance de la tecnología y la inteligencia está bien avance de la tecnología y la inteligencia No termina la frase aquí porque en realidad es avance de la tecnología y la Inteligencia artificial fíjense Cómo empieza el segundo con y la Inteligencia artificial se entiende Entonces de esa manera ven que la primer palabra hubiese sido artificial de este segundo Chan pero me trae y la inteligencia Entonces de esa manera lo que yo hago es justamente recuperar parte de lo anterior y no perder tanto contexto bien eh puesto esto como ejemplo lo que vamos a hacer ahora esaro de lo que hablamos recién que es el tema de evaluar Los costos sí bien en este caso hay una función que está creada aquí que hemos creado aquí pero en realidad pues pueden hacerlo con el código sin que esté en el marco una función pero el hecho que est am no funcion es más práctico en el cual voy a usar a tik token que se acuerdan que al principio yo les dije que tiene que ver con esto de poder precalculo de lo que yo voy a hacer en este caso del embedding y voy a usar justamente tick token punto encoding from model Con qué librería de chat gpt voy a usar para hacer los embedding text embedding Ada se acuerdan que Ho cuando vimos el la página web donde aparecían los precios estaba Este modelo de Ada aquí está para que lo recuerden en vez de models Ada Sí y va a valer 0.01 cada 1000 tokens bien entonces ahora lo que hacemos Es ver eh el en o sea lo que voy a hacer este como código HM por cada página de cada uno de los elementos del texto Sí por cada page de cada texto sí y voy a meder el len la cantidad el largo de cada uno para calcular el total de tokens sí de cada una de las páginas de este texto en realidad acuérdense que estos no son páginas sino que son todos estos split que tengo acá sí o sea va iriendo por cada uno de los párrafos Sí en realidad más que paste podría ser eh for este split Pero bueno no importa Este Pace aquí acuérdense que es el nombre de una variable Así que se pueden poner como ustedes quieran y luego voy a hacer un print diciendo Bueno tengo a lo largo de este texto chat.txt la cantidad de 8181 tokens y vamos a ver cuál es el precio entonces lo que hago es llamar esta función print eding cost de textos Si en este caso textos acuérdase que ese archivo que tenemos aquí es esto textos es esto los textos piteados que lo voy a mandar a el evaluador de costo de embedding ejecut Perdón No cargué primero la función Ahora sí lo ejecutamos y tengo que voy a tener como dijimos recién 81 81 tokens y me van a salir 0.0818 o sea muy muy barato ni mucho muchísimo menos que $ sí así que bueno una vez que sé cuál es el precio y lo veo para tenerlo en consideración lo que hago es crear los embeddings O sea que llamo a Open Y embedding sí les dejo aquí un comentario porque justamente lanch no es una biblioteca que solamente va a trabajar con Open Ai hay otra otro tipo de modelo de modelo largo de gran modelo de lenguaje s o seao un llm que se llama justamente haciendo juego este juego de palabra llama como el animal llama es decir que esto que está escrito acá me permitiría usar el modelo liama y no el modelo openi si esto para que tengan presente que no es l change algo que sirva solamente para un uno de los grandes modelos de lenguaje sino que puede ser por más de uno bueno independientemente de eso seguimos nosotros usando Open Ey Así que ejecuto esto y luego voy a crear la variable embeddings haciendo una instancia de Open e y luego voy a vectorizar Sí una una página para que lo vean como ejemplo sí que sería Esto entonces hago embedding query de textos uno o sea no la primer página insisto el primer párrafo s Por decirlo de alguna manera textos un se acuerdan que qué era Era esto que teníamos acá esto si chat gpt unito Bueno esto es texto todo este texto en realidad lo voy a con este con en ve in query voy a transformar en un vector simplemente para que lo veamos s entonces lo que hago es hacer un plint de qué largo tiene ese vector 1536 por 156 esto ser an porque es como yo defino Este modelo cuando lo creo con opening Ese es el valor por defecto que tien los vectores de Open y luego si los valores cuando de vector me muestra los 1536 valores del vector que representa a a el primer párrafo a este que está AC este párrafo que está acá está representado por todo este vector que está acá por justamente con eding y eding es una instancia de Open bien hecho esto lo que vamos a ver ahora cómo voy a insertar todos los en de todos los split de mi texto dentro de pyon Así que vamos al punto 43 que es insertar en en p para eso voy a importar os voy a importar el pycon y voy a importar pycon de Lan Chain vector Store se de nuevo Lan Chain vector Store porque se especifica python porque no es l change una aplicación que solamente va a poder usar un tipo de almacenamiento de vectores hay varios nosotros vamos aar pycon Sí así que bueno importamos estas librerías inicializo python fíjense con python apique y python m que son los elementos que te llan el archivo punto m H que os antes así que inicializo pyon lo que voy a hacer ahora es ver si hay algún índice dentro de Pon y me dice que hay un índice que se llama talum es esta que yo les mostrado hoy que le dije que hicieran caso omiso Esto bueno es una lista que ya está creada es un índice Perdón que ya está creado Sí entonces vuelvo al colap y lo que voy a hacer es borrar ese índice sí Entonces como lo borro en realidad está preparado para borrar todos los índices Sí bueno ejecuto este código que dice Bueno voy a recorrer todos los índices y cada uno de ellos lo voy a hacer un delete entonces ejecuto acá y bueno habré borrado todos los índices sí cosa que puedo corroborar yendo a la aplicación de pyon Ven aquí ya me dice que no tengo ningún índice desapareció el que estaba antes si vuelvo a cola veo que aquí también me dice lo mismo me muestra las las llaves digamos vacías bien Ahora lo que voy a hacer voy a volver a crear un índice sí Y en este caso le voy a poner índice chat gpt Bueno lo ejecutamos y ahí ya me lo de haber creado voy a pycon a ver si me lo creó y allí vemos que me lo ha creado si entro voy a ver que en este caso sí eh No tengo absolutamente todavía ningún vector vector con cero pero ya sé que la dimensión de los vectores que van a entrar aquí van a ser de Dimensión 15 porque provienen de eh lo que es openi bien Ahora vamos a hacer justamente el embedding de todos los vectores de todos los split de mi texto chat gpt txt como con p con Front document diciéndole Qué texto quiero eh vectorizar con cuál eh variable que me referencia en este caso a la que yo he instanciado de aquí arriba de Open a lo voy a hacer con un embedding de Open a y finalmente En qué índice de python lo voy a meter en el índice índice que índice es justamente lo que yo he puesto aquí como nombre de variable podría poner literalmente gpt Pero bueno lo pongo una variable para que sea más Mane bueno Esto va a tomar un tiempito Así que ahora dentro de vector van a estar todos los vectores de todas las partes del texto de ch bueno terminado esto van a ver la siguiente imagen que está aquí que esto es una imagen pegada al colap pero vamos al pyon en la web y veo que aquí esto no es una imagen pegada en el colap sino que es la realidad tengo 18 218 vectores Por qué Porque es justamente la cantidad de vectores en el cual fue splite el texto chat gpt text vamos a buscarlo para salir de Aquí está de acuerdan 218 vectores fue la división que yo hice justamente de ese texto bien entonces con esto ya tenemos Sí nuestra nuestros vectores dentro de esa base de datos con lo cual ahora voy a poder hacer lo que dice aquí el punto 4.4 Perdón que es preguntas de similitud viejo tema que venimos trabajando de las últimas clases pero queremos de alguna manera reflotar aquí teniendo en cuenta que ya no no lo estoy haciendo con eddings que están en memoria sino con embeddings que están en una base de datos para eso se llama pycon bien Vamos con preguntas de similitud esto ya lo hemos visto en muchos lados anteriores en este caso la pregunta cuál es la evolución de chp la voy a poner dentro de una variable pregunta y luego con the vector Store el método similarity Search le voy a poner justamente dentro la pregunta para que me dé la respuesta en respuesta Recuerden que el vector Store es esto que genera aquí es donde están todos los vectores que generé con embeddings y que guardé en python con lo cual ahora esa ese elemento me va a servir justamente para hacer una búsqueda de similitud y la voy a poner en respuesta y luego de eso voy a ver las respuestas Y veo en realidad más que respuestas en donde cuestiones similares o que encuentran similitud con este tipo de preguntas Este es el concepto raíz para empezar a buscar la respuesta fíjense que lo que eh está escrito aquí digamos es indicar En qué partes de todo este texto de estas páginas que en realidad no son páginas son splits sí puede estar la respuesta esa pregunta simplemente una búsqueda de similitud no es la respuesta concretamente como deje recién la respuesta la vamos a encontrar aquí usando el large Language model por eso vamos a utilizar eh vamos a importar perdón retrial qa y chat Open ea como ya lo hemos venido haciendo antes y vamos a crear en principio un large la Wi model con chat opena tomando el modelo gp4 cambiamos un poquito a un modelo mejor más caro y con una temperatura de una luego vamos a crear un retriever el retriever es lo que me va a permitir indagar esa búsqueda en el contexto de la base de datos de vectores de pycon Recuerden que todos esos vectores siguen estando dentro de esta variable vector Store y ese retriever va a tener determinadas configuraciones una de ellas es decirle Qué tipo de búsqueda quiero que haga de similitud y con searchs lo que hace es indicarle que justamente voy a buscar los k3 es decir las tres mejores respuestas y de ellas me voy a quedar con la mejor de todas sí bien eh o mejor dicho hecho modo voy a buscar acuérdense que el K es el el concepto de vecino cercano sí algo que me determina de todo el contexto los tres que están más cerca Qué cantidad de cercas voy a buscar los tres que estén más cerca y de los tres que estén más cerca obviamente me voy a quedar con el mejor vector que va a ser la mejor respuesta para esa pregunta todo eso lo configuro con reter para que después justamente creo un Chain que va a ser lo que la cadena que me va a ejecutar la búsqueda que eh justamente va a llamar al método Front Chain type diciéndole de qué modelo que es este que crea acá en llm y con qué retriever es con el cual voy a hacer esa búsqueda a través de este objeto chine Esto es lo que voy a hacer a continuación no me acuerdo si cargué las librerías las ejecuto todas nuevamente por las dudas para seguir el orden y ahora con Chain ron poniéndole adentro de los paréntesis la pregunta que en este caso lo he hecho de una manera muy prolija creando una variable para algar la pregunta y lo de la pregunta pero esto que está aquí cuándo se lanzó gpt lo podría escribir directamente aquí y voy a poner la respuesta en una var respuesta H lo ejecuto y vamos a ver el contenido de la vara respuesta y me dice que gpt3 se lanzó en junio del 2020 Esto sí ven que es una respuesta deir no es como lo que me pasó antes que me lanzó un montón de valores que representaban los espacios de similitud respecto de eh esta otra pregunta que obviamente era otra pregunta diferente a la de recién en cuanto Cuál eran los vectores que estaban más similares más llegados a eh esa pregunta que yo había hecho aquí aquí lo armó completamente solo determinó automáticamente de eh respecto de esta pregunta Cuál eran los vectores que estaban más cerca eligí eligió tres perdón porque yo le indiqué que elija los tres mejores y de los tres mejores se quedó con el mejor de todos y la mejor respuesta para esa pregunta fue gpt3 se lanzó en junio del 2020 fíjense que en este caso no se olviden de lo que estamos hablando aquí la change a través de gpt me está dando una respuesta No desde la base de datos de chat gpt o de gpt sino desde mi documento Es decir de este texto que está aquí es decir uso chat gpt con mi propia base de datos tema que vamos a profundizar con un caso bien complejo desde la clase que viene así que sin más por esta clase nos vemos en la clase que vi Hemos llegado al final de esta clase nos vemos en la próxima  clase Titulo: Clase 30 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/eEO4d-c15lU  \\n 1226 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número    30  Hola Bienvenidos a todos a la clase número 30 del curso de Inteligencia artificial de ies el tema de hoy continúa siendo como en la clase pasada l change pero vamos a tomar varios de los conceptos que vimos la clase pasada donde pudimos entender todos los tipos de servicios que brinda l change para aplicarlos a un un objetivo muy concreto que es justamente generar un sistema de respuesta con una base de datos propia de qué se trata esto básicamente puer utilizar a través de l Chain chat gpt como vimos la clase pasada pero para que pueda responder preguntas con la inteligencia que tiene chat gpt con toda la la capacidad que tiene de generar respuestas con lenguaje natural pero sobre la base de una base de datos propia esta base de datos propia va a poder ser un archivo de tipo PDF Word también se pueden tomar cosas de Wikipedia o cualquier aplicación que tenga una app y me brinde sí un conjunto de textos sobre el cual yo pueda trabajar esta aplicación que vamos a hacer hoy y también más adelante vamos a ver que con los agentes voy a poder justamente trabajar con otro tipo de base de datos no de tipo texto pero que también me va a generar una respuesta personalizada como lo que estamos buscando aquí en este caso particular lo vamos a hacer sobre la base de uno de los libros de recursos humanos CR usamos varias clases hacia atrás puntualmente el libro de marth ases que aquí estamos mostrando ubicados aquí en el la pll 11 en este Notebook que es el que vamos a usar para la primer parte de esta clase bueno el título representa un poco lo que hablamos recién de la variedad de tipos de archivos que pueden tomarse como input para que chat gpt conteste con nuestra base de datos propia lo primero que vamos a hacer es preparar los datos más que preparar los datos tenemos que instalar todas las aplicaciones que vamos a usar en en este lap Así que justamente vamos a empezar por esto bien terminado este proceso y al igual que como lo hicimos la clase pasada tenemos que levantar el archivo de environment el punto m que tiene los datos de las apik de chat gpt y pyon que son dos aplicaciones con la cual hoy voy a volver a interactuar Así que vamos a montar el drive y una vez concluida esta tarea vamos a conectarnos con el archivo vamos a abrir concretamente el archivo punem que tiene la información que Recién mencionábamos ahora vamos a instalar una serie de aplicaciones que tienen que ver con el tipo de archivo que yo podría abrir para generar este diálogo con una base de datos propia Aquí vamos a instalar tres archivos pero no vamos a usar los tres Solamente vamos a usar el primero de ellos dado que nuestro archivo que vamos a tomar va a ser un archivo de tipo PDF pero sí les dejo de alguna manera aquí y lo vamos a comentar para no hacer esta instalación en vano que esta sería la aplicación que deberían instalar si el archivo fuese de tipo Word y esta otra si Tomás demos una información desde Wikipedia Así que solamente instalamos lo que nosotros vamos a utilizar y ahora como bien Dice el título Aquí vamos a cargar el documento para eso vamos a hacer una función que le voy a poner cargar documento en el marco de la cual voy a poner como parámetro el archivo o el documento que quiero empezar a cargar en principio al igual que con el criterio que dijimos recién les dejo expresada aquí las tres líneas que corresponderían a levantar las tres los tres tipos diferentes de librería de las tres situaciones que que mencionábamos recién y con el mismo criterio Aquí también dejo expresada tres líneas que tienen que ver con Cómo cargaría el archivo de acuerdo al caso que correspondía nosotros vamos a tomar aquí en principio el PDF porque es el tipo de archivo que tenemos como ingreso pero es importante que les comente que en el caso que es un archivo de tipo Word fíjense que no cambia demasiado el código simplemente lo que cambia la librería pero la estructura de código que voy a escribir Para eso es similar pero en el caso que sea Wikipedia obviamente Wikipedia no es un archivo es un espacio del cual yo puedo sacar información por lo tanto cambia no la librería sino la forma de expresar esta línea a la hora de cargar esa información desde Wikipedia Por qué Porque tengo que ponerle un query No un nombre a un archivo se entiende o sea Wikipedia existo no me va a dar documento sino que yo le voy a mandar un query me va a dar un texto producto de haberle mandado ese query con lo cual como Ven aquí justamente Esa esa línea esa forma de llamar al documento en el caso que use Wikipedia como opción va a ser distinto Así que cargamos esta función y ahora la usamos cómo lo usamos poniéndole justamente a cargar documento que como he denominado esta función el archivo concretamente que le vamos a poner archivo 1 PDF ustedes obviamente lo van a tener como siempre dentro del campus virtual de ifi Así que procedemos a cargar ese documento y lo colocamos en una variable que le he llamado documento una vez que vemos esto vamos a analizar un poquito las características del documento con lo cual vamos a imprimir el documento y su largo como resultado vemos obviamente todo el documento obviamente es imposible verlo así de esta manera simplemente para ver cómo está cargado y ven que está justamente separado por marcas de enter HM corresponde digamos a la característica del documento que está cargado aún no está splite aún no está separado por párrafos Ese es el documento su estado natural pero el dato muy importante aquí es este 89 que me marca Cuántas páginas tiene mi documento dato muy importante para lo que voy a hacer a continuación que es la fragmentación de datos aquí estamos entonces en la fragmentación de datos donde vamos a usar esta librería que usamos la clase pasada recursive charter t splitter que lo que hace es separar el texto Sí en una determinada cantidad de Dimensión que le voy a dar no es en párrafos s lo separa digamos en estructuras que se llaman chang y Recuerden que justamente esta característica que hemos utilizado tiene que ver con el criterio de que esta separación guarde en cada una de ellas algunos componentes de El chun anterior es decir tomo el primer chun y luego cuando creo el segundo toma una determinada cantidad de caracteres del anterior Para no perder el contexto tal cual lo explicamos en la clase anterior en este caso eh Al igual que la clase anterior usamos el criterio de que el chun si sea 200 esto también se puede cambiar lo manejan ustedes yamos a criterio así como el overlap que es justamente Cuántos caracteres del Chan anterior se reproducen en el Chan siguiente bien en este caso vamos a poner justamente todo este split con este criterio de 12 y20 de overlap dentro de una Ray que le voy a llamar fragmentos y luego vamos a imprimir justamente eh el contenido de El Largo perdón de fragmentos para ver justamente En cuántos fragmentos me ha explito mi documento de 89 páginas Perdón en cuantos este chunks ha dividido ese documento y me dice que lo ha hecho en 1025 chuns este documento de 89 páginas con este criterio de que cada el tamaño de cada Chan sea 200 caracteres y haya 20 de overlap como siempre miramos un poquito Cuál es el contenido de algunos de estos chuns y uso el elijo digamos el 101 y veo aquí Bueno presentación en este momento aún est fresco el caso ROM Sí este toda esta cuestión que me lleva a una estructura que tiene como lo puse aquí arriba un tamaño de 200 obviamente no se ve aquí 200 porque solamente Reproduce una parte pero cada uno de estos párrafos tiene para justamente tener este criterio también de separar este documento en estos fragmentos y luego tener esta duplicación perdón de 20 o este overlap de 20 entre un Chan y el otro Bien también podemos cambiar esto obviamente podemos ponerle en lugar del 101 el 10 sí y así vamos a ver bueno Esto es la parte de la dedicatoria un poco más introductoria así que vamos a poner algo más este más avanzado por ahí el 201 bueno ven que ya aquí ya habla de cosas que tienen que ver con el contenido propio del documento y no con la parte de la introducción digamos del documento Recuerden que el documento tiene en principio seguramente algún tipo de prólogo índice cosas que no son digamos de relevancia para lo que nosotros queremos preguntarle digamos vía chat gpt a este libro Y que justamente chat gpt nos responda leyendo o buscando la respuesta en el contexto de ese libro bien como lo explicamos también la clase pasada lo siguiente que vamos a hacer es ahora que tenemos todo el texto separado lo próximo que tenemos que hacer es hacer los embedding pero como los embedding ya lo voy a hacer vía chat gpt lo que necesito saber es cuál va a ser el costo dado que esto obviamente haciéndolo a través de chat gpt tiene un costo en este caso vamos a usar esta función para hacer eso y el modelo que usamos para hacer embedding es text embedy Ada 002 vamos a ir a la página como hicimos la clase pasada de chat gpt de Open y acá V justamente donde están todos los productos que vimos la clase pasada también están los modelos de embedding solamente tengo disponible aav2 y el costo es 0.0001 cada 1000 tokens por eso Esto es lo que voy a tomar como criterio para aplicar en esta función que estamos desarrollando en el Notebook sí Entonces como Ven aquí voy a primero calcular como hicimos la clase pasada la cantidad de tokens que tiene en total este documento y luego de ello lo voy a mostrar por pantalla como result toda esta función y voy a calcular dividiendo ese valor por 1000 y multiplicándolo por el valor que vimos recién del precio que eh Me pide digamos que me solicita para este servicio Open Así que esta es la función la cargamos y Ahora la vamos a utilizar justamente pasándole como parámetro los fragmentos Acuérdese que fragmentos es una Ray que es este que está aquí H que tiene justamente el resultado de haber hecho esta separación este spito de este documento en los distintos chuns Sí así que bueno lo ejecutamos y antes de hacer el proceso sabemos que nuestro nuestro trabajo digamos O Nuestro documento ha sido tokenizado en 42182 tokens y Por ende el costo de hacerlos en beding por parte de openi para esa situación es de 0.0042 bueno supongamos que estoy de acuerdo con ello y que me parece bien y estoy de acuerdo gastar ese dinero Voy a pasar al siguiente paso que es justamente crearlos en beding y subirlos a pyon tal cual hicimos en la clase pasada bien aquí estamos con eso voy a importar pycon voy a importar la librería para los embeddings y voy a importar la librería justamente de pycon bien lo primero que hago es crear la variable embedding crear una instancia de Open embedding con lo cual este va a ser el la variable la instancia de la función que me va a permitir ejecutar los embedding propio de Open Ai Pero antes que nada lo que tengo que hacer es conectarme con pyon y para eso utilizo el método init de donde le paso como parámetro la Api y el nombre del environment que son los elementos que tenía en el archivo punto m Así que dicho Todo esto lo ejecutamos y ya tenemos entonces la instancia de embeddings de Open eii sí que va a ser el que va a tomar el trabajo de hacer los eding y estoy conectado a pyon que va a ser la base de datos en la cual voy a poner el resultado de esos ened son los vectores que son propios de recordemos siempre todo este texto que tenemos transformado a vectores bien una vez que tengo esto lo que voy a hacer es limpiar todos los índices que pueda yo tener en pycon así que bueno ejecuto este código para borrar todos los que tengo y lo único lo único que encuentro Perdón es el índice chat gpt que usamos la clase pasada se acuerdan así que bueno lo está borrando en este momento bien terminado esto lo que voy a hacer ahora es crear un nuevo índice al cual le voy a poner tal un por talento humano que es justamente el o parte del nombre de este libro Y bueno pruebo por las dudas si está ya creado o no y mando un mensaje de error en el caso que así sea pero obviamente ya sabemos Porque lo hicimos recién que este python está vacío Así que mal puede encontrar este índice Así que lo ejecutamos a esto y creamos nuestro nuevo índice tal un si me voy a la aplicación de python veo que ya está creado el índice obviamente sin nada dentro porque aún no he hecho los embeddings Así que volvemos ahora entonces al cola habiendo verificado que el índice ya está creado bien habiendo verificado eso ahora sí vamos a crear los embeddings y lo vamos a cargar en la base de datos python justamente con pyon FR document acuérdense esta introducción que vimos la clase pasada también donde le mando todos los fragmentos en los cual dividí el texto Cuál va a ser el embedding O sea la clase embedding que está habíamos creado aquí decimos que el eding lo va a hacer este con chpt y el nombre del donde voy a poner esos vectores que va a generar este proceso de bedding Así que ejecutamos esto y Recuerden que además de esto ir a python va a quedar contenido dentro de vector stor o sea dentro de la variable vector que es una Ray esto obviamente va a quedar en memoria por el momento después va a desaparecer pero lo que no va a desaparecer y va a quedar es el alojamiento de estos vectores que hicimos en Pon bien ahí terminó el proceso y vamos a volver a python como hicimos hoy y ahora fíjense que tengo 1000 25 vectores que es justamente la cantidad de vectores en los cuales que se corresponde con la cantidad de fragmentos que tiene el el texto que estuve justamente spando fragmentando así que bueno habiendo verificado esto volvemos nuevamente a el col bien de nuevo en el colap lo que vamos a hacer como dice el título aquí es hacer las consultas para lo cual vamos a cargar las librerías retrieval qa y chat Open Ai y vamos a a crear el llm que es el modelo justamente con chat Open el modelo gpt 3.5 Turbo con una temperatura de 1o y vamos a crear el retriever que se acuerdan que también la clase pasada lo hicimos justamente diciéndole que vamos a usar un criterio de búsqueda de tipo de similitud y vamos a buscar las tres mejores coincidencias o las tres eh opciones más cercanas a lo que es el vector que representa la consulta que estamos llevando adelante y finalmente en Chain voy a ejecutar la la consulta con el modelo y con las características de re que le puse justamente para eh poder llevar adelante cualquier búsqueda a través de esta variable Chain y empezamos a hacer las preguntas Cuál es el rol de la motivación lo pongo dentro de una variable pregunta y luego Chain que es la variable que cree punto ran de la pregunta y me da la respuesta y voy a ver cuál es el contenido de la variable respuesta Bueno ahí tenemos el rol de la motivación es fundamental el comportamiento humano etcétera etcétera etcétera ejecutamos otra pregunta ahora Cuáles son los tres sistemas importantes de la motivación humana AC tendría que ser un poquito más explícito es importante obiamente ustedes saben el tema del prom así que bueno ejecutamos esta otra pregunta y vamos a ver cuál es la respuesta aquí está la respuesta los tres sistemas importantes de la motivación humana sí los logros luego punto dos el poder como motivación tres la tenencia como motivación hacemos una pregunta más Cuál es la relación entre el comportamiento y la competencia nuevo aquí me estoy olvidando los artículos y la competencia bueno la ejecutamos y aquí está respuesta la relación entre el comportamiento y la competencia es que los los comportamientos son parte visible de las competencias etcétera etc obviamente esta estas preguntas tienen que ver con el tenor del documento que tomamos como ejemplo no este libro de recursos humanos este así que que bueno obviamente quizás estas preguntas no sean no estén en el común de todos ustedes porque seguramente son más propias de alguien que está en el tema de recursos humanos Pero bueno evidentemente son además preguntas que tienen que ver con el contenido del libro Más allá de que sea recursos humanos con el tipo de tema que trata este libro Obviamente que si ustedes ponen una pregunta de algo que no trata el tema no trata el libro Perdón obviamente la respuesta va a ser muy ambigua o quizás sea una una no respuesta porque no encuentra esa información chat gpt en el caso de no encontrar esa información si le pregunto algo no sé de F por decir algo seguramente me va a decir que no contiene ese documento la información que estoy buscando bien Vamos a ver ahora cómo complementar esto con un sistema de memoria que de alguna manera no con tres preguntas que son independientes como en este caso siga el comportamiento que nosotros tenemos habitualmente con chat donde hacemos una pregunta y luego hacemos una repregunta y una repregunta llevando un hilo conductor desde la primera pregunta hasta la última bien aquí seguimos con añadiendo memoria donde vamos a usar esta misma librería que usamos antes chat openi pero vamos a usar en lugar de esta que eran para consultas directas vamos a usar esta otra que va a servir para hacer una conversación más parecida a lo que hago habitualmente con chat gpt el modelo que vamos aar va a ser exactamente el mismo en el caso del reever también va a ser similar a lo que hicimos en el punto se Y en este caso sí Obviamente el objeto que voy a usar va a ser conversational retal change a diferencia de este que utilizé en el punto seis luego como dijimos vamos a trabajar con un concepto de memoria es decir que la primer consulta va a arrancar con la memoria en blanco pero la segunda consulta que haga la voy a hacer con la idea de volver a trabajar sobre la base de la respuesta anterior por lo tanto este array memoria va a tener todos y cada uno de los diálogos previos a la pregunta que tengo en cuestión así que pongo memoria en blanco y hago la primer pregunta cuáles son los tres sistemas importantes de la motivación humana donde le voy a mandar la pregunta Que obviamente es esta variable y la memoria que en este momento está en blanco luego lo que voy a hacer es agregar esta respuesta a la Ray de memoria para que sea una memoria para la próxima pregunta sí y ponemos la respuesta a esta pregunta bien aquí está entonces repite la pregunta el chat history Ahora va a ser la pregunta y la respuesta los tres sistemas importantes de la motivación humana son los logros el poder como motivación y la pertenencia como motivación bien entonces ahora en la próxima pregunta que voy a poner es podrías ampli arme el primero de ellos el primero de estos puntos obviamente ahora memoria va a ser una Ray que va a contener a todo esto que mencionamos antes es decir la pregunta y la respuesta del caso anterior entonces en esta pregunta tenemos esta pregunta y la memoria el chat history de la pregunta anterior Así que la ejecutamos y aquí tenemos la respuesta fíjense entonces en el chat history tengo la pregunta anterior y la respuesta anterior y ahora tengo la respuesta a esta segunda pregunta podrías ampliarlo en el primero de ellos El primer factor mencionado es la experimentación activa etcétera etcétera etcétera bien entonces con este concepto terminamos la primer parte de esta clase y ahora vamos a continuar con la segunda parte de esta clase para complementar todas las cuestiones que tienen que ver con el sistema de preguntas y respuestas incorporando como adelantamos al principio de esta clase algunos conceptos que tienen que ver con audio hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte Titulo: Clase 30 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/q2Jteb_auXU  \\n 1589 segundos de duracion \\n Hola esta es la segunda parte de la clase número 30 te invito a empezar con    ella bueno Vamos a continuar con la segunda parte de esta clase el sistema de respuesta con base de datos propia en el caso nuestro un PDF el documento recordemos que la parte anterior lo que habíamos hecho era cargarlo por supuesto y luego lo splite sí lo dividíamos cada uno de esos esas divisiones esos chun esos splits los este pasábamos a tokens y luego lo que hacíamos eraa vectorizar losos esos vectores a través del proceso de eding que ya conocemos y ya venimos viendo las últimas clases lo colocá dentro de una base que eh sirve específicamente para guardar vectores que se llama pcon ya tenemos entonces esos vectores guardados allí vamos a suponer que nosotros nos hubiésemos ido Sí hemos cerrado la sesión con lo cual Si queremos Volver al punto si en el cual hicimos estas consultas que tenemos aquí arriba con la memoria eh deberíamos supuestamente hacer todos los puntos des el principio sí la primera parte hasta llegar al punto siete obviamente eso no es así gracias a que gracias a que yo tengo los vectores ahora dentro de la base de datos python con lo cual todo ese proceso del punto un al se lo puedo evitar volviendo a entrar a python esta vez no para guardar los vectores sino para tomarlos de allí volverlos a cargar en una variable y volver a ejecutar este código que tenemos en la parte siete donde decíamos la las consultas sobre el libro que tomamos en este caso como modelo bien Vamos a ver entonces como aquí dice el título Cómo recuperamos los vectores desde python Bueno lo que tenemos que hacer es importar python Open Ai embeddings y pyon de vector stores luego creamos una instancia de Open embeddings a través de una variable embeddings como lo hicimos antes y eh e inicializan Perdón pyon mandándole digamos los valores de la apiq y de El entorno ejecutamos esto y ahora vamos a trabajar sobre el índice que creamos en la parte anterior que le habíamos puesto tal un por talento humano guardo su nombre dentro de una variable y lo que voy a hacer es mirar ese índice todavía no lo estoy cargando simplemente lo puedo observar sus características con lo cual con Index de el índice índice que es el tal um lo voy a poner dentro de esta variable de Index y con describe voy a ver características de ese índice donde me dice la dimensión de los vectores que es la fija que tiene Open e luego este el la cantidad de de vectores que tiene este conjunto sí que es 1025 tal cual vimos en la clase pasada que fue la cantidad de splits que hizo el proceso que hicimos con el libro de capital humano bien una vez que tenemos esto Ahora sí vamos a traernos los vectores obviamente esta parte puede obviarse no Simplemente a título de mirar que también lo puedo hacer si yo voy como lo hice en la clase pasada yendo a ver el sitio web de python para ver estas misma características del entorno del sitio we de python es decir esto que está aquí veo exactamente la misma información la dimensión de los vectores y la cantidad de vectores bien volvemos a el colap bueno acá estamos de nuevo en el colap y ahora sí cargo dentro de esta variable vector Store que es el mismo nombre que usé antes eh el todos los vectores que vienen desde python con python Front existen ind es decir Tomar los vectores de un índice existente el índice es el que está en esta variable índice que lo guardé aquí arriba um y el embeddings que es justamente una instancia de Open embeddings que es lo que des acá ar bien con esa información ejecuto esto y voy a tener Entonces al igual que antes dentro de vector stor todos los vectores que en la primera parte de esta clase pasamos desde el texto que subdivididos y transformamos y luego pusimos en python ahora estoy exacta ente en la misma situación que estaba por eso Aquí está esta referencia antes de ejecutar el punto s sí voy un poquito hacia atrás Es decir Estoy en este mismo punto en realidad en este mismo punto seis Sí porque en realidad las consultas en el seis y en el siete eh Son sobre la misma base solamente que en el seis la hacemos sin memoria y en el siete con memoria Sí con lo cual digamos que habré recorrido desde el principio hasta este punto Sí donde aquí cargué los vectores pero los este los hice como en beddings hice el proceso de bedding por primera vez aquí no estoy haciendo proceso de embedding sino que estoy trayéndolos ya habiendo pasado al proceso de embedding desde el índice creado por lo tanto como dice aquí ahora podría volver a ejecutar el punto si de la parte uno bien puedo volver hacia atrás a ese punto y si ejecuto esta instrucción que es la que me Configura los modelos creo la memoria y empiezo a hacer las mismas preguntas que hice antes y voy a tener respuestas no en todos los casos exactamente iguales pero sí obviamente bastante similares a las que recibí antes las respuestas no son siempre exactamente las mismas con char eh obviamente que la aproximación es bastante eh Pero bueno a veces puede haber alguna palabra o alguna forma de expresarlo de un modo un poco distinto bien aquí está la respuesta H los tres sistemas importantes de la motivación humana bueno obviamente está está preguntando por una cosa muy puntual con lo cual este es difícil que de una respuesta distinta desde la concepción obviamente como dije recién las palabras pueden ser diferentes hacemos la segunda pregunta con la memoria como usamos también en el punto s en la ocasión anterior y tengo nuevamente la respuesta entonces repasando hice lo mismo que hice en la primer parte sin haber recorrido todos los pasos del uno al cinco Sí porque obviamente lo que dije recién estas consultas sí la hacían sobre el mismo proceso que había hecho para también el punto s pero eran consulta sin memoria y en el punto s con memoria bien habiendo visto esto ahora lo que vamos a hacer es agregar una especie de este bueno concepto ampliatorio de este objetivo que estamos viendo aquí que es el tema de incorporar a esta cuestión de los textos el concepto de audio Cuál va a ser la idea que vamos a buscar acá Nosotros le dimos una consulta y vamos un poquito hacia atrás nuevamente a través de de un texto Aquí tengo una pregunta Aquí tengo la anterior Sí y La respuesta es todo esto en primer caso o todo esto en el segundo caso Sí en el cual eh se puede tener información de tipo texto es decir entra texto sale texto la idea ahora sería que pudiéramos darle la pregunta en modo audio la convierta texto genere la respuesta en texto y volvamos a recuperarla en modo audio digamos algo parecido a lo que podría pasar que sería ideal tener una plataforma Como por ejemplo eh WhatsApp donde yo le mando una consulta de audio me la recibe Bueno un sistema en el cual justamente la transforma texto para generar la respuesta recubra la red genera la respuesta y me la devuelve en audio tal cual como si yo le mandase una audio a una persona conocida con una consulta y esa persona me lo devuelve con un audio Pero obviamente intermediando en este caso la guía por lo tanto vamos a usar una aplicación que se llama whisper que es una de las más exitosas en cuanto a la capacidad que tiene de transformar audio en texto sí no sirve para las dos acciones después para la acción de texto audio vamos a ver otra aplicación pero whisper es una de las más utilizadas eh whisper es una aplicación de Open source pero Usa los servicios de Open eii sí Por lo tanto es una aplicación eh que requiere muchos recursos de Hardware y por eso les he dejado aquí este mensaje que es muy importante como viene aquí arriba tengo que usar un gpu de tipo t4 porque si no Esta aplicación no va a funcionar sí Entonces lo primero que vamos a hacer es instalar el paquete de whisper lo hacemos directamente desde un github obviamente es una de las tantas formas Se puede instalar de otra de otra manera también Pero bueno es bastante práctico porque me trae toda la la la serie de librerías que eh incluyen digamos la instalación de este paquete así que bueno procedemos a instalarlo bien ahí terminó de instalarlo y les pido que por favor fijen su vista en este botón que me pide y como dice el mensaje aquí que yo debería hacer un restart session para que queden actualizadas definitivamente esta versión de whisper Así que hacemos clic aquí me da este mensaje le digo que sí está reiniciando bueno ahora sí ya puedo continuar trabajando y lo que hago es una vez instalado este paquete es importarla donde le digo que el dispositivo escuda que justamente se refiere a lo que hablamos recién el tipo de GP que voy a usar batch 16 es eh digamos una información que le doy en cuanto a el tipo de lote en que quiero que vaya procesando todo este audio que va a entrar para ser transformado texto ustedes saben que estas aplicaciones no se pueden tomar toda la la capacidad completa por eso también específico un tipo de tipo de computación computer Ty como flat para que vaya tomando referencia de cómo ir Armando esos lotes y pueda ir procesándolos poco a poco y no todo en una sola acción Así que bien ejecutamos Esto Obviamente que estos parámetros son configurables no el del device Sí pero bat size y computer type C Así que es ustedes también lo pueden hacer un bat size más grande sí Obviamente que en la medida que el bat size es más grande va a requerir más recursos de Hardware por supuesto una vez que tenemos instalado whisper vamos a crear nuestro modelo a partir del cual justamente vamos a hacer esta acción de transformar el audio en texto por eso vamos con load model a cargar al modelo de wisper a esta variable a esta instancia que le he llamado model con un tipo de modelo de los que tiene disponible whisper el large b2 existen otros esto lo pueden consultar el sitio oficial de whisper También tienen otros sitios Como por ejemplo High in Face aquí se los muestro donde ya me propone la posibilidad de empezar a utilizar un lash B3 bueno esto como todo va a evolucionando y Me puede mostrar distintos modelos obviamente en la medida que los modelos sean eh más nuevos pueden implicar Eh bueno algún tipo de de de requisito de Hardware sí Que obviamente por ahí no se soporta con lo que tenemos nosotros a disposición Así que eso también lo tenemos que evaluar nosotros vamos a usar en este caso el large b2 el device que dijimos recién que es el cuda el computer type que también lo lo especificamos Aquí recién estos parámetros de aquí y bueno con todo esto lo que hago es crear esta instancia que le he puesto model para hacer el trabajo que dijimos que vamos a hacer bien ahí lo está cargando bien y allí terminó lo que vamos a hacer ahora es crear dos variables audio 1 y audio 2 que lo lo que van a hacer es justamente cargar cargar eh los audios de entrada los audios de input concretamente estos son dos archivos MP3 que contienen las mismas preguntas expresadas con mi voz que le hicimos antes de modo texto en el primero que le preguntaba Cuáles son los tres sistemas de motivación y en la segunda donde le digo que me amplíe el primero de esos tres sistemas de motivación ambos en formato MP3 se lo voy a hacer escuchar para que entiendan de qué se trata porque si no de otra manera no van a poder identificar más all ya que imaginan que en ese audio está esa pregunta vamos a escucharlo eh Para poder apreciarlo mejor Cuáles son los tres sistemas importantes de la motivación humana podrías ampli arme el primero de los sistemas Bueno ahí escuchamos entonces en primer caso este archivo MP3 y luego tres sistemas motivación MP3 y luego este otros que contienen Ni más ni menos que expresada en audio como yo le dije antes eh el contexto de la misma pregunta que hicimos en modo texto anteriormente esto obviamente siempre lo van a tener como siempre lo hacemos en el campus virtual para que ustedes después lo puedan llevar a eh la unidad de drive y puedan usarlo en el colab como lo venimos usando habitualmente así que justamente ahora vamos al colab bien aquí estamos en el colab Entonces vamos a cargar esos dos audios en dos variables el primero en audio uno y el segundo en audio 2s justamente en un whisper lad audio donde voy a llamar a estos archivos MP3 que recién escuchamos ubicados como como dijimos antes en una unidad de Drive yo aquí la que uso siempre ustedes en la que consideren bueno cargamos entonces esos dos audios en audio 1 y ahora lo que tenemos que hacer es convertirlos en texto justamente a través de model que es model model es el modelo que creamos aquí que usa whisper que es una instancia de load model de whisper que usa el modelo large b2 bien justamente con ese model voy a hacer el transcri la transcripción de el audio con el tamaño de batch size que fijamos anteriormente aquí arriba sí 16 HM bien y voy a hacer lo propio también para eh audio do para audio un y para audio dos importante destacar que aquí estoy usando eh una variable prec un y PR 2 y no uso result 1 y result 2 y más bien la estoy reutilizando para crear estas dos variables porque result me arroja cuando ejecuta el modelo de transcription un diccionario con un montón de valores dentro los cuales está el texto que a mí me interesa Por eso justamente en esta expresión recurro a tomar the result del diccionario de la colección de datos que me arroja esta transcripción solamente el texto y lo pongo en pregun y y después perdón Repito el procedimiento con audio 2 lo vuelvo a poner en result pero este segundo audio volviendo a tomar el texto lo pongo en prec 2 con lo cual ejecuto esto y ahora voy a tener en preg un el resultado de la transcripción de tres sistema de motivación MP3 en prec un y en el segundo caso la transcripción del primer ampliar primero punto MP3 en el caso de convertido a texto en el caso de la variable prec 12 bien Esto lo puedo simplemente ejecutar y lo veo sí fíjese que está más que claro en prec uno está Cuáles son los tres sistemas importantes de la motivación humana y en prec dos podrías ampliar el primero de los sistemas eso que la transcripción ha sido más que exitosa bien lo que tenemos que hacer ahora es volver a configurar las eh la característica digamos de lo que teníamos cuando le hicimos las preguntas de modo texto es decir nuevamente sí generar el modelo conversacional eh que teníamos en el punto si HM Entonces vamos a volver a cargar bien y como hicimos antes dejamos en Blanca la memoria para empezar a hacer las dos eh acuérdense que eran dos consultas encadenadas y en el primer caso le pongo pregunta se acuerdan que antes le había escrito el texto de la pregunta que pregunta esta que pusimos acá Cuáles son los tres sistemas importantes de la motivación humana Bueno ahora directamente le pongo el contenido de esa variable que es resultado de haber transcrito el audio sí con lo cual obviamente de aquí esto es un texto más No puede haber forma de que esto tenga algún tipo de problema porque es como si literalmente yo le escribiese esa pregunta H bien pero vamos a probar para ver que funciona bien Igual y una vez que esto se ejecute voy a imprimir la respuesta y Bueno veo que es equivalente porque ahora ya se trata de un texto sí O sea no debería haber ningún problema y hago en este caso la segunda pregunta donde le pongo prec do que prec do es esto que está aquí resultado de haber hecho esta transcripción sí esto de aquí y esto de aquí originado en el ampliar primero MP3 sí bien entonces ejecutamos y tengo con la memoria incluida la respuesta acerca de la ampliación del primero de los puntos de estos tres que necesit allí terminó de generar la segunda respuesta y la imprimimos al igual que como hicimos en la primera respuesta y aquí tenemos una ampliación con seis puntos respecto de el primero de los puntos que trataba de los logros como motivación bien ahora que tengo esto voy a cerrar el ciclo recuperando el audio o el concepto de respuesta en audio por lo tanto así como en su momento tenía la entrada en un audio y lo transformé a texto ahora tengo la respuesta y le voy a transformar de esa respuesta de modo texto nuevamente en audio para que podamos además de leerla como lo estamos haciendo aquí escucharla para eso voy a utilizar una aplicación Open Ai que se llama tts 1 tts por text to Speed spech o sea speech to text Tex spech la operación inversa que es la que tenemos que hacer ahora por lo tanto voy a importar openi y p l p porque lo voy a mandar justamente a que se guarde a una librería la respuesta de este MP3 que va a generar como salida justamente esta transformación de la respuesta texto en audio aquí lo que voy a crear con audio spe create de Open a través de la variable response una instancia de esa herramienta de es de Open voy a tomar el modelo tts 1 fíjense que acá hablamos de modelo para esta tarea que tenemos que hacer justamente de convertir texto en audio así como hablamos también de modelo vamos un poquito hacia atrás cuando hablamos de la operación inversa Perdón Aquí está large 2 es el modelo de whisper que usamos para transformar audio en texto Si y en el caso de esta otra aplicación usamos el modelo tts 1 para hacer la operación inversa en voice Eh aloy sí es una de las opciones de voz que tenemos disponible Aquí sí hay una limitante porque eh No todos los modelos de transformación de texto en audio tienen eh la la expresión o la elección de la persona que habla en castellano con un Castellano puro en realidad se nota que es una persona que habla en castellano pero con un acento de Estados Unidos se enti es Claro pero no es lo mismo que fuese una pregunta eh una persona digamos que hubiese respondido como una nativo digamos no con lo cual Bueno es un tema a verlo no no hay muchas opciones pero las hay hay que buscarlas no es este el caso pero este Bueno este hayos un poquito más avanzados que ya van mejorando ese aspecto no se olviden siempre no nos olvidemos siempre que en realidad Lamentablemente en todo lo que es el mundo del nlp las las soluciones digamos en en castellano no son siempre las que más abundan obviamente siempre la lengua madre para este tipo de cosas es este la lengua inglesa Sí hay mucho material que se puede recurrir a través de eh de hing Face que es lo que le mostraba recién aquí recurrimos nuevamente a recordar ese sitio donde es uno de los sitios que más se empeña en Buscar justamente gracias a la comunidad española en Buscar este alternativas o herramientas que eh usen al lenguaje Castellano bueno para no ser tan larga toda exposición lo que hacemos en principio aquí entonces es generar Este modelo Sí y luego lo que hacemos Es con el método stram to F de esta variable que acabamos de crear darle Cuál es el archivo que quiero que justamente me genere le digo que quiero que me lo genere nuevamente en mi unidad de Drive pero que le ponga como nombre respuesta 1. MP3 así que sin más lo ejecutamos y ya va a poner entonces allí ese documento HM y hacemos lo propio para la segunda respuesta sí en este caso fíjense que uso respuesta dos answer como use en el primer caso respuesta un answer y le puse respuesta 1 MP3 al primer caso y respuesta 2s MP3 el segundo caso ya termino los dos lo he hecho bastante rápido si me voy acá a la unidad voy a Drive mydrive archivos nlp y aquí tengo que tener entonces esos dos archivos Acá está respuesta un Respuesta 2 MP3 lo vamos a bajar Lo descargamos eh Para poder escucharlos Sí así que ahí los vamos a descargar directamente vamos a poner en el escritorio nuevamente respuesta un y lo mismo vamos a hacer con respuesta dos lo vamos a descargar también en el escritorio ustedes lo pueden hacer donde quieran por supuesto como siempre lo decimos para poderlo ver bien Vamos a ejecutar el audio y vamos a poner sobre la pantalla el texto para poder seguir justamente la locución los tres sistemas importantes de motivación humana según David mcclellan son uno los logros como motivación las personas que están motivadas por los logros buscan constantemente desafíos y metas difíciles de alcanzar les gusta recibir reconocimiento por sus logros y se sienten satisfechos cuando obtienen resultados exitosos dos el poder como motivación las personas motivadas por el poder buscan influencia y control sobre los demás les gusta tener autoridad y tomar decisiones importantes obtienen satisfacción al ejercer su poder y lograr el respeto y admiración de los demás tres la pertenencia como motivación las personas motivadas por la pertenencia buscan conexión emocional y vínculos sociales fuertes valoran la relación con los demás y buscan sentirse incluidos y aceptados en grupos o comunidades obtienen satisfacción al tener relaciones interpersonales positivas y sentirse parte de algo más grande que ellos mismos según David mcclelland las personas motivadas por los logros tienen las siguientes características principales un tienen una fuerte necesidad de establecer metas desafiantes y alcanzar resultados significativos se centran en mejorar continuamente su desempeño y superar sus propios logros anteriores tres buscan retroalimentación y reconocimiento por su desempeño y logros cu tienen una alta autoestima y confianza en sus habilidades c la motivación por los logros les impulsa a asumir riesgos calculados y a esforzarse al máximo seis sienten satisfacción y orgullo al alcanzar sus metas y obtener resultados exitosos en sus actividades Bueno hasta aquí Hemos llegado con la segunda parte de esta clase y con la clase como un todo y bueno ha sido una clase bastante larga en la que hemos puesto sobre relieve una aplicación muy importante de mucha proyección futura que es la posibilidad de utilizar un large Language model puntualmente en este caso chat gpt para Que responda con la capacidad interpretativa que tiene y con la capacidad de generación de la respuesta una respuesta en base a nuestra propia base de datos hicimos con un archivo tipotex t también PDF y está la posibilidad de que ustedes Bueno les dejé algunas referencias para que si quieren utilizar Word como como input o hasta Wikipedia no como un archivo de documento sino básicamente como un generador de información que la va a transformar definitivamente en un archivo de texto Eh bueno esas eh formas también como inputs para poder generar este chat gpt personalizado luego hicimos una extensión donde bueno entramos con un audio y no como un texto la pregunta y salimos también con un audio como respuesta esto pensando argumentando la posibilidad de que bueno podamos conectarnos a alguna Api que nos permita un sistema de chatbox con audio puede ser WhatsApp Que obviamente es uno de los más conocidos puede ser cualquier otro bueno cerrar este Ciclo De esta manera y que la pregunta pueda entrar a un posible chatbox que use esta inteligencia de esta posibilidad de chatear con un chat con un chat que PT sí personalizado con una base de datos personalizada no solamente escribiendo una pregunta en modo texto y recibiéndola en mismo en la misma vía Sí en el mismo modo sino también mandársela de modo audio y recibirla de modo Y por qué no hasta que la persona pueda elegir De qué modo quiere recibir la respuesta más allá de que su input haya sido como unud bueno esto tiene muchísimas aristas tiene para explotarse mucho insisto es uno de los temas que más tiene factibilidad de desarrollo a futuro porque bueno los modelos de los llm van buscando justamente tener no solamente más poder inter interpretativo más poder de comprensión sino Además más poder de que pueda entrar información en mayor cantidad y no haya necesidad de lotear para que justamente esto sea más rápido y más efectivo Bueno hasta aquí entonces la clase de hoy nos vemos en la próxima clase Hemos llegado al final de esta clase nos vemos en la próxima  clase i Titulo: Clase 31 (parte 1) Curso de Inteligencia Artificial \\n URL https://youtu.be/dv7dK-Zm89I  \\n 1372 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número    31  Hola Bienvenidos a todos a la clase número 31 del curso de Inteligencia artificial de ifes Vamos a continuar como venimos haciendo las últimas clases en el mundo de nlp concretamente hoy vamos a ver el tema de cómo resumir textos y lo vamos a hacer tomando varias fuentes en principio resumiendo textos desde un propio texto puede ser hecho en PDF en Word en txt o también la opción como habíamos visto en la clase pasada de utilizar Wikipedia como fuente de ingreso de información pero también vamos a resumir desde un archivo de tipo de audio de tipo mp4 o bien desde un eh video alojado en YouTube así que bueno eh en las distintas partes de esta clase vamos a hacer estos objetivos siempre con este elemento transversal que es poder resumir texto y usar algunas cosas que usamos la clase pasada como poder ese resumen pasarlo a un archivo de audio bien vamos a empezar entonces con la primera parte de esta clase viendo cómo resumir textos desde un texto en PDF bien aquí estamos en el colab la pln 12 donde bueno el título resalta lo que dijimos recién lo primero que vamos a hacer además de conectarnos como siempre es instalar esta librería que venimos usando todos estos estas clases anteriores Así que empezamos por ello y como sabemos que una de las cosas que tenemos que hacernos es conectarnos a la Api de Open vamos a montar el Drive donde sabemos que tenemos allí el archivo pun m de environment donde está entre otras cosas la Api ahora que montamos el Drive bueno justamente lo que hacemos Es configurar el entorno para recoger entre otras cosas la apq Así que procedemos a eso y vamos ahora sí habiendo contextualizado todas estas cosas que necesitamos para operar empezar con el propósito o el primer propósito de esta primera parte de esta clase que es resumir un texto en este caso vamos a empezar con un texto corto nuestro ya conocido chat gpt txt que hemos usado en clases anteriores vamos a volver a usarlo con ese objetivo Entonces lo primero que hacemos Es justamente acceder a ese documento y cargarlo dentro de una variable que le ponemos como nombre texto como ya lo hemos hecho antes una vez que hacemos esto lo imprimimos el contenido ya lo conocemos lo hacemos a título de volver a bueno poner sobre relieve eh o recordar Mejor dicho De qué se trataba este contenido que era una explicación bastante breve de de qué se trata el ch gpt bien y ahora para hacer el resumen vamos a cargar dos librerías primero la de chat Open eii que es justamente lo que vamos a usar para que nos haga ese resumen y luego vamos a cargar de lun Chain esema estas Este variables que usamos para hacer el prom acuérdense que yo acá voy a usar openi Pero además tengo que mandarle un prom y el prom se configura usando estas vares que hemos puesto aquí que ya lo hemos usado la clase pasada y por eso voy a cargar estas librerías y a continuación voy a crear una variable mensajes donde voy a cargar justamente Dos De esas tres variables de esos tres parámetros Perdón que uno es el System m recordemos que el System mees era aquel que nos marcaba el contexto como cuando usamos chat gpt y le decimos que queremos que haga algo Como si fuese un profesional en tal cosa destinado a tales tipos profesionales supongamos que en este caso quiero resumir chat gpt como le estoy poniendo aquí usted es un especialista en resumir textos y el destinatario es un tecnólogo o bien el destinatario no es un tecnólogo entonces obviamente la salida va a ser diferente porque el prom es diferente en este caso no hago referencia así Quién es el destinatario de este documento sino hago referencia acerca de quién Debería ser la persona que lo debería hacer o sea Quién es la persona que es un experto en resumen y que le pido que me haga un resumen y una vez que hago eso que son todas las cuestiones de contexto le doy el objetivo en sí mismo que es que quiero que me haga un resumen del siguiente texto y recordemos que en este caso ese es el human mesage sí la parte del prom que se llama human magis y que le pongo una variable que es justamente el texto sobre el cual va a trabajar Obviamente el objetivo es este y puede aplicar a cualquier texto y justamente en la variable texto es lo que va a cambiar el elemento que va a tomar para resumir en este caso chat gpt txt y en otro caso será cualquier otro texto como lo vamos vamos a usar más adelante con un archivo de tipo PDF a continuación voy a crear una instancia llm que justamente es la estancia de chat Open e que es la red que voy a usar con este propósito de hacer un resumen voy a usar el modelo gpt 4 Recuerden que es más caro que el gpt 3.5 Turbo que venimos usando en clases anteriores con una temperatura de cero por qué Porque aquí no quiero creatividad un resumen no debe ser creativo salvo que yo quiera algo así en principio Debería ser algo por eso la temperatura cer0 que Recuerden que cuando está más cerca de dos chat gpt se pone más creativo y cuando está más cerca de cer0 es más preciso más exacto luego lo que voy a hacer va a ser verificar este texto que voy a usar sí del chat gpt txt cuántos token tiene y justamente me dice que tengo 76 76 tokens Sí luego de ello esto es a título de información nada más no no es una un paso que sí o sí tenemos que hacer pero es importante Por qué Porque Recuerden que dependiendo de la cantidad de tokens lo que me cobra chpt H Así que no es un dato menor Más allá de que no es exigible hacerlo y luego Bueno lo que tengo que hacer es el resumen Cómo hago el resumen bueno Llamando al modelo el lm que crea Aquí esta instancia de chat Open a Y qué le voy a pasar le voy a pasar los mensajes y los mensajes son los prom Sí qué son estos dos que tengo aquí arriba es decir Usted es un especialista resumir textos haga un resumen del siguiente texto Cuál es el texto bueno el que hablamos recién sí Entonces ejecutamos esto y luego vamos a ver justamente en la variable resumen el resultado de aplicar este prom en este modelo y este prom qué hace me hace un resumen en base qué en base a texto y cuál es texto el texto nuestro que venimos trabajando bien así que bueno Esto l un tiempito y luego vamos a poder ver a través de la propiedad content de resumen de est de resumen justamente el contenido resumido ya lo he hecho lo imprimimos y aquí tenemos chat gpt es un hito en la Inteligencia artificial cuyo desarrollo bueno etcétera etcétera etcétera aquí continúo un poquito más y ahí podemos ver completamente todo este resumen que tenemos más o menos unos 15 renglones de algo que originalmente ven era obviamente bastante más largo que eso no O sea que es es un buen resumen H bien pero ahora con la intención de entender la potencia del prom acá es muy importante que lo que tenemos que entender es que cuando trabajo con estas grandes redes eh lo que yo tengo que tratar de hacer es eh focalizar sobre el prom y todo lo que yo vaya configurando sobre el prom va a ser bueno algo que me va a permitir ajustar o hacer lo que se llama un tuneo fino de esa red en virtud de mis propósitos con lo cual voy a cambiar un poquito el prom y le voy a decir que me haga ese resumen pero en otro idioma fíjense que es Exactamente igual solamente que aquí al final Le agregué este texto y traduzca el resumen en idioma inglés sí es decir que lo que cambié fue el prom con lo cual reconfiguró esta var de mensajes y vuelvo a pedirle bueno vuelvo a hacerme un resumen sí en este caso cargar la var resumen Perdón sobre la llm con estos mensajes nuevos que en realidad lo único que varía es esto que le he agregado aquí que dice y traduzca el resumen en idioma inglés bien terminado esto veo que aquí está el resumen donde bueno lo veremos un poquito más tiene más o menos la misma extensión que la anterior Pero obviamente es esto mismo que me envío aquí en castellano en idioma inglés sí esto digamos para que entiendan que también estas cuestiones se pueden agregar y todo lo que ustedes quieran agregarle digamos en el prom insisto en lineamiento al objetivo de ustedes bueno va a ser una forma de ajustar el modelo lo más probable a lo que ustedes tengan como intención Bueno hasta aquí parece que este proceso es muy simple pero no siempre es así hemos tomado un texto muy corto y esa es la razón de esa simplicidad Cuál es la situación cualquier llm cualquiera de estos grandes modelos Por más que vayan abriendo su capacidad de recibir inputs de información no son infinitos o sea yo no puedo darle un texto de cualquier tamaño en la última bueno en los últimos cambios digamos ha pasado chat gpt a abrir irse a mayores tamaños a mayores volúmenes Pero insisto esto no quiere decir que siempre por más grande que sea esa amplitud Yo voy a poder poner cualquier texto con lo cual voy a tener que tener presente que es muy probable que lo tenga que ir haciendo en splits que es lo que hemos hecho antes en clases anteriores es decir voy a tener que subdividir el texto e ir entregando eso en etapas con el objetivo de lograr un resumen el resumen lo voy a lograr solamente que el método va a ser diferente por eso aquí lo que vamos a hacer es tomar un documento más grande que el anterior para poder llevar a cabo ese proceso y que ustedes sepan Cómo proceder en caso de que tengan un documento que exceda la capacidad de un input en un solo acto un input simple para tomar completamente todo el documento en este caso vamos a tomar un archivo de los que ya teníamos de recursos humanos que se llama archivo 7.pdf ustedes lo tienen dentro del campo virtual Así que lo primero que hacemos Es instalar esta librería para poder leer archivo de tipo PDF que ya lo Hi antes así que arrancamos con esto y luego vamos a hacer lo propio con algunas librerías de chat Open e con recursive ch splitter que ya lo usamos justamente para splitear el texto y con lumar change que justamente es lo que nos va a permitir hacer un resumen pero en formato de cadena o sea No todo en un solo acto sino en cada teniendo en cuenta que mi texto ahora está splite está subdividido Así que cargamos también esas librerías y paso seguido voy a en lector sí hacer un PDF reader se cargar El lector en base a este archivo como que estamos tomando como input archivo 7.pdf así que bueno creamos esta nueva instancia lector y lo que hacemos a continuación es poder recorrer se acuerdan cada una de estas partes lo que hacía El lector era dividirme en páginas este documento PDF lo que tengo que hacer es tomar cada una de las páginas tomar el texto y poner una variable la siguiente página la agrego a esa variable la siguiente página la agrego esa variable con lo cual tengo cada una de las páginas convertidas a textos y sumadas en una nueva variable de tipo texto que le hemos dado en Llamar texto justamente donde Ahora sí voy a tener en formato de texto todo el texto completo de este archivo 7.pdf Así que ejecutamos este y recorremos bastante rápido ya tengo en texto todo el documento y para poder verlo hago un print justamente de texto y bueno que aquí tengo todo el libro completo dentro de una variable bien ahí pueden ver de Que obviamente esto recorrerlo bastante largo Un poquito más arriba Vamos un poco más arriba aquí Bueno Este es todo el documento es un libro dentro de todo corto no obviamente he tratado de hacer eso para justamente que el gasto después que tengamos cuando usamos este chpt no sea tan largo no no se tan grande perdón pero dentro de los libros que teníamos de los 10 libros que tenemos de recursos humanos es uno de los más cortos HM bien entonces ya insisto tengo dentro de esta variable texto todo el contenido de este archivo 7 PDF HM bien con lo cual vamos entonces a continuación a crear una instancia como hicimos recién que también llamamos llm de chat Open pero cambiando en este caso el tipo de modelo no usamos gpt 4 sino gpt 3.5 Turbo justamente buscando que lo que va a hacer a continuación con la tokenización tenga un costo mucho menor ustedes saben que es bastante diferente eh casi un 10 veces menos el costo de diferencia entre gpt 4 y gpt 3.5 Sí con una temperatura de cero Porque queremos hacer un resumen y como dijimos recién la temperatura es lo que hace que el resumen sea lo más preciso y no sea creativo bien creamos esta instancia y luego al igual que hoy medimos o vemos la dimensión de la cantidad de tokens en este caso tenemos 19958 versus lo que habíamos hecho con el ejemplo anterior que teníamos 7677 Sí más del doble tengo en este caso bien Por qué este número es importante porque también me permite a mí medir la cantidad de caracteres que tengo 6941 tokens palabras Estos son caracteres no es lo mismo obviamente pero esos dos valores me van a llevar a mí a poder ver cuál es la cantidad promedio o estimada de por token es decir cuál va a ser el largo promedio de cada una de las palabras 3.46 caracteres por cada palabra obviamente habrá palabras que tengan más o menos esto es un cálculo Por qué Porque lo que vamos a hacer a continuación es ver cuál es el límite que tiene justamente los modelos de chat gpt para poder mandar los split porque por más que yo lo split el documento si esos split tienen un tamaño más más grande que lo que es capaz de recibir echas gpt seguimos en problema O sea mis eh Mis divisiones mis split tienen que ser menores a la capacidad de input que tiene mis modelo de gpt para que por más que yo lo vaya llevando uno a uno puedan ser recibidos entonces aquí tenemos 4096 es la cantidad máxima de tokens que toma Este modelo gpt 3.5 y 8192 la cantidad máxima de tokens que toma el modelo gpt 4 obviamente hay una ventaja para gpt 4 Pero hay un costo mayor y como dijimos recién 10 veces mayor H Así que eh Bueno aquí tenemos que ver relación costo beneficio de acuerdo al caso que tengamos cada uno nosotros como presente y Eh bueno en el caso de elegir un modelo u otro En consecuencia vamos a ver bien si entendemos qué es lo que tenemos que ver ahora y todo el cálculo matemático que tenemos que hacer para proceder yo aquí he deducido que tengo en promedio por cada token 3,4 tes 3,4 letras está bien bien Por qué es importante ese dato porque yo ahora digo Bueno si tomo el primer modelo que es el más económico que me es capaz de recibir hasta 4,096 tokens tendré que calcular Cuántos caracteres tendrían que tener mis chuns sí mis subdivisiones de textos para que correspondan a 4,096 por qué Porque los chunks se calculan por cantidad de caracteres no por cantidad de toques y yo acá tengo el dato de la cantidad de tokes Entonces yo tengo en cant una cantidad promedio sí 3,4 letras por tokens de mi texto Entonces lo multiplico por 4096 y lo que tengo es un número que me dice que cada chun Debería ser si la cantidad promedio de letras que tienen mis palabras es de 3,4 cada Chan Debería ser de 14000 189 caracteres se entiende o sea 4096 tokens por la cantidad promedio de letras que tiene cada tokens me da el tamaño del chun obviamente esto es un valor estimado y un valor de máxima porque sabemos que eso es un valor promedio no todos los tokens tienen la misma cantidad de carácter con lo cual corro el riesgo de excederme porque yo tomo como unidad de medida la cantidad de caracteres y no la cantidad de tokens Entonces vamos a tomar una medida un poquito hacia atrás Y en lugar de decir que mis chun sean de 14189 le voy a poner 12000 es un valor menor y recordemos también el chun overlap es decir que esto es imprescindible para que justamente Esta división de splits no pierda contexto de la continuidad del texto es decir que con esto Recuerden que de estos 12000 caracteres que tiene cada chank habrá 100 que se repiten del chun  anterior una vez que está claro esto lo que hago justamente es proceder a splitear mi texto y poner todos esos splits dentro de una Ray que voy a llamar fragmentos y Lu hago un print de l de fragmentos para ver En cuántos chuns me explicó mi texto completo de insisto mi variable texto que corresponde a el archivo 7 PDF no perdamos de vista ese tema así que bueno lo ejecutamos y veo que hizo un split de seis fragmentos O sea me subdividió ese documento en seis fragmentos y podemos ver uno de ellos por ejemplo el primero fragmento c. p content Y también vamos a ver la cantidad de tokens que le puso a ese primer fragmento Bueno aquí veo todo el fragmento la primera parte y veo que lo hizo en 36 53 to sea bastante menos que el máximo 4096 que aceptas g o sea está bastante ajustado s Pero bueno este eso no es algo problemático porque en realidad este esto cobra por tokes con lo cual que est splite en más o menos split no hace Elo simplemente al tiempo obviamente por supuesto Bueno pero es difícil establecerlo de una manera que sea Exacto así que bueno visto esto lo que podemos hacer es decir Bueno Este es el tamaño de el primer split en tokens y Cuál será el tamaño de los otros Bueno vamos a hacer un for aquí y podemos ver que ven tengo la cantidad de tokens que tiene cada uno de los seis spit Obviamente el último es el más chico Porque es lo que quedó como residente de texto pero digamos que está bastante cómodo quizás si quisiésemos hacerlo en menos Bueno nos pasaríamos de esos 4096 aquí lo que hacer como una cuenta es sumar todo esto y esa suma nos debería dar cuánto nos debería dar esto es decir la cantidad total de tokens que tiene el documento bueno y ahora tenemos que hacer el resumen Ni más ni menos con lo cual lo primero que hago es crear una instancia de lowat sumar Chain con esta variable change dónde le mando Obviamente el modelo el lm que es lo que habíamos distanciado de aquí arriba s nuestro modelo de Open e sí Este modelo de aquí y también le voy a pasar una serie de parámetros que son importantes si bueno el verb false ya saben que lo usamos o no dependiendo Si queremos ver un resultado digamos de los pasos que va haciendo por pantalla o no pero aquí es muy importante este parámetro que es el chin type este parámetro en donde yo le indico map redus le indica que lo que yo tengo que hacer aquí es un resumen de un texto que está splite con lo cual tiene que mapear cada uno de los splits de este documento para generar un único resumen que tenga contexto del total es decir no voy a hacer Si en este caso Tengo seis split seis resúmenes el resumen es uno solo lo que tengo que hacer es tratar de mapear todo Ese Conjunto que está subido para verlo justamente como un todo y hacer un resumen único para Ese Conjunto bien una vez que he creado esta instancia uso esa instancia con el método Run para mandarle todos los fragmentos todos los chunks y que me haga el resumen y ese resumen me lo ponga dentro de esta variable resumen bien una vez que ejecuto esto voy a imprimir el resultado de ese resumen y veo aquí este resultado que en este caso me hace un resumen en inglés bueno esto tiene que ver con el modelo antes usamos el gpt 4 ahora el GP 3.5 Perdón gpt 3.5 Turbo lo cual me ha dado resultado en este caso justamente con esta forma de manejarlo si sin el prom como lo hicimos en el caso anterior que lo hicimos de l change en inglés pero podemos usar justamente el mismo Open Ai para pasarlo a Castellano bueno creando un modelo que use text Vinci que lo hemos usado antes para tareas justamente que tengan que ver con tratamiento de texto en donde Obviamente con una temperatura de cero porque quio una traducción literal en donde lo que voy a hacer con este modelo que estoy creando aquí gpt3 pasarle como input tradúzcame al cast Llano el siguiente texto que está escrito en inglés y le paso como variable resumen que es Ni más ni menos que esta variable que vimos Aquí recién eh Como resumen del texto anterior pero expresado en inglés Entonces ejecutamos esto que está aquí y vamos a poner la respuesta dentro de una V respuesta y vamos a imprimir la Val de respuesta y voy a ver que justamente ahora me va a mostrar Eh mi texto que antes obtuve como resumen en inglés pero ahora en castellano bien con esto terminamos esta primera parte de la clase Así que nos vemos en la segunda parte de la clase hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte Titulo: Clase 31 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/cAjvXLESveo  \\n 1143 segundos de duracion \\n Hola esta es la segunda parte de la clase número 31 te invito a empezar con    ella  bien continuamos con la segunda parte de esta clase y como dijimos en la primera parte de esta clase Vamos a abordar ahora el tema de tomar como input un archivo de tipo mp4 o bien un video de YouTube eh Para tomar como ejemplo vamos a tomar eh una de las clases de este curso la clase 24 la parte un donde hablamos de visión computacional usando isoc seguramente lo deben recordar bien en donde bueno en el primer caso lo que hice fue tomar del archivo original del del video extraje solamente un archivo de tipo mp4 y en la segunda parte de esta segunda parte valga la redundancia vamos a hacer lo mismo pero tomando el video directamente desde el lugar donde está es de YouTube eh en general antes de continuar es importante que tengamos presente que todo esto que estamos haciendo de los resúmenes no solamente nos da la idea de poder tener el contenido de la Fuente original en una versión más resumida sino que todas aquellas personas que hacen tareas de investigación o que están permanentemente consultando información desde diversas fuentes eh teniendo un objetivo de búsqueda pero sin tener en claro si esa fuente me va a dar lo que yo quiero O no lo que yo busco o no esta tarea es muy importante porque ustedes piensen que puede ser que estemos leyendo un montón de documentos y no encontremos lo que buscamos mir Tenemos un montón de videos O podcast sí como en este caso puede ocupar el lugar Este ejemplo que vamos a hacer con un archivo mp4 Y ninguno de estos recursos ninguna de estas Fuentes tenga la información que nosotros queremos entonces el general resúmenes de todas estas cuestiones nos ayudan a entender rápidamente si allí puede haber algún tipo de información del tipo que buscamos después puede ser que en el detalle no esté exactamente lo que buscamos pero por lo menos líneas generales podemos sin mirar el documento sin escuchar el podcast o sin recorrer De punta a punta la visualización de todo un video entender Si Allí está la información que queremos o así que bueno explicado todo esto que no viene de más porque realmente es importante para valorizar esta este proceso de resumen vamos al colar Y empezamos con la última parte de esta clase bien estamos aquí entonces en la segunda parte de esta clase donde estoy instalando whisper x que justamente ya lo usamos en la clase anterior lo vamos a volver a usar ahora y es muy importante este que está aquí que nos recuerda que tenemos que configurar una gpu de tipo t4 en el contexto del colaps Sí Mientras lo vamos viendo vamos instalando whisper Ahí está arrancando y Recuerden que esto me pide siempre que lo termino de instalar que haga un restart de la sesión con lo cual eso es fundamental para poder utilizarlo Sí entonces hacemos clic aquí le dio Rest de la sesión y ya estoy en condiciones de utilizar ya reinicio allí whisper bien lo que vamos a hacer ahora es montar el colap y ahora importamos whisper x y configuro estos tres valores aquí que lo hicimos la clase pasada primero cuda que es el tipo de gpu que voy a utilizar bat size que lo vamos a usar dentro de un rato s y luego float 16 que es el compute type que voy a usar aquí debajo cuando defino los parámetros del modelo de whisper que voy a usar Qué modelo el modelo large b2 existe un large B3 que salió hace poco también bueno usamos este Obviamente que es más económico y y también más liviano no por supuesto con lo cual creo una instancia del modelo de whisper a partir de una var Que supongo model o le deben Llamar model y creo también una instancia de la carga del audio que voy a usar con whisper a partir de una variable que le llamo audio con lad audio y le pongo aquí justamente el archivo mp4 clase 24 parte 2 mp4 que es el que está tienen ustedes en el campo virtual justamente que es lo que yo les dije que extraje desde lo que es el video de YouTube saqué nada más que el audio y lo puse en este archivo mp4 bien en este caso tenemos Entonces el modelo cargado y el audio cargado Así que ejecutamos esto ahora que tengo eso lo que vamos a hacer es transcribir el contenido del mp4 en un texto para lo cual en principio hago la transcripción tomando el modelo que fue el que tenemos aquí sobre la base del audio que es lo que definimos aquí y con un batch size de tamaño igual al contenido de la variable bat size que lo habíamos fijado en 16 Recuerden que el bat Size fundamental porque Obviamente el audio va a ir entrando en lotes H este y va a ser procesado justamente a partir de esa mecánica Bueno luego de la transcripción también tengo que hacer la alineación que también lo vimos en la clase pasada justamente para que el texto me quede ordenado así que ejecutamos ambas cosas y vamos a ver el resultado que se va a alojar tanto para la transcripción como para la posterior alineación dentro de una variable que le hemos dado en Llamar results result va a tener todos los segmentos de bueno esta transcripción dado que insisto lo hemos hecho por lot no tengo todo en un solo archivo a bien terminado esto yo tengo todos los segmentos como dijimos recién de con lo cual lo que voy a hacer es recorrer todos y cada uno de esos segmentos para armar un gran texto Si bueno obviamente esto ya lo hemos hecho muchas veces con join for segment in result segment tomando una variable que he llamado seg en cada uno de los segmentos de la var result insisto es la que cogió aquí la transcripción Bueno voy a ir concatenando todos estos elementos para tener una única variable donde esté alojado el texto que se va a llamar transcription text y luego la imprimimos para ver Su contenido y lo vamos a ver justamente completo y alineado sí ven aquí está todo el contenido de la transcripción del audio de la clase 24 parte 2 en un un texto sí Bienvenidos a la segunda parte de esta clase Los invito a empezar con ella Hola a todos nuevamente estamos en la segunda parte de la clase número 24 para abordar el tema etcétera etcétera etcétera y Vamos hasta el final donde dice los espero en la próxima clase nos vemos aquí termin la clase los espero en la próxima clase nos vemos sí bien evidente ente ha hecho una transcripción completa de punta a punta de todo el mp4 el cual yo extraje del video de la clase 24 parte 2 bien Ahora que tenemos esto podemos ver cuánto se largo Sí esta transcripción 21017 y una cuestión adicional que podemos hacer porque en este caso yo lo tengo dentro del colap Qué pasa si me lo quiero llevar a un documento Bueno lo que hago justamente es with open ip.txt o sea abrir un archivo que le voy a poner ese nombre como le puedo poner cualquiera tomando como base el contenido de la variable transcription yion bajo text que es todo esto que vimos acá todo esto que vimos acá es transcription text entonces lo que hago es crear un archivo transcription txt en base al contenido de la transcription text y luego lo descargo vamos a hacer esto primero y hago un download de ese archivo en mi máquina lo voy a poner dentro de la zona de descargas Aquí bien si ahora me voy a la zona de descarga Aquí está el archivo Lo abro y veo Ni más ni menos que un archivo de tipo TT donde está Exactamente lo mismo que vimos recién en colap pero en un contexto de archivo pero ahí tenemos nada más que la transcripción ahora tenemos que ver resumen que es lo que es lo más importante de esta clase Así que lo que vamos a hacer es instalar Open y luego configurar su Api para poder usar justamente Open lo que vamos a hacer ahora es de Open Ey ch completions create vamos a usar open sin l change en este caso para hacer este resumen vamos a usar el modelo más caro ft4 y le vamos a poner en este caso rol esto es muy parecido a lo que haos en l Chain Sí el la configuración del prom del contexto y la configuración del prom del usuario donde se le hace el pedido concreto Así que en el primer caso del rol sistema el content es usted es un un experto Perdón en resumir textto o sea lo mismo que hicimos en la primera parte de esta clase y el contenido hágame un resumen del siguiente texto igual igual que el pron que armamos hoy text donde text justamente va a ser el text que proviene de transcription text sí bien ejecutamos y ahora lo que vamos a hacer es visualizar este resumen resumen obviamente es un diccionario dentro de lo cual yo voy a sacar the choices messis contan Sí y lo voy a volver a poner sobre la misma variable resumen para no eh crear una nueva variable Aunque puede hacer con una nueva no hay ningún problema Sí y este lo ejecutamos y vamos a visualizar el resumen de la transcripción del mp4 que era el contenido de la clase número 24 segunda parte sí Y aquí fíjense tenemos este resumen Este es un video tutorial sobre la segunda parte de la clase enfocada en interpretar texto desde una imagen utilizando una librería llamada Easy ocr sí e durante la clase Se preparó bueno el instructor explicó finalmente introdujo el próximo módulo del curso relacionado con el procesamiento del lenguaje natural cosa que nos tiene aquí ocupados sí bien lo mismo puedo hacer como hice con la transcripción con el resumen es decir puedo cargar esto dentro de un archivo tipo txt lo mismo puedo hacer como lo que hicimos este en el caso anterior pasar este contenido a un archivo de tipo txt y luego hacer un Download lo vuelvo a poner dentro de  descargas y ahora voy aquí lo encuentro abro resumen y voy a ver Exactamente lo mismo que vi recién Pero obviamente dentro del contexto un archivo y fuera del ámbito de cola como elemento adicional podemos hacer lo mismo que usamos en la clase pasada es decir este resumen no lo quiero en texto lo quiero en audio con lo cual lo que voy a hacer es volver a usar el modelo tts 1 de Open e con la voz Alo y la podría cambiar por otro si les digo ustedes que busquen otro tipo en la página de opena hay varios nombres de otras personas sí que pueden hacer esto pero siempre sabemos que tenemos este defecto de que escuchamos el la voz de la persona con un gran acento anglosajón por más Que obviamente lo expresa en castellano bien pero se nota que el acento es diferente así que bueno Esto es Ni más ni menos que lo que hicimos la clase pasada Así que lo ejecutamos perdón hay que poner resumen uno aquí que es lo que cambiamos y ahora lo descargamos se de carga nuevamente Este es un video tutorial sobre la segunda parte de una clase enfocada en interpretar texto desde una imagen utilizando una librería llamada easyocr durante la clase Se preparó un nuevo proyecto con pycharm se utilizó la librería opencv python headless y se abordaron distintos problemas que surgieron durante la instalación el instructor explicó Cómo crear y extraer texto de imágenes tanto en español como en inglés y explicó Cómo la librería puede clasificar y guardar la información en un archivo csw para su posterior procesamiento en la segunda parte de la clase se trató sobre cómo utilizar easyocr para extraer párrafos completos de texto de una imagen en lugar de simples palabras finalmente el instructor introdujo el próximo módulo del curso relacionado con el procesamiento del lenguaje natural nlp bueno Y para cerrar esta clase lo que hacemos es esto que dice aquí transcribir el contenido de un video de YouTube es dec ya no el archivo mp4 la transcripción del archivo mp4 del cual yo extrae desde el video original sino directamente linkando el video de YouTube para lo cual vamos a instalar l change pbe y YouTube transcript app una vez que hacemos esto vamos a importar el YouTube loader o el cargador de archivos de YouTube que es unao de las librerías de Lan change lo cargamos y luego vamos a crear un objeto loader que es una instancia de YouTube loader donde le voy a poner justamente from YouTube URL O sea desde qué eh archivo en este caso la URL donde está vinculada eh está vinculado ese video Perdón está alojado ese video en YouTube y le voy a indicar que el lenguaje en este caso tiene que ser español es decir que lo que hago entonces aquí es crear el cargador loader y con ese loader voy a hacer un load es decir cargar justamente ese video pero llevando a que ejecute una transcripción que la va a poner dentro de una variable que le he dado en poner transcripción una vez hecho esto bueno imprimimos el contenido de la var transcripción y v que ahí tenemos bueno toda la información relativa a la transcripción es exactamente la misma que vimos recién es decir hola bienvenido al curso de Inteligencia artificial de IFE Los invito a empezar la clase número 24 etcétera etcétera Pero esto es un diccionario no es un texto ser que tiene distintos componentes donde vemos al final si nos acercamos justamente a yello que tengo datos como por ejemplo el título sí que es el título con el capa aparece en YouTube la descripción bueno unknown porque no tiene ninguna descripción si no aparecería allí la cantidad de veces que se visualizó ese video bueno y otro tipo de datos importantes sí En qué época se publicó largo el autor etcétera etcétera Sí por eso hemos dado aquí en hacer este código donde vamos a imprimir cada uno de estos componentes por separado por un lado el autor los segundos de duración el título y luego page content que es es justamente la parte que tiene concretamente el contenido de la transcripción Así que ejecutamos esto y vemos que me dice video de ifes icono porque el autor es ifes iono eh tiene 2146 segundos de duración el título es clase 24 parte un curso de Inteligencia artificial etcétera etcétera etcétera y bueno Luego me muestra justamente toda la eh la traducción sí de e bueno el texto la transcripción Perdón del texto completo de eh la clase 241 en este caso no hemos hecho la 242 sí Recuerden que la 241 tenía que ver con tracking la 242 era de ioc r la 241 era de tracking no importa en realidad el el tema aquí no es de del video en sí mismo sino que aquí tengo la posibilidad de hacer una en este caso evit o no tuve que hacer eso de pasar el video un4 sí como hicimos al principio est clase con lo cual en este punto en el que estoy en este momento donde tengo el texto en transcripción subcero page content estoy en la misma situación de poder hacer un resumen como lo estaba aquí sí cuando tenía transcripción txt bien Entonces estaría en el punto o con la posibilidad de hacer un resumen del texto transcripto solamente que el texto transcripto anterior lo había hecho utilizando whisper y luego en este otro caso lo usé con el YouTube loger Sí bueno dos formas distintas hacer cosas similares con orígenes diferentes obviamente porque whisper toma desde un archivo de audio que insisto puede ser un podcast acá es como que estamos tomando la misma fuente de origen en formatos diferentes pero en el caso de que sea un mp4 sería un podcast y no provendría de un video de YouTube Pero bueno lo importante es que veamos la la cantidad de variantes que tenemos que para para hacer este tipo de trabajos y la cantidad de opciones que tenemos justamente para eh utilizarlo en el momento que nos parezca más apropiado o de acuerdo a nuestras propias circunstancias con esto terminamos la segunda parte de esta clase y con esto terminamos la clase como un todo nos vemos la próxima clase Hemos llegado al final de esta clase nos vemos en la próxima clase foreign Titulo: Clase 32 (parte 1) Curso Inteligencia Artificial \\n URL https://youtu.be/HntJvF737to  \\n 1731 segundos de duracion \\n Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número    32  Hola a todos Bienvenidos a la clase número 32 del curso de Inteligencia artificial de ifes esta es la última clase no solamente de este módulo de procesamiento de lenguaje natural sino del curso común completo obviamente luego de esta clase va a haber un componente en el cual vamos a explicar algunas consignas que tienen que ver con cómo va a ser el proceso de aprobación de este curso más seguramente algunos videos de tipo bonus Track que van a aportar de alguna manera una ampliación para aquellos que estén interesados en algunos temas en particular lo concreto es que el tema de hoy es Cómo usar las llms que es el tema que nos está ocupando en las últimas clases particularmente gpt 4 para hacer análisis de datos vamos a volver al principio de este curso Sí cuando vimos justamente el tema de análisis de datos pero ahora lo vamos a ver desde un lugar diferente Es decir en lugar de generar código de python para poder justamente sacarle provecho a a los datos y poder analizarlos debidamente vamos a usar inclusive los mismos dataset o básicamente uno de los dataset que usamos en aquella oportunidad pero ahora las consignas van a ser generadas desde un modelo de lenguaje es decir van a ser expresadas en lenguaje natural no como un query sino que el query lo va a armar el propio sistema de lenguaje natural así que bueno sin dilatar mucho más esta aplicación vamos concretamente al lab 13 en el cual vamos a ver Tres formas de abordar este tema que hoy nos ocupa en esta clase bien como dijimos en la introducción Entonces vamos a usar gpt 4 para análisis de datos y lo vamos a ver de Tres formas distintas dos formas en esta primer parte de la clase y la restante en la segunda parte de la clase la primer forma tiene que ver con el uso de l change y particularmente con agentes de tipo Panda Data frame sí concepto que ya hemos visto insisten primer parte de este curso y en todo el curso no un completo el uso de la librería pandas para el manejo de datos en este caso el dataframe de pandas le vamos a agregar la inteligencia de lch para que a través de agentes podamos sacarle información con la lógica de una llm es decir con el lenguaje natural bien para ello vamos a instalar las siguientes  librerías Y luego como sabemos que vamos a usar gpt 4 y siempre lo hemos hecho en estas últimas imas clases vamos a instalar vamos a montar Mejor dicho el Google Drive para poder conectarnos con el archivo punem que tiene la apq que me va a permitir justamente Conectarme a gpt bien y ahora una vez montado el Drive lo que voy a hacer es empezar por incorporar la librería pandas incorporar la librería os y conectarme justamente a chat gpt a través de la apq y justamente ahora que tengo montado el Drive a partir de este archivo m que como lo hemos usado en las últimas clases tiene como dijimos antes la información de la apq y me permite conectarme justamente en el torno de chat ejecutamos eso bien y ahora voy a cargar dos librerías de l change la chat Open ei que es la que vamos a usar justamente para generar Este modelo conversacional ahora con los datos no con textos y voy a cargar el dataframe agent de pandas sí es decir esta librería create Panda dataframe agent es el agente que me va a permitir interactuar gracias a pandas con un conjunto de datos obviamente es una librería de l change cargamos las dos librerías entonces y vamos a hacer como hicimos muchas veces de crear un dataframe a través de un read ccb de la librería pandas en este caso vamos a recurrir al dataframe Eh Al conjunto de datos Boston csb que hemos usado al principio de este curso que saben que tiene toda la información de casas de justamente la Ciudad de Boston Así que ejecutamos esto para crear ese dataframe y hacemos un Head para recordar un poquito de qué se trataba este conjunto de datos bueno tiene un una serie de información de todas las casas de Boston donde tenemos datos que tienen que ver con el índice de criminalidad de la zona el índice de industrialización de la zona bueno la antigüedad la cantidad de cuartos que se acuerdan que nos llevó a a los primeros algoritmos de regresión lineal donde veíamos justamente en virtud de la cantidad de cuartos poder establecer el precio precio que justamente está aquí al final que es un precio promedio Sí expresado en miles de dólares bien Espero que recuerden entonces este conjunto y lo que vamos a hacer ahora es crear nuestro modelo que le vamos a poner chat de tipo chat openi usando el modelo gpt 4 con temperatura cero Porque queremos una Data precisa en este caso los datos no pueden ser Eh no pueden tener una creatividad son o no son está bien Es decir es un valor concreto que estamos buscando y vamos a crear el agente gracias al cual nosotros vamos a gestionar esta conexión esta comunicación con los datos con lo cual lo hago con la librería que mencionábamos recién create pandas Data frame engine donde le voy a pasar Cuál es el modelo conversacional que voy a usar para charlar con los datos sí Y el dataframe sobre el cual justamente voy a gestionar esa conversación y con verbos acuérdense que le vemos para que justamente se genere un detalle de justamente las tareas que van a ser relativas a la creación de este agente el agente le voy a llamar agent agent perdón y bueno justamente ejecutamos estas dos líneas Y a partir de ahora ya tengo la gente al cual yo le voy a poder preguntar como si fuese un chat gpt común Sí pero algo relacionado a contenido de datos por ejemplo fijense que aquí en este caso lo voy a hacer del siguiente modo Cuáles son Perdón Cuáles con est escrito Cuáles son las 10 casas de mayor mdb es decir cuál son las 10 casas de mayor valor promedio sí fíjense que esto es una expresión regular que yo podría hacer en cualquier eh modelo conversacional solamente que en este caso hace alusión no a un texto sino a un conjunto de datos lo que me va a responder son datos vinculados a números y no datos vinculados a textos eh lo hago con el método ram de agent que es lo que creé recién aquí el agente agente basado en un modelo conversacional chat que es gp4 y sobre la base de un conjunto de datos DF que es justamente lo que vimos aquí el conjunto de datos de las casas de Boston así que bueno ejecuto  Esto bueno fíjense que me va mostrando y esto siempre va a pasar la lógica que va llevando adelante para gestionar esa pregunta en el contexto de los datos entonces aquí me hace todo un detalle y Finish change es donde me muestra la respuesta a mi pregunta me dice las 10 casas con el mayor mdb son aquellas cuyos índices o sea los números sí son 283 225 369 370 37 1 372 186 204 257 y 195 cuando se refiere a ese número se refiere a este número sí ven subimos un poquito para ver el Data frame original sí es el número con el cual estaban identificadas sí esas casas bien entonces lo que tenemos aquí es a través de una pregunta expresada en un modo conversacional pero haciendo referencia a datos Porque fíjense que yo hablo ya cuando digo mdb tengo que hacer referencia a cómo se llama s en este caso es la característica no puedo decirle precio promedio porque no lo va a entender se entiende Así que en este caso tengo que tener mucho cuidado En las etiquetas que uso como cabecera en mi dataframe porque en este caso si cada una de estas características quiero que tenga una expresión semántica diferente pues entonces las tendré que definir redefinir Perdón se acuerdan que esto ya lo habíamos hecho en su momento con un nombre que bueno exprese de mejor manera justamente el contenido del dato esto está expresado de un modo que sea muy corto muy práctico para este caso que es conversacional donde el lenguaje es importante seguramente palabras como RM No significan nada Entonces eso me puede llevar a que o yo pongo RM en la expresión semántica Sí en la expresión conversacional o bien lo que pongo acá es cantidad de cuartos sí como para que después el Modelo conversacional Si una persona lo expresa de un modo natural y no con una nomenclatura corta que hace alusión a la forma breve de llamar una característica bueno pueda adaptarse justamente a ese modo de expresar esto es muy importante porque si no obviamente está gestión conversacional va a fracasar Entonces vamos a buscar otra pregunta por ejemplo cuántas casas tienen zn igual a 0 Sí vamos arriba a recordar si zn es el índice de zona Entonces le pregunto cuántas casas tienen zn0 esta por ejemplo esta esta sí Bueno de nuevo con un modelo conversacional así que ejecutamos me va a mostrar ando toda la lógica que va haciendo para encontrar mi respuesta y fíjense que me dice 372 casas tienen zn = 0 en algunos casos puede ser que yo le pida un detalle en este caso no le estoy pidiendo detallle con lo cual me muestra el dato Yo podría decirle bueno dígame cuántas casas tienen zn = 0 y muéstreme cuáles son esas casas muéstreme el detalle de esa casa Bueno en ese caso obviamente me mostraría sí algo más parecido a esto que de alguna manera generó en el caso anterior si por más que es parte de la charla parte de la conclusión que va llevando adelante del proceso que va llevando adelante para darme la respuesta bueno Me podría mostrar el detalle si yo también lo quisiera sí o hasta le podría decir póngalo dentro de un Data frame que se llame bueno con el nombre que ustedes quieran es decir el modelo conversacional en este caso entiende de datos y va interpretando mi pregunta va hacia los datos justamente Con el agente que Acabo de crear y luego me devuelve una expresión de tipo texto o puede ser que me devuelva un conjunto de datos ya vamos a ver más adelante que hasta puedo hacerlo con gráficos bien Vamos a ver otra consulta Cuál es el mdb promedio o sea el precio promedio de todos los precios promedios de todas las casas de Boston Bueno me dice que aproximadamente es 22.53 esto expresado en miles de dólares qué más Cuál es la máxima cream es decir la casa que tiene o las casas que tienen el mayor índice de criminalidad bueno acá me dice que el mayor índice de criminalidad es 88.97 le pregunto Cuáles son los mdb de las 10 casas con mayor crim está bien Es decir cuáles son las 10 casas que tienen el mayor índice de criminalidad y Cuál es el valor de esas casas que se supone que deben ser valores bajos porque ha ser casa que tienen índice de criminalidad alto obviamente tienen deberían tener una cotización inferior vamos a verlo bien y aquí tenemos la respuesta mdb values for houses con el mayor disc criminalidad son 10,4 8,8 50 1550 bueno fíjense que son valores comparados con el promedio que vimos recién cuando le preguntamos Cuál era el valor promedio de las casas 2253 bueno Están por debajo del promedio bastante por debajo algunas son salen nada más que 000 sí bien algo más Cuáles son las casas con ndb entre 45 y 50 y age Entre 10 y 20 esto para que aplique un filtro tipo we un condicional Sí entonces acá le estoy preguntando si las mdb que están entre 45 y 50,000 sí y que tienen una antigüedad Entre 10 y 20 años bueno Esto para que vayamos variando y esto obviamente después ustedes pueden este probarlo con distintas Este lógicas pero aquí estamos probando justamente que me dé un número que me dé un conjunto de datos que me condicione eh poniendo en este caso dos características entre parámetros sí 45 50 10 y 20 bueno Y aquí me dice que las eh casas sí e la casa mejor dicho con índices 228 es la única H es que hay una sola casa que está dentro de las condiciones que yo le estoy poniendo fíjense aquí que me muestra en el action input justamente Cómo armaría el query para poder llegar a ese valor sí a ver si lo podemos ver bien aquí es muy claro esto fíjense que me habla de mdb mayor o igual a 45 and mdb menor o igual a 50 H mayor o igual a 10 and H menor igual a 20 tal cual como nosotros lo expresaríamos en un query solamente que en este caso el query lo arma este agente y me devuelve en este caso como respuesta que hay una sola casa cuyo índice cuyo número está tipificado dentro del dataframe como 228 que cumple con las condiciones de que su precio promedio esté entre 45 y 50,000 y su antigüedad entre 10 y 20 años obviamente es un precio muy alto y por eso hay una sola una sola casa se supone que si es tan alta si es tan alto el valor bueno eh su antigüedad Debería ser mucho menor bien hasta aquí llegamos con esta opción ahora vamos a ver el otro tipo de tipo de agente que existe en este caso para conversar con los datos que es justamente pandas ahí Es decir es una versión de la librería pandas que incluye una Inteligencia artificial así que vamos con ello y para empezar tenemos que instalar justamente pandas ahí así que arrancamos con esa tarea terminado de instalar esto vamos a ver dos formas dentro de esta forma de pandas í la primera es Smart Data frame sí es una librería de pandas ahí como aquí bien podemos ver con lo cual importamos esa librería e importamos Panda Ya lo tenemos importado antes pero bueno como siempre digo voy tratando de que las librerías ías en cada caso volver a repetirlas para que quede manifestado todo lo que tiene que ver con ese caso y no que esté atado a lo que venimos haciendo antes así que bueno Por más que sea redundante lo volvemos a cargar en este caso y volvemos a crear el mismo Data frame que antes con el mismo conjunto de datos de antes de Boston csb y está bueno para poder comparar justamente lo que hicimos antes con lo que vamos a hacer ahora volvemos a hacer un Head Bueno sí sería bastante innecesario porque el datf lo hemos recorrido bien y lo que vamos a hacer ahora entonces es empezar por invocar Qué modelo de lenguaje del llm vamos a usar para conversar con los datos vamos a importar en este caso FR pandas a. llm Open aí y vamos a crear nuestro modelo que como muchas veces llamamos llm lo vamos a hacer con Open a con el modelo gpt 4 y con una temperatura de 07 Vamos a ponerle temperatura cer porque en este caso aplica más a el caso y bueno generamos nuestro modelo y una vez que tenemos nuestro modelo vamos a crear un objeto que le llamamos sdf en base a la librería Smart dataframe donde algo parecido a lo que hicimos antes le voy a pasar los dos parámetros como siempre Cuál es el modelo conversacional y cuál es el conjunto de datos con el cual yo quiero conversar es decir el modelo conversacional es el lm que es el que Acabo de crear aquí arriba y el conjunto de datos es DF es DF que es Ni más ni menos que el Data frame con los datos de las casas de boster así que bueno creo ese objeto y luego lo vamos a usar de un modo más parecido como lo hacamos antes simplemente a título de referencia es decir este mismo objeto yo lo puedo usar poniendo una expresión lógica si o una expresión de tipo query como las que veníamos usando antes digo Bueno quiero que me muestres todos los este en este caso todas las casas s cuyo rad s cuya característica rad que es que está aquí rad sea igual a uno entonces lo ejecuto y me muestra el resultado Obviamente con todos los ítems cuyo rad rad característica Rd es igual a uno pero esto está expresado no mod no en un modo conversacional está expresado con la lógica de un query ahora entonces para comparar lo vamos a hacer de un modo conversacional es decir le voy a mostrar en este caso con chat sobre el mismo objeto cdf por ejemplo muéstrenme eh muéstreme Por ejemplo las cco Z n con menor creen de nuevo en este caso el modo conversacional tiene referencia de Cuál es el nombre de las características si si yo este módulo este esta forma conversacional le digo muéstreme las eh cinco casas cuya zona es igual eh Perdón cuya zona tiene el la menor tasa de criminalidad bueno en este caso va a ser complicado porque justamente la zona no tiene nombres zona podría soer que zn puede decir zona pero no tiene por qué hacerlo en principio y crim podría serer también que es el índice de criminalidad pero tampoco tiene la obligación de hacerlo así que esto sí si yo no lo Expreso de este modo puede que me dé un error podía que no encuentro referencia acerca de lo que usted me consulta por eso insisto Es que es muy importante para este caso que los títulos del Data frame sí sean lo más explícitos posi para que el modelo conversacional tenga el mejor nivel de adaptación con una persona que no necesita necesariamente conoce la estructura de un dataframe así que bueno ejecutamos  esto y acá meest entonces las cinco casas que tienen el menor índice de criminalidad con el zn con el índice de sol bien bien en este caso podemos ver al igual que antes que lo mostraba en la lógica que va operando Cuál es el código que generó para darme esa respuesta Entonces lo hacemos con Last co generated de el objeto sdf le hacemos un print de eso y fíjense que acá me dice justamente que lo que hace es Buscar sí los más chicos los valores más chicos eh de cream porque yo le pedí los que de menor cream y me dice que me va a mostrar zn y cream como características en ese reporte sí que el resultado lo H digamos lo produce en base a un dataframe Sí y el valor es dataframe el valor resultante de esa bueno obviamente esto es una lógica interna que solamente le va a interesar a alguien como nosotros que es un programador un desarrollador Pero por qué es importante esto y por qué es importante en el caso anterior de lch que vimos también Cuál es la lógica que aplica Porque si el resultado que me muestra no es el que yo quiero yo quiero ver cómo pensó la solución en este caso la inteligencia para ver bueno Cómo puedo mejorar mi prom para que me dé el resultado que yo quiero evidentemente yo voy a descubrir allí que no me entendió bien porque la lógica que aplicó no tiene nada que ver o no guarda directa relación con lo que yo le pedí desde mi idea desde Cómo expresé mi prom Está bien entonces esto es muy importante Al igual insisto como lch antes que me mostraba toda la lógica bien pero hay un complemento más que es muy importante que también en este caso pandas ahí incluye la posibilidad Sí este este caso pandas con este modelo de smart Data frame incluye la posibilidad de graficar los resultados Entonces en este caso yo puedo decirle dibujen un gráfico de barras con el mdb promedio por cada zn es decir por cada una de las zonas dígame cuál es el precio promedio bueno en este caso le digo el tipo de gráfico y ejecuto con sdf chat y me va a mostrar el gráfico resultante bien y aquí tenemos el gráfico resultante donde tenemos Entonces en un gráfico de barras cada una de las zonas sí 0 12,5 17,5 18 20 Sí cada una de las zn y aquí el precio promedio Sí para cada una de las zas fíjense que se acuerdan que todas las características que le dábamos justamente con la libera que utilizábamos sí para hacer gráficos al principio de este curso bueno todos los parámetros que tenemos que darle después también lo vimos justamente en la parte de Machine le todos los parámetros que tenemos que darle para generar este gráfico fíjense que aquí lo hago con una expresión semántica de lenguaje natural donde justamente se adapta a ello y Obra En consecuencia pero vamos por más vamos a cambiar el tipo de gráfico en este caso en lugar de pedir un gráfico de barras le voy a pedir un gráfico de torta Entonces le digo dibújame Perdón un gráfico de torta con la cantidad por cada rad es decir la cantidad de casas por cada rad el rad vamos un poquito acá arriba sí es este concepto de aquí se acuerdan que hoy Les pedí justamente que me diera solamente las casas con rad un entonces La idea es que me muestre por cada rad cuántas casas hay bueno y que me lo haga obviamente en un gráfico de torta Así que ejecutamos esto y aquí tenemos el resultado sí es decir por cada uno de los rat 1 8 2 3 24 me da un porcentaje igual a la cantidad de casas que hay obviamente esto de que sea un porcentaje no sea el número también lo obviamente lo puedo configurar aquí yo aquí le puedo decir Muéstrame los resultados de una manera eh numérica y no porcentual Eh bueno obviamente esto da como dijimos recién para poder interactuar poner muchas opciones Y probarlas sí En realidad Aquí tengo una solución muy interesante justamente para que esa persona que no es un codificador como nosotros y que quiere lograr objetivos de gestión de datos simplemente expresando los pedidos como naturalmente lo vamos a empezar nosotros la otra opción dentro de pandas ahí es la opción de smart dat Lake A ver cuál es esta cuestión Por qué habla de un dat Lake dat Lake por general se le llama a una estructura que tiene datos los cuales no están normalizados O no son parte de una base de datos en este caso vamos a hacerlo con un ejemplo que no es exactamente un datal un datal podría ser por ejemplo que yo tenga un conjunto de datos que proviene por ejemplo de Twitter y otro conjunto de datos que proviene de un web scrapping que dice por ejemplo de una web de un sitio como puede ser Mercado Libre por qué es un datalake porque es un lago de datos donde los datos no están formalmente estructurados si con una homogeneidad es decir tengo distintos orígenes y los datos Seguramente yo tengo que procesarlos sí y para acomodarlos luego de una forma que cuando yo haga un query se pueda sacar provecho de ellos si los datos Por más que sean datos que tienen que ver entre sí pero provienen de de lados diferentes no tienen eh Esa estructura Bueno yo no voy a poder tirar un query porque justamente la forma que están registrados en un lado y la forma que están registrados en el otro son incompatibles y justamente A eso se le llama datalake un lago de datos de eh Alo que tiene o necesita ser procesado acomodado estructurado para poder justamente después hacerle un query sacarle provecho a los mismos concretamente en este caso vamos a ver eh un Smart dat Lake o un objeto de tipo Smart dat Lake que es una librería que tiene pandas ahí así que empiezo por cargar esa librería y luego voy a tener en realidad más que un dat Lake dos dataframe el primer dataframe va a ser de empleados de información de empleados y el segundo información de sueldos de esos empleados aquí en realidad eh tengo dos estructuras o dos dataframe que no están conectados porque obviamente en el mundo del dataframe no existe una estructura Como si fuese un armado de un conjunto de datos que está normalizado y vinculado en una estructura de base de datos como puede ser sql bien en este caso tengo yo eh los empleados con tres items el el ID del empleado el nombre del empleado y el depto al cual pertenece en otro conjunto de datos yo tengo también el ID del empleado y el sueldo sí en este caso la lógica implica Es que tengo dos Data frames que no tienen ninguna relación pero yo puedo gestionar de alguna manera un Smart dat Lake para que entienda que existe una relación entre estos dos conjuntos de datos y más existe un em una característica que llama Exactamente igual la cual me da la Pauta que puede ser el elemento vinculante entre un dataframe y el otro me refiero al ID del empleado es decir esto que está aquí por eso a continuación justamente con el objeto la clase Smart dat Lake creo un objeto de ese tipo el que le voy a llamar Lake dónde paso como valor Qué cosa primero Cuáles son los dos dataframe acá no tengo un dataframe tengo dos a los cuales yo busco que es Smart encuentre una conexión entre ambas la conexión que va a encontrar obviamente es la relación a partir de mid como decíamos recién y luego le paso Cuál es el modelo conversacional que voy a aplicar igual que Sí para poder comunicarme con estos conjuntos de datos que en realidad son datos que están desvinculados pero justamente con Smart dat Lake voy a buscar que estén vinculados y conformen un solo conjunto de datos así que bueno creo el objeto Lake y una vez que lo creo Perdón no creé los dos conjuntos de datos sí me olvidé hacer esto Me olvidé de hacer esto y ahora sí creamos el Smart bien Ahora lo que voy a hacer empezar a conversar con lo cual le voy a preguntar cuál es el empleado con mejor sueldo está bien Es decir en este caso lo que le voy a pedir es que me de el nombre que está en este dataframe del empleado con mejor sueldo dato que está en este segundo dataframe bien tiene que encontrar una lógica vinculante entre ambos dataframe y que tome un dato de uno y otro dato de bien Así que lo ejecutamos y me dice que el empleado con el sueldo más alto es Olivia si voy hacia aquí veo que el sueldo más alto es 7000 qu corresponde al cuarto empleado y el cuarto empleado es ol O sea que entendió perfectamente primero el concepto de cuál es empleado con el mejor sueldo entendió perfectamente que tengo dos dataframe que tien que estar relacionados a través de un ítem una característica mid perdón de aquí arriba mid y entendió perfectamente que tiene sacar qué dato de este dataframe y qué dato de este otro dataframe de la misma manera que como hicimos recién puedo ver cuál es el código interno que generó para llegar a darme esa respuesta y me muestra que bueno toda la lógica el merge que hizo fíjense que entendió perfectamente que tengo dataframe y que pueden sí vincularse a través de ID y luego lo que tiene que hacer básicamente es encontrar el Max del sueldo Y obtener el nombre de ese Max de bien hasta aquí llegamos con esta primer parte de la clase vamos a ver la tercer forma de conversar con datos en la segunda parte de esta clase hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte Titulo: Clase 32 (parte 2) Curso de Inteligencia Artificial \\n URL https://youtu.be/VnpQ4YUjWA0  \\n 974 segundos de duracion \\n Hola esta es la segunda parte de la clase número 32 te invito a empezar con    ella bien Estamos aquí en la segunda parte de esta clase para abordar la tercer forma de conversar con los datos de las Tres formas que anunciamos al principio de esta clase vamos a volver a L change l change lo usamos en la primer parte de esta clase para usar el agente de tipo dataframe pandas dataframe en este caso vamos a ver un objeto es un agente de tipo sql database lo cual implica Bueno un modelo mucho más complejo o puede recibir un modelo mucho más complejo con una gran estructura de tabla y todas las relaciones entre ellas para justamente volver a gestionar una petición de análisis de datos con el modelo conversacional que usamos en la primera parte de esta clase Así que vamos nuevamente al cola para ver ese tercer modelo de gestión con los datos a través de gpt bien aquí estamos en el colap Entonces como dijimos Y como dice aquí el título para ver el agente de tipo sql database lo primero que vamos a instalar Ya lo tenemos instalado dijimos como siempre repetimos desde el principio pero lo vamos a volver a hacer al contexto de esta parte de este lap lun change Open tabulate que esto si no lo teníamos y DM Así que vamos a instalar estas cuatro aplicaciones estas cuatro librerías y luego vamos a instalar algunas librerías particulares de l change que tienen que ver con los agentes y el tipo particular de agente que vamos a usar en este caso que es sql database y finalmente lo que tiene que ver con el modelo que va a continuar siendo chat Open e el modelo que vamos a usar de tipo conversacional para conversar con estos datos de la base de datos así que bueno Esperemos que termine esta instalación y luego cargamos estas librerías de lun change bueno luego vamos a montar el drive y vamos a volver a conectarnos sí a través de la apq que la levanto de el archivo punem que justamente está en el Drive bueno como lo hicimos antes obviamente esto ya lo hemos hecho antes lo vamos a volver a repetir H que quede reflejo de que esto es necesario para este paso y lo que vamos a hacer es crear un modelo de tipo llm voy a volver a poner la temperatura en cero aquí y usando el modelo gpt 4 que venimos usando en el resto de este lap Así que creamos el modelo llm y ahora vamos a levantar una estructura de una base de datos Sí vamos a mirarla primero con un browser especial que viene para justamente mirar bases de datos en este caso una base de datos de tipo esite seite es una base de datos muy livianita que se usa mucho bueno para para educar digamos básicamente porque tiene que ver justamente con poder hacer pruebas básicas porque no es una base de datos para obviamente hacer una una aplicación eh corporativa o digamos de plegar un un un proyecto digamos este realmente de envergadura simplemente se usa mucho para este tipo de pruebas y en este caso justamente nos va a venir muy bien así que vamos a ver de qué se trata la estructura de esta base de datos antes de empezar a ver todo lo que tiene que ver con la creación justamente de los objetos que están relacionados con esa base de datos bien aquí estamos entonces en el D brows for sqlite donde lo que tengo es una base de datos que le he puesto Universidad con Cuatro Tablas muy básicas pero que nos van a permitir justamente sacarle provecho a este modelo conversacional para que entienda esa estructura y responda conociendo esa estructura algo no igual a lo que vimos en la primera parte de esta clase donde vimos Este modelo de pandas ahí donde habíamos visto este Data ley que buscaba eh establecer una relación entre dos dataframe en este caso la relación y todas las estructura de relaciones ya está plasmada dentro de la base de datos con lo cual tiene que traerla Sí y obrar en consecuencia esta eh base de datos que le pusimos de nombre universidad tiene una tabla de alumnos muy sencilla número nombre dni una tabla de cursos número nombre y el número del docente que da ese curso Por lo cual después tengo una tabla de docentes número nombre y el título universitario que tiene ese docente obviamente este ítem número está relacionado con este ítem nereo docente está bien Aquí hay una relación HM y luego tenemos cursos detalle donde tengo el número de curso y el número de alumnos sí es decir este curso es la nómina completa de cursos y luego curso detalle bueno es cada uno de los cursos que se va relanzando que alumnos integran cada uno de ellos Entonces ese curso detalle tiene un número de curso que es lo mismo que este ID número de cursos y un número de alumno que es lo mismo o sea está relacionado con este este ítem número de la tabla alumos bueno una estructura muy sencilla Pero obviamente el objeto de esta clase es que justamente bueno ver cómo le saca provecho como Lee esta estructura desde este agente de sql database con lo cual después ustedes le pueden poner otra estructura mucho más compleja y propia también de sql server y no de sql l Que obviamente es una una estructura una base de datos de juguete como se dice en estos casos mucho más liviana y práctica para este tipo de propósitos educativos Así que volvemos al colap bien de nuevo aquí en el colap lo que vamos a hacer es crear la base base de datos sql y el tool kit de qué se trata esto bueno en principio la base de datos se va a llamar como dijimos recién universidad. db ustedes la van a tener allí en el campus virtual O sea que la pueden levantar así y poner en el Drive como yo lo he hecho aquí y justamente lo que se hace Es con el método from Uri escribir s la ruta Donde está esa base de datos sqlite se le especifica el tipo de base de datos que se trata y con sql database se crea el objeto de B luego se crea el toolkit Por qué el toolkit porque lo que venimos haciendo habitualmente en los dos casos anteriores Hay que crear un objeto que tenga la información del modelo de lenguaje conversacional que voy a usar y con qué Estructura de datos voy a conversar bueno en este caso ese componente se llama toolkit por eso se crea justamente con la clase sql database toolkit y lo que le do de información es lo que dijimos recién si db = a db es decir la base de datos es este objeto de B que está vinculado a esta estructura de base de datos estructura con datos sí que tiene dentro Sí y el modelo llm va a ser este llm que tengo aquí sí es los mismos datos ahora en una clase que se llama toolkit propia de este tipo de conexiones que estamos haciendo Perdón ese error que ven Allí se produce porque no estoy utilizando una interfaz de tipo t4 O sea no estoy usando un gpu estoy usando una conexión normal de tipo cpu así que voy a cambiar la conexión sí por eh Perdón la conexión el tipo de gpu a utilizar el tipo entorno a utilizar por un este entorno de tipo tpu o t4 reinicio todo esto y voy a volver a este punto bien ahí lo reiniciamos volvemos al punto que estábamos y ahora sí ejecutamos esto nuevamente donde creamos como dijimos recién el objeto de b y luego el objeto toolkit ahora lo que voy a hacer es crear el agente el agente lo creo con create sql agent que es una una de las librerías Perdón que levantamos sobre una variable que le voy a llamar asent y le voy a poner a esa gente también la información como lo dic en el caso del toolkit del tipo de llm que voy a utilizar el toolkit que voy a utilizar el verbos que ya sabemos si queremos ver o no el resultado de todas las cosas que van procesando en este caso del agente Y luego el tipo de agente que voy a utilizar que es Open Sí así que creamos este objeto de tipo agente y ya tenemos todas las condiciones para empezar a conversar con los datos con el método Run de el objeto agent sí es el que tenamos aquí y le voy a empezar por preguntar que me muestre la estructura de la tabla alumno O sea no todavía algo que tenga que ver con los datos sino de manera estructural para ver cómo me reconoce justamente la estructura de lo que yo le di aquí como universidad db que es lo que vimos hoy en el browser Así que ejecuto esto y me muestra la estructura de la tabla alumnos Sí aquí está me muestra digamos ser el código con el cual yo podría crear esta tabla con este cada uno de los ems y con su primary key y aquí me muestra una pequeña reseña de los tres primeros ems de esa tabla que son alumno un alumno 2 y alumno 3 bien al unas descripciones más fíjense que es bastante completa la información que me muestra de esto que es algo parecido a lo que yo podría ver en el browser que vimos recién es decir esta parte de aquí donde veo Universidad de B dentro del browser tengo las tablas y puedo mirar No solamente la estructura de la tabla como la vimos recién que nos las mostró en el colap sino también hacer un brow de los datos Y aquí tengo bueno he puesto a cada alumno con un nombre genérico sí alumno uno 2s 3 lo mismo con los cursos curso 1 2 3 4 5 lo mismo con los docentes y el detalle si aquí tenemos solamente tres cursos activos cada uno de esos tres cursos tiene cinco alumnos en el caso del curso un y dos Tiene los mismos alumnos del uno al cco y en el caso del curso 3 tiene cinco alumnos también pero 6 7 8 9 10 bien volvemos al cola bueno habiendo visto que muestra el resultado de lo que le he pedido que en este caso es reconocer la estructura de una tabla vamos a preguntarle lo concreto de los datos como por ejemplo muéstreme el nombre de todos los alumnos del curso del docente docente uno fíjese que es bastante compleja la búsqueda porque aquí lo que tiene que darme es información de todos los alumnos de un curso lo cual está en la tabla cursos detalle luego hace referencia al docente uno el docente uno es un nombre que está en la tabla docentes y tiene que ir a buscarlo a la tabla cursos y luego ir a buscar el detalle a la tabla cursos detalle Sí vamos al browser así se entiende bien lo que estoy diciendo Ven aquí yo le Acá está toda la información de qué eh qué alumnos integran cada uno de los cursos yo le estoy preguntando aquí que me muestre los cursos del docente uno cuyo código es uno por eso tengo que ver cuáles son los cursos del docente un están en la tabla cursos veo que el curso uno y el curso dos los dos son del docente uno sí Y curiosamente esos dos cursos el uno y el dos tienen los mismos alumnos del uno al cinco Está bien entonces se entiende que lo que le estoy preguntando involucra a tres tablas o sea que es un query complejo Por decirlo de alguna manera volvemos al cola bueno y lo ejecutamos está mostrando toda la lógica como siempre que va aplicando en este caso fíjense que toma se da cuenta que tiene que tomar información de la tabla curso de la tabla cursos detalle la tabla docentes me muestra el el query que armaría para buscar la información que le estoy pidiendo y el resultado es este los estudiantes en el curso llam perdón el curso del docente docente uno Son alumno uno alumno dos alumno TR alumno cuatro y alumno c Sí este en este caso F que me da una explicación muy buena me dice nótese que esos estudiantes aparecen más de un curso si por eso Solamente me los muestra una sola vez porque son los mismos tanto para el curso uno para el curso en realidad es muy muy buena la la lógica que aplica y hasta la conclusión final que me da aquí bien también le puedo preguntar por ejemplo muéstreme el nombre del docente del curso uno es un cuer un poquito más sencillo que el de recién pero que involucra do en este caso a cursos y a docentes y vamos a ver la respuesta y me dice que el teacher el docente del curso uno es docente vamos con otra pregunta cuántos alumnos tiene el curso tres bueno ejecutamos y vamos viendo toda la lógica que va Armando esto es muy importante muy interesante porque hasta inclusive aún siendo yo programador me podría escribir el query yo no tenía la necesidad de poder este escribir digamos ese query si no valerme de esta herramienta Bueno me dice qu curso tres tiene cinco estudiantes bueno y así podemos seguir preguntándole todo lo que querramos entendiendo una estructura obviamente un poco más compleja Bueno finalmente le dejo otra base de datos que se llama chinook.db que aparece en un github donde expone justamente esto que estamos poniendo aquí en esta última parte de esta clase y tiene una estructura mucho más compleja hace relación a empleados a clientes sobre una estructura de Bueno un servicio que se da vía web en donde bueno es mucho más compleja la estructura tiene mucha más variabilidad no son tan lineales como alumno uno alumno dos o docente uno docente dos y Bueno le pueden sacar mucho más provecho para justamente seguir practicando Bono esto no quiere decir que tampoco eh que digamos que quede por fuera de esto la posibilidad de que como dije al principio ustedes puedan usar su propia base de datos de tipo sql O my sql o cualquiera de las estructuras conocidas y sólidas que se usan hoy en la industria para justamente probar esto realmente Es una herramienta que tiene en realidad mucho futuro no hace mucho ninguno de estos tres elementos que hemos expuesto a lo largo de esta clase eh hace mucho que está en el mercado con lo cual todavía tienen mucho por crecer y mucho por ver có como afinar estos modelos para que la expresión de la persona que no sabe nada de datos Pero quiere conocer un dato digamos no choque con la estructura en cuanto a que la forma de expresión pueda ser bueno ilegible a la hora de buscar esa información en el contexto del dato ya sea por el nombre de las características por el nombre de las tablas bueno por cada uno de los objetos que forman parte de una estructura de datos así que bueno los dejo para que puedan practicar con esto hasta aquí llegamos con esta clase y bueno con esto como dije al principio terminamos nuestro curso de Inteligencia artificial queda como dije al principio también una última clase o un último video por decirlo de alguna manera donde vamos a exponer Cuáles son todas las consignas a cumplir para eh aprobar este curso para hacer el trabajo final para aprobar este curso y también vamos a ir sumando seguramente a futuro algunos eh videos adicionales sí algunos bonus Track para que bueno puedan ampliar algunos temas específicos bueno Muchas gracias por todo Hemos llegado al final de esta clase nos vemos en la próxima clase Titulo: CLASE 01 PYTHON PARA PROGRAMADORES \\n URL https://youtu.be/ScYQUYOx3r8  \\n 5605 segundos de duracion \\n a y en la instalación de payton es muy sencilla vamos a ir a google vamos a escribir python y una vez que busquemos así vamos a ver que nos aparecen al principio las publicidades típicas que aparecen en estos casos y luego si nos vamos a encontrar con el sitio oficial de payton www python punto org y así vamos a hacer clic sobre el link para poder entrar a payton vamos a ver la pantalla de payton que aparece con todas sus instrucciones en inglés me ofrece cambiar las al español puedo hacerlo si quiero no no es una obligación por supuesto y vamos a ver todo el menú y todas las referencias en castellano vamos a ir al menú descarga y allí vemos que nos aparece la última versión de payton 3 10 1 es obviamente puede cambiar porque más adelante cuando él esté vídeo puede ser que haya presionamos hacemos clic en ese botón y vamos a ir a disparar directamente allí vértice inferior izquierdo fíjense la instalación la aceleramos a los fines nos alargó el vídeo cuando termina de baja automáticamente se dispara en la instalación hago o abro a payton 310 al pas y hacemos clic y arranca la instalación no tiene ningún secreto es bastante sencilla aceleramos también este proceso que va a tardar un poquito más en la realidad y así termina simplemente puedo hacer click en el botón si quiero desactivar o no el largo del límite del path y con esto termina la instalación de payton bien vamos a instalar ahora visual studio code es una experiencia similar y sencilla también como la que tuvimos con python vamos a hacer lo mismo vamos a ir nuevamente a google y vamos a escribir en este caso visual estudio bueno no aparece en publicidad es como en la opción que tuvimos con python gracias a dios bueno tenemos él en primera fila digamos el sitio oficial de miso el estilo code fíjese que aquí hay una referencia hacia el producto que es un producto desarrollado por microsoft que no por ello solamente puede utilizarse es en windows sino que también puede utilizarse en otros sistemas operativos como linux y como mac y así como también tiene varias plataformas de utilidad también puse con varios lenguajes de programación no será no solamente con python que para lo que vamos a hacer nosotros en principio sino también para otros lenguajes después vamos a ver cuando hacemos el sitio que hay alguna referencia explícita acciones bueno entramos al sitio y bueno como siempre aparece o como nos apareció también en el caso de payton las opciones de inglés o español para hacer algo diferente en este caso lo vamos a dejar en su lenguaje nativo de inglés y directamente me voy aquí a este botón que muy claramente me dice que si hago clic aquí se dispara el download en este caso obviamente entiende que debe ser por windows porque el sistema que yo estoy utilizando obliga aquí y empieza a bajar el ejecutable el instalador nos vamos a acelerar el vídeo en este caso porque evidentemente es bastante rápido el proceso y no sería para nada necesario ya termina en pocos segundos y al tener aquí ya terminada de bajar el excel hago clic aquí y arrancó la instalación - de a cuadrado obviamente que aceptó a los términos de licencia del software de microsoft y le doy siguiente siguiente me pregunta cómo quiero visualizar con mi nombre eliminó del icio y si quiero o no crear una carpeta no uso la opción no crear carpeta porque la voy a poner este la carpeta dentro de lo inicio y lo siguiente bueno obviamente aquí aparecen las opciones que me permiten crear un acceso directo en el escritorio no voy a usar esa opción yo ahora y luego queremos o si queremos si abre la acción desde el menú contextual del archivo del directorio del programa de windows yo le voy a poner que sí y vamos a darle siguiente instalar y para la instalación que también va a ser bastante rápida por lo tanto no es necesario que la seguiremos es importante que tengamos presente que una vez instalado este producto nos va a ofrecer la posibilidad de abrirlo inmediatamente o no pero vamos a no hacer uso de esa opción para poder hacer todo el proceso que haríamos habitualmente rutinariamente cuando queremos escribir código de pack es este esta opción que está aquí yo voy a sacar esa casilla con lo cual quiero terminar la instalación de visual studio con pero no quiero que el producto se abra en este momento entonces hacemos finalizar aquí en este botón y se termina rápidamente la instalación bien ahora sí vamos a ir a visual equivoco una vez que instalamos el producto y vamos a buscarlo de nuevo aquí en el menú seguir hasta la de corta aquí tenemos el selectivo code abrimos la carpeta accedemos al framework visual escribo como bien no vamos a volver a encontrar con la misma pantalla o similar a la que nos devolvió la finalización de la instalación y bueno para empezar a trabajar aquí tenemos que acceder a esta opción abrir carpeta pero para eso justamente vamos a crear primero la carpeta con lo cual vamos a minimizar esto y vamos a ir a buscar el explorador de windows y vamos a ir hasta la unidad de discos y una vez allí vamos a crear una nueva carpeta que le vamos a llamar simplemente python bien y ahora si sabemos que todo lo que vayamos a escribir lo vamos a hacer dentro de esa carpeta para esto dentro de la cual pueden haber obviamente otras subcarpetas cerramos esto vamos a nuevamente illusion studios code y aquí vamos a la opción abrir carpeta y obviamente vamos a tratar de llegar hasta la unidad se y la carpeta python que está que acabamos de crear la seleccionamos y vamos a hacer clic justamente en el botón seleccionar carpeta bien una vez que está así ya vemos que aquí abre el explorador que va a tener una estructura similar a la nuestra de windows solamente dentro del contexto del proyecto que estamos llevando adelante y aquí con este botón abrimos python o cerramos dependiendo de lo que queremos hacer obviamente aquí tenemos la carpeta python y no más que eso dentro de la carpeta para esto vamos a escribir código de programación para eso ahora estamos parados en la carpeta python y vamos a usar este símbolo que está aquí que sirve justamente para crear un primer archivo de payton entonces hacemos clic aquí y vamos a escribir el nombre de el archivo a poner por ejemplo primer archivo ya lo he terminado de escribir a un olvidado enter y ya aparece el logo de payton y como vimos son dos víboras pitón combinadas bien fíjense que ya me aparece aquí un número éste número siempre va aparecer como un número de línea si en la cual yo voy escribiendo yo puedo ir hacia atrás el tranquilamente que siempre el curso se va parando en la línea que yo elija justamente en el lugar que yo me muevo que puedo hacerlo con las flechas del teclado o puedo hacerlo directamente con el mouse bien aquí cómo vamos a hacer para ejecutar esto bueno fíjense que yo escribí esta instrucción y apareció aquí arriba y en el vértice izquierdo un número uno que quiere decir que en este momento yo tengo fíjense es muy claro el cartel que aparece cuando yo me posiciono sobre ese número uno que dice hay un archivo no guardado quiere decir que me advierte en principio que debería tener cuidado de antes de ejecutar un archivo ver y luego guardado como lo guardo contradice con lo cual yo ahora voy a afectar controles y le pido mucha atención que este valor número uno nuestra referencia número uno va a desaparecer automáticamente lo hacemos y eso fue lo que pasó desapareció el número uno con lo cual ya tengo el archivo escritura más bien ahora como lo ejecutó bueno lo voy a ejecutar haciendo clic en este botón que está aquí arriba que es justamente the run y va a aparecer aquí en la zona de abajo un espacio que se llama terminal la terminal es el espacio donde va a verse la ejecución del programa de payton y todo aquello que yo quiera que salga por pantalla se va a reflejar en esa sección denominada tram ya vamos por ello va a tardar un poquito más de costumbre porque obviamente es la primera vez que lo escribimos así que bueno tiene que arrancar python con algunas cuestiones y luego me va a mostrar el resultado que yo quiero bien así este resultado todo lo otro que está escrito antes de lo más importante que es esta frase es toda una introducción de algunas cuestiones que tienen que ver con el producto bueno acá estamos en el visual studio code viendo a este archivo que se llama primer archivo punto pait el cual representa un poco la idea de este típico primer programa que se usa habitualmente cuando uno acceda a cualquier curso de programación que es el hola mundo acá le vamos a poner un hola cómo están más que hola mundo y bueno este primer archivo punto país se encuentra dentro de la carpeta uno nosotros creamos una estructura que se llama la primera carpeta código python como ven allí y luego carpeta uno y dentro de carpeta uno el primer archivo punto país en realidad recordad el archivo como la carpeta con la estructura de carpeta puede ser la que ustedes quieran por supuesto yo he puesto esta como ejemplo pero ustedes pueden armar la o configurar las del modo que mejor les parezca nos acá lo más importante es el tema de código donde lo que ponemos obviamente es un print de un mensaje que queremos reproducir en pantalla la pantalla en este editor que llama visual studio code se denomina terminal donde se visualizan bueno todas aquellas cuestiones que yo quiero mostrarle al usuario por pantalla obviamente encerrado entre comillas y entre paréntesis pues lo que quiero reproducir y antes lo que hago es blanquear la pantalla porque realmente cuando arranca el programa siempre hay una serie de mensajes que no son importantes a través de la orden se ls que es una orden que se ejecuta a nivel sistema operativo por eso se hace a través del método system de la librería o ese y para poder acceder a ese método y esa librería primero que lo primero que tengo que hacer perdón es importarlo tal cual se ve en la línea número uno lo ejecutó con el botón de ejecutar y ahí sale como ven aquí el texto que dice hola cómo están bien obviamente qué y es algo muy sencillo se pueden agregar todas las tengo unos otros prince digamos impuestos para que puedan yo reproducir más de un texto no ya vamos viendo algunas cuestiones importantes aquí sigue en secreto de este print ya lo tenía escrito no se ejecutó porque porque lo encerré entre un grupo de tres comillas simples y cuando yo pongo todo un texto entre tres comillas simples la ejecución del programa en este caso aviso el truco lo ignora así que este eso es importante para que lo vayan teniendo presente después está el tema de los comentarios que de otra manera pero por lo pronto esto es importante cuando quieren anular ustedes ya saben que muchas veces queremos anular por algún propósito en particular una parte del código bueno sacamos esto sacamos esto que siempre tienen que funcionar de a pares no las tres comillas antes del texto que quiero anular y al final del texto tiene unas bueno ahora estas cuatro líneas se han incorporado como ven aquí como siempre estaba en el misión estudio con me marca que este programa no ha sido guardado con este cambio con controle se lo guardó lo vuelvo a ejecutar y obviamente van a salir cuatro líneas como se ve acá en lugar de la única que salía en el caso del programa anterior ahora vamos a ir al primero del programa yo obviamente los tengo ya todos escritos ustedes lo pueden ir incorporando uno por uno que es variables 1 punto pait bien arrancamos el programa como la amo recién limpiando la pantalla y acá tenemos un comentario bien una nueva cuestión que se relaciona con lo que hablábamos recién en este caso pongo el objetivo del programa no obviamente que es crear una variable y este en este caso de tipo texto crear dos variables concretamente y este vamos a ponerlo bien crear dos variables y bueno formar una impresión final con la concatenación de sus elementos y bien ya vimos que para crear una variable sencillamente no hay que declararle nada por el estilo acá lo que se hace es asignarle el valor cuando yo a una variable que le pongo como nombre como se me dé la gana en este caso justamente use la palabra nombre para nombrar la variable le asignó un valor del tipo texto automáticamente estoy diciendo que la variedad el nombre es de tipo si lo mismo con apellido bien la forma de crear y darle contenido a una variable es esta si si no supiéramos que ponerle se le puede poner mente comillas en blanco con lo cual en algún momento tendrá un valor pero por lo pronto no tiene ningún valor de texto normal sí pero me sirve para inicializar la y luego hago un print si esté con una concatenación la concatenación se hace con más con el símbolo más acá estoy concatenando todos elementos de tipo texto hola y cómo le va son de elementos que están encerrados entre comillas son de tipo texto y nombre apellido son variables de tipo texto si yo imprimo esto obviamente tengo la el resultado por pantalla y obviamente un decía un problemita porque me salió todo juntos y cosa que obviamente vamos a solucionar en otro programa que es variable dos puntos by haciendo qué bueno ustedes se imaginan esté mejorando la salida impresa como hemos puesto aquí lo que hacemos es dejar obviamente un espacio luego de lola un espacio antes de él como le va y un espacio entre la variable nombre y la variable e incorporado algunos cambios y un quinto elemento para esta cadena que antes era de cuatro elementos y ahora es de 5 ejecutó el programa y obviamente el resultado como se ve aquí es el que esperábamos de un punto de vista más este lógico en la forma de expresar con las reglas gramaticales como corresponde una frase si bien avanzamos con otras cuestiones de variables tenemos aquí otro programa donde ya cambia un poco la cuestión porque yo no voy a darle como en el programa anterior un vago en los programas anteriores en realidad un valor fijo a la variable nombre o un valor fijo a la variable pedido lo que hago es a partir de ahora pedirle al usuario que me dé ese valor con y obviamente después a una impresión no por pantalla esto no cambia es parecido a lo que venía haciendo en el programa anterior entonces éste como lo hago con un input o imputó lo que hace es poder pedir un valor por pantalla y asignarse lo a la variable a la cual yo lo estoy acá pasándole el contenido entonces si acá lo que ejecutó va a pedir un valor por pantalla y cuando el usuario lo escriba o yo lo escriba se lo va a poner a la variable nombre y en el caso de apellido es exactamente lo mismo entonces yo ejecuto esto y veo que me va a parecer en principio si ven que yo marque hice clic en la terminal si yo no hago clic en la terminal el curso sigue en el editor de texto con lo cual ojo con eso porque bueno estamos en una fase de prueba de diseño obviamente que esto cuando se ejecute en la interfaz como corresponde para el usuario nos va a tener esta esta suerte de dos opciones si va a tener una única opción que lo que salga por pantalla pero en este caso nuestro este que pasa el input me marca un cursor nada más nos avisó que saber que ahí hay un pedido de un nombre porque porque lo sé simplemente no hay ninguna referencia que me diga eso bueno no importa ya vamos a ver cómo mejoramos eso le pongo juan y ahí me aparece el segundo inputs y el que dice apellido igual el input bueno pérez le doy enter y bueno vuelve a poner el a juan pérez como elevar esta vez porque yo puse juan y puse pérez en esos dos símbolos obviamente que esto nos sirve como como algo estéticamente bien presentado fundamentalmente para darle al usuario una idea de qué es lo que yo quiero decir así que tenemos este otro programa donde al input como ven aquí yo le he agregado un texto de referencia ese texto de referencia lo que hace es bueno ubicar al usuario es decir bueno qué es lo que debería yo escribir acá un nombre y acá abajo un apellido también es decir corriendo nos un poco de la forma que teníamos antes que era medio de adivinar de qué era o cómo lo estamos probando nosotros sabemos que lo que buscamos sabemos que escribir así si esto se lo diésemos hacia un usuario no sabría obviamente que hacer bien entonces con este cambio como vemos aquí el input se pone el texto que quiero que aparezca al lado del input sí o del pedido de datos que general está encerrado entre comillas al igual que como lo hiciera con un primo y entre paréntesis por supuesto bien ejecuto esto y realmente me va a salir lo mismo a cambiar el nombre para no ser tan redonda redundantes si se dice jose garcia si bien y este el resultado obviamente es el mismo la diferencia hasta que en este caso obviamente que me pidió en el input introduzca su nombre y el otro input introduzca su precio cosa que en el programa anterior no había hecho fíjense que cada vez que yo ejecuto algo si la pantalla se borra justamente por ese punto existenciales y la terminal también se puede eliminar fíjense que yo se apoyo aquí el referente digamos del cursor del mouse del puntero del mouse está bueno el típico cesto de basura con lo cual puedo eliminar la terminal eso no hay ningún problema si la terminal se borra se ha habilitado otra cada vez que ejecute un programa sin perdón se usa la terminal que está abierta o bien usa otra terminal si está fue eliminada si eso lo pueden ver ustedes como para poder ver si está muy cargada de cosas o quieren abrir una nueva instancia se puede abrir más un listarse también no esté aquí con el más se pueden abrir más una terminal y que la ejecución de programa vaya a una terminal u otro esto no tiene que ver con python tiene que ver con el visual es truco bien seguimos con esto vamos a otro programa que hemos introducido acá bueno hemos introducido otra variable más en este caso dni así que en este caso en particular me va a permitir a mí hacer ya una en un tercer imputado haya nada nuevo acá en esto porque obviamente es exactamente lo mismo que hice con apellido y con nombre y hago un print con uno dos tres cuatro cinco seis siete elementos de cadena y no cinco como antes porque obviamente si ha agregado un nuevo valor tengo que agregar el valor a la cadena más el espacio en blanco para poder establecer una separación bien sabido ese cambio lo ejecutó y le pongo ven acá lo que pasó yo no marque si el cursor aquí y fíjese dónde me salió lo que estoy escribiendo así bueno siempre es importante para esto tener presente que tengo que estar parado en la terminal bueno laura perea y el dni 22 millones 564 mil 778 le doy enter hola laura perea obviamente el número como le va por ahí lo que estaría lo que sería piola acá poner entonces en todo caso en este lugar porque queda como ahora el euro pero hay un número no sabe qué es ese número le ponemos después de la presión con una coma dni números dos puntos y ahí por ahí va a estar un poquito mejor presentado no creo que les está diciendo que ustedes no manejen ahí está ejecutó eso laura dni 22 millones no me acuerdo que hemos escrito el inventamos otro dni está un poquito mejor presentado o la europea coma dni número tanto como le van bien eso en cuanto a bueno sí que sea acá que también para aprender alguna cuestión más que tiene que ver con con el servicio de estudio que yo aquí 2 estoy parado puedo escribir se ls como escribo aquí arriba con él o ese sistema sí y obviamente la pantalla de la terminal se va a currar bien vamos a variables 6 punto pait esté acá vamos a ver algunas otras cuestiones que tienen que ver con los tipos de variables en lugar de pedir nombre y apellido pidió dos precios con la idea de poder reproducir por pantalla bueno la suma de esos dos valores y que usted tiene que gastar es tan bueno obviamente tengo dos input hago una cuenta total igual a precio más precio creció 1 pero de más precio 2 y luego a un print mostrando el texto y el resultado de la web bien ejecutó y pongo el primer presión suma que son 100 el segundo 200 y lo que me muestra al final es una hermosa concatenación pero de cuentas nada si por qué porque el input lo que hace siempre por defecto es devolver un valor del tipo texto está bien no importa lo que yo quiera tomar de la pantalla siempre va a ser un valor del tipo texto con lo cual si lo que quiero es texto fantástico y si no lo que tengo que hacer es convertir ese valor resultante del input en lo que yo quiera que sea sí entonces en este caso obviamente el objetivo era hacer una cuenta no lo hizo porque el texto que precio 1 y precio 2 son dos variables de tipo texto con lo cual cuando yo creo este total con el más es más no representa una una cuenta sino una concatenación bis que sigue yo me paro acá y me dice el tipo de variable que cada unos preciados es del tipo string precio uno de tipos obviamente total base al tipo stream bien si yo en realidad insisto con el tema de la cuenta vamos a ver qué cambio debería ser acá estamos viendo y lo primero que tendría que hacer es usar la función int si int lo que hace es convertir un texto en entero en número integra sí entonces ahora sí precio uno ya no va a ser ven como marca aquí no va a ser de tipo stain si no va a ser de tipo int y por ende esto va a dejar de ser una concatenación y va a pasar a ser una operación matemática cuyo resultado va a ir a la variable total que también es de tipo y ahora cuál es el problema si yo intento hacer una concatenación me va a ir muy mal este por qué porque yo pongo el peso total es más total como dice en el programa vamos acá un poquito el programa anterior si escribiese esto mismo me va a dar un error porque porque el precio total es es un string es un número entonces eso no va a funcionar si como se trabaja con eso se trabaja con este complemento efe que lo que me dice es que es una expresión de tipo formato en el cual va a existir un valor de tipo numérico como se inserta en el texto ese número es el número ese valor numérico perdón no hay que hacer ningún más hay que ponerlo si incluirlo dentro de la expresión fíjense que está encerrado al igual que en lo que es el texto propiamente dicho entre dos comillas pero ese total si tiene que estar a su vez encerrado entre estas llaves de ese modo entiende que lo que quiero si yo esto vamos a suponer lo pusiese de este modo si escribiría literalmente el precio total es total si no es el val no la verdad si no el texto total entonces lo que yo tengo que hacer acá es encerrar ese valor como decía ahora siempre entre llaves y con el complemento efe si le voy a decir que esto no es una concatenación de un elemento de tipo texto y un leyendo tipo numérico y así ha funcionado grabó porque hice ese cambio acuérdense siempre de grabar las últimas modificaciones porque si no probablemente esté ejecutando una versión anterior del programa bien ahí está el precio si en el segundo 200 y ahora sí obviamente me hace la cuenta y me muestra el resultado matemático que es 300 bien y yendo a la última parte de lo que tiene que ver con esta parte d esta sección digamos de tratamiento de variables y tipos de datos y cálculos numéricos hicimos un programa acá que tiene digamos el complemento de todos y es decir pedimos dos valores si ahora acuérdense usar la función int para cambiar ese valor de tipo texto que arroja naturalmente el input a convertir a un valor numérico ponemos por pantalla un mensaje como para hacer algo un poquito más mejor presentado donde voy a decir bueno los valores que vamos a usar para operar porque la idea de este programa es hacer todas las operaciones matemáticas que vimos recién en la parte teórica son número 1 y número 2 fíjense que acá yo ya nos inserto en esta expresión del pri un valor de tipo texto con uno numérico sino con dos numéricos y bueno se insertan en el lugar que yo quiero que vayan en la descripción del texto y lo que tengo que tener el resguardo que ambos tienen que estar encerrados entre llaves siempre no olvidar de poner él señalador efe si el elemento de aquí donde le indica que este texto va a ser una combinación de elementos diferentes de tipo texto con otros datos de tipo variable digamos si éste esté numéricos en este caso bien luego lo que hago en cada caso es hacer las operaciones matemáticas crear una variable cálculo y el primer caso voy a ser la suma entonces pongo número uno número dos y luego un print donde digo la suma es cálculo sí es decir muestro el resultado de la suma luego hago lo propio con el resto de las operaciones con la resta con la multiplicación con la división etcétera etcétera fíjense que siempre estoy re utilizando la variable cálculo como receptora de la cuenta propiamente dicha o la operación matemática propiamente dicha que yo haga en cada caso bueno esto es importante porque justamente las variables pueden reutilizarse esto es una cuestión que payton obviamente lo permite bien y acá tenemos la división perdón este luego tenemos la potenciación el módulo y la división entera sin la división en teherán ya todos saben que es el módulo es el resto de la división así que bueno no queda más que operar vamos a poner por ejemplo obviamente evitar poner el segundo valor un cero porque si no le va a dar un error en todas aquellas operaciones que sean de tipo división bien bueno fíjense que nos quedó vemos el último la última parte del texto o sea las últimas cuatro líneas la división del módulo la potencia y la división la visión entera perdón el módulo de la potencia y la división porque porque la terminal tiene un tamaño determinado es decir todo lo que pasó antes no es que no está si yo le doy con el scroll está y lo veo más arriba entonces lo que puedo hacer acá es levantar la terminal y ahí voy a poder ver todo en una sola expresión si bien introduzca el precio pero esto es el del programa anterior porque aquí este evidentemente éste nos ha quedado entonces ponemos el primer valor 10 el segundo valor 2 y muestro el texto los valores para operar son 10 y 2 y ejecutó todas las operaciones la suma es esto el arte es esto la multiplicación excepto la división es esto la potencia es esto el módulo sexto y la división en teoría sexto hemos puesto una división dos valores que dar una división exacta así vamos a probar con otro tipo de valor para poder ejecutar algo mejor vamos a eliminar la terminal acá y lo vamos a ejecutar de nuevo así que un poquito más limpios diez y tres cosas que la división no sea exacta sí bueno y ahí sale todo y se puede ver bien es decir pero voy a levantar bien la terminal aquí tengo 10 3 son los valores los valores para operar son 10 y 3 y empiezo la suma y la resta multiplicación la división que no es exacta la potencia el módulo que es 1 por qué porque 10 / 3 da tres y que a un resto de uno y el resultado la división era justamente es 3 bien hasta que llegamos con esta parte de variables de operaciones si éste y de operaciones matemáticas y de tipo de datos bien ahora vamos a adentrarnos en el tema de funciones todos ya entienden que es el concepto de una función bueno cómo se define una función en python con la palabra de el nombre de la función y este par de paréntesis que por ahora están vacíos vamos a ver eso más adelante y con dos puntos cuando yo esté en tema muy importante en python y ya lo vamos a empezar a ver ahora y es parte de la estructura de payton en muchos lenguajes hay definidores de bloques si éste pueden ser un símbolo como una llave bueno después alguna cuestión de algún lenguaje que tenga un gen de fusión en if en force one algo que cierre de alguna manera la estructura en python eso no es así en python cuando se define una estructura se la termina en ese renglón con dos puntos como ven aquí de saludos dos puntos esos dos puntos indican que así empieza una estructura con lo cual todo lo que está debajo y yo pretendo forme parte de esa estructura tiene que estar con una intenta ción clara como la que ven aquí acá yo puse 4 prince que lo que veíamos en el peso del programa anterior de presentación y automáticamente visual studio code me marca con una línea cuáles son todas las instrucciones que están dentro de la función y dónde termina el bloque es donde termina la línea obviamente acá en la línea en la línea número 9 no existe esta línea delgada de aquí y por ende se entienda automáticamente que allí termina ese bloque cuando yo le escriba algo en esta intenta acción y no he intentado hacia adentro si va a entender voy a poner print a secas aquí ese print no le pertenece a la función saludos salud de alumnos si yo pongo esto con el dedos y la intenta ción perdón fíjense que la línea aquí gris del costado se extiende por un renglón más entonces ese print si pertenece en este caso a la función bien lo vamos a borrar dado que no nos interesa ni para una cosa ni para la otra bueno obviamente acá lo que hago es invocar a esta función tres veces si simplemente escribo el nombre de la función y los dos los dos paréntesis de esta manera hago que se ejecute esta función saludo alumnos en algunas cuestiones importantes también aquí hay una cuestión de forma de expresar el nombre de la función es absolutamente irrelevante muchas veces por una cuestión de convenio o de costumbre dentro de la comunidad de payton se usa si hay una frase combinada que la letra capital que es la primera letra de la segunda parte de la palabra en este caso claramente es alumnos empiece con una mayúscula así de esa manera se puede tener como un convenio que también estas cuestiones tienen que ver con lo que se acostumbre dentro del marco de una comunidad o bien de una empresa que tenga una determinada forma de trabajar sí que esto no es definitivo como siempre el nombre puede ser cualquiera si con guión bajo bueno lo que ustedes se leerán bien tenemos entonces la posibilidad aquí vamos a limpiar esta terminal que nos quedó del ejercicio pasado grabamos porque acuérdense miro aquí arriba y veo que el programa está pendiente de grabación le doy control s lo grabo lo ejecutó y bueno obviamente me va a salir ese código este 12 líneas porque son se va ejecutar tres veces como bien los también si bien este en el caso de de todo lo que tenga que ver con las funciones está la función la forma más sencilla de expresar a una función si yo voy a ver ahora otra que hace o tiene un propósito similar que sería este de poder hacer un cálculo numérico si es muy parecido delante no solamente que en lugar de imprimir un 4 línea de texto lo que hace es hacer un cálculo si crea tres variables numéricas nos da un anotador nota tres hace una operación matemática suma las tres y las divide por tres con el propósito de imprimir el trabajo el print como ya lo vimos en el apartado anterior cuando vayamos el tema de las variables se hace con efe y poniendo entre llaves la variable que en este caso de tipo numérico la forma de invocar a la función es como vimos aquí escribiendo su nombre bien no hay mucho más para agregar lo ejecutamos y ya vemos que el resultado obviamente es 7 porque las variables las las notas son 7 6 y 8 ahora vamos a la función la próxima forma de trabajar una función que es el tema de los argumentos si hasta ahora nosotros trabajamos con funciones que tenían esta forma sí donde simplemente lo que tiene que hacer lo tiene que hacer con todos los elementos que están dentro de la función ahora esa función no se va a ejecutar con los elementos que tenga la función sino con la lo que yo le voy a le voy a mandar cuando la invocó cómo se hace bueno poniéndole cada uno de los elementos en este caso no está uno anotados en otra tres sí y cada vez que llamamos la función como ven aquí le pasó los valores en el primer caso 97 87 67 y luego 97 y 10 o sea cada vez que llamamos la función le mando los valores esos valores se van a asociar a cada uno de estos elementos que son declarados aquí y la función lo que va a hacer va a ser reproducir me el resultado de todas esas notas y así que lo ejecutamos y vemos que el resultado es obviamente primero 8 después 7 pues 8.666 6 6 porque obviamente no es una división exacta bien esto es en cuanto la variedad de la función perdón que ya cambia respecto a lo que habíamos ante donde no le mandamos argumentos de entrada acá definimos una función que tiene argumentos de entrada y vamos a ver también ahora cómo sería una función que no solamente recibe argumentos de entrada sino que también da una salida o sea retorna un valor bien bastante sencillo lo que tengo que hacer es en lugar de imprimir me paro en el programa anterior se acuerdan que acá imprimíamos el resultado no lo voy a imprimir el resultado lo que voy a hacer lo voy a mandar si atravesó mejor que lo voy a devolver a ese valor a través de un retorno sí es decir que acá la función va a recibir tres valores 978 va a ser el cálculo matemático y me va a devolver el valor como me devuelve el valor yo ya no puedo llamar a la función como lo hacía antes simplemente escribiendo su nombre y en este caso los argumentos vamos a norma anterior recordamos no puedo llamar a esa función de este modo porque porque me devuelve un valor sin mover un valor tengo que poner un receptor que sea capaz va a ganar una redundancia de recibir ese valor entonces siempre tengo que asociarlo a una variable está bien si ejecute este programa obviamente no va a pasar nada porque lo que está haciendo son todas acciones este internas y que no hacen un eco en la pantalla así lo ejecutaran tal cual todas las veces que quiera y no va a pasar nada porque yo no he hecho ningún principio por ejemplo quisiera hacer ahora un print d pero me dio uno para de alguna manera visualiza algo de lo que ha hecho este programa bueno a lo ejecutó y ahora si veo que me devuelve el promedio del primer caso si hiciese lo propio con los otros dos casos si copiamos y pegamos bueno va a ser lo mismo solamente con este caso sí que sé que siempre me aparece aquí la ayuda no el vicio el estudio que me ayuda a que ellos descubra cuál es este cuál es el resultante digamos de las opciones que pueden salir cuando yo intenté escribir algo por aproximación voy a escribir una palabra parecida promedio y me parece justamente las opciones que puedo tener que pueden ser las de tres lado la 1 cuando creo 3 obviamente se afirma en la que he elegido bien logrado lo ejecutó y tengo las tres salidas parecidas a la que me dio en el caso de funciones 4 solamente si en este caso está en este programa anterior que se llama funciones 4 solamente que en este caso lo que yo hacía era cada vez que lo llamaba imprimía lo llame la primera vez imprimió la segunda y primero en esta serie primero la diferencia acá es que las tres veces lo que hizo fue ejecutar la cuenta y mandar una variable lo mismo acá lo mismo acá y después yo decidí imprimir el resultado de esas variables y bueno obviamente eso es son todas las opciones que podemos tener entonces de las tres formas de funciones no recibe argumentos ni devuelve valor es la que recibe vamos a repasar un poquito loca como esta que está aquí esta no recibe argumentos ni devuelve valores como el a4 que recibe argumentos pero no devuelve valores y en el caso de esta programa es que para mí está número 5 en el cual lo que fue es una función que recibe argumentos y devuelve valores bueno ahora vamos a ver las primeras estructuras de manejo de datos obviamente todas las estructuras que se manejan en memoria y que no escapan a todo lo que ustedes de alguna manera pueden haber manejado nuestro lenguaje de programación hablamos de diccionario hablamos de listas y hablamos de tu plan sí bien empecemos por las listas las listas en este caso son elementos que pueden ser de cualquier tipo las listas pueden alojar números como vemos aquí la lista uno pueden alojar textos como la lista 2 o puedes alojar elementos de distinto tipo así que sé que tenemos en la lista 3 un elemento de tipo texto un elemento de tipo integra y un elemento de tipo booleana si esto se puede hacer en las listas se pueden agregar elementos se pueden eliminar elementos bueno hacer todo el tipo de variantes que tengan que ver con su contenido si su contenido no es fijo y puede alterarse todas las veces que quieran de la manera que quieran justamente eso es lo que vamos a ver ahora aquí debajo donde en principio vamos a ver todas las cuestiones que tienen que ver con acceder a los datos de la lista bueno acá hemos puesto un input después de cada uno de estos renglones fundamentalmente para ir viendo paso a paso el input tiene solamente la función en este caso de detener la ejecución del programa ver el resultado y avanzar el próximo si no no no es algo que sea necesario para la ejecución este programa bien lo primero que tengo es la forma de imprimir toda la lista completa print listados la segunda es imprimir un elemento de la lista el 3 que no es el tercero sino el ordinal 3 que sería el cuarto porque la primera numeración empieza desde cero luego este un rango si éste de 0 a 2 y luego un rango que se toma desde el principio o desde el final si con justamente el complemento o sea sería parecido lo que pondría que cuando describiría un rango pero éste lo que pongo es que falta un elemento al final o falta un elemento al principio con lo cual le estoy diciendo es desde un elemento hasta el final o desde el principio hasta un elemento sí bueno lo ejecutamos para para poder visualizarlo y entenderlo mejor bien vamos con el resultado aquí y vemos bueno se paró en lo primero que es la impresión de toda la lista completa si bien no hay ninguna cosa extraordinaria con esto le doy enter ahora veo el tercer elemento el perdón el ordinal 3 que es el cuarto por eso aparece matías luego tengo un rango que es javier y laura bien acá es un tema importante porque ustedes van a pensar que esto está mal porque 0 2 puntos 2 para ustedes sería el 0 el 1 y el 2 bien no en python no es así cuando yo pongo un elemento final desde describiendo un rango el elemento final indica el elemento posterior al último no el último con lo cual si yo digo de 0 a 2 estoy implicando al 0 y el 1 y por eso tal cual sale javier y laura esto es importante que lo tengan presente porque muchos lenguajes programación esto no es así bien luego tenemos el 15 lista donde voy desde el 1 hasta los 2 puntos y 2 puntos y decir que irá hasta hasta el final con lo cual fíjense que tomando desde elemento 1 que es laura hasta el final que jose ignoró a javier que es el elemento 0 sí y bueno doy otro enter y voy ahora desde el elemento desde el principio por decirle alguna manera porque ante los dos puntos no tengo nada hasta el elemento 3 pero de nuevo aparece lo mismo que deseamos recién con cuando decíamos lista 0 a 2 lista 2 bayas de dos puntos a tres quiere decir que va desde el principio pero no hasta el número ordinal 3 sino va hasta el alimento anterior a 3 bien entonces por eso sale javier laura e irene bueno estos son algunos o algunas gestiones que tienen que ver con las opciones de cómo acceder a los datos a una lista vamos a ver ahora algunas otras cuestiones que tienen que ver con listas que es los métodos para manejar los datos si e ir alterando el contenido de una lista en principio como agregó un elemento con el método aprenda y poniendo entre paréntesis y entre comillas el nombre que quiero agregar bien vamos a hacer después de cada programa perdón después de cada método hacemos un print de un input para ver el resultado de por pantalla de esa acción bien después extend con extreme lo que hago extender la lista agregando dos elementos más en este caso si les pareció lo que pusimos recién solamente que acá fíjense la denominación es diferente porque cuando yo agrega un elemento juan se pone entre comillas como yo voy a extender una lista la lista se define con corchetes como ustedes ven aquí por lo tanto yo voy a extender la lista los elementos con los que yo vaya a intentar extender esa lista también tienen que estar encerrados entre corchetes bien luego la forma de insertar un elemento pero no como up en que up en siempre no me lo va a poner al final si éste acá yo con in ser puedo decir dónde lo quiero insertar entonces estoy siendo que ramiro lo quiero al principio de todo entonces le digo que lo quiere en el lugar 0 y que elemento quiero sí bien y después las formas de eliminar datos con pop eliminó un dato si del final con pop 3 eliminó un dato concreto si por su número ordinal o bien para eliminar un elemento concreto por su contenido uso remote bueno lo vamos repasando ahora así de alguna manera también repasamos este cómo visualizar bien entender bien cada una de estas cosas que hemos explicado ejecutó el programa y este lo primero que voy a ver es el agregado de juan si dijimos yang con apple pasa el elemento que agregó al final si ven aparece juan después de jose bien estoy entero aquí y vamos a extender ven con extend agregó pero ya no agregó un elemento agregó un espacio es como si le sumará una lista a una lista que existe también si hubiera una concatenación de listas siempre va al final ahora vamos con el tema de ramiro recuerda que dijimos que ramiro con la indicación de que del lugar con el 0 le digo que yo que vaya en primer lugar bueno vamos a decir si efectivamente es así ramiro va al lugar número 0 si le pusiera al 112 3 guardaría lugar que corresponde o que yo tengo ganas que vaya bien ahora vamos a los metros de eliminación con pop elimino de donde del final si te acuerdas que dijimos recién eliminó con poco y tantas veces como executive pop va a ir sacando el último elemento de la lista si quiero eliminar un elemento determinado que en este caso sería el elemento 3 es decir irene hago pop sub 3 y fíjense que irene automáticamente desaparece que era la que estaba en el número ordinal 3 y si quiero eliminar un elemento en concreto si éste lo hago con buen ritmo y la descripción del contenido de ese elemento en concreto lo que quiero eliminar el aura pruebo y bueno efectivamente la que ha quedado eliminada ha sido laura bien vamos a otras cuestiones finales que tienen que ver con las listas tenemos otros métodos que tienen que ver con el ordenamiento de la lista si tenemos rivers tenemos sort y sort con el complemento riverside con rivers lo que hace es dar vuelta a una lista pero dar una vuelta una lista con el contenido que tiene que no tiene que ver con una cuestión de orden si ustedes fíjense que la lista acá por ejemplo samuel está antes que eso 6 laura y alfabéticamente eso no sería lo lógico si entonces lo que tienen que entender aquí que rivers es lo que va a hacer es poner o hacer que se ejerza justamente la lista sin que eso implique que haya un ordenamiento ni alfabético ni numérico ni en ningún tipo bien luego tenemos short que son si hace un ordenamiento si y sort con rivers true quiere decir que va a ordenar del final hacia adelante de atrás hacia adelante si el típico ordenamiento zeta como se dice habitualmente bien y luego algunas cuestiones que tienen que ver con la forma de buscar datos si bien en principio en la lista 2 me encuentro con que tiene 2 lauras si ven que en la posición número 1 y en la posición final hay una laura entonces con count me devuelve un valor que es la cantidad de veces que encuentra ese valor en la lista si bien luego hacemos algo parecido poniéndole 4x con la intención de que obviamente me devuelve un 0 si porque obviamente no va a encontrar si no encuentra nada me devuelve el dato de que no encuentra nada si luego es algo parecido pero con index es decir index no me devuelve si cuantas veces está un determinado valor sino en qué posición está ese valor por eso fíjense que hago un ejemplo con juan si luego algún ejemplo con laura y ahora en la prueba vamos a ver que los valores que me devuelve y luego finalmente el tema de index si porque vamos a encontrar un defecto en el caso de index de laura' por eso tenemos un index laura y un index laura como 2,5 porque el aula personal no index está dos veces como dijimos al principio el primer index lo que me va a dar si este que está escrito aquí es como resultado y ya lo vamos a ver en la prueba la primera aparición del aura que se me va a hacer que es uno la posición ignorando que el aura también está en la posición 0 1 2 3 4 5 6 si éste debe de estar esta lista sí entonces luego fíjense que tengo un index laura como a 2.5 que quiere decir le digo empeza a buscar la función del aura pero desde el lugar número 2 y hasta el 5 obviamente en ese caso me va a encontrar la segunda laura y no la primera laura porque como le digo que arranque desde el 2 y laura hasta la posición 1 es como que los dos primeros elementos que son araceli lauras los va a dejar totalmente de lado y voy a empezar a buscar desde juan y cuando empieza a buscar de de juan obviamente me va a detectar la función de la segunda laura y no de la primera bien vamos a ejecutar este programa como venimos haciendo para terminar de entender bien toda esta cuestión que estamos describiendo aquí y lo primero que vemos obviamente es la lista con rivers fíjense que lo que hizo es dar vuelta literalmente la lista la lista original empieza con aracely terminada con laura y lo que hizo fue dar la vuelta si empieza con la obra y termina con aracely ningún tipo de ordenación y de tipo numérico ni de texto ni en nada ahora le doy enter y ahora si la lista a partir del método short si aparece ordenada si éste le doy enter y veo ahora que esté la lista aparece ordenada pero como dijimos antes con la forma zg es decir aquí arriba aparece con la forma normal de ordenamiento alfabético aceptado si fuera 20 numérico sería de 0 al número que sea y en la segunda oportunidad aparece la forma de orden se tapa bien luego tenemos las formas de buscar que habíamos dicho en qué lugar está laura en la posición número dos levantamos un correo vemos un poquito esto aquí si laura está en la posición número 2 efectivamente está la posición número 2 buscamos este a una persona que llame 4x que no existe bien el número que me dice perdón perdón perdón me confunde lo que dije recién voy a corregirme el valor anterior es count con lo cual me dijo no que el aura está en la posición 2 que hay 2 lauras que el aura el valor la aorta dos veces disculpas por el error entonces repasando el primer listados puntocom del aura lo que hace es decirme cuántas veces está laura en la lista y me dice dos porque está dos veces no porque está la segunda posición si bien ahora sí en el caso de buscar una persona que se llame cuatro veces x el valor que me devuelve 0 porque una persona que se llame así no existe en toda la lista está 0 es así bien ahora estamos en index de juan que me dice la posición en la cual está juan y me dice que está en la tercera posición si esto obviamente estamos tomando la lista está no si no la que está dada vuelta bien y finalmente la posición del aura si vemos aquí la posición del aura es uno porque está samuel primero la hora después sí pero si yo ahora le digo como dijimos hoy empieza a buscar desde la posición número 2 va a tomar está laura y está laura la va a ignorar y entonces le doy enter justamente me dice que justamente arrancó buscando de la posición 2 y justamente la posición 2 está laura bien esto es muy importante todas estas cuestiones y hay muchos más métodos ya vamos a ver algunos más de listas porque bueno igual que todo el manejo de archivo de textos es fundamental para todo lo que es ciencia de datos la gestión de ciencia de datos que se puede hacer con compactos si bien luego vamos y algunas cuestiones más que tienen que ver con las listas que son las últimas que es el tema de concatenación de vistas si aquí tengo una lista tengo otra y para concatenar una lista tengo que hacer como si fuese una informes tendríamos por decirlo de una manera no entonces la lista 3 es la concatenación de la lista 1 de la lista 2 y luego hacemos un print para que se vea el resultado con un input para que se detenga la ejecución del programa y podamos ver el resultado de todo esto que estamos viendo por pantalla luego creamos una lista 4 sí y volvemos a crear una lista 3 en base a la lista 1 y de la lista 4 porque hacemos esto porque acá en el caso de la lista 3 yo estoy uniendo 2 listas numéricas si en el caso de lista 3 yo estoy juntando a la lista 1 que es de tipo numérica con lista 4 que no es de tipo numérico con lo cual esto lo que estoy haciendo es afirmando aquella cuestión de que las listas pueden tener elementos de distinto tipo de datos si bueno imprimo la lista 3 y veo su resultado bien luego algunas cuestiones que tienen que ver con el máximo y el mismo con la función del método max si esté alojado en una variable puedo ver el valor máximo una lista a que veo el valor mínimo una lista y finalmente con le puedo contar la cantidad de elementos que tiene una lista sí es decir leen de la lista 2 me dará un valor que todo el mostrando aquí y le entra lista 3 minar otro valor que también lo estoy mostrando aquí pero vamos el programa y vamos paso a paso viena que ejecutamos entonces el programa el primer sprint de lista 3 que ha resultado de la suma de lista 1 + lista 2 bueno está a la vista que eso ha pasado si vamos ahora con el print de lista 3 lista 3 es resultado de en la concatenación de lista 1 + lista 4 donde estoy mezclando tipos distintos de elementos entonces fíjense que tenemos los tres elementos de lista 1 1 2 y 3 y los 5 elementos de la lista 4 bien vamos ahora con el tema de valor máximo vídeo bien entonces estamos entre aquí y tengo el elemento máximo de la lista 1 recordemos que tenemos 1 2 y 3 el máximo estrés vamos al elemento mínimo de la lista 1 de lista 2 perdón que es 45 la lista 2 acuerdo que tenía 45 99 18 el mínimo es 45 bien que tenemos ahora el largo de la lista dos son tres elementos vamos más arriba vemos la lista dos tienen tres elementos y finalmente el largo de la lista tres que son ocho elementos así que la tenemos acá al alcance de la vista si los tres porque se acuerda que hemos juntado una lista la uno con la otra lista de la cuatro entonces tenemos uno dos tres cuatro cinco seis siete ocho elementos seguimos ahora con el tema de tu plan y bueno en la tabla es mucho más sencillo porque justamente sus características son mucho más limitadas la tabla puede tener elementos de un tipo de otro pero son fijos y determinados es decir no se pueden agregar más elementos no se pueden quitar ni se pueden reordenar si entonces éste obviamente en este caso lo complicado es que la cantidad de opciones para las cuales se puede utilizar una luna tula obviamente es limitada pero bueno la idea es abordar también este tema para que ustedes lo entiendan y así tenemos en principio cómo se crea una dupla a diferencia del caso de la lista en lugar de escribirla de definirla perdón escribiendo sus elementos separados por comas pero encerrados entre corchetes en este caso es encierran entre paréntesis y entonces tenemos por ejemplo atup la una que está compuesta por tres elementos de tipo numérico y la to prados que está compuesta de cinco elementos de tipo textos y después tenemos una lista que gestiona por porque pusimos lista justamente acá porque podemos demostrar con lo que está escrito aquí en la línea 8 que yo puedo crear una dupla desde una lista la lista 3 por ejemplo está allí con esos tres elementos que son de tipo diferente como ya lo habíamos visto en el caso que habíamos está hablando recién de las listas a través del método tupper yo puedo haciendo está pluralista en este caso la lista 3 crear una tabla bien esas son las formas que hay para la creación de una tabla y luego bueno ver un poquito este tema de algunas cuestiones que tienen que ver con formas de acceder a los datos o algunas características de la tabla en principio podemos imprimir toda una tabla completa podemos imprimir un elemento una dupla bueno hay dos ejemplos de cómo imprimir un elemento de una tabla el elemento 3 de la tabla 2 y el elemento donde tú pre 3 luego cómo se determina o se puede consultar el largo de una dupla también tenemos dos ejemplos y finalmente lo que puedo ver es éste con count si como en el caso de la lista ver cuántas veces está un determinado elemento en este caso el elemento 45 sí y finalmente lo que es bastante curioso así que puede pasar digamos el tema de que una tu plan puede pasarles sus valores una misma cantidad que la cantidad de valores de variables fíjense que era tu plata 3 es la que armamos a partir de la lista 3 y tiene tres elementos un nombre una edad y una condición una canción juliana es un valor juliana en este caso estruch bien eso hace de que yo pueda poner una lista de tantas variables como elementos tienen a tu plan e igualarla tu plan con lo cual a partir de aquí tengo nombre edad casado igual a tu plata 3 que quiere decir que el valor de juan pérez varió la variable nombre el valor 45 variedad y el valor true va a ir a la variable casado en este caso debe representar como un estado del estado civil digamos si está casado no está casado bien y para mostrar un poquito eso hacemos un print utilizando la resultante de esos de esas variables que obtienen su valor a través de esta igualdad con la tu plan en el cual obviamente tenemos y también está bueno para ver cómo se combina en una expresión de un print un print de una cadena armada con elementos de tipo texto cerrado entre comillas una variable de tipo texto y una variable de tipo numérica en el caso de la variedad de tipo numérica obviamente s encierra como sea sabemos entre llaves y poniendo obviamente el complemento de efe que me permite hacer esa combinación sí y luego obviamente la concatenación con una variable de tipo texto como el nombre bueno se hace con el más y no mucho más que eso bien ejecutamos el programa y empezamos a ver todas estas sociales y en principio lo primero que tenemos sprint de la dupla 2 bueno me muestra la top lados tal cual con sus cinco elementos luego orientar este elemento 3 de la 2 javier laura y tienen y matías matías es el ordinal 3 que recordemos no es el tercer elemento luego el elemento 2 de la plata 3 que sería 0 juan pérez 145 23 por eso me muestra tu bien después el largo de la top la 13 elementos el largo de la tabla 25 elementos finalmente qué cantidad de veces está 45 el valor 45 dentro de la tabla 3 una sola vez y finalmente la expresión de tipo texto que es resultante de haberle pasado los elementos de la tabla a las variables nombre edad y casado bien ahora el tema diccionario sí que es la última estructura de datos de esta que vamos a ver en esta parte de la clase y justamente el diccionario es una estructura muy particular que tiene algunos puntos de común con las listas en términos de que sus elementos pueden ser de cualquier tipo pueden agregarse elementos pueden modificarse elementos pueden eliminarse elementos al igual que como las acciones que hacía con la lista sea a diferencia de la tabla que tiene valores fijos que no se pueden ni modificar ni agregar ni eliminar en este caso el diccionario guardó un patrón de comportamiento similar en ese sentido la lista una diferencia hasta que el diccionario no tiene elementos y no tiene parejas de alimentos si tiene una clave y un valor asociado a esa clave si entonces tenemos que todo el diccionario se crea encerrando una expresión entre llaves pero cada pareja constituida por dos elementos que están separados por dos puntos que a su vez se separan de sus otros elementos colegas a través de una coma aquí tenemos un ejemplo donde lo que tengo es un diccionario con todos clubes de fútbol donde la primera clave es como una berbia tura de ese club y la segunda es la denominación concretamente del club si tenemos cuatro clubes cuatro parejas de datos si bien y luego lo vamos a imprimir diccionario para que lo veamos por pantalla esto luego como accedo a un elemento del diccionario puedo hacerlo a través de su clave es decir a través d la expresión digamos del diccionario entre corchetes el nombre de la clave y ya accedo a su valor luego cómo agregar un elemento bueno como está escrito aquí lo que hago es escribir un diccionario 1 en este caso el nombre el dizionario diccionario 1 entre corchetes la clave del nuevo elemento e igualando la al texto del nuevo elemento bien luego como modificar un elemento y uso de alguna manera como excusa el elemento que recién acabo de agregar porque veo que real madrid lo he escrito con una minúscula y era mi idea si este que la primera letra de cada palabras ya es mayúscula entonces es una excusa como para decir bueno voy a cambiar ese elemento y fíjense que lo que escribo es exactamente lo mismo que lo que escribí en el punto 3 donde decía para agregar un elemento solamente que en este caso lo escribí como quiero que aparezca definitivamente pero en la expresión es exactamente la misma con lo cual acá hay una inteligencia que tiene el diccionario en qué sentido que si voy a agregar un elemento perdón voy a escribir algo con la intención de agregar un elemento el diccionario lo que va a hacer es ver si ese elemento está ya o no si no está lo agrega y si no está entiende que lo que quiero hacer es lo que quieras en el paso 4 que en realidad es modificar el elemento y lo va a modificar también o sea que ya tiene una inteligencia el diccionario que obra en consecuencia a bueno la realidad que el que tiene su diccionario si tiene el elemento uno lo tiene bien y finalmente cómo eliminar un elemento de la lista vamos a eliminar puntualmente el elemento de del club chelsea con el método del sí que éste lo que hace es si yo pongo el diccionario de nuevo entre corchetes la clave de un elemento que quiero eliminar bueno justamente con del lo elimina bien vamos a ejecutar esto para verificar todo esto que hemos dicho bueno aquí tengo en principio la impresión del diccionario 1 luego este la impresión de la clave sh que es obviamente el nombre chelsea luego agregó un nuevo elemento a al diccionario y lo imprimo y así aparece ven al último rm y real madrid que es lo que veo justamente es aquello que decía que quería modificar para que aparezca la rd real en mayúscula y la m de madrid mayúscula también con lo cual hago el cambio imprimo y veo que el cambio tuvo efecto y finalmente quiero con el método de borrar este club chelsea bueno lo ejecutó lo hago y al imprimir veo que el club chelsea ya no está más bien vamos a ver otras cuestiones ahora que tienen que ver con el diccionario y que son relativas a ver este en principio un diccionario que conceptualmente y técnicamente digamos es diferente al anterior porque en el caso del anterior tenía todos clubes de fútbol es decir tengo muchos elementos que tienen que ver con una misma cosa sí o sea mejor ellos son variantes de un mismo concepto acá tengo un diccionario que tiene atributos de algo también es decir antes tenía todos los clubes cada elemento era un club acá cada elemento es un atributo en este caso de lo que sería o sea una prenda de vestir sí bueno eso también para que entienda que conceptualmente el diccionario puede representar distintos tipos de cuestiones y además que vean que los elementos no tienen que ser que al igual que en el caso de la lista todo el mismo tipo si puede haber elementos de tipo texto de tipo numérico o de tipo juliano etcétera entonces aquí tenemos por ejemplo que el en cada bar o en cada pareja de datos que forman parte de este diccionario el primer elemento en la descripción del atributo y el segundo es el valor del atributo si tenemos el código de la prenda el color el talle y el precio bien luego vemos este el método copy el método copy sirve para copiar y crear un diccionario nuevo copiando lo desde otro si creamos el diccionario 3 que en principio toma todos los mismos valores que el diccionario 2 luego vamos a ver el método clear que éste en realidad lo que hace bueno que por ahí se pueden malinterpretar la palabra borrar un diccionario entendiendo que borrar quiere decir borrar el diccionario en realidad sería borrar el contenido del diccionario porque el método clear lo que hace es limpiar el diccionario no eliminarlo bien bien entonces con clear lo que hago es dejarlo en blanco si todos los elementos que le pasó elecciones de 2 con el método copy desaparece y el diccionario queda pero vacío y finalmente dos métodos y valores keys y valores que lo que hacen es éste imprimir todas las claves del diccionario o todos los valores literarios sí es decir en el principio le imprimiría lo que sería código color talles y precio tiene según 2001 verde l y 1200 bueno vamos a ver si esto es así lo ejecutamos y lo primero que encontramos aquí es el diccionario 2 ahora voy a ajustar el método copy para crear el diccionario 3 y luego imprimir leyendo 3 como veo imprimimos legionario 3 y me muestra exactamente lo mismo que me mostró en el diccionario 2 ahora vamos a limpiar el diccionario 3 y lo voy a imprimir y que me aparece un diccionario vacío sí porque lo que hice fue borrarle todos los elementos y ahora volvemos a seleccionar godos en el cual a través del método x aparecen todos los valores y todas las caras perdón y a través del método varios todos los valores fíjense como muestra esto el programa en cuestión todo lo que aparecen como claves y todo lo que aparece como valores aparecen encerrados entre corchetes con lo cual que quiere decir que la lista o el conjunto de valores de todas las claves de toda la x lo muestra en formato de lista de lista de payton y más allá de que constitucionalista del punto de vista simbólico si este y este conjunto de valores de un diccionario python también ve la muestra en formato de lista bien con esto terminamos todas aquellas cuestiones que tienen que ver con estas tres estructuras de datos pasamos ahora a ver las estructuras condicionales las estructuras condicionales concretamente en python s live que ustedes seguramente ya conocen acá tenemos un caso un ejercicio dentro del cual lo que yo hago es bueno e ingresar la nota un alumno y a través de una función que me devuelva la condición si está aprobado o reprobado completamente esta es la excusa para poder probar un if el eve se escribe con su else si éste su opción alternativa pero fijémonos bien que todas estas cuestiones que venimos diciendo que se manejan en python el if luego de la condición tiene que estar acompañado a los dos puntos todo lo que está dentro del ifd en la parte si digamos no no la parte alternativa debería estar intentado como está aquí por eso me dibuja esa línea y luego el else que sería la condición opuesta así pasa lo que la condición de arriba manifiesta que no está sea menor o igual que 5 menor que 5 perdón también tiene que estar ehlers con los dos puntos y lo que está dentro del él es también intentado todo lo que yo quiera poner dentro del if si estan tailers tiene que estar intentado a la misma medida y todo puedo poner todas las líneas que quieran y todo lo que quiera poner dentro del els lo mismo tiene que poner todas las puedo poner toda la seña que quiera pero tiene que estar intentado con la misma intenta ción luego como parte de lo que ya sabemos de la función retorno una condición que puede ser reprobado o aprobada bueno ejecutó el programa para probarlo y bueno obviamente voy a ver que ingresó a la nota del alumno supongamos que 5 me dice que está aprobado si éste si ingresó un 2 me dice que está reprobado muy sencillo el programa no tiene ningún secreto la idea era poder ver en funcionamiento el if cosa que vamos a ver ahora en este otro programa con un complemento más que es el él el if es decir que cuando yo necesito que tenga existe sujeta evaluación una cuestión por más de dos opciones posibles obviamente con él y con él es no me alcanza con lo cual aparece el el ifd el erick funciona como si fuese a veces un els y unif que le pongo después de leer si está sintetizada una sola palabra y por ende también tiene que estar acompañada la expresión a evaluar con dos puntos una vez que se terminó de comprobar esa condición bien acá tenemos un caso parecido al de recién solamente que en lugar de tener dos esté aprobado es aprobado tengo recursantes y saca en tres heridos reprobado saca entre tres y cuatro aprobados a que entre 5 y 6 y promocionados acá entre 7 y 10 por lo tanto tengo un if con cuatro condiciones por lo tanto voy a tener el if 2 el if y un else dado que la última alternativa evidentemente se va a deducir a partir de la no ocurrencia de las alternativas anteriores entretengo if not igual de 2 condición recursantes ellis nota menor igual que 4 condición reprobado el leaf nota menor o igual que ser condición aprobado else si no ninguna de las tres anteriores entonces quiere decir que promocionado bueno agua el return de la condición como ya lo teníamos y bueno acá tenemos un ejemplo de un el eve que es parte del if en las estructuras de payton lo probamos simplemente para ver lo vamos a descubrir nada nuevo bueno recursantes si lo ejecute lo pongo un 9 me dice promocionado bueno y así sucesivamente la nota que yo quiera poner me va a aparecer lo que tenga que aparecer según esta norma de este condiciones que he evaluado a través de la estructura condicional y bien la estructura condicional y también juega acabemos otro ejemplo con el oro y el and si acá vamos a suponer que tenemos un descuento que se le ofrecemos solamente a las personas que viven en córdoba rosario mendoza y si no vive ninguna de las tres ciudades que le ameritaría un descuento de 20% vivo a su edad y si esa edad es entre 18 y 26 años tiene un descuento del 20 por ciento y si no del 0% si bueno nada que ustedes no manejen vuelve a aparecer el if l live y él es sí y este sí es importante que tengan presente que se puede utilizar el orden dentro de la condición que acompaña live y el anc también dentro de la condición de compañía live recuerda algo muy importante que cuando ustedes preguntan o hacen algo expresa una condición por decirlo de modo más sintético a partir de algo que sea igual una variable igual a un valor se hace con 2 100 bolos iguales y no con 1 solo un solo símbolo se usa como tienen aquí a la hora de pasarle un dato a una variable sí pero cuando ustedes quieren preguntar o me valora una condición si algo es igual a otra cosa acuérdense que tienen que hacerlo con dos símbolos iguales y no con uno solo bueno el resto ustedes ven que la expresión de compaña live es más larga que antes pero es una expresión y por lo tanto tiene que terminar con dos puntos y la expresión que acompaña al el if lo mismo es un poco más larga no importa pero tiene que terminar con los dos puntos y cada uno de sus elementos intentado como corresponde bien qué más tenemos como opción condicional 5 otro programita donde lo que vemos aquí es bueno supongamos que acá en condicional 4 teníamos nada más que tres ciudades y hay que acondicionar el 5 en lo mismo pero tengo más ciudades sí entonces lo que hice fue crear una lista de ciudades con seis ciudades para no poner tanto y for your por lo que uso es un if y esto es importante y que quiere decir listas you es ésta ésta que está carrera y lo que hago es como pregunto si la ciudad que la persona escribió por pantalla si está dentro de esta lista como lo pregunto a eso bueno lo pregunto a través de baa de preguntar si es igual a tal cosa o si es igual al de los votos es igual a tal otra lo que hago simplemente es preguntar si ciudad en listas está dentro de la estación si la ciudad que escribe la persona por pantalla a través de input está dentro del estación entonces es otra forma de preguntar algo parecido a lo que veíamos recién en condicional 4 y aquí simplemente es otra expresión este de dif anidados así que ustedes seguramente ya conocen donde aquí el caso en concreto es ver si la persona estoy preguntando si la persona está en una determinada sede si en la centrada en la sede central o en otra hilada de cliente en base a eso voy a deducir el descuento tengo que quien está en la sede 1 tiene un descuento si tiene una edad mayor a 65 y si no nada si en el caso de que no esté en la sede central tiene un descuento si es mayor a 65 mayor igual 85 tiene otro descuento si es mayor o igual a 40 y si no no tiene nada si bueno esta es una idea de estructura de diferida 2 fíjense que se pone un if y dentro de seguir intentado otro día y segundo y obviamente con sus elementos también intentados dentro de swift y swells else el otro el otro dif anidado que también responde a la misma estructura solamente que como tiene tres condiciones a evaluar en lugar de tener un iphone else como el primer y fanny dado que está dentro de ese de uno tiene un irunés yúnes leaf no mucho más que eso y esté bueno no tiene sentido probarlo ya la idea es mostrar más que nada la estructura se los dejo a ustedes se prueba este código simplemente es ver que también tenemos o dentro de payton la estructura de y fanny dados y como se escribe el código cuando tenemos una situación que amerita una estructura condicional de ese tipo bueno como último elemento de la clase de hoy vamos a ver el tema de las estructuras de tipo de bucle si dentro de las cuales en python tenemos dos force y while si en el caso de ford obviamente sigue el mismo patrón de comportamiento que la estructura que vimos antes de if por ejemplo o la definición de las funciones donde no hay una estructura de cierre no hay un en de fort como igual tampoco hay un en while ni tampoco hay un elemento de apertura y cierre como puede presumirse unas llaves por ejemplo bien aquí siempre funciona todo con dos puntos y eso abre la estructura y toda la componente o lo que integra esa estructura tiene que ir intentado y aparece esa línea como vemos ahí que está abajo del foro de la fd for donde me marca qué es o cuáles son todas las líneas que están dentro de esa estructura bien acá tenemos en principio una lista que la estamos tomando como ejemplo a los propósitos del ejemplo de este programa y tenemos un primer foro que es una forma de recorrer una lista tomando cada uno de sus miembros a partir de una estructura sobre la cual no tengo idea cuántos elementos tienen simplemente los recorro todos con una variable que llama list y digo por liz in lista print list es decir por cada elemento de la lista cada elemento lo voy a nominar o lo voy a poner dentro de una variable que he dado en llamar list y lo que hago bueno si cada elemento de la lista lo pongo en list brindis va a ser print de cada elemento de la lista luego tengo otra forma de expresarlo a esto donde yo esa sí conozco la dimensión de la estructura en este caso de la lista y sé que tiene siete elementos por lo tanto yo voy a hacer un foro y un range en el rango range sería el rango de valores de los que yo quiero recorrer si yo pongo 0.7 quiero decir que arrancó desde el 0 y como ustedes ya saben llegó al 61 al 7 no al séptimo elemento o sea esto lo vemos en las otras estructuras anteriores lo que yo pongo como último elemento la referencia de manera ordinal el último elemento sino el próximo al último sí entonces y después obviamente hago un print de lista sub y también porque justamente y me va a ir él me va a ir dando el número ordinal de cada uno de los elementos de la lista bien dos formas de hacer exactamente lo mismo y por ende ojo tener exactamente el mismo resultado lo ejecutamos y acá lo vemos y primero el recorrido de la lista con los siete elementos y recorrerá lista otra vez con los siete elementos fíjense que fue con un título que yo puse aquí en estos print de aquí arriba recorrer la lista y luego recorrer la lista otra vez bien si yo ahora hago algún cambio en range ya no voy a tener ningún resultado vamos a suponer que yo le digo de que quiero que ahora arranque desde el 2 y tomé 3 elementos por decir algo bien perdón tres elementos pero hasta el número tres vamos a otro número 5 que sería yo pongo acá para ejercitar bien si desde el 2 quiere decir que arrancó desde juan y si quiero ir hasta el 5 que decir quisiera ir hasta el 4 entonces voy a imprimir el 2 el 3 y el 4 juan y viene y samuel y vamos a ver si es cierto esto acá está a levantar un poquito la terminal y vemos que tenemos en principio recorrer la lista los siete elementos y luego recorrer alito a través juan y de ni samuel porque porque como yo lo dije lo puse aquí arrancó desde el elemento número 20 0 1 y 2 o sea desde juan y le digo que quiero llegar hasta no hasta el 5 sino hasta el anterior del 5 el 5 es jose entonces juan y denis amo que es lo que me muestra aquí juan ir en izamal bien vamos a ver algo más de bucles el foro también me puede servir para recorrer toda la estructura de texto carácter a carácter como lo hago bueno email es una variable que la tomé en este caso de este ejemplo si acá lo que quiero hacer con este programa es poder ver si la estructura de un email si responde bronce lo que se ingresa por pantalla como un email responde la estructura de un email viendo si tiene arroba o si tiene por error supongamos más de una vez escrito la arroba si entonces hacemos un input para poner su contenido a la variable email vamos a hacer una variable de contador sí para ir contando la cantidad de veces que aparece el símbolo arroba dentro de la estructura de la variable email y acá tenemos for y un email qué quiere decir esto por cada elemento de la variable email obviamente que esto tiene que ser una cadena nos serviría para un número mucho menos para muriano sí entonces pero no es una estructura de datos sino acá no estoy recorriendo una lista no estoy recorriendo un diccionario no estoy recorriendo tu plan recorriendo un texto uno por uno esto es muy muy utilizado en lo que tiene que ver con ciencia de datos es muy común entonces esta estructura es importante que le prestemos atención fort y nivel quiere decir que por cada letra dentro del texto email voy a preguntar if i igual arroba si es igual arroba hago con está más 1 igual a 1 que acuérdense que esta es una metodología de abreviada de poner con está es igual a cuenta más 1 si sería lo mismo que yo escribiese esto que estoy escribiendo acá con está igual con está igual a cuanta más 1 si entonces una manera abreviada de escribir esto es esta que tenemos aquí con está más igual a 1 y luego preguntó si contra 0 que decir que la dirección no tiene arroba si es 1 qué dirección tiene el formato correcto y si tiene más de 1 que la dirección tiene más largo pero más allá del ejercicio que es la excusa simplemente para probar esto lo importante es entender que un foro puede también recorrer todas y cada una de las letras o carácter y vamos decirlo como corresponden o letras de una cadena de texto vamos a otra estructura forma otra textura for que tiene que ver con recorrer un rango de nuevo pero no hace falta a menos que sea necesario de entrada no hace falta que yo ponga por ejemplo si quiero recorrer un rango de 0 a 10 es decir esto que estoy escribiendo aquí si el primer valor es 0 se puede omitir yo popular 10 entonces acá por ejemplo lo que quiero hacer es poner una tabla de multiplicar las de modo de que yo pueda escribir un número y que me muestre bueno ese valor por 1 por 2 porque hasta llegar a 10 pero qué pasa en este caso en digo a veces se puede omitir a menos que sea necesario este es el caso porque porque si yo tomo esta estructura a los propósitos del programa si me daba un error porque el primer elemento no va a ser el uno va a ser el cero con lo cual me va a imprimir una tabla multiplicar no del 1 al 10 sino del 0 al 9 para que sea correcto entonces lo que tengo que hacer si es hacer un range con el par de números el que me dice de dónde y el que me dice como ya sabemos el valor posterior al último bueno probamos los dos para ver que esto efectivamente es así vamos a suponer la tabla del 2 y bueno así me imprime todo donde veo que en el primer caso ben fue del 0 al 9 porque el primero es 0 2 por 0 0 y el otro 2 por 9 18 y después al haberlo hecho arrancar como tenemos aquí desde el 1 a los propósitos del programa si bueno la tabla esté multiplicar es correcta va del 2 al 10 pero acá lo que queremos mostrar es que salvo que sea una cuestión necesaria es la lógica del programa si yo quiero arrancar de cero el range puede recorrerse sin necesidad de ponerle que implícitamente vas desde cero en adelante sino con la omisión ya asume que el primer elemento va a ser cero bien esto es todo lo que tiene que ver con for ahora la otra estructura que es el while bien acá con el wild lo que queremos hacer es lo mismo que hicimos en el ejercicio recién sabiendo de que hay cuestiones que son diferentes a la eufor porque para que el while termine la con termine de ciclar hay que poner una condición y menor o igual a menor a 11 perdón y luego para que el why vaya haciendo que ese valor que tomó como referencia vaya cambiando de valor tengo que hacer una cuenta dentro del white cosa que el foro lo hace de manera implícita entonces como hago una estructura acá les puso un comentario una para que no se ejecuta por supuesto como una estructura white que represente la misma idea que este foro bueno de esta manera arrancando una variable en un valor inicial un 1 para propósito este caso mediante puede arrancar en el valor que ustedes quieran la condición la acción que quiero hacer y el elemento que va a hacer que vaya avanzando el ciclo uno por uno y no me cree un while infinito sí porque si no pongo una cuenta de este tipo para cambiar o ir evaluando la condición si la condición no cambia el 'wild será infinito lo ejecutó para probarlo y veo que el resultado es ni más ni menos que la tabla de multiplicar que vimos recién en el caso del foro si le pongo el 2 tabla multiplicar de ese valor que quiero ver y el resultado es el mismo que vimos y en el caso de bucles 3 bien y finalmente creamos un programa que llama bucle 5 en donde doy otro uso del while si este este es un programa que lo que hace es ingresado los números a través de un menú es decir que quiero hacer con esos números cuál de las cuatro operaciones matemáticas esas que están ahí quiero hacer con ese número bien aquí lo más importante en el receso que como siempre digo es una excusa el 'wild puede utilizarse también o se utiliza muchas veces en cuestiones de validación cuando quiero que algo esté fundamentalmente que tenga que ver con el ingreso de datos esté dando ciclos hasta que una condición esperable se cumpla y yo acá he creado un menú con cuatro opciones sí pero pero no tengo ninguna garantía de que la persona no escriba un cero o un valor mayor a cuatro si con lo cual tengo que hacer una validación porque yo escribo un cero o malo mayor a cuatro esta estructura dice que tengo aquí abajo que responde a los valores 1 2 3 u otros va a hacer que siempre va a ejecutar la división porque es la que queda como con wells digamos que para hacer eso pusiera 5 s bueno es igual a 1 no es igual a 2 no es igual a 3 no entonces a una división cuando la división en realidad responde a la opción 4 no la opción 5 si digo 5 como decir cualquier valor que sea superadora 4 entonces hago un while justamente para poder validar y decir la única manera de que este while deje de circular y se rompa para proceder a hacer la operación concretamente que yo quiera elegir es que yo ponga un valor que sea de 1 a 4 mientras pongo un valor menor a 1 mayor a 4 esto base decirle bueno lo ejecutamos y vemos que lo que sucede es eso voy a poner los números 10 y 2 y vamos a poner la opción 6 vuelvo la opción 7 vuelvo la opción 8 vuelvo a la opción 4 ejecuta la división terminada la cuestión si acá lo que en realidad para emprolijar un poquito de esto como la validación está dentro del while y yo debería repetir esta orden en realidad aquí abajo pidiéndole que por favor vuelva a poner un valor sin que aparezca como veíamos recién que aparecía uno abajo del otro el pedido de rectificación entonces pongo dos números el 10 y el 2 y en pieza y digo bueno quiero un 6 no quiero un 9 en que ya no pone un abajo del otro porque dentro del wild pues el cls bien y hasta que ponga supongamos está en este caso la opción 2 que es la resta y me muestra resultado bien hasta acá llegamos con todos los temas relativos a esta clase y este bueno a partir de la próxima clase vamos a ir con los elementos que son un poquito más avanzados que estos que son las cadenas los elementos de cadena y luego de los elementos de cadena vamos a hacer una primera práctica aunando todas estas cuestiones y vamos a avanzar a otros elementos que son un poquito más avanzado que es el manejo de datos ya en estructuras que no se pierden que no son volátiles como las que vimos como en el caso de listas diccionarios como los archivos de texto muy importante como dije hoy en el tema del manejo de la gestión de datos para ciencia de datos y luego este el tema de las estructuras de base de datos bueno quedamos así en la próxima clase entonces seguimos viendo todos estos temas Titulo: CLASE 02 PROGRAMACION PYTHON \\n URL https://youtu.be/pXvL-uOaRH8  \\n 4000 segundos de duracion \\n  bueno hola a todos que estamos en la segunda clase de este curso de payton para programadores así que vamos a repasar un poquitito a empezar con la clase qué temas vamos a ver en esta clase en principio el tema de expresiones de manipulación de cadena tema muy importante básico para todos aquellos que quieran darle una orientación a su iniciativa de estudiar python hacia lo que es la gestión de datos o la ciencia de datos vamos a ver algunos métodos propios de payton más algunas cuestiones que se manejan a través de algunos algunos métodos que vienen del manejo de algunas librerías con todo ello vamos a poder buscar datos y poder bueno verlos analizarlos y yo cambiarlos si esto es muy importante insisto porque en la ciencia de datos es vital que los datos sean limpios como se dice en la jerga porque si no obviamente no no van a tener este éxito cualquier análisis que podemos hacer si el dato no es bueno si después tenemos el tema también del manejo de archivo de texto que justamente tiene que ver con esto que estamos hablando muchas de las cuestiones que vamos a manejar ciencia de datos son archivos de textos que vienen perdón en formato de archivo de textos etc muchas veces éste formateados separados por espacios por coma por punto y coma o muchas veces no formateado sí por eso justamente importante el tema de la manipulación de cadenas de estas expresiones de archivo de texto para poder acomodar los bienes y poder después procesarlo mejor bien vamos a ver entonces todas las cuestiones que tienen que ver con archivo de texto de que tengan que ver básicamente con crear un archivo de texto con poder este incorporarle texto con poder leer ese texto con poder de borrar con poder este bueno este agregar algún elemento o algún texto más a lo que ya está creado bueno las acciones que ustedes se imaginan son básicas para este caso para el tema de archivo de texto en el caso de payton insisto es muy importante y finalmente el tema de manejo de base de datos aquí cuando hablamos de manejo de base datos parece que va a apuntar algo que es mucho más grande de lo que la palabra puede significar en realidad está aquí lo que vamos a ver básicamente son cuestiones básicas de manejo de base de datos cómo es crear una tabla como es insertar le de actos en esa tabla y acceder a los datos de esa tabla luego éste eliminar datos esa tabla o actualizarlos y o cambiar los datos de esa tabla si obviamente que no vamos a ver a profundizar en todo lo que tenga que ver con el diseño de base de datos sino con cuando hablamos de manejo hablamos de desde python con qué instrucciones hago estas operaciones básicas que acabo de mencionar recién bueno y con esto estaríamos terminando en la clase de hoy así que bueno le vamos a dar paso al primero de los temas aquí mencionados bueno como primer tema de la clase número 2 vamos a ver el tema de los métodos de manipulación de cadenas que ha hecho un resumen de las principales de los principales método de manipulación de cadenas obviamente que como siempre digo si entramos al sitio de payton vamos a ver muchas más que éstas rescate las más importantes las que considero son más importantes pero obviamente yendo al sitio de payton tienen todo el resto de las otras y obviamente como siempre con muchos ejemplos y la explicación este en ambas lenguas en la mayoría de los casos para que puedan verlo bien más allá que siempre es importante que puedan hacerlo en inglés bien aquí tenemos en principio upper que tiene la utilidad de convertir texto a mayúsculas lower que hace lo contrario capitales que lo que hace es convertir la letra capital digamos un texto que no tiene su primera letra mayúscula la pone mayúscula y el resto lo conserva minúscula count que sirve para ver la cantidad de veces que en el marco de una cadena aparece una palabra o un pedazo de palabra que en la búsqueda de cadenas recordemos que no se trata de palabras sino de cualquier expresión del tipo de texto que puede constituir una palabra o no con fin también podemos retomar la posición si en la cual se encuentra ese valor si llegó esa expresión dentro de una cadena si obviamente si no está devuelve un -1 después tenemos algunas algunos métodos de validación y salud que me permite ver si una cadena tiene solo valores de tipos letras o números de alfa si sólo tiene valores alfabéticos is this it si tiene solo valores numéricos después tenemos este la función split que sirve para dividir una cadena en partes y split es lo opuesto a y acá nos vamos al final de esta lista que está a la derecha de join que yo en hacer justamente lo contrario une este espacio de cadena conformando una única cadena después tenemos algunos métodos que tienen que ver con los espacios en blanco con streep se quitan los espacios en blanco a la derecha de la izquierda de una expresión de cadena con el strip quita los de left y por eso en la l si los espacios están a la izquierda y con el reloj the right ons en los espacios que está a la derecha de una cadena y finalmente tenemos el método de replace que lo que hace justamente además complementando el find que vimos hace un rato bueno replace lo que hace es no solamente buscar sino reemplazarlo por un determinado valor vamos a ver ahora una una expresión digamos un trabajo práctico a partir del cual podemos hacer como tenemos aquí bueno un simple ejemplo donde podemos involucrar a todas estas expresiones entre estos métodos que hemos visto entonces este con este sencillo programa lo que hacemos es tomar un texto de entrada que lo pedimos con un input perdón este y a partir de allí bien con la expresión que corresponda a cada caso ponemos un print y al lado de ello la el método aplicado a la variable cadena en este caso que es la que recoge el valor que escribe la persona a través del link bueno y así poder ver por pantalla el resultado de eso vamos a verlo ahora en visual studio code bueno acá tenemos el código de programación de prueba de estas dos métodos de búsqueda de cadena que veíamos recién en la teoría ahora tenemos emisor estudio code así que lo vamos a ejecutar levantamos un poquito cada terminal para poder verlo mejor arrancamos y le ponemos por ejemplo hoy es viernes el último viernes del mes bien oyente y ahí me aparecen todas las variantes resultante de aplicar cada uno de estos métodos en el primer caso transforma todas la expresiones mayúsculas en el segundo todo minúscula en el tercer caso pone la letra capital es la h de adelante digamos en mayúscula el resto menos cura después bueno obviamente lo que tiene que ver sin blancos a la izquierda a la derecha o en ambas partes bueno no lo van a poder ver si en este caso hubiese blanco sería más visible pero bueno esto es creer básicamente que esto ocurrió y fíjense lo curioso de las últimas tres expresiones de que dice que es alfanumérico falls alfabético falls y numérico fotos numérico evidentemente no es numérico si alfabético uno puede decir como falls bueno y también falls alfanumérico alfanumérico porque no tiene números y letras y el tema es que en ambos casos tanto alfabético son como alfanumérico están teniendo en cuenta la coma sí y la coma es un valor que no representa no está en el con el valor del tipo alfabético porque no es una letra ni tampoco en el ámbito de los números entonces es por eso que pone esfuerzo si si esta expresión no hubiese sido sin esa como en el medio si de esos tres foros el que diría que esto es alfabético porque realmente todas las presiones esas fáctica y finalmente la letra el método fine digamos donde dice con perdón donde podemos ver qué texto desea buscar en la cadena bueno vamos a poner viernes que el que aparece dos veces y me dice por qué texto lo va a reemplazar cuando jueves sí y me dice que bueno que lo reemplazó y con él con como decíamos recién me dice cuántas veces está en el texto y después en qué posición está si y finalmente me muestra el texto hemos reemplazado tiene que represó el jueves por los viernes en ammán en ambas oportunidades bueno entonces veamos lo que tiene que ver con cuántas veces está el texto obviamente la expresión de cadena que yo puse viernes aparece dos veces está claro que aparece dos veces y el tema de la posición tiene que ver con que la de corta de viernes empieza en la séptima posición obviamente hace caso omiso de la segunda cuando yo busco con una posición hay una forma de aplicar ese método que es decir desde donde quiero empezar a buscar si yo no pongo nada va a empezar a buscar del principio con lo cual la palabra viernes empieza en la séptima posición porque insisto como peso del principio agarra la la expresión viernes toma la expresión viernes que primero encuentra y si no bueno si yo le hubiese puesto aquí que voy a buscar desde la posición 10 por decir algo o un poquito más 12-13 bueno de ahí en adelante me ubicaría si empezaría a contar y por ende ubicaría de la segunda expresión viernes y no la primera bien esto es un rápido repaso de todo lo que tiene que ver con estas expresiones de cadenas vamos a complementar las con estas otras dos sí que son la joint y la split y tenemos para eso una lista hemos elegido una lista la lista que creamos aquí con juan pedro lucas la obra y stella bueno le imprimimos el recibo para ver justamente el contenido la lista nos asombra de eso porque realmente vamos a ver lo mismo que estamos viendo aquí en el código pero después podemos aplicar la función juntar si éste pero la función join en la variable juntar y lo que vemos aquí es que join no solamente me junta en este caso los elementos de una lista sino que va a formar como una expresión que alfabéticamente bueno está dentro de la lógica porque porque lo que hace acá es tomar cada uno de los elementos de una lista y el perdón insertarle una y para que diga juan y pedro y lukas etcétera etcétera es decir que join puede ir acompañada de una expresión que sea aquella a partir de la cual todo lo que está separado y yo intento juntar cuál va a ser el elemento de concatenación esto porque yo pretendo formar una especie de frase pero yo podría querer estructurar algo parecido a archivos de tipo de cbs y bueno es ese perdón y poner una coma en lugar de esto un punto y coma para separar cada uno de estos elementos porque en ese caso tengo la intención de alguna manera formatear estos datos y no deformar una expresión alfabética común o parte de lo que es el lenguaje nuestro sí y después bueno imprimimos juntar para que se vea el resultado de eso y finalmente usamos ahora la expresión o el método contrario ante ese split y lo ponemos en la variable dividir en este caso cuando uno hace clic también tiene que decirle bueno yo quiero dividir esta expresión pero decime cuál es el elemento que separa uno del otro para saber justamente bueno cuando empieza un elemento y cuando empiece cuando termino elemento pero en cuando empieza a otro para ir tomando uno por uno tal cual como éste vos querés tomar es decir que en un caso pongo justamente concatenar y en otro caso pongo y para decirle cuál es el elemento separador que me va a separar valga la redundancia un elemento del otro un dato del otro y hacemos una impresión de desarrollar dividir para ver el resultado rápidamente lo ejecutamos levantamos un poquito aquí la terminal y vemos el resultado si en principio va a aparecer la lista completa si quiero que está aquí arriba luego me aparece en la lista ven ya con todos los y es decir los cinco valores con los cuatro y insertados y luego me vuelve a generar la lista tal cual en el original porque lo que hice fue lo inverso así como pase de esta primera expresión a esta otra insertando letras y bueno paso a esta última y quitándole o tomando mejor hecho como elemento de separación la i y tomando cada uno de esos elementos por separado para conformar nuevamente una lectura esto un ejemplo muy sencillo obviamente este no tiene mucho misterio esto simplemente lo que pasa aquí es por saber cuál es el elemento separador o elemento que yo voy a usar para separarlos pero una vez unidos otros para tomarlo como referencia si lo quiera dividir bien hasta aquí llegamos con estas cuestiones de cadena y ahora no vamos a meter en el contexto de las expresiones regulares si las expresiones regulares persiguen un objetivo similar al de las cadenas sí pero están en el marco de una librería o de librerías en este caso estamos vamos a empezar a trabajar con la librería red la cual tiene algunos métodos que también sirven para la búsqueda de cadena pero qué bueno no son métodos directamente de pactos sino que responden digamos a esta librería que tiene bueno obviamente propósitos un poquito más avanzados que los implementos que ya vienen compactos bien aquí tengo dentro de la línea r el método search sí que se sirve para buscar dentro de una cadena sí entonces yo puedo tener aquí en este simple ejemplo hice un force simplemente para que esto pase cuatro veces sin tener que ejecutar el este código cuatro veces digamos es un pequeño truco así que yo puedo ejecutar esto y ver que por ejemplo en este caso puedo poner un texto cualquiera que hayamos puesto hoy hoy es viernes y y pongo el primero viernes del mes siguiente el río que lo que quiero buscar es viernes y ahí me dice que red con el método search donde yo le digo que quiero buscar lo que está en la variable buscar que lo que escribí aquí en la cadena cadena que es lo que puse antes sí y me dice que lo encontró y que lo encontró en la posición 7 y que va de las 7 a la 14 también y fíjense ahora lo ejecutó nuevamente y voy a escribir lo mismo luego un poquito más corto vuelo buscar viernes pero me dice que en este caso éste lo encuentra a pesar de que lo he escrito en minúscula sí fíjense que yo lo escribo minúscula este pero el texto original la frase hoy es viernes lo tienen en mayúsculas bien esto sucede si yo lo quisiese así gracias a lo que he puesto aquí que es ignore case sí es decir que ignoren la mayúscula es decir que busque la palabra por su esencia dejando de lado el tema de la mayúscula o minúscula así obviamente que si yo escribo otro texto aquí hoy es jueves sí y le digo que los viernes me voy a decir que no necesito obviamente que no funciona esto por ende el search no da un resultado este efectivo así que bueno esto en principio es el primer método de la libia red que también sirve para hacer búsquedas de determinadas expresiones en el marco de una cadena las expresiones regulares también tienen otro método este en la librería de red puntualmente perdón que es el find o si el find all es ya no buscar algo concretamente dentro de la cadena sino buscar a lo largo toda la cadena si por eso este el fine oil no solamente me va a traer una situación diferente sino que también me puede dar otro tipo de resultados vamos a ejecutarlo vamos a poner un texto de nuevo hoy es viernes el primer viernes del mes y le digo que quiero buscar viernes fíjense cuál es el resultado ahora ya no me dice lo encontré en esta posición y me muestra resultado pensando que quizás ese valor está en otras posiciones más a lo largo de la cadena eso fue con search con final como les comenté recién mira toda la cadena de punta a punta no se queda con la primera aparición está bien entonces fíjense que en este caso me marca todas y cada una de las apariciones de la palabra de búsqueda que encuentro en la cadena como una lista me lo devuelve como elemento es una lista porque sé que me aparece justamente una lista viernes coma viernes y así todas las veces que aparezca la palabra y finalmente con len puedo no traer el resultado de todas las apariencias desde texto de búsqueda sino simplemente la cantidad de veces que aparece esa expresión en el marco de la cadena vamos con otros dos ejemplos de la utilización de fine variando un poquitito en la situación en este caso vamos a hablar de los meta caracteres o comodines como se llama habitualmente también en la jerga en este caso vamos a tomar como ejemplo una lista esta que está que con bueno algunos nombres que tienen algún patrón que se repite tenemos los juanes 2 lópez si para poder jugar juego con eso y ver algunas variantes de red fine hoy cuáles serían en principio el meta carácter puesto delante de una expresión lo que hace es buscar todas aquellas expresiones de cadena que empiecen con ese valor en este caso todos los que empiezan con juan por el contrario el símbolo pesos puesto al final lo que buscaría sería justamente lo contrario todas aquellas expresiones que terminan con esa expresión que está pegada al símbolo pesos bien y luego bueno hacemos ahí un print de cada uno de estos porque vamos a recorrer la lista con un short y bueno si es el primer caso lo va a poner a la izquierda y sigue el segundo lo corremos unos 2 espacios 12 espacios a la derecha y saldrá digamos con esa con esas formales formatos y así que lo ejecutamos y ya vemos rápidamente el resultado realmente se cree que ya tenemos en vista no obviamente van a aparecer por el caso de este primer red final los 2 juanes y por el caso del segundo red fine all bueno los 2 lópez si éste fíjense algo interesante que es que también aparece juan amaral esto que quiere decir que no se toma la expresión de punta a punta sino que lo que está diciendo me aquí esta expresión es busquen métodos los que empiezan con jota y a tiene no solamente con esta palabra sino que las cuatro primeras letras sean éstas lo mismo pasaría con lópez si yo hubiese algún apellido que fuera no sé a lópez pues decir algo que venga al caso si también saldría porque lo que está pidiendo aquí no es que termine con lópez como una expresión única sino que las últimas cinco letras de la expresión sean lopezdoriga1 t eso justamente que juan amaral aparece porque sus primeras cuatro letras responden a lo que tiene que ver con el patrón que hemos puesto aquí el patrón de búsqueda que hemos puesto aquí y luego con un caso más que también se da mucho se utiliza mucho en estos casos con final que es el caso de tener una lista que suponemos que tienen todas direcciones de correo electrónico si tenemos alguna duda de bueno este esos datos si responden al patrón de lo que yo quiero o están como se dice habitualmente las heras sucios o datos y que no no es tan buenos digamos para después poder procesarlos bien entonces lo primero que hemos puesto aquí bueno es un título que lo que yo busco son mails con acentos o letras en ustedes sabe muy bien que el patrón de eso puedo utilizar hoy dentro de lo que es una dirección de correo electrónico no van ninguna de esas dos expresiones ni les en y ni los acentos por eso hemos puesto aquí a propósito bueno en el caso del primero el segundo tienen acentos y en el caso del último tiene una ñ entonces con red find yo puedo darle todo un conjunto de elementos así como elementos de búsqueda pero separados como una lista si ya no busco una expresión no busco un lópez novo con juan le digo que quiero buscar dentro de la cadena elemento si este que está recorriendo cada uno de los pagaron danza elementos de la lista quiero buscar cada por cada nombre que tome cualquier aparición de cualquiera de estos seis elementos es decir las cinco vocales con el con el acento o bien la ñ y luego bueno imprimo el elemento que responda esa búsqueda a ese patrón de búsqueda y finalmente a otra cuestión similar que es una búsqueda por la negativa es en lugar de buscar lo que sí tiene busco lo que no tienen por eso recordó nuevamente la lista y ahora usó un not refine all y el elemento de búsqueda va a ser el arroba es decir lo que estoy buscando son mails cuya expresión no contenga una arroba entonces en lugar de buscar por si busco por no bien lo ejecutamos y ya creo que vamos a prever cuáles van a ser resultados por la lógica de los del contenido que hemos puesto en la lista obviamente que los mails con acentos o letras ñ son juan pérez juan garcía y j ni ar al uruguay email y los mails y nos roba simplemente l lópez el lugar que tenga la roba tiene este l la empresa yahoo.com sí entonces este bueno esto es lo que tenemos también como opciones que insisto mucho para depurar los datos en ciencia de datos es muy importante la depuración de datos porque si no tenemos datos buenos después las conclusiones que vamos a hacer del análisis de los datos obviamente no van a ser buenas porque los datos no responden al patrón que deben tener para poder ser analizados correctamente bueno vamos a ver el segundo tema de esta clase número 2 que es justamente el tema del tratamiento de los archivos de texto los archivos de texto bueno obviamente que tienen la posibilidad de a través de payton manejar las operaciones clásicas que se hacen con las estructuras de datos donde bueno se puede trabajar con expresiones que tienen que ver con la creación del archivo con la lectura del archivo con leer una línea del archivo y con agregarle texto a un archivo que ya tiene texto o que ya existen bien aquí esté todas estas operaciones la vamos a ir ejecutando paso a paso vamos a ponerle justamente como son tareas que queremos ir viendo de esa manera vamos a poner un comentario aquí o una forma de marcar esto como un bloque comentado que se hace con tres comillas al principio y tres comillas simples al final bien a partir de esto yo puedo tener fíjense que aparece una flechita que lo que hace es permitirme ocultar todo ese bloque o mostrarlos y de esta manera para que me sea más fácil la visualización del código que realmente me interesa porque muchas veces lo que está comentado es algo que parcialmente no me interesa y por ende no debería yo de interesarme mucho mirar eso bien les pido que miren la carpeta número 9 donde están estos archivos que van a tener que ver con los distintos ejemplos que vamos a abordar en esta clase porque ahora vamos a ejecutar primero el método open el método open va a abrir un archivo de tipo texto para escribirlo como se dice esto nos indica el nombre del archivo la carpeta y con w se le dice que lo que yo voy a abrir la forma que abre este archivo hacer para insertarle un texto para escribirlo a w yo lo que hago en realidad es empezar a escribir desde cero el contenido de un archivo es decir w lo que hace es empezar desde cero con ese archivo por eso en este caso el documento txt fíjense que no está dentro de la carpeta 9 por ende con esta modalidad apertura al no estar lo va a crear y si estuviese creado lo va a volver a inicializar de cero con lo cual este texto que está aquí por más que que va a ser justamente lo que yo voy a insertar dentro de ese archivo de texto a través del método right lo va a hacer siempre desde cero si yo lo que hago es usar el método open con el nombre del archivo acompañado de la letra w y luego cierro con close el archivo bien esto es importante que lo tengan presente porque si yo ejecuto esto tantas veces como sea el resultado va a ser siempre el mismo volvemos al tema en la carpeta 9 no está el archivo de documentos txt pero pero justamente como yo lo abro con este complemento w lo que va a hacer es que como no está lo va a crear y además de que darlo con el método right le va a insertar este texto lo probamos y vemos rápidamente obviamente por por terminar no se va a ver nada porque toda una cuestión interna pero ya ven aquí que aparece el archivo documento txt que si yo le hago un clic lo abro y lo pudo visualizar tranquilamente no si acá tengo las tres líneas que justamente puse aquí ven que iban a aparecer porque aparecen en tres líneas porque en el medio de la expresión de tipo texto y yo he insertado estos elementos barra n barra n que lo que hace es abrir un nuevo renglón debajo de la última expresión que precede hace barra n es decir después de la s de juan carlos a abrir un nuevo renglón como gerente general y después de la n gerente general va a abrir un nuevo renglón para darle espacio a esta última parte de la expresión de la cadena que es 12 de mayo de 2022 por eso esto sale de este modo si yo en esta expresión no hubiese utilizado estos barra en obviamente hubiera encontrado aquí en documento txt todo un solo renglón este mismo texto dispuesto insisto en un solo renglón este mismo texto bien ahora lo que vamos a hacer es pasar a la segunda tarea que vamos a hacer justamente con datos que es con un archivo de texto perdón que es leer el archivo por lo cual esto lo voy a esa carrera me lo voy a llevar a trabajo para que quede comentado de haciencia abajo y no la parte de leer archivo bueno como se lee un archivo bueno primero hay que abrirlo para leerlo ahora con el complemento r donde yo le estoy indicando ya no quiero abrir el documento txt para escribirle algo como yo puse acá quiero abrirlo para leerlo se lee justamente a través del método read de la variable archivo que se supone que es la que recoge a través del método la apertura de ese archivo en este caso en modo read y después lo cierro y finalmente vamos a hacer un print del contenido para ver ya no desde la apertura del documento sino por pantalla verlo a través de ejecución de payton el contenido de ese archivo lo ejecutamos levantamos un poquito la terminal para prepararnos para lograrlo ni acabemos justamente que lo mismo que tenía acá en documentos dxt es lo que me parece aquí en la terminal aquí debajo señor juan carlos gerente general 12 de mayo 2022 bien y ahora vamos a ver el próximo caso que no es leer todo el archivo completamente si no saben solamente leer un renglón una línea de ese archivo con lo cual me llevó nuevamente este comentario de acá luego el traslado hacia abajo y ahora lo que voy a ver es justamente la posibilidad de ver como que estoy recién una línea de un documento de texto que se supone que tiene varias líneas en este caso no uso el método read cómo usaba aquí sino el mito read lines que está destinado para ello y cuando hago el print en lugar de hacer print de la variable contenido lo que hago es print en la línea número uno también es decir si yo hiciese read y quisiera acceder al alien número uno no podría porque red me leyó todo el documento en cambio red line sem el hotel idea líneas y por ende yo ya puedo referenciar a la variable contenido con un número de índice que este caso aparece que el 1 representa el segundo renglón con lo cual me estaría mostrando esta palabra gerente general bien lo ejecutamos para ver si esto produce de ese modo levantado en el terminal para ver que no aparezca el resultado ejecutamos y vemos que efectivamente es así en el primer caso me está mostrando el resultado de esta expresión donde uso read como ya lo habíamos visto antes si obviamente aparecen los tres renglones y esta expresión gerente general obedece a la ejecución de esta segunda parte del código donde a través del método de read lines yo muestro ante acaso el contenido de la línea número obviamente que si yo aquí en esta expresión usase un foro podría tener exactamente el mismo resultado que tengo con el print del contenido a través del método read es decir si yo muestro todo el documento de una o bien lo hablo con reel dance pero voy mostrando de a uno cada uno de los renglones pues el resultado va a ser exactamente el mismo este caso obviamente se entiende que yo no quiero leer todo el documento y por eso uso read lines y no re bien y finalmente vamos cerrando todas estas terminales de aquí y finalmente tengo el método ahora sí para agregar texto que en realidad lo que hace borramos los comentarios lo que hace o lo que pretendemos hacer es que a este documento txt que tengo aquí le aparezca un renglón más que dice nos dirigimos a usted precedido de estos barra en es que lo que hacen justamente es habilitar a que eso ocurra renglones más abajo sí entonces cómo hago para insertar o para agregar texto a un documento que ya está bien cambia la cuestión en la modalidad de apertura yo ya no abro el documento ni con el que lo que hacía de inicializar lo ni con el re que me sería solamente para leer en este caso hablo con él a de apem que me permite tomar este archivo siempre y cuando exista por supuesto y si existe le agrega el texto que yo le ponga con wright bueno si no pusiera los barra en reglones abajo si pongo a la reina bien entonces este me queda esta última parte del texto volvemos a ejecutar el código me vuelve a aparecer todo lo mismo que veíamos veníamos viendo anteriormente pero qué pasa obviamente que el efecto de este último código y de esta última parte no se ve aquí porque no era el print que yo tengo precede a este hasta ampliación del contenido del documento pero si voy aquí a documento txt lo veo justamente que la línea que yo he agregado con el apple apareció aquí debajo reunión debajo de lo que yo tenía fíjense que yo puse dos barra n por lo cual el primer barren y lo que hizo fue ponerme en este renglón pero al darle otro barra en lo que hice fue ponerme en otro renglón abajo dejando uno blanco si hubiese puesto solamente un solo barra n bueno pues entonces el archivo hubiera quedado de esta manera como puse dos barra n bien hasta aquí vamos con esta parte de lo que tiene que ver el tratamiento de archivos de texto con las cuatro operaciones básicas y pasamos ahora a lo que tiene que ver con cómo poder ya leer un archivo desde una determinada posición aquí he puesto como título leer un archivo dos veces porque lo que vamos a intentar hacer aquí es justamente esto si yo voy a comentar esta parte que está aquí vamos a perderla de vista por un ratito sí y lo que voy a hacer aquí es abrir el archivo diciéndole que lo que quiero hacer es leerlo lo voy a leer y voy a imprimir el contenido y después voy a volver a hacer lo mismo voy a volver a leer el archivo y voy a volver a imprimir el contenido y después tierra el archivo si para qué y mi primo ejecutó el archivo y fíjese lo que pasa hice un print después de haber leído y acá está el resultado y ese otro print después de haberlo vuelto a leer pero no aparece dos veces porque porque esto tiene un concepto de puntero o de cursor que que es lo que pasa una vez que yo leo el archivo si el puntero se pasa al final del archivo es decir que el puntero estaría vamos abriendo comentó que aparenten estaría aquí si yo vuelvo ejecutaron ruiz que es lo que hace melee todo desde el principio no me lento desde la última posición en la cual está el cursor con lo cual si lee desde esto que está tirando en adelante no hay nada y por ende no es que no imprimió nada e imprimió lo que encontró después el cursor que no es nada se entiende o sea el primero ejecutó pero bueno como el contenido que muestra es nada bueno no aparece nada pero no quiere decir que se olvidó de la instrucción print la extrusión print la ejecutó solamente que la variable contenido en este caso es vacía porque el ruido arranca desde aquí en adelante bien como alteramos eso muy fácil lo que hacemos es usar el método cic y ahora sí voy a desconectar esto para que se entienda si lo que hace es reubicar el puntero con lo cual ahora con archivo puntos y sub zero que quiere decir mándame el cursor de nuevo a la posición 0 con lo cual si yo lo veo en el documento es como si yo hiciese manualmente esto que es poner el cursor aquí arriba adelante era ese y me va a volver a leer todo el archivo completamente en 1.090 -1 o lo puedo poner en otra parte y arranca leyendo desde el lugar que a mí se me diera gana porque el sexo lo pongo aquí en cero porque pretendo leer todo el archivo nuevamente pero si no fuese así lo podría poner donde yo quisiera ese curso y empezar a leer ese chico vamos a probar ambas cosas igual así que bueno grabamos esto lo ejecutamos y vamos a ver qué el resultado es que ahora así tiene que aparecer dos veces correctamente como se ve aquí el contenido del archivo porque la primera bueno lo primero como siempre la segunda lo volvió imprimir porque yo móvil el curso insisto aquí a esta posición que se vuelve a leer todo el principio y me lo muestra aquí debajo bueno como dije recién el cic no hace falta que me den siempre de cero vamos a suponer que lo pongo desde el elemento 10 y nos del 0 grabó por la terminal la levantamos un poquito para verla mejor y ejecutamos y vemos qué pasa la primera vez me muestra todo el archivo completo pero ya la segunda fíjense que arranca desde la de carlos porque porque lo que evitó fueron los nueve primeros los días primero porque la posición 10 no es la primera sino la décimo primera este arranca desde esa posición en adelante o sea que lee desde la posición 10 este elemento de perdón que sería la posición 11 en adelante y así puedo jugar todas las veces que quiera con lo que se me ocurra y ir alterando el cic y poniendo donde a mí se me dé la gana y enrique secara empieza desde el aire de gerente general bueno esto yo creo que está más que claro más que entendido es una cuestión de que ustedes prueben todas las veces que quieran y bueno vean los resultados en virtud de la prueba que hace bien ahora vamos a otras cuestiones que tienen que ver con justamente tratamiento de archivos que esté bueno puede ser como una especie de algún programa que me permita de alguna manera mostrar algunas cuestiones de manejo de archivos y este primer programa sin medicar una lista de pasajeros suponiendo un piso juan quiero que se descargan dos y pedro quiero seguir cargando así ahora quiero seguir dando sí jose tirase declarando no bien se terminó la historia es simplemente para mostrar algunas cuestiones fíjense que lo que hizo justamente es crearme un documento 1 punto txt en este caso donde están los cuatro nombres que acabe de cargar bueno puestos uno debajo del otro acuérdense que esto si en realidad yo no puedo manejar como quiera es decir en este caso lo que hice fue a través del archivo punto right si de este programa de ejemplo simplemente ir agregando nombre más la perle el símbolo de apertura de 1 regla si yo pusiera nombre más coma aparecería o punto y coma uno al lado del otro sí lo podemos demostrar fácilmente si cambiamos esto bueno aquí lo puse como el archivo vamos a abrirlo para que empiece a trabajar desde el principio si ese cambio lo ejecutó y voy a hacer lo mismo que hicimos recién si pedro si ahora si lo cortamos a disparo solo tan largo bien yo ahora voy a documento un vídeo que pasa que yo no dije que ahora de entre pasa abajo dije que quiero que por cada nuevo elemento que haya aparezca separado por niko bueno esto insisto es un ejemplo muy sencillo de aplicación de lo que tiene que ver el manejo de datos y la inserción en un archivo y después éste lo mismo para aquellas cuestiones que tienen que ver con este bueno si yo quiero de alguna manera ir esté ejecutando algunas cuestiones que tengan que ver con una lista de pasajeros y usando en este caso el rate lines y entonces lo que voy a hacer es poner un programa vamos a borrar esto para verlo bien donde lo que se pretende en este caso es cargar una lista de pasajeros cargar el nombre y el destino y después separarlo por coma el nombre del destino y abrir por cada nueva persona que aparezca con su destino un nuevo renglón o sea una alteración a lo que vemos recién y luego abrir el archivo y poder leerlo línea por línea y tratar de separar ese nombre y ese destino a la hora de imprimirlo sí porque si yo tomo todo completamente el expresión me aparece el nombre como destino punto como destino yo lo que quiero que me aparezca el nombre por otro lado el destino para eso tengo que separarlo con explico vamos a verlo paso a paso si lo entendemos mejor el ejemplo porque está aquí carrera lista de pasajeros me dice dice cargar o ampliar perdón esto no lo había comentado hasta aquí al principio que es también algo interesante para poder ir viendo porque yo lo que puedo hacer es si quiero crear una nueva lista o si quiero agregarle elementos a una lista existente con el que queremos crear la lista y vamos a poner un nombre de pasajero de nuevo juan bah salta si quiere seguir hablando pero va a capital si quiere seguir cargando y jose que va a neuquén y no quiero que además nada bien fíjense que aquí me sale la lista de pasajeros bien como qué pasó acá en el archivo de documento 1 y vamos a abrir juan punto coma salta pedro punto y coma capital jose punto coma no cree que eso los datos que yo cargué y que yo puse a través de este programa y que los quería insertar de ese modo con wright nombre es una expresión de cadena punto y coma destino enter y paso hacia bajamos a cerrar todos estos otros archivos que no están haciendo otra cosa más que entorpecer no nada más y vengo acá a documento de 1x t y tengo juan punto más alta pero punto como capital jose punto como nunca que esto que está aquí bien lo que yo ahora estoy tratando de hacer es decir bueno yo no quiero imprimir algo donde se ve esto tal cual está acá quiero imprimirlo sin el punto y coma bueno fantástico entonces me voy acá abajo recorro primero abro por donde perdón el archivo para arriba le digo que lo quiero ver línea por línea y en cada ocurrencia de esa línea voy a aplicar el método split y le digo que lo que quiero es que me explique que me los separe que nos divida con el elemento separador punto y coma también entonces por cada elemento el punto split o sea cuando sé que el hilo que va siempre tomando cada uno de los elementos el primer y cuál va a ser este juan punto y coma salta bueno le digo fantástico en el primer elemento sácame la toma y dame el dato superó por un lado y el dato suma uno por el otro porque datos sub serios en uno porque antes de esta expresión no existía datos subsidios o no existía una sola expresión como yo le aplicó el split ese dato se subdivide en dos en este caso en dos porque son dos elementos se puede subdividir en tantos como elementos aparezca separada un punto y coma esto es yo ahora sí puedo referenciar a datos sub zero de datos según y decirles bueno ahora quiero que cuando me imprima por pantalla me lo enviamos por separado sin esa coma que aparece allí en el documento entonces me lo separan perdón la terminal en la terminal ven aparece juan espacios alta pero espacio capital jose espacio no que bueno es simplemente un ejemplo para para ver algunas cuestiones que ya hemos visto antes como el caso del split y cómo enriquecer la ahora a partir de jugar con esto con el contenido de un archivo de texto bien obviamente como siempre digo hay muchas más cosas para trabajar con archivo de texto no solamente obviamente con con las librerías específicas de ciencias de datos sino bueno en el marco de este el contenido del sitio de payton puedan sacar otros métodos que hacen referencia a otras cuestiones que son un poquito más específicas no tan generales o tan usuales como éstas pero que ustedes pueden llegar a necesitar siempre es importante ir así que payton para tener una mejor documentación desde ya cualquier libro cualquier cuestión también ayuda pero el sitio de pacto siempre tiene la última información y la más actualizada y la unidad siempre aparece tiene muchos tips y muchas cuestiones que nos van a ayudar mucho y mucho mejor que en ningún otro sitio siempre cuando analizamos en inglés y en inglés siempre aparecen obviamente las últimas versiones y las últimas cuestiones que van apareciendo hay también en instagram y en otras redes sociales en muchos sitios que hacen referencia a tips y estas cuestiones que nos van a ayudar mucho para algunos casos en particular así que siempre es importante recurrir a toda la información que tengamos a través de la red formal como el sitio de payton o u otra que pueda aparecer desde grupos de comunidad de payton bueno hasta aquí llegamos con el tema de archivo de textos bien ahora vamos con el tercer tema de esta clase número 2 que es el tema de acceso a datos pero no en archivo de tipo txt sino en bases de datos si bien en este caso vamos a hablar básicamente de base de datos haciendo referencia a una base que ya viene con python que se llama es eco light si en este caso este es una base que es muy utilizada para toda aquella instancia de práctica o aprendizaje que podemos llevar adelante en instancias como ésta pero está claro de que en realidad hay una posibilidad de trabajar con cualquier sistema de base de datos de los conocidos y obviamente pueden trabajar con mysql con pobres pone sqlserver wear con todas las bases de datos que son las más conocidas del mercado en este caso esta base circular es lo que tiene de bueno que obviamente no tienen ningún tipo de costos ya bien incluye en el paquete no hay que instalarla y por ende se puede empezar a utilizar directamente para hacer bueno todas las tareas que puede conllevar una gestión de base de datos o un programa que a accesibilidad al sistema de base de datos como una fase de entrenamiento y después obviamente cuando tengamos alguna cuestión de un trabajo mucho más profesional o de mucho más rigor obviamente tenemos que recurrir a otros sistemas de base de datos pero mientras tanto tenemos una base de datos que es gratuita y que nos permite estar probando o aprendiendo cosas bien vamos a el primer programa de ejemplo que estamos tomando acá para hacer la primera tarea que hay que hacer con una base de datos que es crearla así como nosotros vimos es un rato del tema del archivo de texto como lo crea vamos bien aquí entonces vamos a trabajar con una base de datos sea base de datos en realidad esté algo parecido lo que digamos recién con archivos de textos se puede en el caso de secular ver si existe o si no existe y en el caso de que no exista crearla en este caso lo primero que tenemos que ver es qué para trabajar justamente con la base de datos sql tenemos que importar la alegría es es cool hay 3 el 3 hace referencia a la versión de ese q light que estábamos usando en esta versión de payton después obviamente será otro en el caso de que haya otra versión lo primero que tenemos que hacer es usar el método conecto para conectarnos a una base de datos que va a estar alojada entre una carpeta en este caso elegimos la carpeta 10 porque todos los programas que tengamos que ver con esta este apartado de la clase referida de base de datos lo vamos a poner dentro de la carpeta 10 obviamente puede estar en cualquier parte bien y con este método connect esté establecemos la conexión con una variable o asignándole esta conexión a una variable que se puede llamar como ustedes quiera en este caso le hemos dado en poner mi conexión y luego hay que crear un cursor si este curso va a ser vital para hacer todas las acciones que tengan que ver con los datos de una base de datos tales como crear una tabla insertar datos o buscar esos datos eliminar esos datos o eliminar esos datos bien son las cinco tareas básicas por decirlo de algún modo entonces al yo crear este cursor desde la variable de conexión recientemente creada mi conexión punto cursor me creó la variable mi cursor que le he puesto ese nombre pero se puede llamar como ustedes quieran y llamo al método execute y dentro de paréntesis y dentro de comillas lo que hago es utilizar el lenguaje transa que se cuele que seguramente esto ya conocen que tiene que ver con poder acceder a las tareas tradicionales de gestión con una base de datos y en este caso completamente la de creación de la tabla que lo primero que vamos a hacer porque porque ya con esto que escribimos aquí arriba con connect como dijimos recién como la base no está creada la vaquera y una vez que está la estructura de base creada bueno empezamos por poder crear la primera estable en este caso estamos creando la tabla mis contactos la orden y table orden de tranza qué sql y le ponemos en este caso dos campos nombre de tipo chart de 50 y teléfono del tipo entero bien recuerden que en este caso como en otros lenguajes lo que se utiliza para ejecutar una tarea vinculada a una base de datos o una tabla una base de datos es en primer lugar hacer referencia al método propio del lenguaje y luego al lenguaje propio de la base de datos en este caso el lenguaje sql del propio de la base curar bien y una vez que está hecho esto cerramos la base de datos y ya queda cerrar una conexión la base de datos ya queda digamos esté creada la base de datos y en este caso una tabla que es mis contactos procedemos a ejecutarlo lo hacemos y ya vamos a ver en un rato que va a aparecer dentro de la carpeta cádiz aquí está la base base obviamente le hacemos clic no se va mostrar nada porque no podemos verlo como veíamos antes se acuerdan en el caso del archivo de texto documento txt o documento 1 txt bueno en este caso la base la podemos ver que está aquí pero no la podemos abrir entonces para poder abrir esto tenemos que instalar un programa que éste es un browser de ese q light sí que se llama the browser de secular esto sí hay que instalarlo no viene con el paquete original yo ya lo tengo instalado con lo cual lo voy a abrir bueno y aquí está es un browser muy sencillo si este mínimo digamos tan básico si se quiere como la base circular ustedes seguramente han manejado a otros otras este entornos de gestión de datos más complejos y obviamente tienen seguramente más opciones son más más ricos en alternativas pero aquí tiene lo mínimo y bastante bien es llevado a cabo luego lo que tenemos que hacer es llegar a esta 10 bien aquí está y sencillamente ahora que fíjense que lo que estoy viendo aquí son los archivos punto pait lo mismo que estaba viendo aquí y la base base entonces tenemos esta base va a sacar y lo que hago es arrastrar y soltar directamente si el resto y ya veo que tengo bien la tabla mis contactos con los 22 campos digamos no con el campo nombre del campo teléfono que vemos referenciado recién bien este una vez que tengo esto puedo ir al brou data y ver allí los registros la tabla me contactó que obviamente como todavía recién acaba de crear no tiene ningún dato pero lo puedo ver de aquí así que esto lo voy a dejar minimizado porque podemos tener que volver a este espacio y bien con este programa entonces base de datos punto cero lo que hice fue entonces crear la base que no existía y crear la tabla ahora vamos a ver las otras acciones que vamos a hacer con esta base de datos y su tabla bueno habiendo visto recién con el programa base de datos cero punto para esta cuestión de cómo crear una base de datos y cómo que era una tabla vamos a empezar a ponerle datos dentro de la tabla bueno recurrimos a una instrucción que seguramente ustedes ya conocen de transar que sql insert into el nombre de la tabla valores y bueno los dos valores que hacen referencia a los campos que creamos en la tabla mis contactos en el programa base de datos 0 punto país es decir al nombre y al teléfono el nombre de tipo guarch ar teléfono tipo entero por ende los datos que pasemos aquí tienen que estar el primero encerrado entre comillas y el segundo no puede tratar un número ago 2 vamos a hacer dos inserciones pero bueno esto es básicamente el lenguaje sql distancia que se cuele ahora vamos a ver concretamente al contexto de payton que hago en python bueno importar la alegría segura de tres como hice antes conectarme a la base de datos crear el cursor ahora con ese cursor voy a volver a recurrir al método execute ya no para hacer un create table como el que hicimos aquí ya no para hacer este crédito de bloque como hicimos aquí sino para hacer el inserto es decir que la estructura es igual lo que cambie destrucción que está encerrada entre paréntesis entre comillas pero pero en los casos de inserciones de eliminaciones o de actualizaciones tengo que invocar al método commit sobre la conexión si yo no hago esto esta instrucción que está aquí arriba no se habrá ejecutado por ende para que le insert que yo ejecutó a través del método s que realmente sea efectivo tengo que acompañarlo de commit todas y cada vez que se ejecute una instrucción de gestión de datos - obviamente la búsqueda por supuesto aquella gestión me refiero a insertar eliminar o modificar y luego cierro la conexión obviamente que yo ejecuto este programa que lo que vamos a hacer ahora cerramos la terminal lo ejecutamos no vamos a ver nada porque acá lo que está haciendo es una operación que va contra la base de datos y no me refleja absolutamente nada más que obviamente mensaje como veo aquí la terminal tendríamos como resultado visible destinado a un usuario común no tengo ningún este elemento para poder visualizar cómo hago para ver si esto que hice realmente pasó o no pasó bueno vuelvo al libi browser y acá tengo la tabla de mis contactos si bruce está tratada mi contacto no aparece nada porque tengo que actualizarla cuando el actualizó con este botón aparecen los dos datos que acabo de insertar con lo cual la gestión que hemos hecho ha sido exitosa y obviamente con el cierre de idea de la base de datos como corresponde bueno vamos a ver ahora con base de datos 2 punto pait y la instrucción select que tiene alguna diferencia con esto sí pero que básicamente de nuevo utiliza el lenguaje transa que se cuele bueno a fin de ser redundante así que problemas importante nivel secular 3 en este caso hoy se también porque vamos a mostrar un resultado por pantalla cosa que no hicimos cuando creamos la tabla que hicimos cuando insertamos datos en la tabla por ende los primeros dos regímenes son iguales a los que utilice en los casos anteriores y lo que voy a hacer acá es con cursor execute volver a ejecutar una opción una operación ese cuello una instrucción perdón sql en este caso select asterisco from contacto bien una vez que creó esto lo que hago perdón no es contactos vamos a corregirlos es parecida es mis contactos sí porque si yo hago por llevar un hermoso error que ser vamos a ver más adelante este que hay este tipo de cuestión que se pueden ir esté viendo de evitar si si hay algún error como éste que acabo de comentar bien entonces insisto nuevamente como utilice mi cursor existe acá para hacer un criterio ejecuta mi curso de éxito para hacer un share también utilizo en mi curso para hacer un selecto una vez que hago todo eso para ese resultado ese data set que yo tengo de la tabla mis contactos lo tengo que alojar en una variable en este caso de mayo de objeto se llama datos tengo que acceder al método fets el que quiere ser algo así como de método de mi curso es decir que acá abro el select y una vez que abro el select hago el pfets el para que definitivamente el resultado de este select vaya a parar al la variable de objeto datos una vez que tengo eso bueno puedo usar esa variable objeto para hacer en este caso recorrer los por ejemplo para imprimir lo puedo hacer lo que quiera con los datos lo que hago en este caso como ejemplo hace un ford diego por dato sin datos por cada elemento de la variable de objetos datos que tiene un data set entonces por cada registro en este caso sería 100 por cada renglón de ese ataque voy a tomar el dato sub 0 dejó un espacio y el dato sobre lo que ocasiona es un print combinando tres elementos con más sino con comas sí que esto es otra forma también de poder hacer un print diferente de lo que veníamos haciendo antes y luego cierra la conexión muy sencillo el programa no tiene nada del otro mundo y lo ejecutamos en este caso sí vamos a ver por la terminal el resultado de esto porque lo que tenemos que ver como vemos aquí son los dos datos que vi recién en el data browser ven que tengo aquí juani su número y laura y su número bien es lo que estoy viendo aquí debajo en la terminal que no es ni más ni menos lo que yo a través del programa base de datos uno inserte con estas dos intrusiones sin ser bien esto entonces es el manejo de select ahora vamos a base de datos 3 punto pait donde veo la otra instrucción del conjunto que es update o actualizar sí bien insisto con estas cuestiones si ya manejan trasera que se cuelen no va a haber nada nuevo que estén observando aquí simplemente lo que hago acá es un update contactos set con lo que leo asigne l al teléfono este número mientras el nombre o cuando el nombre sea igual a juan y haga lo mismo con el teléfono cuando el nombre sea laura y ponemos un tercer ejemplo justamente para acá lo que ustedes seguramente esa maneja que obviamente si yo hago una búsqueda que una ante una condición en este caso en un web que no es exitosa pues entonces este update nunca tendrá lugar cada vez se ejecuta con mi cursor execute al igual que lo hicimos aquí para leer aquí para insertar y aquí para crear la tabla lo mismo aquí y al igual que en el caso del insert tiene que ir así como en aquella oportunidad por cada in ser un cómic aquí también por cada update un cómic más allá de que este último no va a ser exitoso pero no importa porque es obviamente a veces no se sabe si va a ser así o no y entonces recordemos un poquito los datos que teníamos que son estos que están aquí debajo y yo voy a ejecuta el programa y fíjense que acá tenemos justamente medio un hermoso error que justamente me dice que la tabla contactos no existe porque es mis contactos vamos a corregir nuestros casos bueno insisto esos errores después vamos a ver cómo tratarlos porque hay una forma de justamente poder hacerlo para que no nos dé este mensaje de error ejecutamos vamos a poner ahora de aquí la pantalla de acs y ejecutamos nuevamente el programa y ahora si fue exitoso porque no me larga ningún mensaje error porque evidentemente no voy a ver nada por pantalla pero si acuérdense que lo que he hecho aquí es cambiar el teléfono a juana laura este y si yo miro acá fíjense juan tiene 1 5 6 11 22 33 y acá el número es otro y laura también unos 55 33 44 55 y acá unos 55 15 51 55 bueno esto ya se ejecutó tiene que mostrar la cara por qué no por qué bueno porque aún no mejor dicho porque no lo que ahora fresco dijo que juan y laura han tenido el cambio de teléfono y este update ha sido exitoso y obviamente el tercero no porque no hay ninguna persona cuyo nombre sea j y vamos este último caso sí que es el del dígito delete from contactos where nombre sea la aura de litro contactos guard nombre se ajusta esto es más o menos lo mismo que hicimos un rato con el update en poner un agente una instrucción que va a ser exitosa porque existe una persona que se llama laura y otra que no lo va a hacer porque no hay ninguno que sea mejor bueno de nuevo el patrón es el mismo acá no hay nada nuevo que podemos poner es decir es el execute para ejecutar la instrucción sql y el commit que la tiene que acompañar para que sea definitivamente efectiva sabemos que acá tenemos mal nombre la tabla lo vamos a cambiar para que no vuelva para hacer el mismo mensaje de error con lo cual después de hacer esto y ejecutamos esta instrucción lo que deberíamos ver que en la tabla la opción el registro laura si éste no va a estar más si así que lo ejecutamos bueno este se supone que ya ocurrió así que voy al grow share veo que siguen los dos no debería estar laura cuando ago efe veo que laura desaparece bien nada esté de otro mundo que seguramente usted no ya no manejen ya que manejan otros lenguajes de programación y seguramente todo lo que gestión de datos así que estas son las instrucciones básicas que hay en cada caso demás está decir que siempre el delete debe ir acompañado no web porque si yo pongo directo en contacto sin web obviamente lo que estoy haciendo es actualizando absolutamente borrando perdón de eliminando todo el ojito de la tabla y eso obviamente ya sabemos los peligros que contiene igual que obviamente con el tema de las otras instrucciones ya que son direccionadas obviamente la medida que no consiga nada este no no no voy a no voy a lograr nada pero si yo también le quitó el where a la pdi también puedo tener este alguna situación no esperada por llamarla de alguna manera bien esto entonces es el recorrido por las operaciones básicas de creación de tabla de inserción de registros de búsqueda este caso sería no sería una búsqueda por no está acompañado en web recuerda que yo esto lo acompaño en web ya no voy a tomar todos los registros una tabla sino algunos esto ustedes ya la manejan simplemente acá decidimos con el tercer es tomar todos los registros de la tabla y todos los campos y update y milito bien con esto terminamos la parte de gestión de base de datos y ya la próxima clase vamos a encarar un pequeño programa como para repasar todo lo que más oído está aquí incluyendo también esta cuestión de las bases de datos vamos nos va a quedar un tema pendiente para la clase siguiente que lo vamos a incorporar antes de hacer esa práctica y que es el tema del manejo de excepciones que justamente sirve entre otras cosas para poder anticiparse a problemas como el que vimos aquí que yo había escrito mal el nombre de la tabla y justamente al tratarlos debidamente esos errores de modo de que no aborten el programa sino que le den un tratamiento con un mensaje de error que el usuario entienda y pueda cerrar ese programa de una manera prolija bueno hasta aquí llegamos con la clase de hoy muchas gracias Titulo: CLASE PYTHON 03 PROGRAMADORES \\n URL https://youtu.be/TBhFBnko0Gk  \\n 3334 segundos de duracion \\n tercera clase de el curso de payton para programadores y estos serían los temas de esta clase el primer punto es las interrupciones cómo podemos trabajar los errores críticos que habitualmente interrumpen la ejecución del programa y la idea justamente es capturarlos para poder tratarlos de manera adecuada estos errores habitualmente abordan la ejecución del programa envían un mensaje previamente es inentendible para el común de la gente y bueno obviamente lo que queremos hacer es evitar esa situación capturar el error que muchas veces bueno hace aborto programa porque requiere alguna cuestión en la que tiene que intervenir un programador pero por lo menos que se interrumpa de una manera mucho más cuidada el programa y con un mensaje de error que alerte al usuario o le haga entender la gravedad de la situación luego vamos a ver una práctica integral con un club sencillo es un programa muy sencillo que haga great read update y delete de los datos de una base de datos y una tabla una base de datos tiene algo de esto hicimos la clase anterior pero bueno la idea ahora sería contextualizar un poquito más en algo que integre todos los conceptos que hemos visto atacar y le vamos a agregar algunas cuestiones que no hemos trabajado por eso el tercer punto habla de nuevos conceptos sobre metodologías de lo que tiene que ver la forma en que yo voy a expresar las instrucciones de acceso a datos y además algunos métodos que no hemos visto y qué bueno que ustedes tengan presente para complementar algunas situaciones que no son las habituales pero que en algún momento les puede hacer falta y por eso es bueno que conozcan esos métodos y también los exploren mucho más cuando puedan tener la necesidad de usarlos bueno vamos a la clase entonces bueno para empezar a ver el tema de las excepciones vamos a hacer un programa que obviamente va a arrojar errores con la idea de poder ver después cómo tratar de evitarlos con el manejo de excepciones en primer lugar tenemos acá bueno muy sencillo en el cual se introducen los números y el resumen nuevo elijo cuál de las cuatro operaciones matemáticas básicas quiero hacer con esos dos números lo vamos a ejecutar ya inmediato y vamos a ver la primera situación en la cual por ejemplo pasa si yo pongo un número y en el segundo caso oprimo otro elemento que no es un número este por ejemplo una letra puede ser un entero puede ser un siempre lo que fuere y lo que inmediatamente me va a dar es un error porque me dice que no puede aplicar la función int dado que no es un número por tanto a través de in no lo va a convertir jamás en un valor entero y bueno acá tenemos lo que sería la expresión típica de un mensaje de error que obviamente para los usuarios es absolutamente entendible esto lo podemos trabajar en principio con un white para que bueno mientras se escriba mal el usuario no ingrese esos números siga y creando allí pero además con la incorporación justamente de la instrucción que nos va a permitir a trabajar con el manejo de excepciones ahí vamos entonces aplicando esto a este programa bien visto el error que justamente estamos percibiendo hace unos instantes acá tenemos la instrucción que nos va a permitir justamente capturar este error recepción y bueno trabajarlo de la manera más adecuada como hablábamos antes aquí para hacer eso justamente aparece el par de instrucciones try excepto try que quiere ser intentar en inglés dará lugar a la el bloque de instrucciones que se presume puede producir algún tipo de error en este caso los dos imputados kings este que ya sabemos que escribiendo un valor que los numéricos nos va a dar y luego a través del excel bueno hago lo que yo quiera hacer cuando veo que ese error en la mayoría de los casos voy a mirar un mensaje de error y así se terminará el programa o no le dará continuidad será un mensaje de advertencia para darle lugar a que se vuelva a intentar escribir algo si es que es un error a partir del cual se puede continuar fíjense que aquí éste lo pongo todo en un bloque en un ciclo pero estruch eso está bueno para que lo vayan es un ciclo infinito acá no hay o iluso una variable no es white una variable igual a true con lo cual después cambio el truco falls o fuerza true como sea la lógica del programa y el cual se termina acá con el cual tú es un bloque infinito la única manera de cortar ese bloque infinito es a través de break si bien esto es un uso sencillo de la de una de un taller set obviamente piensa en ellos podría preguntar si el valor es numérico y después hacer el link por supuesto y sería una forma de trabajarlo pero me parece que esto es una forma un poquito más cuidada más prolija de ver en este caso particular bien lo vamos a echar a andar el programa y vamos a ver su resultado bien aquí está los exponemos un número y ponemos una letra y nos dice bueno obviamente ese mensaje de error que los valores no son los que corresponden con los dos letras ya la primera electro perdón y ya me dice fíjense no ejecuta la segunda situación porque ya ese intento de la primera ya me manda al excel bien acá vemos entonces dos números como corresponde y ahí si le da lugar a la secuencia que sigue a este while true que se rompe por qué porque en este caso yo he puesto dos valores numéricos con lo cual se ejecuta el break el guay se rompe y pasó al otro wild el menú donde yo voy a poner en este caso que voy a elegir la operación suma nuestro resultado ese terminal clave bien ahora cuál puede ser el tema bueno hay un tema particular el respecto de la situación de la división yo voy a ejecutar nuevamente este programa y voy a poner dos valores numéricos no sé que voy a superar la instancia de este try voy a hacer algo que realmente está dentro de las reglas de lo que es esperable que sean dos valores numéricos y voy a poner un 1 y el segundo elemento hace un 0 si esto hasta que está todo bien porque justamente el intento del try próspero pero voy a tener un problema cuando elijo un tipo de operación usted ya se la imaginaba que la división sería poner el 4 y me va a dar obviamente un mensaje de error porque me dice que la división por 0 si maria bien acá tenemos otro otra situación de que amerita un 3 excel y es lo que vamos a hacer a continuación aplicándolo sobre esta parte del programa y acá está justamente como aplicamos trier para justamente lo que hablábamos recién del error que nos daba cada vez que el dividendo era un valor cero bien acá ponemos el try hacemos el print del resultado de la división que es lo que podemos presumir puede dar un error y después en el excel voy a hacer voy a decir mejor dicho que la ella no puede llevarse a cabo que revise los valores y esté bueno será la forma de salir prolijamente de este error que ha sucedido aquí por la mala introducción de los valores para poder operar matemáticamente en el caso de la división vamos a echar a andar el programa sí y vamos a estar un poquito que la terminal bien vamos a poner los números 1 2 y un 0 sí y ahora elijo división bien y ahí me dice el rol ha dicho nos pusimos a cabo revisiones valores bien y ahí se termina el programa podría haber hecho un 'wild para que vuelva a introducir este el número correcto pero bueno la idea era poder ver la cuestión de estrés no tanto el tema de aplicar en un ciclo obviamente debería englobar a este while 14 otro while que también sigue envuelto en este caso podría poner que hace un break cuando hago el print con nuestro trago no corresponde y aquí obviamente volvería a sellar es exactamente lo mismo que hicimos aquí arriba ya no es perder tiempo con eso que ya lo sabe ya lo pueden manejar y lo pueden aplicar obviamente ellas intentan hacerlo y tienen algún problema desde ya podemos comer pueden consultar por supuesto a través del campo deporte bien vamos a pasar ahora a ver el tema de manejo decepciones con otro caso ya vinculado al tema de base de datos bien acá tenemos lo que hablábamos reciente sistema de trabajar con algo relacionado con acceso a datos fíjese que yo aquí ya puse desde el vamos para hacerlo más rápido el try excel qué es lo que estoy buscando aquí lo que estoy buscando aquí es que yo intenté hacer una conexión con una base de datos e intente hacer un se les dé una tabla acá puede suceder distintos problemas primero que no me pueda conectar a la base de datos y después que el select por algún problema pueda dar un error entonces todo eso para que te lo pongo dentro de trae y después aún es dónde si pregunto si la excepción es que es un error operacional de circular 3 que es el error es el código de error que lanzará si pasan algunas cosas del amor 100 bueno a un mensaje de error y terminó el programa bien en este caso vamos a probar dos cosas en principio tenemos acá ustedes recordarán que la base si no está cuando yo intento conectarme es secular y la crea bien eso va a pasar siempre y cuando la carpeta la tema referencia exista que no es este caso pensé que la carpeta 12 no existe existe una carpeta 11 entonces lo ejecutamos ahora y vamos sobre el resultado éste justamente lo que nos sale aquí abajo es que es un error nos marca y nos manda el mensaje que hemos puesto porque se fue por el excel si bien vamos a corregirlo ahora vamos a la carpeta 11 sí bueno fantástico entonces ahora seguramente no me va a dar ese error nunca porque la carpeta 11 existe con lo cual no está la base la crea y fíjense aquí ven la base la creo pero cuál es el problema el problema perdón si la base recién la crea bueno mal poder tener una tabla dentro de la base porque recién acaba crear con lo cual justamente select asterisco son contactos es quien me está dando el error ahora y por ende vuelve a irse a la parte del excel bien obviamente que para que se supere esto debe de generar la tabla sino lo voy a demostrar para no hacer tan largo este ejemplo este bien éste sí creó la tabla dentro de la base obviamente éste select va a funcionar bien y va a terminar de serrat más allá de ejecutar este efecto all y este foro va a cerrar la base de datos con este close y va a terminar ortodoxamente por de alguna manera la ejecución este programa sin haber entrado nunca al excel ya habiendo podido ejecutar todo este bloque dentro del traje con esto cerramos el tema decepciones sobre el cual seguramente vamos a volver en la práctica que vamos a ver a continuación dónde vamos a integrar todo lo que hemos visto hasta aquí vinculado al tema de base de datos y como dije antes vamos a incorporar algunos elementos que hemos visto para enriquecer un poquito más esa práctica bien ahora vamos con lo que habíamos hablado la segunda parte de la clase el tema de poder hacer una pequeña práctica con acceso a base de datos de repasando un poco todos los contenidos que hemos visto aquí e incorporando algunas otras cuestiones que no hemos visto y que hacen a esta práctica para seguir aprendiendo cosas nuevas por supuesto bueno el propósito de esto es hacer un cruz como habíamos hablado un programa de altas bajas modificaciones agregamos algunas cuestiones también como enlistar los datos el tema de hacer una búsqueda fina y también exportar datos para repasar un poquito el tema este de los archivos de texto y lo que vamos a trabajar es sobre la base que venimos trabajando sobre la tabla que venimos trabajando que recuerda usted ustedes es la la tabla de contactos con nombre de la persona y el número telefónico correspondiente bien aquí estamos viendo el cuerpo principal del programa el que va a hacer de alguna manera el centro de todo este programa y vamos en primer término la cuestión de la conexión con la base de datos la apertura del cursor y a continuación un menú que está enmarcado dentro un bar cualquier no hemos elegido la opción del wild infinito sí o sin fin el mejor nuestro este simplemente hemos trabajado con la variable op y estamos diciendo que este ciclo simple tanto en cuanto no sea 7 bueno esto lo podríamos haber hecho un buen truco como vimos a su ratito pero bueno hemos optado para para estar otra alternativa estable una como la otra bien tenemos el menú con las opciones que veíamos recién y bueno en virtud de base a la elección del usuario de la opción se disparará la ejecución de una función obviamente que es y la opción elegida es mayor a 7 menor a 1 me dice error en varios y gracias al gwail vuelve a ciclar luego cierro la conexión y finalmente tengo el tema del try except sospechando que puede haber algún problema con la colección de la base de datos todo esto la conexión y el menú lo encierra un try la tierra de la colección también por supuesto y en el excel caería todo aquello que tenga que ver con el mensaje de error propio de algún problema que pueda surgir con la conexión de la base de datos no obstante es importante que ella que vea que yo acá estoy considerando que el único error posible que ameritó entre excepto tiene que ver con la conexión de la base de datos podría haber otro en el caso que hubiera otro bueno en lugar de poner un excel así genérico como aquí podría un excel haciendo referencia al tipo de error y basel tipo de un mensaje que quiero decir con esto el excel no solamente es capaz de receptar o ser la excepción de un error sino de distinto tipo de errores dentro del mismo drive que en este caso no pongo nada pero bueno si hubiese más de un tipo de error pondría excepto el tipo de código con su mensaje y el otro tipo de código con el otro mensaje y así tantas veces como se me ocurra cosas para poder validar bien esto luego me lleva a que en toda la parte anterior al cuerpo presión principal del programa están los las funciones sí es decir todas estas visiones amo desde aquí agregar datos mostrar datos modificar eliminar a buscar datos y exportar datos están [Música] anterior a ese menú que podemos ver más adelante que en realidad este programa armado de este modo es mejorable sí por supuesto porque no me refiero al contenido obviamente tiene muchas cosas por preparar eso no es un cruce sencillo como como elegante porque la idea es repasar y continuar pero digo toda esta cuestión desde un punto de vista más genérico de las funciones lo lógico sería que estuvieran en otro archivo que justamente proponga una forma modular de trabajar entonces concretamente estarían en un módulo y lo que yo haría aquí además de importar es importar ese q light supongamos que el módulo le pongamos gestión de datos en oregón imports de gestión de datos y accedería a cada uno de estos de que serían bueno cada uno de los métodos si de esa librería o ese archivo mejor dicho módulos y lo tomaría justamente sin necesidad de escribirlo dentro de este principal datos punto para que tengo aquí sino que el principal dato punto país sería bueno que cada uno de estos de estas funciones estos métodos justamente por haber importado el módulo que lo contiene ya lo vamos a ver está más relacionado con la cuestión propia del de la promoción orientada a objetos ya vamos a avanzar un poquito así eso vamos hablar un poco más adelante quizás en la práctica tenemos tiempo lo ponemos sino también puede ser un buen desafío para que podamos trabajar lo esté para continuar en la próxima clase bien pero centrándonos en lo importante aquí vamos entonces a la primera función de las propuestas aquí que es la de agregar datos bien la función agregar datos propone en principio un while para encerrar toda esta esta estructura dentro de algo que esté circulando en tanto en cuanto a algunas cuestiones que ya vamos a de variación no se cumplan de nuevo no hacemos un while infinito como el true sino que igual a files y le digo que mientras oxia force que justamente que juste es todo este ciclo y si lo que terminara cuando pase esto a true que va a ser justamente cuando bueno esté todas las cuestiones vinculadas al a los posibles errores si terminen o la persona ponga no como vemos aquí cuando vamos en concreto al caso y ya lo vamos a repasar paso por paso al principio bueno nada nuevo pongo un título el input de los dos elementos el número y el nombre nombre y número pregunto si cogen o la carga ción o si es si en principio verificó con es alfa es digite si los valores tanto el nombre es un valor poético y el número es un valor de tipo numérico y inmediatamente trabajo con el excepción para hacer el insert primer tema que tenemos que ver aquí primer cuestión que hemos trabajado nosotros en la clase anterior trabajamos con un insert into contacto valores con valores fijos si en este caso no lo vamos a hacer porque sabemos que eso simplemente fue lo que volcamos a la clase para empezar a introducirnos en cómo trabajar con la instrucción sql más el éxito entonces lo que representa la realidad es que lo que yo voy a insertar siempre va a ser algo diferente y va a provenir de dos variables o más depende los campos que tenga la tabla acaso la variedad en hombre la verdad número y fíjese la técnica que se usa para insertar esto que es poner un comodín en cada uno de los valores si un comodín para el nombre un comodín para el número el comodín se representa con un porcentaje ese y se pone en el caso de que sea un valor de tipo texto las correspondientes comillas y si no obviamente él es como dicen las comillas luego se cierra la comilla y se antepone un porcentaje y entre paréntesis un listado separado por coma de cada uno de los variables que yo entiendo o pretendo que ocupen el lugar donde aparece el comodín y sigue en este caso nombre será la primera de ellas seguirá el primer comodín y números en la segunda que decir al segundo comodín hay opciones parecidas a éstas en algún caso también se puede trabajar en lugar de éste el porcentaje s con un signo de pregunta se van a ver seguramente se empiezan a buscar cuestiones que tienen que ver con parte lo van a encontrar en alguna documentación y también lo mismo se pone en el listado de éste de elementos está mal ya una opción como la otra descartando algunas opciones más tradicionales que tienen que ver con el uso de otros lenguajes que pueden aún tener vigencia por supuesto que es el armado una cadena yo puedo poner insert into contacto valores y ahí en la primera comilla simple cierro la doble más nombre más si el resto a la instrucción es decir hacer una cadena como la que ya ustedes hemos la hemos trabajado con un print en algunas de estas instituciones de este tipo que hemos trabajado en ejemplos anteriores eso tiene algunas cuestiones de seguridad muy cuestionables y ningún riesgosas mucho más que esta opción que estoy poniendo aquí o la de el signo de pregunta que le comentaba antes entonces no quiere decir que no funcione se pueden usar tranquilamente porque queda todo dentro de este literal digamos no o sea que está dentro de los paréntesis y las las comillas dobles pero como recomendable no es recomendable ese tipo de formas de escribir la instrucción francia que se cuele es preferible esta hora de los signos puntos bien continuamos tenemos el comité cuando que dijimos que por cada excepción tiene que haber un comité porque decir perdón que tenga que ver con insert abdú willy tiene que haber un comité y bueno cambio la variable a true para que rompa el guay bueno me mandó un mensaje de confirmación y de terminar la historia y si los datos no son buenos pon un mensaje de error y volver a igual y si no también si pongo que no que no confirmó como obstruir para que rompa el cual pues supone que no quiero agregar más nada bien eso en cuanto a la opción de agregar datos a la opción número dos era la de listar datos sí o mostrar datos que se está que está aquí es muy sencilla lo que tenemos simplemente es de nuevo el exactitud pero en este caso es un ser casta y con contacto lo cual el resultado de este selectivo que ir a parar a la variable datos y recuerden que la variable dato se llena de de ésta del registro de la tabla contactos una vez que hago el fecha entonces lo vimos así que bueno lo que simplemente hago aquí es parecido a lo que vimos también días pasados en la clase anterior que es recorrer el data set de datos con la variable dato y bueno uno por uno tomando el dato sub zero de datos uno dado que obviamente como son dos campos yo voy a tener un vector con dos elementos el 0 y el 1 el resto es una cuestión cosmética pongo el input 9 que termina el foro para que se detenga la ejecución el usuario puede observar el listado de contactos le da enter y continuar bien luego tenemos la opción de modificar datos bueno en este caso lo que va a pedir es el nombre del contacto para que para buscarlo y bueno la búsqueda la hace con un select y acá de nuevo aparece esta cuestión de el porcentaje ese como comodín cierra la comida doble si obviamente la simple que cierran el dato desde el comodín dado que el nombre es un campo de tipo texto porcentaje nombre fíjese a diferencia del caso anterior se había puesto porcentaje paréntesis porque eran dos los valores que hacía referencia a los elementos del comodín en este caso es un solo elemento del comodín un solo elemento al que va a ocupar lugar del comodín por lo tanto no dirían los paréntesis sino que simplemente sería porcentaje y la variable y bueno se arma la instrucción de ese modo luego de este select a diferencia de que vimos aquí arriba no voy a hacer datos punto flex ol los datos mi cursor perdón punto fech won qué es esto bien page one es cuando se quiere buscar un elemento es una búsqueda puntual con lo cual puede arrojar como resultado que está o no está no va a buscar los al no buscar todos si yo pusiese un valor vamos a suponer que ahí pongo nombre pérez y además de pérez para buscar uno de ellos nos va a traer todo esto no funciona como flexfold es encuentro o lo encuentro por lo tanto me da una situación de encuentro encuentro y en el caso de que no encuentre el resultado de gato va a ser no sé entonces perdón tos indígena bueno sí entonces lo que yo hago aquí luego es preguntar si datos es diferente no es que quiere decir que es diferente nada que sí encontró algo bien ese algo lo voy a mostrar acuérdense igual que acá arriba tenía datos 0 dato 1 en este caso lo que voy a mostrar es el número de teléfono el teléfono ese segundo datos con lo cual es él dato 1 según print como el teléfono contacto 1 y le hago un input para que pongo un nuevo valor que reemplace a este que mostré aquí arriba y bueno preguntó si confirmaron la carga si digo que si bien de nuevo agregó si el número que han incorporado representa un número o li shishi y hago el ex equipo y de nuevo aparece esta forma de plantear la instrucción tracsa de sql que sería de contactos el teléfono comodín uber nombre comodín bien al igual que en el caso de lindsay más allá de que en el caso del isr los dos comodines estaban juntitos acá cada uno está en su lugar y terminada la expresión aquí con la comilla doble paso a poner porcentaje y dentro de paréntesis los dos elementos que van a ocupar cada uno de los lugares estos como dice fíjense aquí tenía nombre número porque porque nérin ser primero está el nombre después está el número aquí en el update obviamente diferente porque no tiene que ver las posiciones comodín con cómo están los campos en la tabla sino cómo se expresa la instrucción primero hasta el número y después el nombre por eso aquí el orden digamos de los campos en este caso de las variables cambios y luego el commit como corresponde y bueno el resto es un mensaje de contacto modificado sino que es un mensaje de error porque número no es un dígito y luego sí supongo que no digo que la confianza en la modificación no ha sido confirmada sea el mismo usuario que canceló la modificación o la confirmación de la modificación y el último es hace referencia a sí datos no es diferente no sé si no si es nole si él no le quiere decir que la búsqueda fue equivocada inválida por lo tanto le doy un mensaje donde dice que el contacto no existe y aquí se termina todo bueno acá lo estoy planteando una manera muy directa todas estas cuestiones van a volver al menú ustedes si quieren o simplemente de manera diferente y poner algo que yo pueda modificar modificar aplicar y poner algo está decir bueno claro si lo modifica hermana de nuevo menú o dejarlo como esta que si la persona quiere hacer cinco modificaciones va a tener que poner cinco veces canciones menos obviamente esto es algo muy rudimentario ya vamos a llegar a un programa de interfaz gráfica con otra calidad pero con los elementos que tenemos alguien esto es lo que más o menos podemos hacer para hacer un ejemplo rápido y sencillo y que podamos continuar luego de la función de eliminar datos que bueno no reviste demasiadas grandes diferencias respecto del de modificar a excepción de que obviamente acá no voy a validar si este como en el caso de modificar datos si el dato del teléfono es bueno o malo porque acá no quiero modificar el dato del teléfono sino directamente quiero eliminar el registro por lo tanto hago nuevamente un select usando el comodín y uso efecto normalmente para ver si esa y su usuario está en ese contacto perdón está o no y luego bueno imprimo el teléfono en el contacto como para completar completamente vale la redundancia todos los datos de el contacto ha buscado y con pregunto si confirma o no sí confirmó savoldelli de nuevo aquí aparece con el executive tu contacto wear nombre igual a comodín y después nuevamente aparece el porcentaje con el valor de la variable que iba a ocupar el guardés es como el cómic de confirmación mensaje de eliminación no confirmada por el usuario no puso ese puso en él y el error que marca el hecho de que bueno puedes saber que lo que yo he puesto aquí como nombre para buscar para eliminar no existe finalmente prácticamente es el anteúltimo buscar datos buscar datos es una búsqueda final de contactos no es un listado de todos los contactos o si depende como se de la la búsqueda bueno acá directamente lo que hago es hacer una búsqueda con like si por una búsqueda aproximada en una búsqueda exacta ahora cómo armó toda esta historia igual lo primero que hago es pedir un texto y pongo es esta estación el like hay que expresar lo con una palabra y un porcentaje y nada tiene que ver con el porcentaje del del comodín por supuesto es decir que si yo quisiera carbonero y el costado buscar todos los parecidos a juan el texto concretamente que tendría que aparecer donde está esto en realidad es lo que ocupa la variable texto debería ser juan porcentaje se conjugan porcentajes o el play o búscame todas las que son o contienen juan y algo más bien entonces para armar esa estructura siga esa forma de expresión lo que hago es primero hacer un strip de texto para que un strip porque yo acá cuando escribo el texto de búsqueda quizás deje un espacio en blanco con ese que streep lo que hace es sacar los espacios en blanco a la izquierda de la derecha con lo cual limpio de alguna manera la expresión de búsqueda y luego le agrego en la parte final derecha y bien pegado a la última letra de la derecha de la expresión el símbolo presenta que ya tengo mi variable armadita el texto armadita como para poder insertarla en donde en este comodín que va a jugar con el like entonces de esta manera buscó si todos los parecidos todos los light todos los como lo que yo haya escrito si en el input de tales tiempos bueno luego a un fit oleaga vuelvo nuevamente los efectos porque casi se supone que estoy buscando bueno más de una ocurrencia puede ser que sea uno puede ser que no sea ninguna así porque obviamente esto es parte de una búsqueda fina donde yo puedo que la búsqueda final me dé un resultado con todas las opciones o la variabilidad de opciones que pueda existir una vez que hago eso a un force parecida nada aquí se arriba enlistar y bueno a comprende uno emprende el otro tanto como haya encontrado este select que insisto es un número absolutamente determinado por como no saber entradas depende de lo que yo ponga como texto búsqueda cuántos registros pueden responder a ese texto de búsqueda y bueno pongo un input amor stop para que la persona pueda ver el resultado de la búsqueda y luego de darle enter continuar y finalmente exportar datos en esta opción lo que se pretende sería poder tomar íntegramente el contenido de la tabla de contactos y exportarlo a bueno un archivo se llama documento txt como es una exportación que se supone la voy a hacer desde el vamos y sin tener en cuenta nada previo o anterior es que arrancó con el complemento w a por el circuito w arrancaba un documento desde cero sino con la con el complemento a lo que hacía era apenas agregarle algo a un documento que existía y al contenido que existía dentro del en este caso es un documento que se trabaja de cero si el documento existe lo pone en blanco y le agrega lo que tiene agregado o sea no le pone lo que tiene que ponerle si el documento no existe además de eso lo crea bien y hace un select y luego un facebook y luego padecido lo que vemos aquí arriba jabón fort solamente que este foro en lugar de hacer un print va a a armar un literal y luego a poner dentro a través del método right de este el archivo archivo y grabar el archivo que identifica como ven aquí arriba a documento txt y bueno el literal que hago acá bueno es para que quede más o menos prolijo va a aparecer el primer dato luego 20 espacios y el segundo acto acuérdense que este post está separado por una coma en lugar de 20 pasos un punto y coma por eso es esa elección de la persona o el grabador en este caso al fis edad que tenga observen también que como yo intento formar aquí una cadena el dato 1 que es un número a través string lo convierto en una cadena si yo no voy a poder concatenar esto y finalmente el barra n que lo que hace es habilitar un nuevo renglón para que el próximo dato que venga en el cni for aparezca debajo del anterior bien y esto paso a paso hasta que se concluya con todos los registros de la tabla contactos con el club cerró el archivo y mandó un último mensaje que diga datos exportados para que el usuario lo conozca y eso es todo con lo cual ahora lo que vamos a hacer va a ser probar este programa para ver si todo esto que dijimos si hablamos realmente funciona bien ahora programas el programa le damos aquí ejecutar bien vamos a crear un contacto ahora y para carga me dice que si contacto agregado bien el agua y vamos a poner a un lado bien ahora voy a la opción de listar agenda ahí tengo todos los elementos bueno aquí obviamente que podríamos haber hecho una operación con ese string ese blanco que deja en el medio que este script está al lado del nombre entonces ellos juego alguna operación matemática entre el link y los espacios que yo querría dejar el nombre del número puedo hacer un cálculo para que el número no me deje no me quede digamos de esa manera desarticulada eso yo pero ustedes ya lo pueden manejar tranquilamente saben el valor de la función le sabe manejarlo y seguramente saben hacer esta pequeña el cambio para que justamente pueda quedar los números en columnas como siempre digo cualquier cosa que no se den cuenta desde ya estoy disponible el campus virtual seguimos vamos a la opción de modificar vamos a agarrar a juan y vamos a invertir los nombres de atrás ese lugar decía todo en 33 44 44 33 22 le digo que si contacto modificado si voy a listar la agenda ya veo que el libro de juan cambio bien obviamente que si quiero modificar algo y no existe me va a decir error si quiero modificar algo y existe el cosa y le digo que no así que la expresión no fue confirmada y lo vais a asombrar en eliminar lo mismo si pongo un nombre que nos corresponda no se va a eliminar si quiero eliminar el primero juan le dijo que si contacto eliminado pero lista la gente vio que juan no está más bien obviamente que se quisiera mismo decir que no confirmo no se produce la eliminación laura si así activa en la búsqueda fina teníamos a la volea lautaro por eso voy a poner como texto de bucle allow y me aparecen los dos si luego tenemos exportar datos con la opción 6 bueno simplemente elijo la opción mis datos exportados fíjese que automáticamente que yo puesto acá esta opción y me dice este mensaje automáticamente se vislumbra dentro de la carpeta practicados el documento documento txt si yo sin cerrar esto w que veo que tengo los tres registros que en este momento están dentro de la tabla contactos si bien aquí le doy entre el mensaje y no me queda más nada simplemente salir y bueno con esto pudimos ver que todas las opciones toda la programación piscina justamente de esta de este sencillo funcionamiento bien vamos a seguir incorporando cosas a este programa como dijimos al principio que si bien es un cruz sencillo vamos a utilizarlo para ir incorporando algunas cuestiones que no habíamos visto al momento una de ellas era la forma de plantear las instrucciones sql con esta estrategia de los comodines y ahora ver un poco a lo que habíamos adelantado a su ratito que decíamos que le vamos a dar más en profundidad cuando viéramos el tema de la promoción de orientado a objetos que es modular izar un poquito mejor este programa porque como ya dijimos este hoy más temprano en esta clase está todo escrito sí tanto a las funciones como el cuerpo principal del programa en el mismo documento archivo que hemos dado en llamar principal datos punto país fíjense que aquí el costado siempre visual estilo code muestra sí como una especie de miniatura en donde se ve este todo el largo del del programa no obviamente esto así no es lo más para modular que se puede hacer si lo más ortodoxo que se puede hacer como una programación bien concebida así que vamos a hacer algo muy sencillo para poder empezar a planificar esto al mal estado de una manera esté más apropiada en principio vamos a crear un archivo que se llame principal datos 1 punto pait y otro precisamente acciones datos punto pait vamos a copiar el contenido del principal datos punto país en este principal datos 1 punto pait y a este archivo resultante le vamos a borrar todo todo lo que está por encima de este comentario que dice cuerpo principal del programa excepto las dos primeras líneas que son el importe del school hay tres que lo voy a necesitar para aquí hacer la conexión y el importe o ese que lo necesito bueno por cuestiones de limpieza de pantalla si bien ahora todo el resto del programa lo vamos a pasar en un nuevo archivo que se llama acciones datos punto país todo todo el resto va a ir ahí solamente le vamos a agregar esta línea que dice importó ese porque también la necesito aquí pero para limpiar la pantalla sí entonces en principio lo que he hecho repito es dividir el archivo principal datos punto país en dos lo que es el menú propiamente dicho más el importe o es el importe s cuerda y 3 lo pongo en o lo dejo mejor dicho en este principal datos 1 punto pait que lo había copiado literalmente de principal de autor punto país y el resto más el importe se lo pongo el archivo acciones dato punto más bien como ven los datos aquí están a la misma altura porque digo esto porque qué pasa yo ahora en principal datos 1 punto país no tengo todo lo que está en acción es datos punto p pero necesito entonces que hago lo importó que es la tercera importación que lo estaba dejando un poco de lado contradecía el comentario justamente para poder explicarlo bien ahora es decir éste importa acciones punto datos me trae concretamente el contenido de todos los leds de todas las funciones que están dentro del archivo acciones datos punto país bien y una vez que se ha hecho esto que ha agregado esto procedo a ver como vínculo ambas cosas si es que simplemente yo tengo que usar las funciones de acciones datos tal como estaban o tengo que agregarle algo volvemos a probar principal dato punto pait como usaba yo las funciones agregar datos de esta manera que quiere decir esta manera sin argumentos porque porque todo lo que necesitaba negar la funcionalidad de datos está dentro del cuerpo de programa qué pasa ahora que lo tengo separados dos reacciones datos punto país y tengo el cursor y la conexión que no están definidas dentro de este programa sino de principal datos 1 punto pait entonces que tengo que hacer tengo que asumir de que esta función ahora no puede ser sin parámetro de entrada sino que tiene que tener tiene que recibir dos parámetros entrada que es la colección y el cursor que son dos variables que le pongo a ver cómo se me dé la gana aquí ha dado con el fmi con y me cura y después más adelante ustedes recuerdan que acá decía mi cursor o mi conexión bueno como yo ahora lo parámetro le puse mi con y vincule directamente uso esas dos expresiones y hago exactamente lo mismo en el resto de las funciones en este caso en nuestros datos no pasó la conexión porque no la necesito entonces vamos simplemente el cursor en modificar datos de conexión y mi cursor y kurt courtney con en eliminar datos mi conexión en mi curso sónico ni me cura bien lo mismo en buscar datos solamente la verdad el cursor y en exportar exportar datos perdón solamente la variable del curso por lo tanto hecho esto está bien a la hora de llamar a esos está esa función es si esos métodos tengo que hacerlo de una manera diferente antes de ir a agregar datos paréntesis vacío ahora es agregar datos y le pasó los dos parámetros o el parámetro dependiendo del caso si en agregar datos modificarlas de eliminarlas necesita conexión y curso en mostrar datos en buscar datos y en exportar datos simplemente el curso sea como sea le pongo entre los paréntesis de llamado de cada una de estas funciones o metros de esta de este programa bien cada uno de los parámetros de correos y le antepongo además el nombre del objeto que estoy importando que esa acción es datos que es este archivo aquí obviamente que en tanto y en cuanto por eso decía que puedes visualizar los aquí en el explorador que están a la misma altura dentro de la carpeta practicados tanto principal datos uno como acciones datos si eso no pasa hace bueno obviamente de proponerle toda la ruta si estuviese dentro de por ejemplo la carpeta 11 ya no podría poner importe acciones datos si yo tendría que salir de la carpeta practicados para meterme la carpeta práctica 11 persona carpeta 11 sí entonces éste debería poner la ruta sí para poder llegar al candidato como en este caso está en el mismo nivel no hay que poner sin más que simplemente el nombre del archivo si bien obviamente que esto más adelante vamos a poder ver que también se puede compilar y ni siquiera formar parte del mismo proyecto acá por ejemplo ese q light yo ese no está en el proyecto sí o sea el código de o s o de circulante del proyecto se importa son librerías compiladas que son los importes de caso acciones datos la que está dentro el proyecto puede no estar y ya vamos a ver que más adelante vamos a hacerlo lo correspondiente para que eso pase que es un paquete si es el módulo que se transforma en un paquete y el paquete se importa como un código totalmente ajeno siguiente de lo que es la visualización de su contenido de lo que es el proyecto bueno y esto es todo lo cerramos el programa lo probamos ahora y bueno vamos a poder ver que esté ejecutamos esto sin ningún tipo de inconveniente tal cual lo hicimos antes voy a listar la agenda veo que está la agenda puede agregar un nuevo louis que lo estaba bien ahí lo agrega a louis quien busque afín a la aparece el abogado tower todas las mismas opciones que teníamos antes exportar datos si vemos lo mismo que éste nos venía pasando con todas las cuestiones esté acá juan justamente no existe vamos a laura ingle cambio el dato bueno ahí está lista ahí aparece y salvo bueno un elemento más de todo lo que tenemos que aprender y ahora vamos simplemente es la teoría a mencionar unos métodos también que son propios de ese cura y tres y es importante que también los tenga presentes vamos a eso vienen algunos métodos más de ese cool hay 31 de ellos además del éxito que ya lo hemos visto muchos es decir hasta el cansancio execute men y qué quiere decir existir me dice para que en uno de los programas que hemos visto la clase anterior justamente cuando estábamos viendo el inserto o el update pero más concretamente en el caso del isr porque toma el ejemplo parecido al que tengo aquí en la situación pusimos tres insert uno abajo del otro y cada execute concert y después cada uno con su respectivo commit bien cuando tengo que hacer eso también tengo otra opción que justamente es el examen y fíjense qué bueno hace la conexión como ya sabemos él no solo como ya sabemos y crea aquí una lista con tres duplas donde cada tabla tiene un nombre de usuario de un email es decir que lo que está haciendo aquí es como si virtualmente está preparando una lista con tres registros para una tabla que tiene los campos nombre de usuario y correo es decir que preparó un conjunto de datos como para hacer tres insert con un solo y sharp por dichos tres inserciones con un solo instante como bueno en lugar de usar execute que me serviría para hacer solamente un isr usa execute men y con lo cual fíjense que más allá de la sintaxis tradicional donde puedan ser into you' ser los paréntesis con el nombre de los campos que eso no lo usamos nosotros no no es algo obligatorio vallières con los dos signos de pregunta que se acuerdan que habíamos hablado de que podía ser los comodines expresado como porcentaje es o bien con signo de preguntas aquí está el caso de selecciones de pregunta viene bien para que vean las dos opciones a usuarios y usuarios bueno este es el conjunto de estos tres datos que van a ser correspondidos con tres inserts si o mejor dicho tres inserciones con un solo insert bien y después obviamente hago el commit para que efectivamente ocurran las tres inserciones bien que más tenemos también este con este la próxima placa excel script bueno este script tiene una finalidad más amplia que la que vimos recién porque no es solamente el hecho de que pueda bueno hacer por una sola acción replicar en varias acciones del mismo tipo sino que acá puede ejecutar muchas instrucciones de distinto tipo si esto es más parecido a un justamente porque se llama escrita un script de sql o store procedió como se sabe llamar justamente también dentro de algunas bases de datos como sql ser por ejemplo donde fíjense que acá ya redacta todo un literal dónde está en principio es en un drop table qué es lo que hace es eliminar una tabla en caso que exista porque se estabilice existe luego crea la tabla justamente porque previamente la eliminó de la tabla con dos campos esto se lo vimos nosotros y después a su insert y civilizar no tiene nada nuevo el cliente no tiene nada nuevo porque ya eso lo hemos visto el drop dave él no lo hemos visto este pero bueno si ustedes ya manejan secuela tras secuela creo que ya conocen esa instrucción acá el secreto es que todo esto es como tres instrucciones que están dentro de una variable empieza más órdenes y luego hago una excepción script y directamente le mando la variable objeto órdenes y entonces ejecuta esas tres instrucciones sí entonces de ese modo éste se puede ejecutar esta suerte de néstor procedió esta suerte de script de instrucciones de sql con de nuevo una sola instrucción que se llevará a cabo una vez que haga el cómic como ustedes bien y finalmente algunos métodos más que es importante que los tengan presente conexión total change is con conexión total 6 yo puedo ver la cantidad en algunas este unas instrucciones propias de sql o de lenguajes similares sabe decir rose affective si es la cantidad de filas afectadas total chains y quiere decir algo parecido a decir cuántas cuatros registros fueron afectados por los cambios cambios que pueden ser eliminaciones inserciones o modificaciones bueno con eso puedo tener un estatus de si hago más que nada cuando una eliminación en base un patrón y no sé cuántos registros pueden responder a ese patrón o una actualización en base también a ese patrón no sé cuántos son en el caso analizar es más fácil porque seguramente yo se salvaba un inserto un poco especial como un inter se les esté en general este yo en mi ser puedo tener una presunción de cuantos registros pueden estar afectados en este caso de un update o un delito con un patrón de búsqueda con un huevo por el que no lo sepa por lo cual está este método me da ese resultado roll back your back es volver atrás una instrucción esto muchas veces puede pasar como vimos hoy algún tipo de error decepción o alguna otra cosa que querramos validar y se usa mucho cuando tengo como una cadena de instrucciones en el caso de que recién por ejemplo queríamos también el exceso de script y hago un cómic final entonces qué pasa si yo hago tres instrucciones y después hago el cómic no lo hago por cada instrucción yo puedo usar rollback y esto que me da la posibilidad a que si yo vamos a suponer que hago tres instrucciones en la tercera detectó algún problema rollback me vuelve atrás la ejecución de las dos instrucciones precedentes se entiende vamos a nuevo es decir tengo tres instrucciones y la tercera por decir un ejemplo me da una falla yo después que detectó que la tercera me dio una falla a un conexión rollback y la instrucción previa a esa tercera o sea la segunda y la primera se retrotraen es decir no se ejecuta con lo cual cuando tengo esta cuestión están encadenadas una otra es muy importante porque si no me puede quedar un conjunto de datos como un poco desarticulado porque algunas cosas que formaron parte de un proceso pasaron y las otras no entonces esto es una situación muy muy importante especialmente cuando tengo estos estos scripts largos de instrucciones que están secuenciados uno después y luego éste en el cursor fexme ni el cursor femenino a diferencia del face el que me trae todos los resultados de un selecto o el face one que me trae 11 net como ya lo vimos en el script del programa que vimos reciente y es para y por eso está acompañado de un site que es para decirle tomar desde una determinada posición un lote de registros y el site con ésta es justamente le digo cuál es la magnitud de ese lote la cantidad de registros por ejemplo y le digo este bueno el cursor éste en el cual yo un array donde voy a poner el resultado de esa selección parcial de algunos registros es tomar un lote digamos que tiene un tamaño y desde un punto determinado hacer que de un punto determinado que yo tomar los 20 próximos registros web of x men y puedo hacer eso bueno hasta aquí sigamos con la clase de hoy estos tres métodos lo dejo para que ustedes lo pueden investigar y probar son en realidad son muy útiles en tanto en cuanto obviamente sus lecciones muy particulares tengan este tipo de situación a problema pero siempre está bueno que lo que lo investiguen porque esas situaciones cuando menos esperamos siempre aparecen y bueno cualquier dificultad que tengan con esto de nuevo visitó como ya lo ha dicho en esta clase está disposición me contactó a través del campus virtual bueno muchas gracias y nos vemos la próxima clase\", metadata={'source': '/content/drive/MyDrive/Archivos/NLP/Modificado/contenido_combinado.txt'})]\n",
            "1596568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(documento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYCp-dowHUrI",
        "outputId": "f69a3d2a-5da1-4afb-9580-4a3d04f37702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3** Realizamos la fragmentación"
      ],
      "metadata": {
        "id": "yUD718VgD2Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fragmentacion\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter as RC\n",
        "text_splitter=RC(chunk_size=200,chunk_overlap=20,length_function=len)\n",
        "fragmentos=text_splitter.split_documents(documento)\n",
        "print(len(fragmentos))"
      ],
      "metadata": {
        "id": "Q1RZaXZxDv4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1335ea-6b28-44fe-c245-073e67de5e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fragmentos[8936].page_content"
      ],
      "metadata": {
        "id": "CQgqph2uEDTj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "30b7243e-eddb-41e6-dff7-ce2ffc44aae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sus lecciones muy particulares tengan este tipo de situación a problema pero siempre está bueno que lo que lo investiguen porque esas situaciones cuando menos esperamos siempre aparecen y bueno'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fragmentos[8935].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GDBl0p1vHnYf",
        "outputId": "cdb6c19c-5655-40aa-f348-a6593ef428b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bueno hasta aquí sigamos con la clase de hoy estos tres métodos lo dejo para que ustedes lo pueden investigar y probar son en realidad son muy útiles en tanto en cuanto obviamente sus lecciones muy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.4** Calculamos el costo o presupuesto del uso de OpenIA"
      ],
      "metadata": {
        "id": "01wsJ0oPENeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para sacar el presupuesto\n",
        "def print_embedding_cost(texts):\n",
        "    import tiktoken\n",
        "    # Obtener el codificador para el modelo de embedding\n",
        "    enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
        "    # Calcular el número total de tokens para todos los textos\n",
        "    total_tokens = sum([len(enc.encode(text)) for text in texts])\n",
        "    # Calcular el costo estimado del embedding (suponiendo $0.0001 por cada 1000 tokens)\n",
        "    embedding_cost = (total_tokens / 1000) * 0.0001\n",
        "    # Imprimir el costo total y el costo por token\n",
        "    print(f\"Total Tokens: {total_tokens}\")\n",
        "    print(f\"Embedding Cost (USD): {embedding_cost:.5f}\")"
      ],
      "metadata": {
        "id": "Nsh8Cs8GEUxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular el costo del embedding de los fragmentos\n",
        "# aca se complico por que no me toma el array de fragmentos, entonces decidi sacarlo del documento directamente el costo seria mayor por el overlap\n",
        "print_embedding_cost(documento[0].page_content)"
      ],
      "metadata": {
        "id": "4e7MZdQHEW1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e7a880-fc5a-48e9-e924-ad75a77cbdbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 1596568\n",
            "Embedding Cost (USD): 0.15966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.5** Generamos indice en Pinecone"
      ],
      "metadata": {
        "id": "TqhHBYwbEeBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conectarse con Pinecone\n",
        "from pinecone import Pinecone, PodSpec\n",
        "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))"
      ],
      "metadata": {
        "id": "7Uk4AdvkEjlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes()"
      ],
      "metadata": {
        "id": "oe1McFQqEpQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab3ce7c-b731-4466-932b-4486e0c1a4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'indexes': [{'dimension': 1536,\n",
              "              'host': 'videos-rujfu32.svc.gcp-starter.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'videos',\n",
              "              'spec': {'pod': {'environment': 'gcp-starter',\n",
              "                               'pod_type': 'starter',\n",
              "                               'pods': 1,\n",
              "                               'replicas': 1,\n",
              "                               'shards': 1}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes().names()"
      ],
      "metadata": {
        "id": "PrxKfeREEtCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbde775-382c-405c-8d99-735b904afe31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['videos']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc.describe_index(\"indice\")"
      ],
      "metadata": {
        "id": "1Vc-nBDcEwhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in pc.list_indexes():\n",
        "    print(index.name)\n",
        "    print(index.dimension)\n",
        "    print(index.metric)\n",
        "    print(index.status)\n",
        "    print(index.host)\n",
        "    print(index.spec)"
      ],
      "metadata": {
        "id": "k6ii_rkmE1FJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d94a4ffe-69b5-4b6b-9718-b8ad048e4f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "videos\n",
            "1536\n",
            "cosine\n",
            "{'ready': True, 'state': 'Ready'}\n",
            "videos-rujfu32.svc.gcp-starter.pinecone.io\n",
            "{'pod': {'environment': 'gcp-starter',\n",
            "         'pod_type': 'starter',\n",
            "         'pods': 1,\n",
            "         'replicas': 1,\n",
            "         'shards': 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Borrar todos los indices\n",
        "for i in pc.list_indexes():\n",
        "    #print(i.name)\n",
        "    pc.delete_index(i.name)"
      ],
      "metadata": {
        "id": "tANwkvBOE7NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear Indice usar (spec=PodSpec) cambio la forma de crearlo\n",
        "indice=\"videos\"\n",
        "if indice not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "      name=indice,\n",
        "      dimension=1536,\n",
        "      metric=\"cosine\",\n",
        "      spec=PodSpec(\n",
        "        environment=\"gcp-starter\",\n",
        "        pod_type=\"p1.x1\",\n",
        "        pods=1\n",
        "      )\n",
        "    )"
      ],
      "metadata": {
        "id": "usxG6ygcE_IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.6** Generamos el embedding y guardamos los vectores en Pinecone"
      ],
      "metadata": {
        "id": "HGE8ewMwFEdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "#embeddings=OpenAIEmbeddings()\n",
        "model_name = 'text-embedding-ada-002'\n",
        "OPENAI_API_KEY=os.environ.get(\"OPENAI_API_KEY\")\n",
        "embeddings= OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "RiiL7h7_FZE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75426810-8014-4742-f9ac-c404fdf89fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Subir a pinecone los vectores\n",
        "from langchain_pinecone import Pinecone\n",
        "index_name = \"videos\"\n",
        "docsearch = Pinecone.from_documents(fragmentos, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "eDjKXfmzFaCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fragmentos[1].page_content"
      ],
      "metadata": {
        "id": "96hoSUIaFeYY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2e164805-d384-4704-cc70-8c25a5ba5732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cargos Actuales: Director Académico de IFES Educación Superior, Neuquén, Argentina. Director del Centro de Formación y Empleo en Informática de IFES, Neuquén, Argentina. Cofundador de grupo IFES'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar vectores a un indice existente En caso que querramos incorporar mas datos a nuestro embedding\n",
        "from langchain_pinecone import Pinecone\n",
        "index_name = \"videos\"\n",
        "vectorstore = Pinecone(index_name=index_name, embedding=embeddings)\n",
        "texts = [doc.page_content for doc in fragmentos]\n",
        "vectorstore.add_texts(texts)"
      ],
      "metadata": {
        "id": "KBxKbiJfFlu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HEMOS TERMINADO DE CARGAR NUESTROS DATOS VECTORIZADOS**"
      ],
      "metadata": {
        "id": "tcD4LcuvLxlg"
      }
    }
  ]
}