 Titulo: Clase 27 (parte 2) Curso de Inteligencia Artificial 
 URL https://youtu.be/XLAdvjbQe6M  
 2313 segundos de duracion 
 Esta es la segunda parte de la clase número 27 te invito a empezar con [Música] ella [Música] bueno como dijimos en la primer parte de esta clase vamos a trabajar ahora con embeddings pero con un Corpus nuestro y no A diferencia de como hicimos en la primera parte de esta clase que trabajamos con un Corpus que habíamos bajado desde el sitio cer en principio vamos a usar tres archivos que tienen diferente nivel de volumen sí son diferentes niveles de tamaño yo se lo voy a estar pasando como siempre sipr a través del campus virtual y vamos a verlo rápidamente de qué se trata en cada uno de ellos bueno el primer archivo se llama chat gpt txt y es Ni más ni menos un archivo que me generó justamente esa herramienta en tanto y en cuanto yo le puse como prom que me dijera Cuál era la historia del ch gpt así que bueno me dio todo un texto que yo lo he volcado a un archivo de tipo txt que es el que le estoy pasando aquí el segundo archivo es eh el libro 20,000 leguas de viaje submarino que lo bajé en formato pdf que es Obviamente el famoso libro de julio berne y eh lo tercero es una carpeta recursos humanos que he bajado Bueno también de un sitio que se puede bajar documentos gratis 10 libros relativos a recursos humanos y este es el Corpus más grande que vamos a tener por lo tanto como dije al principio tenemos tres niveles diferentes de tamaños de Corpus volviendo a la pnl 6 vamos en principio a instalar una librería que se llama pip df2 que sirve para leer desde un documento de tipo PDF y transformarlo digamos en un formato txt o más propio del tipo de librerías que usamos nosotros para el tratamiento de los textos es decir que instalamos esa librería y luego vamos a importar la librería String hensim models del cual vamos a sacar el Word to que es el que nos va a permitir bueno hacer la transferencia de palabras en vectores y finalmente vamos a importar la librería que recién acabamos de instalar vamos con ello y luego como siempre montamos El colap Bueno allí tenemos ya activado el Google Drive donde vamos a acceder como dijimos antes al archivo de chap PT al archivo de 20,000 leguas de viaje submarino y a la carpeta con los 10 archivos de recursos humanos nuestro primer trabajo va a ser con el archivo más chico y vamos a ir gradualmente pasando a los de mayor tamaño con lo cual vamos a hacer aquí un Open del archivo chat gpt que yo lo tengo dentro de una carpeta nlp dentro de archivos dentro de mi Drive ustedes siempre le pueden poner aquí la ruta que corresponda al lugar del Drive donde ustedes hayan montado este archivo luego le decimos que lo queremos sair solamente para lectura y este encoding es uno de los encoding que tenemos como opciones para poder bueno evitar algunas cuestiones que tiene que ver con el formato en que nosotros traemos el archivo txt Bueno no es el único hay varios después les puedo dejar todas las opciones que hay no siempre este va a funcionar para todos los archivos de tipo txt con lo cual esta tarea de ubicar el encoding inapropiado es una de las tareas que siempre tenemos que ver más que nada cuando abrimos archivos perdón de tipo txt Bueno cuando leemos el contenido de este archivo chat gpt txt lo vamos a volcar a través del F read en un elemento un objeto que se va a llamar documento lo ejecutamos e inmediatamente vamos a ver qué tiene ese documento documento que no va a tener otra cosa más que todo el documento chat gpt.com eh en documento está todo el documento completo Si recordamos lo que vimos hace un rato se trata de este documento que está acá sí obviamente como dijimos recién solamente por eh el límite que tiene de visualización El cola me muestra una parte Pero dentro de la variable documento está todo este texto este Corpus que está dentro de este archivo que hemos puesto o le hemos dado a llamar chat gpt txt bien una vez que hicimos esto vamos a medir el largo del documento el largo del documento no es Ni más ni menos que ver Cuántas palabras tiene el documento es decir con len de la variable documento voy a ver Cuántas palabras tiene la variable documento hecho justamente en base a chat gpt txt Así que literalmente como que dijera Cuántas palabras tiene este archivo chat gpt txt lo ejecuto y me dice que tengo 31168 palabras Bueno lo que tenemos que hacer a continuación es el preprocesamiento de datos ya sabemos que los datos como vienen en un solo lote no podemos procesarlo sino que tenemos que hacerlo de al lotes Sí entonces lo que se hace primero en estos casos es separar todo el Corpus en oraciones ahora cuál es el criterio para separar o para darle a entender a esta inteligencia hasta dónde llega una oración y dónde empieza la otra bueno se puede hacer con diferentes este tipos de puntuación habitualmente con las comas y los puntos eh En este caso vamos a usar la coma porque en algunos casos de algunos Corpus el punto eh representa oraciones demasiado largas y quizás nos convenga que sea un poco más corto y la coma siempre representa una entidad que tiene un sentido también los Corpus se pueden dividir en oraciones que tengan una determinada cantidad de caracteres eso también es posible lo que pasa es que en ese caso le quitamos la entidad a la oración una oración si yo le pongo que lo corte cada 100 cada 100 palabras Eh quizás en esa eh palabra número 100 No termina de representar un sentido semántico simplemente es una cantidad de 100 palabras bueno eso hay que tenerlo en cuenta porque el entrenamiento tiene que basarse en que entienda esa inteligencia el sentido de lo que está escrito y a partir de ello empieza a hacer todo el tipo de actividades que ya vamos a ver qué podemos hacer con ello Así que en este caso vamos a tomar como símbolo de split o como elemento para separar oraciones la coma lo hacemos ponemos ahora todas las oraciones del documento separadas por comas dentro de una Ray que le voy a llamar oraciones y eh imprimo e Perdón imprimo a través del l la cantidad de elementos que tiene ese Ray y veo que tiene 200 elementos con lo cual este Corpus que tiene 31168 palabras ha sido dividido en 200 oraciones si podemos ver eh cualquiera de ellas simplemente bueno como es una Ray colocando colocándole Perdón aquí el número de ítem que quiero ver sabiendo que puedo ir de 0 a 199 porque tiene 200 oraciones y voy a tomar el primero de ellos el cero y lo vamos a imprimir porque así puedo chequear justamente si funcionó bien el concepto de splite a través de la coma Por qué Porque me fijo que todo este texto termina en amente el desarrollo de chat gpt si voy al texto original veo que justamente en esa parte está la coma con lo cual la primera oración que ha identificado es toda la que precede a la primer coma y por supuesto en todo el resto de los elementos de la Ray con el mismo criterio está haciendo Exactamente lo mismo con lo cual veo que en principio con esta simple observación veo que está funcionando bien este este sistema de tokenización de splite a través de el uso de la coma la siguiente tarea que vamos a hacer va a ser la de limpiar las oraciones por qué miremos un poquito aquí la primera oración la cero que pusimos como ejemplo y ya estoy viendo que por ejemplo aparece un barra n que representa un enter Sí este igual es un documento muy sencillo De hecho ya viene de parte de skpt Que obviamente da un texto muy este muy accesible pero vamos a ver más adelante que los textos vienen muchas veces con un formato muy difícil de tomar Así tal cual está en algunos casos como en el caso que vamos a ver por ejemplo de documentos de tipo PDF bien hasta con imágenes y las imágenes obviamente es un elemento que no debe procesarse y debe quitarse cuando se transfiere de PDF a texto pero eso ya lo vamos a ver un poquito más adelante Lo importante es entender que siempre el texto del Corpus tiene elementos extraños que debemos limpiar Pero además también queremos quitar todos los elementos de puntuación es decir que los puntos las comas u otros elementos similares también van a ser quitados Por ende tengo que con el array oraciones crear un nuevo Ray que se llama ahora que se puede llamar como seaas yo le he puesto oraciones limpias Pero bueno Al fin y al cabo un nuevo Ray que tenga como su nombre lo indica aquí oraciones que estén limpias de todos esos tipos de elementos que acabo de mencionar recién por lo tanto creo nueva Ray y voy a con este for recorrer todos los elementos de la Ray oraciones como for oración y oraciones es decir tomando cada uno de los elementos de esta Ray Oraciones al tomar una oración cualquiera lo que voy a hacer es con el método Translate y con el método make trans d str voy a hacer una operación de división pero a la vez o de tokenización Mejor dicho pero a la vez quitando todos los símbolos de puntuación justamente esta este método mtr lo que me permite es reemplazar un elemento con otro elemento y en el tercer argumento de esa función le puedo decir Cuáles son los elementos que quiero eliminar en este caso particular yo no voy a usar ni el primer argumento ni el segundo porque no quiero reemplazar un elemento con otro sino solamente quiero la tercer parte de esta función o el tercer objetivo que puede cumplir esta función que es justamente quitarme todos los elementos de puntuación con esto Entonces quito todos los elementos de puntuación de cada una de las oraciones y con split simplemente lo que hago es dividir cada una de esas oraciones en palabras Es decir aplicando el concepto de tokenización que ya vimos antes una vez que tengo todos los tokens dentro de la r tokens voy a hacer el proceso de quitar de ese contenido todos los elementos que no sean de orden alfabético y además los paso todo a m esto ya lo hemos hecho en las las clases anteriores con lo cual no aporto Nada nuevo con lo cual repasamos aquí quito los símbolos de puntuación y splite para tokenizar la oración aquí entonces tengo cada una de las palabras luego con cada una de las palabras lo que hago es pasarlas a minúscula y sacarlas o no incorporarlas dentro de la Ray oraciones limpias Perdón que estoy creando si no son caracteres de tipo alfabéticos bien una vez que logro ese objetivo pregunto If tokens por qué porque no va a ser este caso pero ya vamos a ver más adelante que puede ser que luego de este proceso de limpieza de las oraciones una oración esté conformada enteramente por elementos que no quiero con lo cual es probable que yo tenga una oración menos del total de oraciones que tenía originalmente Por eso pregunto de la limpieza que hice En esta oración quedaron tokens Bueno si quedaron tokens quiere decir que tengo elementos para agregar a el array oraciones lipas bueno dada esta explicación lo ejecutamos en virtud de lo que decía recién voy a volver a hacer un len de la cantidad de oraciones por si llegase a dar el caso de que hubiese alguna oración que luego de ser eh tokenizado Y bueno con todos estos pasos que vimos anteriormente sacándole el símbolo de puntuación este y eh quitándole caracteres que no son alfabéticos pueda eliminarse de El la cantidad original que había de oraciones pero veo que tengo en oraciones limpias 200 oraciones como tenía en el array anterior que no estaba procesado no estaba limpiado y depurado con lo cual no he perdido ninguna oración esto se ve claramente porque insisto es un texto de charg PT no cabía otra posibilidad que fuera eso vamos a ver cómo quedó así como hicimos antes se acuerdan aquí arriba que miramos Cómo era oraciones subcero ahora voy a ver cómo es oraciones limpias su cero lo imprimimos y veo que tengo el mismo array de antes sí ven el contenido este que está acá sí el primer elemento pero ya no aparece como todo un texto conjunto como está aquí todo un texto conjunto sino que aparece tokenizado es decir palabra por palabra separada sí sin elementos que no sean de tipo alfabético y sin ningún tipo de símbolo de puntuación ni puntos ni comas sí bien ya tengo entonces eh este texto procesado con lo cual lo siguiente que tengo que hacer tal cual dice Este título que está aquí es tomar ese texto para entrenar un modelo con Word tob es decir tengo que pasar todos estas estas oraciones que han sido depuradas y limpiadas a formato de lector bien Ahora vamos a entrenar Entonces el modelo Word tob con lo cual vamos a crear una variable model como la hemos usado muchas veces para crear Este modelo y vamos a aplicar el método Word to Back para ponerle todos los parámetros con los cuales queremos que efectúe esta tarea de transformar palabras en vectores en principio de dónde tiene que sacar la información de las oraciones para hacer eso obviamente de la Ray que cargamos de crear oraciones limpias a través de el parámetro sentesis pero existen otros parámetros que en este caso lo vamos a ir eh Bueno aquí le he puesto una ayuda para que quede constancia de esto pero lo vamos a explicar rápidamente el primero esos es muy fácil de de entender le estoy diciendo que todos los vectores que me generé Word tob con este proceso de embedding que voy a hacer ahora todos los vectores tengan dimensión de 500 esto obviamente es un parámetro que ustedes pueden manejar en virtud de la dimensión del lenguaje que tienen luego tengo window que es para darle contexto acuérdense que en este caso es muy importante que la se entienda que la palabra no es un ent aislado que para poder entrenar esta inteligencia esa palabra tiene que estar dentro de un contexto con Windows le digo Cuántas palabras antes de la palabra que estoy procesando Y cuántas eh palabras después debe entender que forman parte del contexto de esas palabras luego con mincom le puedo decir cuántas veces tiene que aparecer una palabra para que pueda ser considerada si puede ser tenida en cuenta o no para entrenar el modelo con uno le digo que cualquier palabra Que aparezca obviamente va a aparecer una vez ya es lo mínimo que puede aparecer va a ser tenida en cuenta y finalmente workers 8 indica esto es una cuestión muy técnica si ustedes tienen la posibilidad de contar con un cpu que haga procesamiento en paralelo bueno con el ocho le pueden decir que están utilizando ocho núcleos para que trabajen en paralelo bueno Esta es una cuestión insisto muy técnica y hay que ver si Más allá de la de que sea una cuestión técnica ustedes tienen un equipo que tenga es esa posibilidad perdón de procesamiento dicho todo esto ejecutamos Esta instrucción y ya qué tenemos en model en model tenemos ahora todas las oraciones limpias transformadas a vectores y esto lo puedo ver justamente poniendo a través de este método wb corta una palabra cualquiera chat gpt que estoy seguro que está obviamente esa palabra Entonces le voy a decir que me muestre Cuál es el vector de la palabra charg PT H Entonces lo voy a poner dentro de esta variable vector para después poder imprimirlo Sí ahí me cargo el vector de esta palabra en la variable vector y luego voy a imprimir el vector vector h y voy a ver que en el caso concreto que estamos viendo aquí vamos aaro un poquito para verlo mejor s tengo que la palabra chat gpt está representada por todo este array que tiene cuántas tiene 500 dimensiones dado que cada uno de los vectores con que está representado el vocabulario de todo este Corpus en particular en este caso de la palabra chatp como una de las palabras tiene o está representada insisto por un vector de 500 posiciones esto es a título de curiosidad para que se entienda Cómo ha hecho esta transferencia este Word tob Sí este sistema o Esta técnica de Word to que recuerden como vimos en la teoría existen además de glob hay dos métodos muy conocidos para ello Nosotros hemos utilizado a los fines de este práctico Word luego una vez que ya tengo eh cargado todo este conjunto de vectores entiendan lo siguiente Estoy parado en la misma situación que la que estuve al principio del lab anterior del lab 5 por qué Porque el lab 5 yo lo que hice fue traerme todo un vocabulario que ya estaba vectorizado es decir que ya tenía todo este trabajo que hemos hecho aquí desde que empezamos este lap hasta ahora yo ya lo tenía resuelto cuando En el laba anterior me traje ese archivo en este caso el archivo lo generé yo entonces ahora estoy en condiciones de hacer lo mismo que hicimos En el lab anterior es decir poner una palabra y que me di Cuáles son las similares a esa palabra entonces justamente Aquí está el método que usamos recién m similar y le voy a poner chat gpt y le voy a decir que me muestre las 20 palabras más parecidas sí lo pongo dentro de una variable de palabras cercanas e imprimo el contenido de palabras cercanas vamos con ello y Aquí tengo las 20 palabras más cercanas de hgpt siempre van a ver que van a aparecer entre las eh primeras seguramente fundamentalmente cuando es un cuerp muy chico esto ya se va a ir depurando y a lo largo de este práctico que vamos a ver que hay Corpus más este más grandes que las palabras que van a aparecer al principio son las palabras comunes que no es que tengan una implicancia directa con ch gpt Pero obviamente son las palabras más utilizadas que tienen más relación no con ch gpt sino con cualquier palabra que esté en el Corpus y al final aparecen palabras Un poco más propias lenguaje medida y a aplicaciones y respuesta hago lo mismo con red y Bueno aquí hay una excepción apareció la primera variedad HM pero después aparecen palabras más comunes pero sí después a diferencia de la anterior aparecen ya eh más palabras que no son dan de uso genérico como máquinas uso aplicaciones luego ia luego avances lenguajes garantizar siempre fíjense que aparece un índice de score que lo vimos también en el lab anterior que es la palabra que tiene más eh concurrencia con eh Esa palabra que yo he puesto aquí más asociatividad más similitud bien y finalmente lo hacemos También con otra palabra máquinas y vemos que tenemos Bueno aquí la experiencia es similar a el primer caso me parecen siempre palabras muy comunes hasta que luego aparece medida chat gpt respuestas Y a y lenguaje ahora volvamos a lo que hablamos ha un rato respecto de que yo les decía que en el lab anterior habíamos tomado un archivo que ya estaba vectorizado Y en este caso el archivo lo tuvimos que hacer nosotros resulta que cuando yo hago mi propio archivo no es que lo tengo que procesar permanentemente ente yo puedo hacer ese archivo y guardarlo para que después si lo quiero volver a usar no tenga que volver a hacer todo lo que hicimos hasta ahora y cuando yo cargo ese archivo que lo hice yo va a ser lo mismo que lo hicimos Perdón que lo que hicimos En el lab anterior cuando cargamos un archivo que no era nuestro Pero no importa en realidad yo estoy cargando un archivo que ya está vectorizado el origen es lo de menos obviamente en este caso va a ser el origen un archivo que hice yo y en el caso del lado anterior un archivo que lo tomé desde C para hacer eso es importante que veamos aquí que tengo un método save que me permite guardar mi archivo de vectores con extensión punto model y luego voy a hacer lo inverso que es bueno volver a traerlo como si nunca lo hubiese generado en este propio momento sino que ya hubiese estado generado de antes y vuelvo a recurrir a él como con Word to lad lo vuelvo a cargar y lo cargo En una variable que se va a llamar modelo cargado bien Ahora yo no voy a trabajar ya con model voy a trabajando antes sino que voy a trabajar con modelo cargado usando el mismo método mod similar es decir voy a hacer Voy a simular la situación como dije recién que es un modelo que yo no hice ahora sino que lo tenía de antes y ahora recurro y lo cargo insisto igual que como hicimos con ese archivo que no nos pertenecía en el Lama anterior bien a partir de que tengo Esto bueno ejecuto y veo que el resultado que tengo por ejemplo para para chat gpt exactamente el mismo que el que tuve con el modelo que fue el que generamos ahora mismo sí de este modo se pueden dar cuenta que el modelo va a actuar de igual manera si yo lo proco el mismo momento o si me lo traigo luego de un tiempo para utilizar bien vamos a hacer ahora el mismo recorrido pero con un libro en este caso 200000 leguas de viaje submarino libro de Julio berner es decir este libro que muestro Aquí bueno que es un PDF y que tiene Bueno una cantidad bastante integr Grand de 327 páginas Sí bueno no tiene muchas imágenes Sí al principio lo que les comentaba hoy pero seguramente no va a ser un texto tan depurado como en el caso de que utilizamos en el perdón en la práctica reciente este documento tengo que preprocesal como siempre pero en el caso particular insisto no puedo tomarlo como el el caso del ejercicio anterior porque no es un archivo de tipo texto yo tengo que transformarlo ahora en un archivo de tipo texto Sí para ello voy a crear una función que le voy a poner extraer texto desde PDF al cual le voy a pasar la ruta del archivo y el nombre del archivo por supuesto y voy a retornar el texto completamente eh diferente al formato original más propio de el formato que vimos Recién con el archivo gpt txt bien el hecho de hacer una función siempre tiene que ver con una cuestión de practicidad lo pueden poner como un código completo sin que esté en el formato de una función desde ya bien lo que voy a hacer en principio con el Wi Open es abrir el archivo y una vez que abro el archivo voy a ejecutar el método PDF reader de la librería que incorporamos para leer justamente contenidos de archivos tipo PDF de el archivo que Acabo de abrir y voy a poner todo ese contenido dentro de una array lector con lo cual Dentro de este lector dentro de esta variable que va a ser el tipo array lector voy a tener Ni más ni menos que todas las páginas de este archivo es decir del libro que acabamos de mencionar recién y luego vamos a crear un String donde voy a tener todo ese contenido de este array pero en un formato de texto parecido insisto al del ejercicio anterior para ello voy a hacer un for y voy a recorrer justamente a través de lector pages sí toda la cantidad de páginas que tiene lector la cantidad de páginas que tiene El lector es la cantidad de páginas que pude traerme desde el archivo de 20,000 leguas de viaje en sumaría bien cada vez que tome cada una de esas páginas por eso por página in Range Sí o sea tant tantos recorridos tanto forer como páginas tenga ese documento lo que voy a hacer Voy a aplicar el método stract text de lector pages su página es decir voy a tomar la página cero y voy a extraer el texto y lo voy a poner en la variable texto en el siguiente Ford voy a extraer la página número uno voy a extraer el texto y lo voy a ir acumulando en esta variable de texto con lo que vengo trayendo desde la páginas anteriores a fin de que ejecute esta función pues entonces en texto tendré todas las páginas una al lado de la otra dentro de una variable con un formato similar insisto al tipo txt que usamos antes bien para ejecutar esta función que la cargo con esta con esta con este rank que le voy a poner ahora voy a colocarle aquí justamente el uso de esa función y lo voy a poner dentro de una variable documento y le voy a mandar a esa función Ni más ni menos que toda la ruta y el nombre del archivo que voy a usar bien una vez que cargué todo voy a ver qué tiene la variable de documento y veo no me voy a sorprender tengo todo el contenido del libro Pero obviamente como siempre me muestra los primeros la primera parte del documento fíjense que aparecen ya muchos más eh mayor cantidad de caracteres extraños que los que tenía en el caso del documento anterior obviamente era mucho más sencillo y mucho más chico bueno Esto es importante que lo tengamos presente porque ahora vamos a ver primero Cuántas palabras tiene este documento y veo que tiene 86949 palabras y a continuación lo que voy a hacer es dividir ese documento como hicimos antes en oraciones también con el mismo símbolo de coma lo ejecuto y voy a poder ver que tengo en este caso en base a esa cantidad de palabras 10729 oraciones recuerdan el caso anterior que tenía 31,18 palabras y tenía 200 oraciones Bien voy a ver también como lo hice antes el contenido de la primera oración Bueno aquí está el contenido de la primera oración y voy a ver otra por ejemplo la 221 y ahí tengo ejemplos de distintas oraciones obviamente la medida que yo le ponga un un número aquí Perdón que esté digamos dentro del Rango de este de esta cifra Bueno me va a mostrar el contenido de esa oración lo que vamos a hacer ahora es el mismo proceso de oraciones limpias que hicimos en el ejercio anterior Así que no hay mucho que explicar aquí simplemente lo ejecutamos y voy a ver ahora que la cantidad de oraciones limpias que tengo es 10720 Qué pasó Aquí ya no es lo mismo que en el caso anterior se acuerdan que le dije que en este proceso de oraciones limpias podía darse la situación de que tenga oraciones que estén íntegramente conformadas por caracteres no deseados con lo cual esa oración deja de existir y es lo que ha pasado aquí fíjense que yo tenía originalmente 10,700 estas 29 oraciones y ahora oraciones limpias tengo exactamente nueve menos si ahora miro oraciones limpias con el índice 221 que es el que miré antes sí este que está aquí lo voy a ver pero en un formato de oraciones limpias justamente donde están las palabras bien tokenizadas y sin ningún carácter extraño luego de esto lo que voy a hacer es entrenar un nuevo modelo al igual que como lo dice antes con los mismos valores de los parámetros Sí con vectores de 500 buen Exactamente igual y empiezo a buscar palabras similares como eh en el contenido de este libro refiere a cuestiones de un viaje submarino la primer palabra que voy a usar va a ser mar y fíjense que ya no aparecen palabras de uso general como antes salvo del h es muy excepcional la mayoría de las palabras tienen que ver específicamente con el tema en sí que es mar nautilus que era el nombre de submarino fondo su rojo bueno Asia también es una palabra uso general salón interior Horizonte océano agua cielo golfen pruebo con nautilus que insisto es el nombre Cómo se llamaba el submarino del cuento sí bien mar dirigió Sur Horizonte Polo salón rojo bueno etcétera etcétera uso una más Nemo que era el nombre del capitán del submarino bueno fíjense que casi no existen palabras que sean o representen uso general si aparecen algunas palabras que hay que ver después bien cómo ha sido el splite más que nada con los enters muchas veces puedo tener eh palabras que están por ejemplo este hijo que está acá o este guo evidentemente es parte de un enter que en el cual yo eh está dividido una palabra que no es lo usual sí supone que el enter tiene que ser que la palabra quedó en la línea anterior o pasó a la próxima bueno son parte de las cosas que hay que hay que depurar pero evidentemente ya tenemos una evolución en cuanto a la similitud de palabras respecto de el caso del ejercicio anterior bien y la última parte de este lap sería con el caso más complejo donde tengo 10 libros de recursos humanos en formato pdf lo primero que voy a hacer va a ser instalar la librería tqdm la cual me va a dar la posibilidad de este proceso que va a ser un poquito largo que voy a llevar a continuación me muestre una barra de Progreso y no solamente eso sino que me pueda dar información respecto de En qué nivel de tiempo lleva ese proceso y cuánto me falta para poder llegar al final luego voy a importar Eh bueno estas librerías algunas ya las importado antes esto importa Porque evidentemente queremos contextualizar cada ejercicio a sí mismo por más que a lo largo de este colab ya lo hemos hecho obviamente cada uno de ustedes sabe perfectamente qué tiene que volver a cargar y queé no con lo cual cargo os el librería que me sirve para leer los PDF y esta librería que acabamos de instalar lo que vamos a hacer a continuación es si una función de extracción del texto desde PDF que es Exactamente igual a lo anterior de nuevo la vuelvo a cargar a los fines de contextualizar esta esta parte del práctico pero no sería necesario hacerlo y luego lo que voy a hacer ahora sí que es diferente es poder ir Armando todo este gran Corpus ya no leyendo un documento como en los dos casos anteriores porque Más allá de que uno era txt y otro era PDF igual era un documento sino que tengo que recorrer un conjunto de documentos que está en una carpeta e irlos incorporando uno por uno por eso en principio voy a crear una matriz que le voy a poner todos los textos y voy a recorrer for archivo en tqdm de os listdir Comando que ya lo conocemos muy bien porque me da todo el contenido de una carpeta de ruta carpeta Qué es ruta carpeta esta ruta que les estoy mostrando aquí arriba sí acuérdense que esta carpeta rr hh Perdón es la que yo le pasé por el campus y el resto del pas tiene que ver con lo que yo establecí dentro de mi Drive Y ustedes tienen que contextualizar de acuerdo al a lo que ustedes tengan en su propio Drive Bueno luego lo que hace es pregun ar si el archivo que encuentra dentro de esa carpeta termina en PDF Entonces lo tiene en cuenta y arma la ruta completa y tra Acuérdese que tra es intento intenta tomar todo el contenido de ese archivo que está en PDF y ponerlo dentro de estab de documento es igual a lo que hacíamos antes solamente que ahora lo estoy recorriendo en el contexto de un Ford por eso en este caso necesito este aray todos los textos para juntar cada uno de los documentos cada uno de los contenidos de los 10 documentos que es lo que estoy haciendo aquí con el apen es decir tomo cada uno de los documentos y los agrego a este array todos los textos luego le pongo un excep por el caso de que la apertura de algún documento pueda generarme algún tipo de error como usamos el tqdm voy a ver la barra de Progreso de este proceso que va a ser como dije antes un poquito largo bien una vez terminado este proceso voy a ver el largo que tiene este Ray Que obviamente tiene que ser 10 porque había 10 documentos constato que eso es así y bueno ahora también tengo que hacer un proceso diferente respecto de lo que hacía antes antes ejecutaba solamente esta instrucción Qué pasa ahora obviamente tengo como dije antes una Ray Por qué Porque tengo 10 documentos entonces voy a crear una aray oraciones totales y voy a crear una variable tamaño más que nada a título de curiosidad porque tengo que determinar como lo dice antes la cantidad de caracteres totales que tengo o sea en este gran Corpus conformado por 10 documentos con lo cual voy a hacer un for voy a tomar for documento en todos los textos Sí todos los textos acuérdense que es lo que tengo aquí el contenido de cada uno de los 10 documentos y lo que voy a hacer en principio es tamaño igual a tamaño más len de documento es de cada uno de los documentos contabilizando Sí el total del Corpus tomando los len de cada uno de los documentos luego hago el splite del documento en concreto que estoy tomando sí es del documento 1 2 3 cu Sí cada uno de ellos lo pongo pongo oraciones y ese oraciones lo voy a colocar dentro de El array oraciones totales en este caso como bien dice aquí la explicación No puedo usar apen porque oraciones es una ar rray entonces voy a agregar una rray a otro ar rray se usa extend y no apen bien ejecuto esa [Música] instrucción y ahora vemos Cuántos caracteres tiene este gran Corpus tiene 5,1666 palabras las cuales están separadas en 7049 oraciones a continuación hago el mismo proceso de limpieza de operaciones con el mismo aray que le vuelvo poner el mismo nombre de oraciones limpias y voy a ver cuál es el largo de oraciones limpias que tengo ahora y veo que tengo 37904 oraciones vamos a medad del original tenía 7049 y ahora tengo poco más de la la mitad en este caso el proceso de operaciones limpias ha sido muy eh significativo Porque ha eliminado un número muy importante de oraciones que como dijimos en otras oportunidades porque tienen solo símbolos de puntuación o solo elementos que no son alfabéticos eh Son eliminadas digamos de esta de este proceso de limpieza de oraciones es decir que del total del Corpus que tenía 7049 oraciones tenía bueno unas 33,000 y un poquito más oraciones que en realidad no me servían y no me aportaba nada al Corpus esto es muy bueno porque lo que voy a hacer ahora es una vectorización con las oraciones que realmente sirven al igual que como lo hicimos antes vamos a mirar un determinado elemento de ese radio de oraciones limpias vamos a tomar el elemento 36571 Pero obviamente puede ser cualquiera lo ejecutamos y vemos aquí bueno todo un conjunto de eh caracteres de palabras que forman parte de la Ray número 36571 bueno Esto pues ustedes pueden poner el nomo que quiera y pueden ver justamente el resultado de las oraciones limpias esto es tal cual lo que hemos hecho antes por eso tal cual lo que hemos hecho antes también es la creación del modelo y ejecutamos la misma línea de código que venimos usando en los dos casos anteriores Y ya tengo mi modelo Que obviamente va a tardar un poco más ahora pero lo voy a volver a probar como lo hicimos anteriormente bien terminado este proceso vuelvo a hacer lo mismo que en los casos anteriores en cuanto a ver palabras similares y elijo empleado al principio ahora voy a elegir top 10 porque ya este Corpus es más grande y fíjense que no hay ninguna palabra que sea de tipo de uso general artículos el la d que veíamos más que nada en el primer caso y un poco menos en el segundo caso en este caso no aparece y justamente esto eh de que aparezcan primero las palabras Eh más generales hacía que yo pidiese que el top n fuera 20 para poder ver Bueno más adelante para ver que sí tuviesen más que ver con la que yo utilizaba aquí como referencia en este caso puedo reducir justamente a 10 porque ya insisto las palabras que son de uso general no aparecen hago lo mismo con la palabra trabajo y bueno vean que aparece puesto bueno acá aparece de nuevo una palabra pegada que son cosas que hay que depurar puesto lugar empleo laboral valor relativo cargo equipo comportamiento eh F aporte supervisor currículum desastre hay que ver porque desastre instructor movimiento bueno Esto lo dejo para que ustedes aquí con humano también puedan este bueno poner la palabra que ustedes quieran y puedan practicar y ver qué sale competencias bueno y insisto ya ustedes pueden poner la palabra que quieren Bueno un último dato que les dejo aquí al final de esta clase que ha sido bastante larga es este resumen interesante para poder comparar justamente los tres Corpus que hemos usado en este lab en principio en el caso de chat gpt txt la cantidad de caracteres que generamos fue 31168 200 oraciones y 200 oraciones limpias ya cuando pasamos a un documento que tenía 327 hojas recuerden pasamos a 86949 caracteres 10729 operaciones y una diferencia solamente de nueve cuando hacía el proceso de limpieza de operaciones de oraciones perdón y luego en el caso de los 10 documentos de recursos humanos ya tengo bueno una cantidad de caracteres mucho mayor prácticamente Eh sí casi seis veces un poquito más de seis veces respecto del libro de 20,000 leguas de viaje submarino 70.000 oraciones pero una reducción como ya hablamos antes casi de poco menos de la mitad de oraciones limpias con lo cual me quedaron 37904 en todos los casos Recuerden que tomamos la postura de cuando creamos el modelo usar vectores de 500 posiciones si bueno Esto también es un tema a ver porque en algunos casos puede ser que sea exagerado y en otros casos puede ser que sea prudente y no sirve en realidad esto es un parámetro que nosotros tenemos que manejar y podemos ver también a la hora de analizar el modelo que tenemos Si es el valor ideal bien con esto terminamos esta clase nos vemos en la próxima clase Hemos llegado al final de esta clase nos vemos en la próxima clase [Música] foreign