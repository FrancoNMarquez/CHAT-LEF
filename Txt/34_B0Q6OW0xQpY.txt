 Titulo: Clase18 (parte 1) Curso Inteligencia Artificial 
 URL https://youtu.be/B0Q6OW0xQpY  
 1046 segundos de duracion 
 Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número [Música] [Música] [Música] 18 [Música] Hola a todos Bienvenidos a la clase número 18 del curso de Inteligencia artificial de ifes donde vamos a seguir con redes neuronales convolucionales dentro del marco de este módulo de Deep learning en principio el propósito de la clase de hoy es resolver algunas cosas que nos quedaron pendientes de la clase pasada cuando decíamos que teníamos un modelo con sobreajuste y hoy vamos a ver algunas técnicas para mejorar y superar ese problema por eso inmediatamente pasamos al colab a empezar a programar justamente estas soluciones de las que estábamos hablando bueno Y aquí estamos en el colap de la clase 18 y el título justamente de este Notebook hace referencia a lo que decíamos recién en principio cuando tenemos un problema de sobreajuste tenemos dos opciones para superar Uno de ellos es el aumento de datos Y el otro el Drop out Pero antes que nada vamos a empezar como siempre por usar un dataset para justamente demostrar todo esto que estamos diciendo y vamos a volver a cargar el dataset emist como ya sabemos que lo venimos haciendo habitualmente Así que empezamos por eso importando también las librerías como siempre lo hacemos Ahí está cargado verificamos sí que tenemos la cantidad de observaciones que ya conocemos que tiene este dataset 60,000 observaciones En el conjunto de entrenamiento y 10,000 observaciones En el conjunto de Test Y corregimos de paso aquí que est mal escrito Así que lo ejecutamos y seguimos Sí luego lo que también hemos este de alguna manera explicado en las clases anteriores que necesitamos reformatear para que justamente el input se adapte a lo que solicita una red de tipo neuronal convolucional que es hacer un resave para convertir la entrada en un formato de tipo tensor Recuerden que el tensor tiene cuatro elementos el primero de ellos describe la cantidad de observaciones el segundo la altura de la imagen el tercero el ancho de la imagen y finalmente el canal de colores aquí tenemos con el reshape len de X3 que sea medir el largo de El X3 que son los 60,000 que tenemos aquí arriba podría haber puesto 6000 directamente también por supuesto 28 por 28 las dimensiones de la imagen y uno que es el canal de color de tonos de gris H lo mismo el resap para el conjunto de Test y luego hacemos Perdón no ejecuté esto Luego hacemos la normalización como ya sabemos dividiendo por 255 bien y acá estamos con la primera de las técnicas para superar el problema de sobreajuste que es el aumento de datos Qué es el aumento de datos Bueno aquí hay toda una larga explicación que les va a venir muy bien como ya hemos hablado estamos poniendo mucha información eh de conceptos en el propio colap para que ustedes tengan bien a mano pero lo podemos explicar del siguiente modo en principio yo tengo un problema de sobreajuste que puede verse en el caso de el ejercicio de la clase anterior a una muestra muy chica de imágenes Recuerden que teníamos solamente 10000 observaciones En total en este caso del dataset emis o Fashion emis tenemos 60,000 que es un número mucho más lógico pero muchas veces sucede de que yo no tengo forma de poder hacerme de más imágenes para que justamente tenga mayor variedad y el modelo pueda generar esa mejor entonces una de las opciones es llevar a cabo un proceso de aumento de datos A qué se refiere a aumento de datos bien la idea es Buscar algunos mecanismos que generen nuevas imágenes distorsionando las originales A qué nos referimos cuando hablamos de distorsionar imágenes Bueno aquí hay un texto que es seleccionado para que leamos juntos y podamos entender este concepto mejor dice para una imagen se pueden aplicar variaciones por ejemplo rotar en 90 gr en 45 gr se podría achicar o alargar la imagen saturar los colores aumentar o disminuir el brillo o contraste cambiar la iluminación agregar ruido a la imagen y varias opciones más hay entre 20 y 30 opciones que se pueden aplicar para este cambio esta transformación de datos Ya lo vamos a ver este en en la práctica y le voy a dejar después un link para que ustedes vean toda la cantidad de variantes que existen en este método eh En este caso particular dice que si yo eh agrego cuatro variaciones sí tengo la imagen original más las cuatro variaciones paso a tener de una imagen a tener cinco imágenes con lo cual mi dataset se quintuplica y Por ende puedo generar un modelo mejor sin la necesidad de recurrir a por mis medios hacerme de más cantidad de imágenes habiendo explicado esto vamos a pasar a aplicarlo al dataset emist bien lo primero que vamos a hacer vamos a importar image Data generator de esta librería que justamente es lo que nos va a permitir hacer o aplicar este aumento de datos Antes que nada les dejo aquí una URL Para que vean todas las opciones que existen para este proceso de aumento de datos lo voy a mostrar aquí rápidamente para que vean fíjense una lista larguísima de opciones que tengo para alterar la imagen esa esas opciones están aquí en esa web que yo les dejo allí y con la explicación de Para qué sirve cada uno y ejemplo Sí este es el sitio oficial de tensor flow donde está toda esta valiosa información que se las dejo para que ustedes miren volviendo a la práctica vamos a aplicar solamente cuatro casos de estos que vamos a hacer una rotación un movimiento en ancho un movimiento en alto y un Rango de acercamiento estas son variables que yo creo con los datos que yo quiero para cada caso y lo que hago es crear una variable Data que va a ser la variable del generador la voy a crear justamente con image Data generator y voy a ponerle a cada uno de sus parámetros la variable que cree con los valores yo aquí arriba luego lo que tengo que hacer es entrenar este generador de datos Sí el entrenarlo es que conozca los datos originales porque si no conoce los datos originales cómo va a ser un buen entrenamiento conociéndolos bien para poder hacer alteraciones que correspondan bien a ese origen Así que esto es lo que que tengo que hacer y lo ejecutamos terminado esto vamos a hacer un código que va a ser muy práctico y muy orientativo para lo que eh queremos demostrar que estamos haciendo Aquí como siempre tratando de dar un efecto visual para que ustedes lo puedan entender mejor vamos a imprimir bueno con el código que usamos muchas veces con ML blip dos tiras de eh ocho imágenes es decir vamos a imprimir 16 primeras imágenes eh de el dataset original y luego vamos a imprimir la misma cantidad pero de las imágenes alteradas y ustedes visualmente van a poder comparar una cosa con [Música] otra bien y aquí tenemos el resultado bueno Estas son las imágenes que ya conocemos con las formas que ya conocemos y tenemos 5 0 4 1 9 bueno y el resto estas son como dijimos hoy dos tiras de ocho imágenes tenemos las 16 primeras imágenes del dataset emis original y luego hacemos la impresión de las imágenes alteradas alguna de las imágenes alteradas también las 16 primeras pero se corresponden con la original fíjense las diferencias Sí el cinco original vamos a achicar esto un poquito para que se pueda ver mejor bien el cco original con el cco modificado el cer original con el modificado fíjense el cu cómo lo transformó lo saca un poquito del cuadrado sí del límite fíjense que acá hay una parte dos partes del cuatro que no aparecen el uno está muy inclinado de acuerdan que uno de los parámetros le decía la rotación bien Eso es lo que aplico aquí aquí el nu está como alejado fíjense el dos también está alejado y fuera de foco el uno el tres seguimos después con el uno con el cuatro el uno fíjese en este caso el cinco lo recostó mucho más que lo que está allá en la imagen en original el tres fíjense que hasta pareciera que fuera la parte de abajo un cinco El Seis el un bueno de esta manera es más eficiente la búsqueda de un patrón que corresponda exactamente con lo que buscamos o queremos identificar en cada caso bien Si volvemos al código vamos a ver que en este caso yo para ver esa última tira apliqué esto que está aquí que es el dat Flow Qué es Data sh Flow dat sh es lo que creamos aquí arriba el Data generator si la imagen que tiene todo el Data generator creado con las características que yo quiero y el Flow es lo que hace fluir que ya hemos visto este concepto sí que justamente trae las imágenes y las altera esto no implica que ya están alteradas las imágenes esto lo estamos haciendo solamente para ese grupo de 16 primeras imágenes para hacer esta comparación todavía no Hemos llegado a hacer la la transformación efectiva sobre el conjunto de datos ahora antes de avanzar con la transformación definitiva de los datos vamos al otro método porque acá vamos a aplicar las dos cosas juntas sí eh Para justamente Este modelo que estamos generando buscando evitar eh el sobreajuste de la red la segunda técnica es el dropout como ya lo habíamos mencionado antes Qué es el dropout es una técnica que lo que busca es Aunque parezca incoherente tratar de que la red no aprenda tanto sí Parece que va en contra de todo lo que dijimos antes Cuál es el tema es que la idea es que no aprenda tanto en el sentido de que si aprende demasiado puede llegar a idealizar el conjunto con que se entrenó como ya sabemos y después no generalizar bien cómo evitamos para que no aprenda tanto y copie tanto fielmente eh el conjunto con el que está entrenando bueno La idea es Apagar algunas neuronas es decir por cada entrenamiento no usar todas las neuronas algunas desactivarlas Entonces de esa manera en el próximo entrenamiento Esto va a cambiar o sea no siempre van a estar apagadas las mismas neuronas sino que eso va a ir cambiando de ese modo como las neuronas van a ir participando de manera alternada no va a generarse una copia o una idealización de los datos de entrenamiento y Por ende no voy a tener el problema del sobreajuste e Por lo general el Drop out se puede aplicar en todas las capas en algunas s por lo general eh es muy poco aplicado en la capa de entrada y muchas veces se recomienda eh tomar como opción ya aplicarlo en la capa de entrada eh el nivel o la cantidad de neuronas a pagar Eh puede ser también variado se puede eh arrancar desde el 20% y llegar a no más del 50% por supuesto bueno así que explicada Esta técnica vamos nuevamente a a implementarlo pero antes les dejo aquí una animación para que ustedes intenten ver de qué se trata un sistema de dropout vamos a hacer clic aquí y acá ven fíjense cómo se van apagando solamente aquí tengo una capa en la cual estoy aplicando el dropout evidentemente pero fíjense que las neuronas que se apagan no son siempre las mismas Y eso justamente es lo que le da valor a este método bueno Esto es interesante simplemente Siempre busco cuestiones gráficas para que puedan ustedes entenderme mejor los conceptos bien volviendo Aquí vamos entonces al diseño de nuestro modelo al cual le hemos agregado una capa de Drop out que es esta que está aquí antes de la capa flaten hemos puesto Aquí esta opción eh Recuerden que la puedo poner Tantas veces como yo quiera y en el lugar que yo quiera Sí el resto tengo dos capas de convoluciones la primera de 32 y la segunda de 64 tengo la capa Drop out que en este caso le he dado un 50% o sea va a estar estar apagando la mitad de las neuronas y luego la capa flatten la capa dens y la capa de salida de 10 neuronas como ya hemos usado antes Perdón por el tema de las 10 salidas que tiene el conjunto emist y luego la compilación usamos Adam spars categorical Cross entropy acuérdense que esta función de pérdida tiene que ver con un problema de categorías y no binario y el la métrica a través de el método accuracy Sí así que gener el modelo bien y ahora sí vamos a generar o sea volvemos al concepto anterior si ya tenemos el Drop aplicado tenemos que volver al tema de la generación de imágenes alterando la imagen original como dije antes lo que hicimos hasta ahora sí fue Simplemente crear el generador darle todas las características que queremos que tenga pero hasta ahora no lo hemos aplicado sí lo hemos entrenado lo hemos generado pero no lo hemos aplicado ahora vamos a hacerlo aquí donde tomo el xtrain el etrain le pongo un batch size de 32 Recuerden que esto es un concepto muy importante en el manejo de las imágenes para ir manejándola por lote y con Data Gen Flow pongo Ese Conjunto nuevo con todas esas modificaciones y alteraciones en Data gent TR lo ejecutamos y una vez que tengo todo esto lo único que me queda es entrenar el modelo bien aquí lo vamos a entrenar por 60 épocas y otro dato importante que vamos a poner aquí Bueno aquí hemos puesto en algunos casos ustedes van a ver que yo puedo crear una variable para poner el dato o poner el dato concretamente Yo podría haber puesto aquí ep 60 y el lote 32 Pero bueno creé variables que contienen ese valor son formas alternativas de hacer esto steps per epoch y validation steps se acuerdan que lo obtenían dividiendo la cantidad de observaciones por el el tamaño del lot por ejemplo 60,000 di 32 y 10,000 di 32 bueno lo hago con una fórmula matemática que es esta que está aquí np sale sirve para redondear el valor bueno es una un método de npai Sí y aquí lo mismo para el validation Step Sí así que bueno ahora entrenamos el modelo paciencia va a tardar mucho pero bueno es lo que nos toca Así que vamos con ello bien habiendo terminado el entrenamiento vemos Que logramos un 91 por en el conjunto de entrenamiento y un 98 por en el conjunto de Test lo cual habla más que claramente que hemos superado El problema del sobreajuste dado que tenemos una situación totalmente inversa a la que teníamos en la clase pasada donde teníamos un 100% para el primer conjunto y un 80% para el segundo es necesario que ambas técnicas se apliquen en conjunto definitivamente no se puede hacer solo dropout solo aumento de datos o ambas en combinación como hemos hecho en esta práctica También es importante que podamos ver la posibilidad de mejorar este 98 por del conjunto de Test como en principio por ejemplo con la cantidad de entrenamientos hemos aplicado en esta práctica solo 60 entrenamientos Los invito a poder Probar con 100 o 200 entrenamientos y finalmente bueno variar estas dos técnicas que por más que las apliquemos en conjunto por separado hemos utilizado aquí es decir más capa de Drop out variando también además de más capas el el valor de 02 o 05 por y en el caso del aumento de datos bueno poner más variantes más opciones de variabilidad ya viemos que hay aproximadamente 24 opciones para variar eh el aumento de datos o la la deformación de estas imágenes bueno eso también es otro parámetro para variar y probar a ver si ese escor del 98 por lo podemos mejorar Bueno hasta aquí la primer parte de esta clase número 18 vamos a ahora en la segunda parte ver una forma muy práctica de cómo implementar Este modelo que hemos creado Así que nos vemos en la segunda parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte