 Titulo: Clase 11 (parte2) del Curso de Inteligencia Artificial 
 URL https://youtu.be/3vfEIhqZCnY  
 1380 segundos de duracion 
 Hola bienvenidos Esta es la segunda parte de la clase número 11 del curso de Inteligencia artificial de ifes en ella vamos a poner en práctica todo lo que aprendimos en la primera parte vamos por ello [Música] Hola nuevamente Bienvenidos a la segunda parte de la clase número 11 donde vamos a poner en práctica todo lo que vimos recién en la primer parte de esta clase donde abordamos los conceptos teóricos de este nuevo algoritmo de Machine learning máquinas de soporte vectorial lo primero que vamos a hacer luego de haber hecho la conexión como siempre de El cola es importar las librerías pandas nampai sbm es Support vector Machine que es la forma que se llama movimiento en inglés este nuevo algoritmo de máquinas de soporte vectorial y obviamente siempre de la librería Side Land luego vamos a seguir utilizando otras librerías que ya hemos utilizado antes la que sirve para separar el conjunto De tez y entrenamiento gritch ser cb y confusión Matrix y finalmente warnings que bueno son ya clásicas en todas las prácticas que estamos haciendo Así que empezamos por esto y luego inmediatamente hacemos lo propio para incorporar el conjunto de datos bien ahí lo estamos importando ya ha sido importado y creamos el Data frame con el nombre de iris y hacemos un Head bueno Nada nuevo hasta aquí y ya este conjunto lo hemos usado mucho y lo que tenemos que hacer como siempre separar las variables de entrada de las variables de salida Solamente vamos a usar dos variables de entrada dado que vamos a tener que trabajar más adelante con el truco kernel para lo cual vamos a usar justamente dos variables para poder verlo bien y vamos a usar para este caso el largo del sépalo y el ancho del sépalo y obviamente la variable y la especie como ya lo hemos hecho antes luego de esto separamos en el siguiente paso el conjunto de entrenamiento del conjunto de Test y aquí empezamos con lo nuevo que sería bueno crear nuestro primer modelo de soporte vectorial en el cual vamos a incorporar los hiper parámetros que hablamos en la teoría como recordarán donde tenemos kernel que vamos a usar el linear el c lo vamos a fijar en 0,01 y el Gamma una vez logrado esto Bueno voy a entrenar el modelo como lo he hecho antes con los otros algoritmos y medimos su precisión lo ejecutamos bien vemos que el modelo de entrenamiento y el modelo de Test tienen escorts muy bajos obviamente esto tiene que ver con el tipo de terno el que he elegido el tipo de carne lineal va a proponer una solución en la cual la separación pueda ser lineal Y esto no siempre va a poder ser factible dado que si la solución sugiriera que por la forma que tienen los puntos hay que hacer una curva O tiene que tener una cierta cobertura esa hiper plano tridimensional en este caso va a ser un plano porque tengo un problema de tres dimensiones dos variables de entrada más la variable de salida bueno el problema que voy a traer es que esa linealidad que obtengo con el lineal no va a darme como resultado un buen algoritmo y aquí se ve en el score con lo cual lo que voy a hacer va a ser cambiar el kernel y a su vez también voy a cambiar los valores de c y de gama el carnet que voy a utilizar es el rdf si vamos a recordar la clase teórica veíamos que ese era el tipo gausiano Recuerden que tenemos el lineal que ya lo usamos recién en la práctica el gauciano que lo vamos a usar ahora y el polinómico obviamente para este caso que no se adapte a alinear el polinómico y de océano son las mejores opciones de esas dos vamos a usar Esta última volvemos a la práctica bien entonces también tenemos un salto y un gama alto Recuerden que eso nos puede traer el problema del sobre ajuste bueno el resto es igual a lo que hicimos recién lo ejecutamos y medimos la precisión cuando mido la precisión veo que tengo una muy buena precisión para el conjunto de entrenamiento pero una muy mala para el conjunto de Test Por qué será eso bueno vuelvo a la teoría se acuerdan que dijimos que si tengo un gama alto como está este gráfico aquí de la derecha y un c alto pero probablemente copie también la realidad del entrenamiento que me va a dar una muy buena solución del conjunto de entrenamiento pero con un nivel de sobreajuste tal que justamente después no va a generalizar bien Esto se ve claramente aquí cuando miro los score el 04 el conjunto de Test me habla de que es un modelo muy sobre ajustado dado que el score que tengo para el modelo de entrenamiento es muy Superior y muy bueno a su vez y el conjunto de Test es muy inferior y muy malo dado que con valores bajos he obtenido scores bajos para ambos conjuntos y con valores altos he obtenido un buen valor para el conjunto de entrenamiento sea un mal valor para el otro conjunto vamos a usar el gris ser se ve para que nos sugiera como ya sabemos y conocemos de esta herramienta el mejor valor de c y el mejor valor de gama para tener buenos scores para ambos conjuntos dado que voy a usar grip ser cb para que me Determine automáticamente como ya sabemos el valor adecuado de ese y de gama para este modelo voy a crear un nuevo modelo que le voy a poner como nombre sbsg por la G de grid y solamente voy a especificar Cuál es el tipo de truco kernel que voy a utilizar el cual voy a utilizar el truco de tipo gausiano que es el mismo que usamos en el ejemplo anterior una vez creado el modelo Lo acabamos de entrenarlo con los conjuntos de entrenamiento y test como es habitualmente utiliza ahora vamos a armar un diccionario con el conjunto de posibles valores para cada uno de los hiper parámetros que quiero que se incorporen a Gris ser cb para que con ellos grid me Determine Cuáles serían los mejores o los más adecuados para este modelo en el caso de c voy a poner 0,0011 10 y 100 En el caso de dama 0,11 y 5 los valores los pueden poner ustedes de acuerdo a lo que les parezca Y también poder usar la función que usamos antes de nampai Land space para que Determine un conjunto de valores en base algunos parámetros que le damos a esa función luego de ello generamos justamente el Grisel se ve en donde le especifico Qué modelo quiere usar que es el que Acabo de crear acá arriba y cuáles son los parámetros que son los que recién acabamos de referenciar lo hacemos ya tengo mi grid creado a partir del Grisel CV lo que voy a hacer ahora es con ese grid entrenarlo con los conjuntos de entrenamiento y texto y con todo ello ahora poder averiguar bien qué determinó como valores adecuados de gama y DC el glister cb y veo que los valores adecuados son 1 y 0,1 que son justamente algunos de los valores que yo ya le había dado aquí en el conjunto de posibles valores miedo a la precisión y veo que tanto en el conjunto de entrenamiento como el conjunto de Test no llegan al 0.9 y un poco más que sería lo ideal para este caso Sí pero por lo menos 0.9 pero son mucho mejores que los dos casos anteriores Y más allá de eso es importante que ven aquí que el conjunto de entrenamiento no tiene tan buen score como antes Si 080 versus 094 pero el conjunto de Test es mucho mejor y no está tan alejado en su score respecto del conjunto de entrenamiento como era antes que había 0.5 de diferencia y ahora hay 0.1 de diferencia de todos modos lo ideal sería Buscar aquí una solución para que este score no solamente sea mejor sino que sea superior al del conjunto entre para terminar de validar la precisión del modelo además de hacerlo con el método score sabemos que tenemos una herramienta que ya venimos usando las últimas clases que es la matriz de confusión recordemos que antes de aplicar la matriz de confusión tengo que establecer las predicciones Es decir para el conjunto de Test saber usando el modelo grid en este caso que es el producto de haber utilizado la herramienta griser cb y el modelo bid con predic obtener las predicciones para ese conjunto de Test y luego compararlas con las variables de salida de ese conjunto de Test lo ejecutamos y vemos el resultado Recuerden que lo que estamos viendo aquí es la precisión para tres salidas para el caso de cetosa versicolor y virginica por eso en la diagonal principal veo que tengo seis aciertos en la primera variante cetosa 7 en versicolor y 8 en virginica en el caso de versicolor tengo dos valores que no han sido predecidos de manera correcta y en el caso de Virginia que es la última tengo Siete valores que no han sido predecidos de manera correcta con lo cual aquí es donde tengo el mayor nivel de desaciertos y por eso justamente si saco el valor total voy a poder saber sobre la cantidad de aciertos versus la cantidad de errores justamente este 07 evidentemente el 30% de esta información que está en la matriz de confusión es desacertada por eso el 07 ahora vamos a ver otro caso de aplicación de El algoritmo de máquinas de soporte vectorial con kernel donde vamos a utilizar un conjunto de datos mucho más grande y que ya hemos utilizado la clase pasada recordarán el conjunto mnist sí que tenía dígitos escritos a mano entre 0 y 9 y que tenía una composición de 70.000 observaciones de las cuales 60.000 ya estaban configuradas o preconfiguradas para el conjunto de entrenamiento Y 10.000 para el conjunto de Test vamos a tomar este caso para ir incorporando algunos conceptos y adelantándonos un poquito a lo que tenga que ver con visión computacional que bueno aplica este caso y ya es una buena oportunidad para ir manejando algunos conceptos que vamos a utilizar a futuro como siempre lo primero que vamos a hacer es trabajar con la incorporación de las librerías y verías que son similares a las que usamos anteriormente solamente que en este caso vamos a usar fecha Open nml que la que usamos para incorporar este Data set del cual estamos hablando y estándar escáner la vamos a sumar también dado que vamos a probar lo que vamos a hacer ahora en el ejercicio también en una parte con el escalamiento de datos bien una vez hecho esto voy a incorporar en la cassette como dije recién y ya sabemos que es muy grande por lo tanto este proceso de la incorporación de este conjunto de datos al cola va a tardar mucho más que en los casos anteriores bien allí terminó y tardó 27 segundos como dice aquí luego imprimo el diccionario de las claves de todas las Kiss y las dos más importantes son el Data y el target por lo tanto voy a hacer un shape del Data y un shape del Target que lo que estoy haciendo aquí en el primer caso veo que tengo Las 70.000 observaciones que ya hablamos y 784 características Cómo es esto vamos a tomar este gráfico como ejemplo Este es un número 5 la información de este número 5 está dispuesta en una estructura matricial que tiene 28 por 28 píxeles 28 por 28 son 784 píxeles que en esta estructura están dispuestos en una raid unidimensional es decir que cada una de estas 28 filas de 28 píxeles están todas juntas en una sola tira por decirlo de alguna manera en un array unidimensional Por eso sumadas cada una de estas 28 filas de 28 píxeles me dan los 783 784 Perdón características que menciona y luego está el target que tiene 70.000 Qué información tiene el target bueno cada uno de estos dibujos qué número corresponde en este caso yo tendría la información de estos 28 por 28 y el tar te diría 5 porque esto es un 5 bien esa es la información y por eso tenemos de esta manera una explicación de estos números que vemos aquí ahora para ver un poquito más de cómo guarda esta información vamos a tomar la primer fila de estas 70.000 y la vamos a mostrar por pantalla no hace falta poner un print simplemente poniendo mnist punto Data sub 0 que sería la referencia de la primer fila y aquí está lo que le decía recién el array de los 784 píxeles de esta imagen similar a esta que está aquí bien cada uno de estos píxeles es un valor que va entre 0 y 255 que es la nomenclatura que tienen los colores en el formato rgb y con esto tengo toda la información si yo ahora quiero imprimir esta información para poder verlo como estoy viendo aquí esta imagen qué debería hacer debería a toda esta tira de 784 píxeles reformatearla en una estructura matricial de 28 por 28 es decir como esto que tengo aquí por lo tanto lo que voy a hacer a continuación justamente es Eso es imagen la variable digo que es igual a MX de Data sub 0 que es m&s Data sub 0 es lo que imprimir acá sí o sea todo este conjunto de valores lo quiero imprimir y para imprimir lo tengo que hacer un shape de 28 por 28 Luego de eso lo que hago es un plt o sea imprimo Qué cosa imagen con un semeaba que es un mapa de tipo binario y finalmente lo que voy a hacer es imprimir el target es decir que número corresponde con ese dibujo que estoy transformando en esta estructura de 28 por 28 dispuesto a imprimir bien lo ejecutamos y vemos que al igual que el caso que veíamos recién en el ejemplo es se trata de un 5 Entonces tenemos este número 5 que está aquí arriba es la impresión del Target y todo este dibujo que está aquí es la impresión de imagen de la variable imagen que es la resultante de haber tomado la primer fila Data Sub Zero insisto es todo este conjunto de números que está acá era una estructurada de 28 por 28 todo lo que tengo aquí es un cuadrado de 28 píxeles por 28 píxeles y que en este caso me muestra un 5 como en otro caso Me podría mostrar otra cosa podemos hacer una prueba acá y cambiar un poco esta Consigna esperemos que nosotros cinco imprimimos bueno en este caso es un cero un poco torcido pero es un cero fíjense que de nuevo aquí arriba está el target y aquí la impresión bueno Esto Pueden seguir ustedes también jugando todo lo que quieran cambiando este valor siempre que sea un valor que esté entre 0 y no 70.000 sino 69.999 bueno pueden poner el número que quieran y ver bueno tuvimos suerte que en todos los casos es un valor diferente en este caso el dibujo representa un 4 y el target Me dice que esos son cuatro Aquí es importante entender que la clave de esto es encontrar un algoritmo que viendo esta estructura de 28 por 28 y la forma que tiene puede interpretar que esto responde a un patrón de un número determinado en este caso lo que determina Es que este patrón a pesar de que es un número muy regular es lo más parecido a un 4 si eso es lo que o de esa manera trabaja el algoritmo no nos adelantemos ya la vamos a a ver bien en Visión computacional pasemos Entonces ahora a crear el modelo de máquina de soporte electoral antes debería hacer como sabemos en este caso dividir el conjunto de entrenamiento y test Recuerden que en este caso particular de este conjunto de datos que ya viene preparada la separación de datos de entrenamiento Entonces no voy a usar la librería 30 split sino que directamente voy a hacer la asignación según el número de orden de la muestra de la observación es decir los 60.000 desde el principio hasta el valor anterior al 60.000 para los primeros 60.000 datos Y desde el 60.000 hasta el final para las 10.000 restantes observaciones que corresponden Al conjunto de Test una vez que hago eso creo Ahora sí el modelo que en este caso lo voy a hacer sin manejar los hiper parámetros que vimos antes gama y s simplemente lo único que voy a poner es el tipo de capa bueno Esto obviamente por la magnitud de el conjunto de datos va a tardar bastante tiempo Bueno ahí tenemos que ha terminado la el entrenamiento del modelo y ha tomado cuatro minutos saben que cuando empezamos a trabajar con datasettes más grandes y con Data set que tienen alguna cuestión como los que manejan imágenes a pesar de que estas imágenes son blanco y negro de 28 x 28 que es una porción digamos una muestra muy chica respecto de otro tipo de imágenes que tiene más colores y más dimensión ya con eso vemos que el tiempo que invertimos en crear modelo va cambiando radicalmente respecto de los ejercicios que veníamos haciendo antes bien Ahora vamos a medir entonces la precisión del modelo para el conjunto de entrenamiento de Test como hacemos habitualmente bien como Ven aquí la medición del score también tardó mucho y mucho más aún que el entrenamiento del modelo 13 minutos la precisión es muy buena 0.98 y 097 la idea podía hacer ahora probar escalar los datos Porque porque justamente lo que hablábamos de la diferencia de las cifras en este caso tenemos 784 características y sus valores van entre 0 255 puede ser que esa ese conjunto de posibles valores marca una distancia muy grande con lo cual podría ser una alternativa escalar los datos y ver si la precisión con esa técnica mejora o no vamos por ello bueno en este caso se han invertido 7 minutos para el escalamiento de datos y la obtención del modelo con el escalamiento de datos es decir que tomó cuatro minutos que cuando generamos el modelo sin escalar los datos y ahora vamos a medir la precisión y bien hay concluyó la medición del score volviendo a repasar un poquito todo este tiempo que nos está llevando este Data set importante vemos que tenemos 4 minutos para la generación del modelo y 13 minutos para medir su precisión luego con el modelo escalado tenemos 7 minutos para la generación del modelo y 17 minutos para la medición de la presión la precisión no ha mejorado significativamente respecto del anterior hay una pequeñísima diferencia en el caso del modelo de entrenamiento y una pequeñísima diferencia en el caso del modelo de Test pero en menos es decir no he mejorado el score en este caso tengamos presente que no utilizamos recordemos lo que pusimos aquí arriba no utilizamos hiper parámetros se puede mejorar este score se puede mejorar a pesar de que es muy bueno justamente lo podríamos probar incorporando los hiper parámetros Gamma y C el tema es que aquí obviamente cada prueba nos va a llevar un tiempo muy importante si utilizásemos el grid se ve obviamente mucho tiempo más pero bueno Esto es parte de lo que tenemos que empezar a aprender a manejar y a tener paciencia porque obviamente todas estas cuestiones llevan mucho tiempo hay algunas herramientas inclusive en el Google que ya vamos a ver más adelante que pueden hacer que este proceso sea más rápido y dos herramientas que tiene también Google que son pagas que también aceleran los tiempos de desarrollo de este tipo de procesos Pero bueno lo concreto que sabemos que esta es nuestra primera experiencia con el tema del tiempo así que bueno Los invito si alguno quiere probar el hacer este alguna prueba incorporando algún hiper parámetro para mejorar este algoritmo y bueno tener una primera experiencia de lo que se trata de empezar a trabajar en el mundo de el desarrollo de inteligencia artificiales y algoritmos y cada tiempo que lleva digamos en la búsqueda de este tipo de perfeccionamiento bien hasta aquí llegamos con esta segunda parte de la clase número 11 y cerrando la clase número 11 como un todo Solo me queda despedirme y decirles que nos vemos en la próxima clase número 12 hasta entonces hasta aquí llegamos con esta clase con ella ahora podés manejar un nuevo tipo de algoritmo máquinas de soporte vectorial Te espero en la clase 12 donde vamos a comenzar a aprender los algoritmos de tipo no supervisado nos vemos [Música]