 Titulo: Clase12 (parte2) Curso de Inteligencia Artificial 
 URL https://youtu.be/_hinMLX5VnI  
 2221 segundos de duracion 
 Hola bienvenidos Esta es la segunda parte de la clase número 12 del curso de ia de ifes en ella empezaremos a practicar con el algoritmo camins  Bueno Hola a todos nuevamente Vamos a continuar con la segunda parte de esta clase número 12 donde vamos a ver una práctica con kamins que es el algoritmo en el cual nos empezamos a incorporar al mundo de los algoritmos no supervisados que tratamos en la primer parte de esta clase así que ya pasamos al colar Y empezamos con esta práctica bien entonces aquí estamos en el cola cola número 12 en el cual vamos a ver lo que titula justamente este documento algoritmo no supervisados kamin clustering lo primero que vamos a hacer como siempre es incorporar las librerías que vamos a usar en esta práctica pandas nampai matter y aquí aparece kamins dentro de side in the cluster si es obviamente la librería específica que vamos a usar para este algoritmo nuevo que estamos aprendiendo Y también vamos a ver algo de pre procesamiento de datos algo que charlamos hace dos clases atrás donde hablamos del escalamiento de datos y la necesidad de eso cuando es necesario y porque es necesario Bueno vamos a volver a abordar este tema un poquito más en profundidad aquí con dos opciones que tienen que ver con el objetivo del escalamiento de datos Y que por ahí a veces suenan como sinónimos cuando no lo son tienen un mismo objetivo final pero no son Exactamente lo mismo así que bueno vamos a aprovechar esta clase para involucrar como siempre un nuevo tema a desarrollar y luego obviamente warnis bueno incorporamos estas librerías y pasamos a trabajar a traer digamos el conjunto de datos con el cual vamos a hacer esta práctica que es el de los vinos rojos los vinos tintos digamos no que ya usamos algunas clases hacia atrás lo vamos a volver a utilizar Así que vamos hacia ello como hacemos siempre aquí está y lo traemos al cola como siempre lo verificamos que aparezca Aquí bien aquí tenemos obviamente aparece dos veces porque obviamente ya lo hice en el marco del repaso de esta práctica la preparación de esta práctica Pero bueno Nos volvemos a hacer para demostrar justamente como siempre paso a paso cada una de las cosas que hacemos en la práctica y luego creamos un Data frame DF con esos datos y hacemos un Head recordemos un poquito de qué se trataba este conjunto de datos datos de vinos obviamente como dijimos Recién tintos con características que tienen que ver con la composición y con algunos aspectos químicos digamos no de la composición del vino pH sulfatos alcohol densidad Bueno este residuo azúcar ácido cítrico Bueno una serie de elementos que tienen que ver con las características del vino y aquí recordemos que el valor Target u objetivo era el Quality es la calificación del la calificación tiene que ver con bueno la determinación de un profesional con catador digamos de vino que le da una determinada calidad en virtud de las características del vino con los conocimientos que tiene la gente que se dedica a esto y le pone un número de calificación de acuerdo a la calidad que es especialista determina el vino bien Esto lo podemos este conjunto de datos podemos ver la composición que tienen son 1599 observaciones y 12 columnas que vimos recién y podemos justamente hablarlo del tema de la composición de la columna Quality Aquí vemos Algunos números el 5 al 6 pero no sabemos Cuántas variedades hay entonces lo que hacemos es utilizar de DF Quality es decir en Data frame DF punto Quality porque la característica que vamos a analizar es Quality el método única con Unique ya lo usamos Sí justamente en la clase análisis de datos donde podemos ver los valores únicos de una característica decir bueno cuántas variantes tiene este una determinada característica este caso Quality y cuando lo ejecuto me da como resultado una raíz donde me da todos y cada uno de los valores que tiene esa columna es decir que las calificaciones de los vinos van de 3 a 8 5 6 7 4 8 y 3 sí Esas son las variantes Así que lo único lo que uno podría llegar a presumir viendo estos números que podría ser de uno a cinco o de 1 a 6 Perdón o de 0 a 6 supongamos bueno no es así la calificación mínima es 3 y la calificación máxima es 8 bien a partir de eso vamos a involucrarnos en el tema de transformación de datos escalamiento y estandarización qué es lo que poco comentaba al principio de la clase Porque es conveniente o necesario bien para incorporarnos en este tema primero vamos a tratar de reconocer el contexto que tenemos para después encontrar la justificación de la transformación de datos para eso vamos a recurrir a un método que también usamos en las clases de análisis de datos que se llama discribe y distrive si ustedes recordarán se aplica obviamente sobre el Data frame que quiero analizar en este caso DF lo ejecutamos me va a dar una serie de indicadores muy utilizados en el ámbito de la necesidad y la estadística que bueno los puedo visualizar en una sola vista como aquí veo y por cada uno de los las características de este conjunto de datos Sí qué es lo que me da me da la cantidad de elementos que hay de cada una de las características obviamente son en todos los casos igual 1599 el promedio la dimensión estándar el valor mínimo de esa característica el valor máximo de esa característica y los percentiles 25 50 y 75 en este caso porque hago esto porque quiero ver el rango de valores numéricos entre los entre las distintas características de este conjunto de datos es decir Quiero ver el valor mínimo de todas las características y el valor máximo de todas las características esta herramienta me sirve mucho para ver ese tipo de información con lo cual voy a ir el caso de la fila mínimo y voy a recorrer todas y cada una de las características y voy a ver que por ejemplo tengo 460 0 12 0 0 90 0 0 12 16 29 274 033 843 es decir que la expresión mínima en este caso es esta que está aquí 0.012 es la mínima de esta característica Pero además es la mínima porque estoy justamente en la fila de los valores mínimos y si es la mínima de la fila de los valores mínimos quiere decir que es el valor mínimo de todo el conjunto de datos del mismo modo y con la misma lógica voy a la fila de los máximos y veo recorriendo cada una de las características el mismo proceso 1590 158 1 15 0611 bueno todos los valores que estamos viendo aquí y veo que este 289 me doy cuenta que es el valor máximo de nuevo es el valor máximo de esta característica pero es el valor máximo de todo el conjunto de datos con esto puedo ver que yo tengo una diferencia entre el valor mínimo de todo el conjunto de datos que recordemos era este valor 0012 y este valor máximo 289 esto representa una distancia muy grande en el medio obviamente todas las variantes de todos los valores que están entre esos extremos no esto representa un Rango muy grande de valores y puede llegar a distorsionar el modelo que voy a generar Por qué Porque puede entender que el valor más grande representa a una característica más importante y el valor más chico lo contrario esto en realidad no es así si yo tengo una característica como en este caso que tiene una cifra muy grande eso no debería implicar que esta característica es más importante que otra tiene más peso que otra a la hora de encontrar un patrón dentro de el modelo que estoy generando en el caso contrario lo mismo o sea la que tiene la cifra más chica no Debería ser como en este caso la menos importante esa distorsión se puede generar justamente por la magnitud de las cifras y lo que se busca transformando los datos con alguna de las tres variantes que ya vamos a ver es justamente imitar esa gran diferencia y que justamente el modelo no se equivoque en ese sentido y pueda tener una percepción más apropiada en virtud de la importancia de cada variable de entrada en virtud del algoritmo de predicción y no en relación a la magnitud de la cifra que representa recién hablábamos de Tres formas o tres métodos de transformación de datos Aquí vamos a ver dos de ellos pero vamos a mencionar Cuáles son los tres estandarización de datos es uno de ellos escalamiento de datos es el otro y el tercero es normalización de datos como dije recién de los tres vamos a ver aquí dos de ellos el primero el que titula aquí el punto 2 1 estandarización de datos es una transformación de datos en el cual se determina una media cero y una desviación estándar 1 generando si pudiese graficar esos valores una estructura de tipo acampanada que justamente lo que demuestra es que existe un nivel similar de datos hacia la izquierda y hacia la derecha del cero obviamente esto arrojaría valores que están positivos al cero O sea a la derecha del cero o a la izquierda del 0 negativos si el rango sería entre un valor negativo y un valor positivo lo más cerca posible del cero porque justamente la desviación estándar es 1 este método como dice aquí no es conveniente cuando el conjunto de datos tiene muchos outliers sí los outliers Recuerden que son los puntos O valores atípicos sí que están muy alejados del resto y que muchas veces nos hacen pensar si es conveniente que permanezcan en el conjunto de datos o directamente quitarlos porque distorsionan el modelo o pueden llegar a distorsionar el modelo Entonces en este caso si usamos esta forma de transformación de datos y Tenemos muchos que puede darse la situación y quizás no podamos quitarlos a todos ellos bueno no sería conveniente utilizar este método Bueno pero salvo ese caso vamos a aplicarlo en esta situación para poder conocerlo y practicar con él y aquí tenemos el código para eso por eso lo primero que hacemos Es crear una variable que le llamamos estándar como siempre cualquier variable que querramos y lo hacemos a partir de el método estándar que yendo un poco hacia arriba es uno de importamos aquí de cyclecic bien abajo y lo que hacemos a continuación es crear ese mismo conjunto de datos pero estandarizado bien Luego de eso que transformó creo el Data frame y le hago un Head y vemos el resultado que los valores son muy chicos las diferencias no son tan grandes como antes y tengo valores como dijimos recién algunos negativos y algunos positivos siempre centrados en el cero vamos a tomar por ejemplo uno de los valores que teníamos Aquí vamos al Data frame original tenemos 747878 11 12 11 2 Perdón 74 sí a ver si recordamos un poquito el primero y el último son iguales el segundo y tercero son iguales y el cuarto es el más alto esa lógica debería trasladarse después de haber sido transformado si a través del método de estabilización Sí este los datos de este Data es decir fíjense que en el caso este como dijimos el primero y el último son iguales el segundo y el tercero también y el cuarto es más grande que todos los otros bien el otro método es el escalamiento de datos en este caso el método funciona del siguiente modo le da el valor cero al valor más bajo de todo el conjunto de datos Y uno al valor más alto del conjunto de datos el resto de los valores están entre 0 y 1 de manera proporcional este método no es afectado a diferencia del método que vimos recién por la presencia de outliers Bueno lo aplicamos con una lógica similar a la que vimos recién el otro caso simplemente que en esta situación usamos min Mac Scanner que es uno de los este elementos que incorporamos aquí está si de la misma librería que sacamos estando en escáner al principio de esta práctica bien volvemos al lugar y bueno el proceso es similar porque tiene un Data frame que en este caso le pongo DF guión bajo scale y a un Head para verificar los datos al verificar los datos puedo hacer la misma comprobación que hice En el caso anterior solamente que en este caso tengo que entender que los valores no van entre un valor negativo y un valor positivo centrado en el cero o con desviación estándar uno sino que son valores que van entre 0 y el uno pero tiene que mantener esta cuestión de la proporcionalidad como en el caso anterior y puedo chequearlo yendo a comparar este conjunto de datos con el conjunto original no lo vamos a hacer para acelerar la clase pero lo dejo para que lo puedan hacer ustedes vamos a crear Entonces ahora el modelo camins habiendo visto toda esta introducción al procesamiento de datos y vamos a usar de las dos transformaciones que hicimos los datos que han sido escalados bien en el caso de camín tengo hiper parámetros como en todos los algoritmos que hemos visto últimamente y los sirve parámetros son los siguientes tengo el número de clusters que si no lo fijo va a ser definido en 8 por defecto luego n y net que por defecto es 10 que es el número de veces que se ejecuta el algoritmo camis con semilla de sectores diferentes es decir recordarán en la teoría que lo que hacía era tratar de buscar la mejor ubicación del centroide tratando de medir todas las diferencias o todas las distancias que existe entre cada punto y el centroide propuesto bueno eso lo hace Cuántas veces antes de pasar a la próxima iteración 10 veces por eso habla de 10 veces por cada semilla Luego pasamos a maxter que es la cantidad de iteraciones que en la animación que vimos en la clase teórica habíamos visto un proceso de 14 alteraciones Que obviamente es muy chico Porque por defecto fíjense que aquí el valor prefijado es de 300 O sea que es bastante mayor la situación solamente que vimos un gráfico para poder entender rápidamente Cómo funcionaba visualmente este nuevo algoritmo pero el maxi pero en este caso se puede fijar con el número que ustedes quieran pero por defecto será 300 y finalmente el hipermetro en it que fija el método de inicialización de los centroides las opciones pueden ser camíns o Random en el caso de camines que es lo que usa por defecto establece una un primer punto candidato por llamarlo de alguna manera que guarda cierta relación con la estructura de los puntos y en otro caso es Random Que obviamente es aleatorio y no guarda ninguna relación o no observa la distribución de los puntos Para proponer unos primeros candidatos bien Sabiendo esa información ahora vamos a crear nuestro primer modelo con kamins y vamos a fijar el número de cluster en 6 porque bueno viendo Que cuando vimos el conjunto de datos teníamos seis calidades de vino podemos tomar ese parámetro para intentar generar un primer camis con seis clusters luego en el hiper parametriums en enit 10 maxter 300 y Random State Nou es Si no vamos a usar en este caso un Random State Así que creamos el modelo y ahora vamos a ver la información importante justamente de este método que es a donde se ubicaron los seis centroides de cada uno de los clusters esto lo podemos ver justamente con model que es el nombre del modelo y cluster guión bajo centers ejecutamos y ahí vemos en el método de array las coordenadas de los seis centroides fíjense que tengo 6 conjuntos Sí se ve claramente y cada uno de estos conjuntos que tiene tiene 12 valores porque porque justamente estamos en un espacio de 12 variables y Por ende tengo un valor para cada una de las variables y ese valor me da las coordenadas con las cuales va a estar ubicado cada uno de los centroides fíjense que los valores que tengo aquí son los valores que vienen del producto de haber sido escalonados sí es decir él sí el método de transformación que dijimos que utilizamos aquí o sea no son los valores originales pero justamente esto lo que me describe insisto son los valores finales es donde este proceso de modelo de camines ubicó a cada uno de los seis centroides pero también me gustaría ver además de la ubicación de los centroides luego este proceso de creación del modelo finalmente cada uno de los puntos de este conjunto de datos a qué cluster fue asociado Y eso lo puedo ver con el método bajo de model lo ejecutamos Y tenemos una raíz obviamente vamos a ver algunos valores porque es un array que Cuántos elementos tiene tantos como observaciones tiene este conjunto de datos recordemos cuando hicimos el shape que la cantidad de observaciones que tiene este conjunto de otras 1599 Por ende obviamente tenemos solamente una parte pero lo podríamos poner en otro tipo de estructura para poder visualizarlo mejor y justamente es lo que vamos a hacer a continuación vamos a crear un Data frame que le vamos a llamar DF guión bajo View al cual le vamos a dar todos los datos del Data frame DF y luego vamos a agregar una con una nueva que vamos a poner cluster 6 y justamente en esa columna voy a poner estos valores que acabo de muy parcialmente aquí pero ahora los voy a poder ver de mejor manera porque porque justamente van a estar como una columna más del Data frame original en un nuevo Data frame obviamente para poder no confundir un Data frame con otro no alterar el original y voy a poder a través de esta visualización solamente ver los primeros registros pero puedo verlos todos por supuesto poniendo una relación entre el valor de la calidad que presumir podía tener que ver con la identificación del cluster y el número de cluster que finalmente le dio bien hago eso lo ejecuto me voy hasta el final y aquí tengo Quality y cluster 6 obviamente es difícil que sean iguales porque ustedes Recuerden que Quality va de 3 a 8 y los cluster empiezan desde cero pero aquí Lo importante es poder tratar de visualizar si hay una relación entre la calidad del vino y el cluster que no se altera por ejemplo acá a las 5 le da el cluster 4 a las 5 al 4 a las 5 al 4 sé que hay una relación Más allá de la parte numérica que dijimos Nova coincidir porque son distintos rangos de valores Pero esto es importante a las 6 El 3 a las 5 el 4 es que fíjense que hay una relación aquí la relación no está marcada 71 74 554 pero fíjense que en el caso de el 5 con el 4 es bastante marcada la relación y eso es lo que podemos investigar a ver si existe esa relación pero ahora viendo este resultado de esta comparación que mencionábamos recién podemos llegar a preguntarnos si adoptamos un criterio lógico a la hora de determinar la cantidad de clase tomando como referencia la cantidad o la cantidad de opciones de variantes de calidad de vino que existían eso lo podemos verificar justamente con una técnica que se llama técnica del ya vamos a entender porque se llama técnica del codo porque tiene que ver con una forma gráfica que vamos a ver a partir de este código que tenemos aquí en este caso voy a crear una matriz inercia ya vamos a ver por qué y luego voy a crear un Ford que va a alterar de nueve veces a partir de una variable que le he dado Llamar n cluster Cuál es la idea de esto es la idea de poder crear 9 modelos diferentes cambiando el valor de la cantidad de clutch en el primer caso va a ser un cluster de uno nada más y después de allí un modelo con dos cluster tres cluster el resto de los hiper parámetros están con los mismos valores que he utilizado antes voy a volver a crear un modelo con el conjunto y una vez que lo haga voy a incorporar a esta matriz inercia un valor que se llama justamente Inercia de cada uno de los modelos y lo ejecuto y a continuación voy a graficar la resultante de esta matriz que Acabo de crear aquí pasamos a la parte del gráfico voy a crear justamente un gráfico donde Ven aquí voy a utilizar al valor de los valores perdón de la matriz inercia le digo que voy a imprimir un Rango de una y este concepto final o este elemento final se llama marker es el tipo de dibujo en el cual voy a poner cada uno de los puntos está OK quiere decir que va a dibujar un círculo en cada uno de los puntos Sí el resto son los títulos y bueno el formato del tamaño de el gráfico que voy a dibujar bien a partir del gráfico esto se ve mucho mejor vamos a ponerlo en un tamaño más chico para que se vea mejor ahí lo tenemos que me muestra Esto me muestra un gráfico que me dice en cada uno de los casos Cuál es la inercia para cada uno de los modelos que he creado desde el cluster número uno al cluster número 9 este valor de Inercia que está dentro de este Rango de valores que está aquí me indica algo que me va a permitir encontrar un elemento que se llama codo aquí se llama codo qué es lo que se busca como codo es la parte que se asemeja digamos el codo de un brazo de un ser humano es la parte donde la inercia empieza a dejar de tener fíjense que en este caso Cuando paso del valor 1 al valor 2 Tengo una línea que va muy marcadamente hacia abajo de 2 a 3 eso va siendo un poquito menos intenso de 3 a 4 de 4 a 5 Es decir va a llegar un momento en el cual Este cambio o este nivel de cambio no va a ser tan abrupto Por ende en ese punto que por él no puede ser uno puede ser uno o dos que en donde se marque ese cambio va a ser la referencia de la cantidad de cluster que debería ser la adecuada para ese modelo en este caso podemos presumir que esta inercia deja de tener efecto en el 5 y en el 4 si quizás en el 6 ya vemos que ya esta tendencia no cambia mucho ya el nivel de graduación o de dependiente no tiene una gran alteración prácticamente es del 5 pasa eso en el 4 ya hay un cambio un poco menos significativo pero significativo al fin y ya fíjense que prácticamente del 5 al 9 parece que es una recta con lo cual esa tendencia no tiene una alteración al menos una fuente con lo cual Esto me indica que probablemente más que el 6 los valores candidatos mejores para generar un buen modelo en relación la cantidad de cluster puede ser cuatro o puede ser Cinco más que 6 con lo cual de estas dos opciones el 5 y vamos a crear un modelo ahora en lugar de con 6 cluster como decimos antes con 5 plazas viene aquí está entonces lo mismo que hicimos antes pero solamente que en N cluster en lugar de poner 6 le ponemos 5 Así que lo creamos al modelo vemos ahora los centroides obviamente tengo un centro de menos si tengo o sea un conjunto de valores que identifica un centroide menos que antes pero siempre referido con 12 valores Porque sigo teniendo las mismas 12 variables que antes sí puedo ver rápidamente las etiquetas como antes pero prefiero hacerlo también como lo hicimos antes a partir de DF View con lo cual ahora en DF View voy a agregar una nueva columna que le voy a poner cluster 5 con este modelo o los Label de este modelo que Acabo de crear que es el modelo el 5 a diferencia del otro que tenía otro nombre lo ejecutamos y acá tengo Entonces ahora si me voy hasta el final para cada observación el valor de la calidad el número de cluster que le asignó con el modelo de clusters y el número de cluster que le asignó con el modelo de cinco plast obviamente acá en algunos casos como en estos tres primeros casos hay una relación bastante marcada Aquí también 540 Así que se mantiene con un patrón pero en el momento ese patrón se va a alterar Porque bueno porque tengo un cluster menos obviamente no puede haber una linealidad si tengo una opción de cluster menos pero bueno es interesante para poder insisto como antes investigar justamente Cuál fue el resultado de una opción o de otra lo importante que este gráfico que tenemos aquí nos permite poder saber cuál es la cantidad de clusters ideal para bueno el modelo o el conjunto de datos que intentamos modelar ahora vamos a ver cómo hacer una predicción con este modelo camins y lo hacemos como hicimos antes con el método predic sobre el modelo el último modelo que hicimos que le hemos puesto model 5 y luego lo ponemos una variable resulte a partir de la cual vamos a ver qué cluster le corresponde a ese conjunto de datos que le voy a pasar para que haga la predicción estos datos los voy a sacar de el datasette original lo voy a tomar el primer registro y tengan presente que si yo he armado un modelo en base a datos escalados pues entonces los valores que tengo que ponerle de entrada para que arme la previsión También tienen que estar escalados sí Entonces esto es importante tenerlo presente por eso estoy tomando el primer registro los datos de primer registro que tenía aquí de el conjunto original después de haber sido escalado Entonces tenemos 024 039 0 0 6 8 bueno son todos los datos que estoy poniendo aquí justamente como valores de entrada para establecer la predicción y luego imprimo la variable resulte nos da como resultado 0 es decir que este conjunto de valores correspondientes a esta primera observación tiene pero asignado pero como predicción el cluster número 0 si vamos a la el Data frame que habíamos armado aquí donde teníamos el valor original recuerdo a la calidad digamos que había determinado el especialista el valor en el caso de 6 cluster y el valor en el caso de cinco cluster vemos Que en la primera observación el cluster que le corresponde es el 0 Obviamente que esto lo podríamos variar con otros valores diferentes y ver a qué cluster le asigna Pero bueno quise tomar Este primer elemento para ver justamente que lo que de alguna manera había sido estipulado de entrada cuando se definió el modelo sigue teniendo vigencia cuando lo hago dentro del contexto de una predicción o sea los mismos valores que usé y que se determinó cuando se creó el modelo que correspondían al cluster 0 al ponerlos como predicción también arrojan el valor con lo cual es una buena manera de testear este este conjunto de datos Y este modelo bien Ahora nos preguntamos si esto de algún modo así como lo vimos en algoritmos anteriores en el caso de el bosque del árbol y de otros algoritmos podemos verlo en un gráfico bueno el problema aquí que no se olviden de que tenemos una gran cantidad de variables de entrada Sí y eso hace de que es complicado poder representarlas en un gráfico ya que tenemos Claro que un gráfico de más de tres dimensiones no se puede visualizar entonces la respuesta a esta pregunta es si lo podemos ver en un gráfico pero aún teniendo esta cantidad de variables de entrada no esta cantidad de dimensiones pero para eso tenemos que aplicar un concepto que ya vimos que es el tema de la reducción de la dimensionalidad Sí pero aquí no con los propósitos que no habíamos hecho antes que era para poder desarrollar un modelo que fuera más rápido sino para poder graficarlo es decir vamos a cambiar una estructura de 12 dimensiones a una estructura de solamente dos dimensiones para poder graficarlo como corresponde así que bueno vamos a tomar el caso del modelo de cinco clusters y vamos a aplicar el psa que ya no vimos antes que es esta librería que me permite justamente poder este hacer una reducción y aquí lo que le voy a poner es en lugar de como hicimos en otra oportunidad de decirle que quiero un cambio una reducción de dimensionalidad que no me dé una pérdida superior al 005% aquí le pongo Cuántos componentes y cuántas dimensiones quiero que tenga en este momento este traspaso esta transformación Así que está la información que le pongo aquí luego le pongo justamente el conjunto de datos que deseo transformar y finalmente lo que hago es crear justamente este ese modelo transformado para poder determinar a partir de ahora cuáles son los nuevos clusters Por qué Porque ya no voy a tener la cantidad de cluster que tenía antes en 12 dimensiones voy a tener otros clusters porque mi realidad ahora es de dos dimensiones entonces lo que tengo que hacer es también redimensionar la distribución de los clusters sí acuérdense Con 12 dimensiones en una cuestión con dos dimensiones va a ser otra completamente distinta y finalmente un Ford de 5 ciclos porque tengo no se olviden este cinco cluster y en cada uno de ellos voy a generar las siguientes acciones en principio la distribución de los puntos en dos dimensiones Sí para esta nueva este nuevo formato que adquirido luego se establece un color diferente para cada cluster si con esta línea que está aquí se determina Qué tipo de dibujo quiero Que aparezca en el punto que es de nuevo esta o que representa un punto un círculo Sí luego si ese ese círculo lo quiero con un borde Y en este caso le digo que sí que quiero que ese círculo tenga un borde y que ese borde sea de color negro y finalmente una leyenda que ya vamos a ver que va a aparecer donde se va a indicar el número de cluster como cuando se hace una gráfico en Excel que aparece una tabla con la referencia de cada uno de los elementos esta gráficando en este caso la referencia de los ciclo bien Luego de que hago ese dibujo para poder representar porque hago un Recuerda que son cinco cluster y tengo que hacer estos cinco veces con cinco colores distintos y con cinco números y referencias de cluster distinto después lo que hago es representar cada uno de los centroides lo que grafiqué hasta el momento hasta ahora son los puntos es decir cada una de Las observaciones que están insisto en un gráfico de dos dimensiones ahora lo que tengo que hacer a continuación es poner el centro aire que le corresponde a cada caso ese centro Lo voy a hacer con un marker asterisco que lo que va a hacer va a dibujar una estrella y este lo que quiero que esa estrella sea de color negro y justamente la idea también es bueno tengo que referenciar la ubicación de cada centroide y finalmente la leyenda centroides que va a aparecer insisto en esa sección donde se bueno identifica cada uno de los componentes del gráfico explicado todo esto pasamos a dar finalmente el gráfico y vemos aquí la resultante de todo lo que explicamos el título general que lo último que hicimos el gráfico de dos coordenadas los puntos fíjense que cada uno de los cinco grupos de puntos tienen un color diferente Violeta azul rojo verde y naranja cada uno de estos de estas referencias están aquí en esta tabla ven que dice que las terceras y el color cluster 1 en color así hasta el cuarto que era lo que yo les había comentado antes y también me dice que las estrellas van a representarnos fíjense que en cada cluster existe una estrella negra que representa al centroide por eso dijimos recién todos estos puntos no estaban en un gráfico de dos dimensiones los pasamos a un gráfico de emisiones por eso hicimos un pc una transformación de la ubicación de cada punto y con los centroides también Hicimos lo mismo si una transformación de la ubicación de centroide Porque si esos puntos estaban en una estructura de 12 dimensiones que es difícil imaginarla pero este es así tenían esos centros de 12 coordenadas ahora tendrán que tener dos coordenadas por eso también tienen que hacer haber pasado por un proceso de capacidad de transformación y de ese modo cada punto tiene dos coordenadas y cada centroide tiene dos coordenadas bien en este caso todo lo que explicamos antes se ve claramente Pero puede ser que nos surjan una duda como esta pregunta que tenemos aquí este gráfico me muestra muchos puntos superpuestos especialmente en nuestros colores azul y violeta sí azul y rojo también este No tanto digamos el rojo con naranja rojo con verde fundamentalmente rojo con azul vamos a decirlo Entonces esto me puede hacer pensar de que quizás la distribución que yo he tenido aquí no es la más acertada sin riesgo a equivocar Me podría decir que el Violeta está bastante bien representado en naranja también el verde también Pero quizás esta distribución de los puntos rojo y azul con tanto nivel superposición nos llevaría a pensar que quizás el rojo y el azul podría ser solo cluster y Por ende tener un modelo de cuatro cluster y no de cinco ojo con esto este gráfico no nos tiene que confundir esto es una forma de intentar poder visualizar Este modelo en un gráfico Que obviamente tiene una pérdida de información muy grande pasé de 12 dimensiones a dos una imaginen ustedes esto con lo cual esta superposición en un ambiente de 12 dimensiones no es tal esto pasa Simplemente porque transforme algo de 12 dimensiones a dos y entonces a partir de ese aplastamiento de datos justamente hay datos que superponen bastante bien ha salido porque fíjense que hay tres cluster que son bastante diferentes tienen muy pocos puntos que superpone quizás lo más este representativo en cuanto a superposición lo representan los grupos azules los que nazcan Azules y rojo Pero insisto esto es simplemente una cuestión de percepción porque es una transformación en la realidad estos grupos son independientes y no se mezcla como aquí finalmente les dejo un apéndice como siempre algunos ejercicios adicionales para que los vean ustedes es un ejercicio muy interesante porque permite ver cómo puede utilizar un modelo de kamins para reducir la dimensionalidad de una imagen es muy interesante el ejercicio véanlo después me consultan pero está explicado obviamente paso a paso para que ustedes lo puedan ejecutar bien hay un archivo que tienen que levantar desde esta URL y se trata básicamente de esta imagen que tiene una dimensión muy grande es transformada en otra imagen que tiene una dimensión mucho más chica que ustedes la Ven aquí y la dimensión no pasa por el tamaño de la imagen sino por la cantidad de píxeles que tiene que obviamente aquí ustedes no lo pueden percibir pero lo van a percibir cuando termine el ejercicio porque van a poder comprobar que esta imagen original tiene una dimensión de cabeza que no es la misma que es la que tiene esta otra bien hasta aquí llegamos con esta clase y Por ende nos vemos en la próxima clase hasta aquí llegamos con esta clase Te espero en la próxima y última clase del módulo de Machine learning 