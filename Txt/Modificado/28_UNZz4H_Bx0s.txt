 Titulo: Clase15 (parte 1) Curso Inteligencia Artificial 
 URL https://youtu.be/UNZz4H_Bx0s  
 665 segundos de duracion 
 Hola bienvenidos al curso de Inteligencia artificial de ifes Los invito a empezar con nuestra clase número 15 Hola a todos empezamos con esta primer parte de la clase número 15 donde Nuestro objetivo va a ser dar respuesta a lo que nos quedó pendiente de la clase anterior los tres problemas que quedaron pendientes de resolver en la casa anterior fueron los siguientes funciones de activación manejo de peso de las neuronas y volubilidad de las redes problemas que se solucionaron con el artículo que mencionamos al fin de la clase pasada uno de los temas más importantes sobre los que hubo que trabajar en aquel momento fue el tema de las funciones de activación nosotros ya trabajamos con una de ellas que estaba en el perceptrón que era las funciones escalonada Pero luego se descubrieron muchas más y mejores que la vamos a nombrar a continuación muchos de los problemas que resolvemos constantemente con nuestro propio cerebro podemos verlos como funciones de entrada y salidas Por ejemplo si estamos viendo un número escrito manualmente como el del Data set Eminem y Queremos saber que número es podemos entender a este problema como una función la función de entrada es la imagen y la salida la respuesta A dilucidar qué número es el que corresponde esa imagen como dijimos en la clase pasada el propósito de los científicos que crearon las redes neuronales era emular el conocimiento del cerebro humano con una red computacional que tuviese el mismo nivel de aprendizaje profundo para alcanzar ese aprendizaje profundo dijimos que teníamos que crear una estructura con múltiples neuronas conectadas de forma secuencial y si recordamos también lo que dijimos en la clase pasada lo que hace cada una de estas neuronas es un problema de regresión lineal es decir que lo que estamos haciendo si lo planteamos matemáticamente es concatenar diferentes operaciones de regresión lineal el problema aquí es que sumar distintas operaciones de regresión lineal va a dar como resultado inexorablemente un valor lineal es decir una recta y un espacio de dos dimensiones un plano en un espacio de tres dimensiones o un hiperplano en un espacio unidimensional bajo este problema no importa la estructura de la red no importa Cuántas capas haya no importa Cuántas neuronas haya porque en realidad si toda esta concatenación de neuronas lineales dan resultado lineal toda esta estructura es lo mismo que tener una sola neurona los problemas del mundo real son mucho más complejos que una simple función lineal por lo que una red así no podría aprender nada interesante para conseguir que nuestra red No colapse necesitamos que esta suma de aquí dé como resultado algo diferente a una línea recta y para eso necesitaríamos que cada una de estas líneas sufra alguna manipulación no lineal de las distorsiones Cómo conseguimos Esto justamente con las funciones de activación esta sencilla función escalonada que nos ayudó la clase pasada a resolver con un umbral una decisión de si vamos a empezar o no a jugar al padre es ahora un elemento fundamental en todas y cada una de las neuronas de una red neuronal para que gracias al aporte de su distorsión no lineal podamos encadenar de forma efectiva la computación de varias neuronas obviamente que la forma en que la salida cambia según las entradas depende de qué función de activación estemos utilizando y por eso vamos a ver en detalle Cuáles son los tipos de funciones de activación más importantes que hay hoy en día Comencemos con la más simple que es la que ya venimos usando la función escalón o Step esta función realiza cero para todos los valores que están debajo de un valor llamado umbral y uno para todos los que están encima de ese valor llamado umbral nos da el componente no lineal que necesitamos sin embargo no es apta para el aprendizaje profundo actual ya que se limita a problemas muy sencillos de salida de tipo binario Esta es la función sigmoide y como vemos la distorsión que produce hace que los valores muy grandes se saturen en uno y los valores muy pequeños por tanto con esta función sigmoide no solo conseguimos Añadir la deformación que estamos buscando sino que también nos sirve para representar probabilidades que siempre vienen en el rango de 0 a 1 seguramente la forma de la figura te debe parecer familiar sí es la misma que usamos para representar el comportamiento de un algoritmo de revisión logística en el módulo de otra función similar a la chimoide es la función tangente hiperbólica o tan cuya forma es similar a las inmóviles pero cuyo Rango varía de menos uno a uno softmax es la función más utilizada para las redes de clasificaciones no binarias y muy necesario como ya dijimos para que las salidas sean de tipo probabilístico Por ejemplo si la entrada a una red es un número 5 escrito manualmente y digitalizado como en el caso de lo que tenemos en Data de salida tiene 10 neuronas como salida de 0 al 9 Entonces Esperamos que la neurona que representa al 5 sea la que probabilísticamente tenga Las mayores posibilidades y finalmente esta función de activación que es la más utilizada hoy en el campo del dip de Darling la unidad rectificada lineal o básicamente se comporta como una función lineal cuando es positiva y constante acero cuando el valor de la entrada es negativo se representa con esta expresión o esta fórmula que está aquí no obstante Los investigadores siguen buscando mejores funciones de activación y algunas de las que hoy están en pleno proceso de investigación son estas que están aquí o lrelu tiene una función con esta forma es muy similar a relu pero en lugar de dar cero para los números negativos multiplica el valor por un número muy pequeño 0.01 generando una gráfica como esta parametric reloj o prelu es prácticamente lo mismo que el relu pero en lugar de multiplicarse ese valor por 0.01 se utiliza un parámetro que puede ajustarse Hello la cual es muy similar sin embargo es más suave y es usada por el hoy ya mundialmente conocido gpt hay muchas opciones más que hoy están en instancia de investigación pero los científicos aún no la consideran formalmente parte de todas las que te mencioné antes ahora Es lógico y encuentro razonable que te preguntes después de ver tantas funciones de activación cuando aplica uno y cuando aplicó otra la función logística o sin móviles se recomienda solamente en la capa de salida cuando requerimos un resultado binario 0 o 1 verdadero o falso la tangente hiperbólica o tan es superior a la función logística para capas ocultas sin embargo en muy pocos casos da resultados superiores a Reno relu es la que normalmente vas a utilizar A menos que quieras experimentar mucho con otras funciones y Por ende puedas experimentar con el reloj o PR elu es muy nueva pero de manera general tiene mejores resultados que relu especialmente en redes muy grandes y finalmente está softback que es una variante muy importante que no mencionamos antes en realidad como dijimos antes conviene usar la función sigmoide Cuando tenemos una salida binaria es decir solamente dos valores posibles pero si tenemos más de dos valores posibles la función para la salida es softmax con esto terminamos la primera parte de esta clase en la segunda parte de esta clase vamos a ver los dos temas que nos quedan pendientes la volubilidad de las redes y el manejo de los pesos nos vemos en la siguiente parte aquí terminamos con la primera parte de esta clase nos vemos en la segunda parte