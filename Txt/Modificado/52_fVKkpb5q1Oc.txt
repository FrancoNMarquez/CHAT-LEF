 Titulo: Clase 27 (parte 1) Curso de Inteligencia Artificial 
 URL https://youtu.be/fVKkpb5q1Oc  
 1469 segundos de duracion 
  Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número  27  Hola a todos Bienvenidos a la clase número 27 del curso de Inteligencia artificial de ifes hoy vamos a ver uno de los temas más importantes del mundo del nlp el tema  embeddings Qué es un embedding un embedding es una forma de vectorización similar a la que vimos pero que plantea una evolución respecto a esas formas para recorrer esas formas vamos a recurrir a esta imagen que graficab612 y de poner un ejemplo bastante sencillo habíamos tomado como que cada uno de estos renglones era un eh documento y cada una de estas columnas que tenos tenemos aquí dibujadas era el vector que representaba a cada uno de sus documentos y todo lo que tenemos aquí a la izquierda el abecedario me gusta el deporte hasta llegar a que odio y sí eran 18 palabras con lo cual tenía una representación de cada uno de los documentos con vectores de 18 posiciones y cada una de las posiciones tenía un número que representaba Cuántas veces estaba Esa palabra que estaba a la izquierda estaba en el contexto de ese documento Pero cuál es el problema aquí aquí tenemos vectores de 18 posiciones pero fíjense cuántas posiciones están realmente ocupadas en el primer caso tenemos ocupadas seis posiciones con lo cual tenemos 12 posiciones que no tienen ningún tipo de información en el caso del documento dos está un poquito digamos más relleno tenemos 1 2 3 4 5 6 7 8 pero tenemos igual más del 50% de las posiciones Sí están siendo ocupadas por espacios en blanco por ceros en el caso del documento 13 es lo mismo bueno Esto pensemos que es un ejemplo muy muy sencillo pero justamente si lo tratáramos de llevar a un ejemplo más complejo más real donde puedo tener un vocabulario de 2000 3000 y hasta por qué no 20,000 palabras ustedes piensen si en ese contexto yo dejara de utilizar O estaría sub utilizando de alguna manera Por decirlo este las posiciones de los vectores en esta misma proporción en una proporción del 60 por por decir algo estaríamos hablando de vectores de 20,000 posiciones de las cuales estaríamos usando solamente el 40% dec 8,000 es decir sería un despropósito y estaríamos Obviamente con una sobrecarga de información que no ayudaría nada a nuestro algoritmo y nuestro proyecto los enedis vienen a proponer una forma diferente de vectorizar un concepto diferente de vectorizar justamente tratando de proponer una solución que no eh bueno que proponga justamente una postura diferente que evite desperdiciar la cantidad de espacio que estamos desperdiciando en vectores que no tienen ningún tipo de información por eso este concepto de embeddings me lleva a una idea que sería la siguiente supongamos que en el caso de recién yo pensara en tener vectores en lugar de 18 posiciones que tengan con un vocabulario de 18 palabras supongamos ocho posiciones sí es decir un número significativamente menor con el método del contol de palabra Eso no se puede hacer pero sí lo podría hacer pensando en tener vectores que tengan valores que no tengan que ver como en el caso anterior que tenemos aquí la cantidad de veces que aparece una palabra sino otro tipo de valor que tuviera que ver con justamente ahorrar la cantidad de dimensiones pero que cada una de las posiciones del vector tenga un valor es decir que no haya posiciones que tengan valores cero o que estén subutilizadas en este caso fíjense que yo tengo por ejemplo Un ejemplo muy sencillo en el cual supongamos que yo pudiera estar representando un conjunto de palabras solamente con dos dimensiones y aquí tengo por ejemplo la palabra auto que está representada por un vector 0109 o sea de dos posiciones que tengan los valores 0.1 y 0.9 y la palabra camioneta eh representada por otro vector de dos posiciones pero que tiene valores 015 y 085 veamos que más allá de esta cuestión de que las palabras no están representadas por una gran cantidad de dimensiones sino solamente por dos los valores que tiene auto y camioneta son bastante parecidos esto es importante y lo tenemos visto ya desde la clase pasada donde hablamos justamente de la similitud de vectores Porque estos valores de camioneta y de auto representan la similitud que tienen sem iamente esas dos palabras entonces obviamente los vectores tienen que actuar en consecuencia tienen que tener valores en consecuencia del mismo modo tengo el ejemplo de casa y departamento donde veo que los valores 075 027 o 07 y 02 son muy similares y muy distintos a su vez de auto y camioneta es decir casa es similar a departamento auto es similar a camioneta pero ni auto ni camioneta son similares a casa y departamento y viceversa bien importante aquí es que el concepto de embedding me lleva a utilizar vectores que no tienen la misma cantidad de Dimensión que la cantidad de de palabras que tiene el vocabulario tiene una dimensión mucho menor y los valores de cada una de esas dimensiones representan un valor en un espacio vectorial de la cantidad de posiciones que sea pero que no tiene que ver con la ya con la cantidad de palabras que o la cantidad de veces perdón que aparece esa palabra en el documento sino con un número que en el espacio me permite Sí con un valor determinado establecer una similitud entre una palabra y otra o diferenciarla con otra palabra diferente los modelos para en bedim utilizan eh conceptos vinculados al Deep learning al mundo del Deep learning que nosotros Ya estuvimos incursionando en él justamente con lo que tenemos aquí como primer opción de tipo de de red para eh realizar o llevar a cabo el embedding que son las redes neuronales convolucionales bueno es una de las opciones no es la opción más utilizada la opción más utilizada es las redes neuronales recurrentes que es lo que aparece aquí arriba de la derecha y dentro de ese contexto Y con esa ampliación que plantea esta rnr hay un concepto que es concretamente el que se usa hoy en día que es el de Transformers que es el concepto de mayor evolución y de mayor uso en la actualidad vinculado a los procesos de embedding técnicas para eh llevar a cabo los procesos de embeddings hay muchas nosotros vamos a empezar por ver dos de las que hoy son muy importantes en el mercado Pero después vamos a ver otras que plantea justamente la evolución de las últimas tendencias esta primera que está aquí arriba es Word to Back es decir básicamente un juego de palabras que es o implica la transformación de una palabra en un vector y justamente es un algoritmo como dice aquí que utiliza un modelo ronal para aprender asociaciones de palabras a partir de un gran Corpus de texto o sea cuando hablamos de Corpus hablamos de un conjunto de palabras que puede estar representado por uno y hasta por varios documentos bueno el Corpus de texto es todo ese gran conjunto de texto a partir del cual yo voy a entrenar una red como bien dice aquí una vez que entreno el modelo puedo detectar palabras sinónimas o sugerir palabras adicionales para una frase sin terminar es tal cual lo que vimos lo que venimos hablando perdón de lo que nos pasa cuando estamos en gmail y escribimos un texto y nos aparecen palabras sugeridas a continuación de lo que estamos escribiendo Word tob representa cada palabra distinta con una lista particular de números llamada vector es decir lo que vimos recién en el ejemplo de esta relación aut camioneta o casa departamento Bueno eso es Ni más ni menos lo que hace Word o lo que hacen los enedis en general Sí y eh A partir de eso puedo justamente ver la similitud que existe entre vectores y en consecuencia ver la similitud que existe entre los textos a través de la teoría o el teorema del coseno que vimos la clase pasada que justamente midiendo el ángulo que existe entre los vectores puedo ver el nivel de similitud entre las palabras que representan esos vectores otra técnica para embedding es glow glow es un desarrollo de la Universidad de Stanford y significa vectores globales es una opción similar a Word desarrollada por otro proveedor como muchas veces pasa en el mundo de la informática y en el mundo de la ia y eh si bien tiene propósitos muy similares tiene una particularidad tiene en cuenta las estadísticas de coocurrencia del Corpus es decir se fija o va registrando estadísticamente los conjuntos de palabras o los pares de palabras Cuántas veces aparecen juntas en el Corpus para de esa manera cuando una palabra determinada esté siendo redactada por una persona por ejemplo en gmail como referenciamos recién la palabra que le sugiera a continuación sea la que más frecuentemente aparece cuando esa persona escribe al lado de la palabra anterior el próximo tema que tenemos que ver es el concepto de analogía de palabras en el ámbito de los embeddings y aquí tengo un ejemplo en un espacio bidimensional donde tengo países Argentina y Uruguay y sus respectivas capitales pero básicamente son ciudades Buenos Aires y Montevideo la idea aquí es que Argentina y Uruguay están cerca entre ellas porque en realidad identifica que se trata de dos países y del mismo modo Buenos Aires está cerca Dentro de este contexto de esta estructura bidimensional de Montevideo porque entiende que son dos ciudades pero más allá de eso también se puede plantear un concepto de analogía de qué se trata la analogía que yo pueda decir que si hago una relación Argentina Buenos Aires pueda descubrir Cuál es la potencial relación que puede tener Uruguay o Montevideo es decir que si yo digo que Buenos Aires es a Argentina como Montevideo es a Uruguay en realidad cuando yo digo estoy diciendo esto yo pretendo que la ia sea quien me lo diga por ejemplo yo puedo decir Buenos Aires es Argentina con Montevideo es a y ahí la ia me va a responder Uruguay Ese es el planteamiento de analogías y es parte de lo que vamos a ver a continuación en el lap número cinco donde vamos a ver justamente similitud de vectores con embedding y el concepto de analogías utilizando un dataset cagle de palabras en castellano nosotros vamos a hacer un primer lab en esta primera parte de la clase en el cual Vamos a abordar el tema de utilizar un embedding o un archivo que tiene los embedding ya hechos y en la segunda parte de esta clase Vamos a abordar el tema de eh hacer nuestros propios embedding con nuestro propio conjunto de palabras o nuestro propio Corpus bien aquí estamos en el apn l5 en el ámbito de colab y vamos a empezar con el propósito que aquí tenemos similitud de vectores con embedding utilizando una de kaggle con palabras en castellano antes de empezar a configurar todo este trabajo vamos a hablar un poquito de este eh dataset que vamos a bajar este Corpus hablando más en términos de nlp que bajamos de kaggle eh este dataset se llama pretrained Word vector for spanish el el título de El dataset como se lo identifica en kaggle es un dat que contiene 1,653 Word embeddings de Dimensión 300 o sea 1,653 palabras cada una de esas palabras está representada por un vector de 300 dimensiones fíjense eh lo importante de lo que hablamos hace un rato Es decir yo quizás con el conteo de palabras hubiera tenido un vocabulario de 1,653 palabras pero a su vez vectores para cada una de esas palabras de 1,653 en este caso fíjense la enorme diferencia que hay entre la dimensión del vocabulario y la dimensión de los vectores bien esto ha sido entrenado justamente con Word tob que es una de las herramientas que vimos recién en la teoría y obviamente ha sido entrenado con Corpus de billones de palabras en español esto es muy común hay muchos archivos que ya tienen los vectores armados con un vocabulario muy grande de palabras y me sirven mucho para esta fase de aprendizaje para probar cosas y no tener que formar yo mi propio Corpus pero como dijimos hace un rato en la segunda parte de la clase eso es lo que vamos a hacer obviamente no vamos a llevar a un Corpus tan grande como esto pero es importante que sepamos usar lo que ya está hecho bienvenido Que esté hecho y también empezar a usarlo nuestro dicho esto vamos entonces al principio y vamos a importar pandas y luego de ello vamos a montar el Drive como siempre hacemos dado que allí tiene que estar el archivo que hablábamos recién sbw vectors 3001 5.txt obviamente Este es un archivo que ustedes van a tener en el campus virtual Y ustedes lo van a tener que poner como siempre animos haciendo en el Drive que ustedes quieran utilizar Bueno ya tenemos montado el Drive lo que vamos a instalar ahora es una librería que se llama hensim que Eh no la tenemos seguramente instalada con lo cual vamos a hacer primero un pip install genim Recuerden que siempre que les digo que eh cada vez que no tengan alguna librería en la actividad que estén llevando adelante bueno pueden instalarla con pip install enim el signo de admiración refiere a que este Comando no se ejecuta dentro de colap sino fuera de colap y el menos q es para que no me muestre mucha de la información que muestra habitualmente un proceso de instalación es un detalle Simplemente no pasa nada si no pongo el menú q bien luego de importar el genim eh Perdón luego de instalar el genim lo importamos y como lo hicimos tantas veces si creamos un dataframe con pandas con rsb en base a este archivo que referenciamos recién y lo ponemos dentro de una refame que le ponemos como nombre DF bien Ahora vamos a ver el contenido de DF y podemos ver rápidamente queé tenemos en principio la palabra d dígito la enl O sea toda la palabra que tiene este gran voc formulario de 1,653 palabras y cada una de esas palabras está representada como dijimos antes por un vector de 300 posiciones en el cual aquí estamos viendo las primeras cuatro y un poquito de la quinta posición en algunos casos aquí solamente las cuatro Pero podemos observar las cuatro primeras posiciones de cada una de estas palabras con valores que son los que tiene justamente cada uno de esos vectores en el contexto de un espacio dimensional de 300 dimensiones sí bien eh aquí tenemos entonces toda la la información en una sola columna o sea está junto la dimensión del vector con el texto de la palabra y como dijimos antes las eh 1,653 palabras bien una vez que vemos esto vamos a empezar a crear los vectores o traernos los vectores a nuestra estructura a través de una variable que le voy a poner vectores y la voy a eh llenar justamente a través de la biblioteca gensin que acabamos de instalar recién y con lad Word tob format le voy a decir que vaya a buscar esa información de los vectores si a el archivo que recién acabamos de observar a través de este Data frame entonces a partir de esto todos estos vectores que estamos viendo aquí s todos estos vectores que están aquí al lado de cada una de las palabras van a ir o van a ubicarse dentro de esta variable de vectores en este práctico como dijimos antes vamos a ver dos cuestiones propias de los embedding la similitud y las analogías aquí por ejemplo Argentina y Uruguay son palabras similares Buenos Aires y Montevideo son palabras similares ahora Argentina y Buenos Aires son palabras análogas y Uruguay y Montevideo lo mismo eso lo vamos a ver justamente ahora en la codificación de El colap aquí estamos entonces luego de haber haber eh colocado todos los vectores en esta variable de vectores lo cual nos llevó un buen tiempo pero ya tenemos esta cantidad enorme de eh vectores dentro de esta variable con lo cual vamos a definir en principio una función analogía para ir poniendo distintos valores y no tener que estar reescribiendo el código tantas veces esta función le voy a poner ver analogía le vamos a dar como input tres valores La idea es como dijimos hoy en el ejemplo de la de la clase teórica que yo le pase por ejemplo Argentina y Buenos Aires y luego le pase Uruguay y automáticamente la inteligencia me dé la palabra Montevideo con lo cual siempre le voy a tener que dar tres valores y va a aparecer automáticamente el cuarto que va a ser la analogía de uno de esos valores para ello vamos a usar de eh la variable que creamos aquí vectores el método m similar en el cual le voy a pasar la el valor de entrada p1 y p3 que se dirían las dos palabras similares pero no análogas y luego con el argumento negativo le voy a pasar la palabra análoga de la primer palabra que puse Aquí concretamente vamos un ejemplo supongamos que en p1 yo pongo Argentina en p2 podría Buenos Aires en p3 pondría Uruguay y estaría la la representación de la palabra faltante como algo pendiente a resolver por parte de justamente m similar de vectores eso lo voy a poner dentro de esta variable similitud y luego voy a hacer un print donde voy a poner que p1 es ap2 como similitud 00 que es justamente el valor que me va a tirar la ia es ap3 bien con esta lógica que vamos a cargar porque esto acuérdense que las funciones la tenemos que declarar y ejecutarla no como que vaya a hacer una acción sino que esté identificada y dispuesta eh disponible Perdón esta función vamos a usar en principio con rey y hombre y luego la palabra mujer lo cual nos tiene que dar la posibilidad de que la palabra análoga que nos tiene que brindar esta función y Por ende la ia es la palabra reina lo ejecutamos y justamente el resultado es rey esa hombre como reina esa mujer lo probamos con Argentina argentino y español y dice que Argentina es argentino como español es a español como española Perdón es a español bien en el caso anterior la palabra que me brindó la ia es reina y en el caso este es española en este caso fíjense que toma Argentina como una mujer es decir no como la la patria como el país porque justamente la primer letra está puesta en minúscula si yo ahora pruebo poner la primer palabra en mayúscula va a tomar Argentina como país y o como una persona de sexo femenino propia de nuestro país sí Entonces ahora la respuesta va a ser Argentina es argentino como España es a español bien y así puedo probar aquí con taller mecánico y médico talleres a mecánico como consultorio ex a médico y con algunas ciudades Madrid es España como montevo es Uruguay bien con eso tenemos varios ejemplos de analogías ahora vamos con el tema de similitud con lo cual vamos a hacer otra función que le vamos a llamar ver cercanía donde aquí no entran eh un conjunto de palabras como en el caso de la función anterior sino simplemente una palabra a la cual le vamos a buscar cuál es el vector más similar o que representa una mayor similitud con esa palabra con lo cual en este caso vamos a usar el Moss similar pero ahora solamente con un argumento positivo antes teníamos el positivo los positivos y el negativo por esta cuestión de la analogía ahora simplemente Busco algo que es similar al argumento que entra con lo cual Busco algo que sea similar a lo que yo tengo aquí como argumento y luego hago un print de eh Cuáles son los elementos cercanos a p Por qué digo los porque justamente lo que vamos a ver aquí es tratar de determinar una cantidad de valores similares o sea no voy a tomar la sugerencia de solamente el vector que tiene mayor similitud sino voy a tomar una serie de candidatos eh de similitud con lo cual me va a dar un conjunto de palabras similares a la que yo le doy por eso hago un for con el cual recorro Sí justamente la variable vecinos que es la que aloja a todo el conjunto de vectores similares a la palabra ejemplo que yo le estoy dando y Forward Score In vecinos voy a mostrar justamente cada una de esas palabras y en este caso no estoy mostrando el score lo podría hacer también con lo cual puedo estar mostrando la palabra y el nivel de factibilidad que me da la la inteligencia respecto de cuán similar entiende es esa palabra a la que yo he candidatado bueno habiendo cargado esta función vamos a probarla con lo cual voy a poner la palabra fútbol y aquí tengo balonpie fútbol soccer béisbol basketbol o sea palabras similares recuerde que similares no quiere decir que sea concretamente algo que sea Exactamente igual sino palabras que tengan un alto nivel nivel de asociatividad pongo Argentina en minúscula representando como hoy sí una una persona de exceso femenino que pertenece a nuestro país no el nombre de la patria bueno y aquí me aparecen las recomendaciones de similitud ahora Argentina mayúscula y me aparecen países ven que en este caso aparecen países y en este caso aparecen otras cuestiones que tienen que ver justamente con Eh bueno las bolivianas las mendosinas las paraguayas las brasilenas tomando el femenino de una persona que está representada por esa parte bien con esto Terminamos el lab y em les dejo para ustedes a ver si quieren ponerles aquí Este esta variable score Sí este parámetro score para ver justamente Eh bueno qué nivel de score le da a cada una de estas palabras que consider las similares obviamente esto es importante porque después pu en base a eso pueden determinar con cuál de todas estas palabras ustedes se van a quedar a la hora de hacer una una inteligencia o programar un un algoritmo de nl bien con esto terminamos decía este lab con esto terminamos también esta primer parte de esta clase lo que nos queda para la segunda parte Entonces es algo similar de un proceso de embedding pero con nuestro propio Corpus hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte i