 Titulo: Clase 11 (parte1) del Curso de Inteligencia Artificial 
 URL https://youtu.be/3Ssh1SsiW6s  
 1183 segundos de duracion 
 Hola bienvenidos Esta es la primera parte de la clase número 11 del curso de Inteligencia artificial de ifes en ella Vamos a aprender un nuevo algoritmo de Machine learning máquinas de soporte vectorial empezamos  Hola a todos Bienvenidos a la clase número 11 del curso de Inteligencia artificial de ifes seguimos en el módulo de Machine learning y seguimos con el propósito justamente de conocer hoy un nuevo algoritmo de Machine learning que se llama máquina de soporte vectorial que es un algoritmo de máquinas de soporte vectorial es un algoritmo de aprendizaje automático de tipo supervisado recordarán en la primer clase cuando introdujimos justamente el concepto de algoritmo de Machine learning y dimos un listado de todos los de tipo supervisado Este era uno de los últimos justamente este algoritmo se puede usar tanto para problemas de clasificación como de regresión Aunque claramente tiene su potencial mayor en los casos de clasificación en este algoritmo si se trazan los puntos en un ámbito unidimensional y luego se realiza la clasificación de segregación entre ambos puntos con un hiperplano que es el que mejor separa ambas clases para entender este concepto vamos a utilizar no n dimensiones sino dos dimensiones x y cada uno de estos puntos que están en ese espacio unidimensional en este caso va a ser en un ámbito de dos dimensiones tenemos Aquí estos puntos rojos que están por un lado y estos puntos verdes que están por el otro bien justamente ese hiperplano que mejor separa las dos clases en este caso dado que tenemos un ámbito de dos coordenadas va a ser una recta y la recta justamente es lo que tienen gráfica Ahora aquí esta recta es la mejor recta mejor hiperplano en un ámbito de dimensiones que separa este conjunto de este conjunto Pero qué son los vectores de soporte y Qué es el hiperplano bueno justamente el hiperplano hablamos recién en este caso en la recta puede ser trasladado si tenemos un ámbito de una dimensión mayor a un plano o un hiperplano en el caso que sean más de tres dimensiones y los vectores de soporte son estos puntos que fíjense que están referenciados aquí es decir este rojo y estos dos verdes Por qué son vectores de soporte porque son los puntos de cada una de las clases que están más cerca del hiperplano plano o recta depende la dimensión que mejor separa ambas clases vamos a ir analizando cómo funciona este nuevo algoritmo de máquina de soporte vectorial ya nos quedó claro que lo que buscamos es un hiperplano que separe de la mejor manera posible grupos distintos de puntos ahora Cómo podemos identificar cuál es el plano correcto porque justamente pueden haber distintas formas de distribución de puntos como justamente pueden haber muchas situaciones es que nosotros vamos a tomar para esta clase cinco casos diferentes de distribución de puntos cinco problemas diferentes para ver justamente en cada uno de ellos Cuál sería la solución óptima de cómo es la forma de encontrar el hiperplano correcto en este primer caso tenemos dos grupos de puntos por un lado de las estrellas celestes y por el otro lado los puntos rojos vamos a suponer tres propuestas diferentes de rectas ya que este caso el hiperplano corresponde a un espacio bidimensional que podrían ser las ideales para separar esos dos conjuntos de puntos Tenemos aquí la rectada la recta b y la recta c Cuál sería la mejor recta que separa estos dos conjuntos de puntos se debe recordar una regla general antes de dar esta respuesta que es seleccionar el hiperplano que mejor separa en este caso las dos clases en esta situación el mejor hiperplano que cumple con esa Consigna es el hiperplano B ahora vamos a ver un caso más en el cual vuelvo a tener las estrellas por un lado y los puntos rojos por el otro Y supongo que una recta dispuesta en diagonal puede ser la mejor forma de separar esto estos dos grupos de puntos Bueno pero me propongo dibujar esa recta Y supongo que puedo tener tres rectas que pueden cumplir con esa Consigna por eso Aquí tengo la recta a la recta b y la recta c que cumplen con esa Consigna pero no del mismo modo porque evidentemente no están dispuestos en el mismo lugar Cuál será la mejor recta que cumpla justamente con la Consigna de ser el mejor hiperplano que separan ambas clases la que cumple esa Consigna del mejor modo es la recta c porque porque si bien tanto la B como la Seo como la separan ambos grupos Es evidente que la B está mucho más cerca de las Estrellas y Por ende podría haber un margen de error mucho más chico la a lo mismo pero en el sentido contrario hacia los puntos rojos y la c tiene una distancia muy buena hacia un grupo y hacia el otro con lo cual el margen de error Sería mucho menor y Por ende c es la mejor recta el mejor hiper plano para separar estas dos clases luego tenemos otro caso más caso 3 vuelvo a tener los mismos grupos pero fíjense esta Estrella Azul Aquí está dispuesta de un modo bastante alejado del resto del grupo tengo como propuesta la recta a y la recta B Cuál será la mejor uno puede presumir en primera instancia que la recta de es la que mantiene la mejor distancia con el grupo de las Estrellas y con el grupo de los puntos teniendo en cuenta que tengo una estrella que estaría mal clasificada la recta a por el contrario no tiene el mismo nivel de margen respecto de las dos clases pero clasifica bien porque en este caso la estrella a pesar de estar alejada del resto Está bien clasificada como estrella y no como punto rojo en este caso de nuevo surge la pregunta Cuál será la mejor de las dos opciones la mejor es la opción a porque porque en esta disyuntiva que puede presentarse qué voy a priorizar la distancia respecto de las dos clases o la correcta clasificación lo que ganas esto último yo primero tengo que cerciorarme de que el hiperplano sea un buen clasificador y después obviamente lo ideal sería que tuviese una buena distancia respecto de las dos clases ahora tenemos el caso 4 que es una situación más o menos similar a la de recién Por qué Porque si bien la estrella está separada de la clase del resto de las estrellas está tan alejada que parece que formará concretamente parte del grupo de los puntos rojos en este caso Cuál será la mejor opción la mejor opción es esta recta que está aquí pero ustedes me van a decir Bueno pero no clasifica bien esta estrella bueno esta estrella es considerada un valor atípico y justamente este algoritmo de máquina de soporte vectorial tiene una forma de ignorar justamente los valores atípicos para poder clasificar bien y Por ende esta recta para este caso es la mejor solución y finalmente tenemos el caso 5 que es el más complejo fíjense que en este caso tengo las estrellas como si tuviesen una coordinación o formaran un grupo pero en un sentido circular y en el mismo sentido del grupo están de una manera agrupada en sentido más circular pero más juntas todo el grupo de los puntos rojos lo que yo voy a hacer en este caso es introducir un algoritmo que lo que hace es crear una dimensión Z la dimensión Z que no existe hasta hoy porque está la x y la y lo que hace es a través de un algoritmo Elevar al cuadrado la x y elevar al cuadrado la y como está esta fórmula que está aquí abajo y sumadas crear Z eso produce un efecto que cuál es los puntos que están en este gráfico distribuidos de esta manera se distribuyen en este otro gráfico de esta otra manera aquí la coordenada z que está aquí arriba y la coordenada x que la que está aquí al costado estos puntos son los mismos de aquí pero al haber sido producto de aplicar esta fórmula se distribuye de este modo concretamente a partir de esa realidad lo que voy a obtener es la posibilidad de separar ambos grupos quizás con una recta que pase por esta zona que no tengo en este gráfico donde es imposible trazar una recta que separe correctamente estos dos grupos de puntos entonces lo que hice aplique un truco a través de un algoritmo donde transforme los puntos para que se distribuyan de otra manera teniendo como resultado una manera distribuida como esta que está aquí que sí me permite dividir las dos clases cosa que no podía hacer con este otro gráfico tomando como base lo que vimos recién vamos a pasar un problema mucho más complejo nosotros tenemos en realidad todos los ejemplos que hemos visto hasta ahora en un gráfico de dos dimensiones y nos podemos llegar a preguntar si siempre Esta división la tengo que hacer como lo hicimos nosotros hasta ahora es decir de una manera visual La respuesta es que no obviamente yo tengo un algoritmo que es quien toma esa ese papel ese rol Pero si yo llevo esto ya a problemas donde tengo más dimensiones que dos puedo tener un problema que es el siguiente de que justamente los puntos parecido al problema que vimos en el último caso en el caso 5 este distribuido de una manera que sea imposible encontrar un hiperplano que lo separe con lo cual lo que tengo que recurrir es a un concepto a un truco kernel así se llama este concepto que lo que hace justamente es poder redistribuir los puntos de una manera diferente para que eso favorezca la posibilidad de encontrar un hiper plano que separe Dos clases para lograr eso lo que yo tengo que hacer es incorporar una dimensión más cada que justamente esa separación pueda darse de un modo de que yo justamente encuentre esa solución esto parece un contrasentido respecto de lo que hablamos en la clase pasada donde lo que buscamos era reducir la dimensionalidad Pero bueno con un propósito de poder tener un algoritmo que fuera tan eficiente como el que tenía una dimensión más pero fuera más rápido en este caso es un sentido totalmente inverso es decir lo que yo busco es justamente aumentar una dimensión porque con la dimensión que tengo la separación es imposible aquí tenemos algunos gráficos que pueden representar fácilmente esa idea fíjense que yo tengo un primer caso donde tengo todos estos puntos que están aquí y cómo se ven están mezclados los puntos rojos con los negros de esta manera es imposible encontrar en este caso en un ámbito dedimensional bidimensional Perdón una recta que separa ambos puntos yo voy a llevar esta estructura de dos dimensiones a una estructura de tres dimensiones y el resultado se transforma en este gráfico que está aquí a la derecha y si me da la posibilidad de en este ámbito de tres dimensiones donde los puntos sean distribuidos de otra manera encontrar un hiperplano que separa ambas clases en un sentido similar tengo otro ejemplo como este que está aquí donde tengo un grupo de puntos que están concentrados en un círculo y otros que están concentrados en un círculo pero más grande y que abarca la anterior de nuevo no puedo encontrar en este espacio bidimensional una recta que separa ambas clases con lo cual incorporo nuevamente una dimensión más y llego a una situación como la que está aquí en este gráfico en donde justamente esos puntos se separan claramente y me permiten a mí encontrar un hiperplano que pueda separar ambas clases como tu algoritmo de Machine learning y en línea A lo que hemos venido tratando en Las dos últimas clases desde el momento que incorporamos el concepto de tuneo de hiper parámetros el algoritmo de Machine learning este algoritmo de máquina de soporte vectorial también tiene sus hiper parámetros el primero de ellos es el carnet lo que acabamos de ver recién esta cuestión del truco kernel de esta manera de redistribuir los puntos agregando una dimensión tiene tres opciones posibles luego vienen los hiper parámetros gama y C Que funcionaba habitualmente en conjunto el valor de gama es un valor numérico y ese valor en tanto sea más alto intentará ajustar exactamente el algoritmo a las características de conjunto de entrenamiento tema que ya vimos Es peligroso porque justamente nos lleva a un sobre ajuste que es uno de los tipos de resultados que no queremos tener por lo tanto hay que ajustar el valor de Gama de manera cuidadosa Para no caer en ese error un tema importante este hiper parámetro gama no se usa si la opción de carne elegida es la lineal luego viene el parámetro de penalización c del término de error Ese es el nombre que tiene este hiper parámetro set este preparametro lo que controla es el equilibrio entre los límites de decisión y la clasificación correcta de los puntos de entrenamiento para entender un poco mejor esto vamos a recurrir al caso 3 que vimos hace un rato donde teníamos una estrella que estaba del lado del resto de las estrellas pero un poco alejada del grupo mayoritario y por otro lado el grupo de los puntos rojos nos preguntamos en este caso si lo mejor era la recta b o la recta viendo que priorizamos la clasificación en el caso de la recta o la buena separación de los grupos con una distancia muy buena respecto de un grupo o de otro que proponía la recta B esto es una cuestión que tenemos que equilibrar y justamente Este es el rol que tiene este hiper parámetro C para poder encontrar un equilibrio entre el nivel óptimo de separación y el nivel óptimo de clasificación Finalmente y al igual que en el caso del hiper parámetro Gamma el hiper parámetro c no se usa si la opción elegida de kernel es la lineal a los fines de terminar de entender estos dos últimos siete parámetros y esta cuestión de verlos en conjunto hemos puesto aquí cuatro gráficos donde lo que hacemos es ir variando los valores de ambos desde valores bajos a valores exactos Qué pasa cuando tengo un gama que aquí se representa con un valor similar con una letra similar a una y tengo un gama bajo y un sebajo bueno la división que se produce es esta Sí para este ejemplo fíjense que la división propone una buena separación pero no copia tal cual la realidad del conjunto entrenamiento porque digo esto porque fíjense que hay triángulos verdes del lado que no corresponden y hay cuadrados azules también del lado que no corresponde en el siguiente gráfico paso a pasar a el c a un valor muy alto y el gama lo dejo el mismo valor que antes Cuál es la diferencia con el caso anterior esa recta si esa línea pasa a ser una curva Y esa curva lo que intenta hacer es hacer una separación tal que esos elementos que estaban del lado incorrecto ya no lo estén fíjense la diferencia es muy clara respecto del caso anterior ahora todos los triángulos están de un lado y todos los rectángulos están del otro en el siguiente gráfico paso a subir al gama a un valor alto y vuelvo hacia atrás el valor de c que produce Esto bueno acuérdense que el gama alto lo que nos llevaba era un problema de sobreajuste y aquí está claro porque fíjense que el grupo Trata de hacerlo dejando la menor cantidad posible de valores del lado incorrecto como recién vimos con el caso de El c alto Pero cuál es la diferencia fíjense que en el caso del sea alto buscaba que esa división fuese lo más estricta posible pero abarcaba un montón de lugares donde no había ningunos de estos cuadrados Azules es decir sectores donde no había datos en este caso eso no lo hace es decir se cierra exclusivamente hacia los lugares donde están esos valores pero tiene un efecto aún fíjense que hay aquí cuadrados del lado que no corresponde y un triángulo también del lado que no corresponde Cómo se usan a eso bueno poniendo justamente un sea alto nuevamente con lo cual esto sería el mejor modelo probablemente para el conjunto de entrenamiento mejor modelo porque fíjense que ahora esta nueva figura No deja ningún cuadrado azul del lado incorrecto ni ningún triángulo verde del lado del correcto esto obviamente es un modelo que va a dar un score muy bueno pero seguramente me va a llevar a un problema sobre ajuste que no va a generalizar para el futuro Cuál es el método ideal Bueno tengo que buscar algo que sea equilibrado entre los valores de gama y C y por eso tenemos que usar como ya lo hemos hecho antes un algoritmo como gris se ve que nos sugiera Cuál es el valor más equilibrado para tener el mejor score pero con una mayor probabilidad de generalización a futuro para terminar esta primer parte de esta clase Vamos como siempre a las ventajas y desventajas de este nuevo algoritmo máquina de soporte electoral por el lado de las ventajas funciona muy bien con un claro margen de separación es decir cuando los conjuntos están bien separados Sí bueno este conjunto Funciona muy bien aunque tenga que hacer un hiperplano recto o con alguna forma Curva pero que estén esos grupos separados favorece que este algoritmo funcione bien es eficaz en espacio de Gran dimensión acuérdense que hablamos de espacios unidimensionales estamos graficando espacios de dos coordenadas y tres coordenadas justamente para poder visualizarlo de un modo que lo podamos entender mejor pero evidentemente esto puede llegar a otra dimensiones superiores a 3 utiliza un subconjunto del conjunto de entrenamiento en la función de decisión llamado vectores de soporte recordarán el gráfico el primer gráfico de esta clase donde hablábamos de los vectores de soporte que son los que están más cercanos a la línea o curva que me separa los grupos Es decir si yo tomo solamente esos valores como referencia ya suficiente porque sé que los otros que están más alejados esa curva si se para eso que está más cerca también lo va a hacer con el resto con lo cual solamente haciendo referencia a esos puntos sería suficiente para poder Establecer un buen modelo y sin tener que tener en cuenta a los restantes puntos y por el lado de las desventajas dos cuestiones por un lado no funciona bien cuando tenemos un grupo grande de datos Porque porque justamente aquí si usamos esta opción del truco kernel voy a tener que agregar una dimensión Y si eso lo hago para un conjunto muy grande de datos Obviamente el entrenamiento va a tardar y mucho finalmente tampoco funciona bien cuando el conjunto de datos tiene más ruido es decir cuando las clases que en este caso estoy intentando Separar superponen en algún caso Aunque yo utilice el truco kernel e intente agregando una dimensión separar esos conjuntos quizás esa separación no sea efectiva y ambas clases tengan puntos que siguen mezclados Bueno si eso sucede evidentemente Esta es una de las desventajas de este algoritmo y Por ende este algoritmo nos va a funcionar bien Bueno hasta aquí esta primer parte de la clase número 11 ahora como siempre nos vemos en la segunda parte para implementar todos estos conocimientos a través de cola en la práctica hasta aquí llegamos con esta primera parte los esperamos para poner en práctica todo lo aprendido en la segunda parte 