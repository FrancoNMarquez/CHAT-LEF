 Titulo: Clase 26 (parte  1) Curso de inteligencia Artificial 
 URL https://youtu.be/s-Yg52wSNKc  
 1540 segundos de duracion 
  Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número  26 Hola a todos Bienvenidos a la clase número 26 del curso de Inteligencia artificial de ifes continuamos con el módulo de procesamiento del lenguaje natural y esta clase que vamos a dividir en dos partes como la mayoría de nuestras clases va a tener una primer parte destinada a hacer una práctica integral que refuerce todos los conceptos que vimos en la clase pasada y en la segunda parte de esta clase vamos a ver nuevos conceptos y también algunas prácticas adicionales para también reforzar esos nuevos conceptos así que sin más empecemos con la primer parte de esta clase bien es momento de avanzar a otra práctica a otro lab con lo cual vamos a ver un ejemplo completo de tokenización ese ejemplo completo lo vamos a llevar a cabo utilizando un dataset de noticias que vamos a bajar del sitio kag este lab se va a llamar la pnl 3 y como todos los que vimos anteriormente los pueden bajar desde el campo virtual de ifes bien en este lap vamos a desarrollar un ejemplo completo de tokenización como bien dice aquí el título y vamos a hacerlo tomando un dataset del sitio que ya conocemos kaggle con Eh bueno dat que tiene información de noticias y cada una de esas noticias está caratulada como un tipo de noticias Sí policiales deportivas etcétera Ya lo vamos a ver bien y todas las características como está rotuladas cada una de estas noticias cuando hablo de rotuladas ya van teniendo idea de que justamente me enfrento a un dataset que tiene etiquetas y este algoritmo que vamos a tratar de construir desde nlp tiene algunos contactos con lo que ya hemos hecho en Deep learning y en Machine learning porque se trata justamente de leyendo el contenido de una noticia tratar de inferir qué tipo de noticia es esa Está bien como dijimos recién deportiva política polical etcétera etcétera La idea es que justamente yo usé este dataset para qué Para lo que hemos hecho un montón de veces entrenar una inteligencia Y a partir ir de entrenar es inteligencia puede inteligencia empezar a deducir o a inferir qué tipo como dije recién de noticia es un determinado texto vamos a incorporar librerías que ya algunas las conocemos muy bien pandas por un lado py it learn con nuestro ya conocido trest split pero no tan conocido con vectorizer esta es una aplicación que tiene que ver con lo que vimos recién la teoría trata justamente de transformar textos en vectores a través de la contabilización de la la cantidad de veces que aparece en determinado texto en determinado documento eh una palabra y finalmente vamos a usar multinomial nb de night bice que es un tipo de algoritmo que no vimos en Machine learning pero es un algoritmo muy liviano muy efectivo y muy aplicable para casos de nlp Así que empezamos por cargar todas esas librerías y luego vamos a montar el Drive como ya lo hemos hecho muchas veces para Acceder al dataset que les mencionaba recién y habiendo hecho esto lo que voy a hacer es hacer nuestro ya muy conocido rit csb voy a tomar esta información de este dataset desde un dataset que tiene formato csb con el encoding utf8 y lo voy a poner dentro de una variable que le voy a llamar DF y luego hago un Head como lo hemos hecho infinitas veces y tengo en realidad un dataset con tres características nada más la URL donde está la noticia el texto de la noticia y el tipo de noticia que es cada uno por lo tanto tengo un eh si hago un shape pued darme cuenta que tengo un dataframe de 1217 noticias y como dije recién tres características si miramos eh el primer la primer característica de la primera observación por eso DF news porque la news es el nombre como llama esa característica el subcero indicando que es el primer valor bueno ejecuto eso y voy a ver todo Sí esto que aparece aquí muy recortado Sí todo el texto completo y así todo continúa sí bien luego lo que voy a hacer como insisto tantas veces hicimos decir bueno Cuál es mi característica x y cuál es mi característica i o Target bien la x va a ser el news y el target va a ser type que t justamente lo que vemos aquí es el tipo de noticia insisto lo que queremos aquí es a partir de un texto poder hacer una inteligencia que luego me permita predecir Qué texto puede tener que ver con cuál de los types con cuál de los tipos así que bueno creamos la x creamos la i y luego vamos a hacer una contabilización de eh los los tipos y además vamos ver cuáles son todas las variantes de tipo que existen ver dos propósitos en uno en principio puedo ver que los tipos que son macroeconomía alianzas Innovación regulaciones sostenibilidad otra y reputación y que bueno la cantidad de noticias que hay dentro de cada rubro o sea nada que ver lo que yo le había dicho política deporte Pero no importa era la esencia era transmitirle cuá es la la característica de lo que se quería pretender con este dataset Pero bueno tenemos 340 noticias de macroeconomía 247 de alanzas etcétera etcétera como siempre decimos no está muy balanceado porque ser ideal que hubiese la misma cantidad de noticias de cada uno de estos tipos Pero bueno tampoco es algo que nos vaya a complicar demasiado bien visto esto hago como Insisto fíjense que este este proceso es muy parecido a lo que hemos hecho muchas veces en Machine learning lo que hago es separar el conjunto de prueba del conjunto de Test tomando la x tomando la i y considerando un test size Es decir para el conjunto de test de El 20% quedando el 80 en el conjunto de entrenamiento bien para vericar como qued como quedaron esas cantidades hago un print de x trin y de X test viendo que a partir de ahora de Las 1217 observaciones tengo una división de 973 observaciones En en el conjunto de entrenamiento y 244 en el conjunto de Test bien lo que viene ahora es la vectorización recuerden vamos a ir justamente a la parte de la teoría donde decimos un ejempl muy sencillo Pero efectivo que era el de estos tres documentos que eran virtualmente tres líneas pero lo que vamos con el espíritu de tres documentos para hacer un ejemplo chico que se entienda bien lo que hacía era generar todo un vocabulario conformado por 18 palabras porque justamente la suma de las palabras que había entre estos tres documentos sin repetir la que aparecía más de una vez representaban 18 palabras bueno Esto lo que vamos a hacer ahora justamente con este vectorizer con lo cual hago una instancia que le pongo vectorizer de con vectorizer con vectorizer lo tenemos aquí arriba como una de las librerías de psych learn que acabamos de incorporar al principio de este lap y luego lo que voy a hacer es algo insisto parecido a lo que hicimos muchas veces con Machine learning un fit transform de los datos que van a usar para entrenamiento y un transform de los datos que van a usarse para Test en este caso por qué el transform Bueno por lo que venimos hablando porque tengo que transformar cada cada uno de los contenidos de los documentos en vectores entonces una vez que hago Esto me interesa sa ver así como en aquella oportunidad yo supe que tenía vectores de cuánto de 18 posiciones o sea documento uno tenía un vector de 18 posiciones documento dos lo mismo y documento 3 lo mismo lo que variaba es que elemento Qué valor tenía en cada una de esas posiciones bien en este caso es lo mismo y lo que hago es un shape de x30f para ve eso puedo descubrir o empiezo a poder descubrir que sigo teniendo mis 973 observaciones pero esas 973 observaciones tienen 26807 posiciones es decir que cada una de esas observaciones son vectores de 26807 posiciones Por qué Porque evidentemente a los a lo largo de todas esas 973 news noticias o descripciones de noticias se ha podido determinar o llegar a la conclusión que todo ese conjunto de palabras conformó un vocabulario no repetitivo recuérdense este este tema importante que no no repite no es la cantidad literal de palabras sino la cantidad de palabras diferentes por decirlo de alguna manera de 260807 palabras bien Es decir insisto Este ejemplo sencillo que ten tenos acá de 18 de vectores 18 posiciones ahora se transforma en este ejemplo de este lap en vectores de 2687 Cuántas 973 tan igual que la cantidad de observaciones Obviamente que esta transformación también tiene lugar en las 2 44 que representan el conjunto de bien aclarado esto vamos a crear el modelo bien el modelo lo creo con model multinal nave byes como si hubiera creado modelo de regresión logística de árbol de decisión de bosque aleatorio bueno el que fuere de soport vector Machine que fuera en este caso otro modelo diferente hago un fit igual que como lo hacía antes y mido si a través del predict la posibilidad de tener la el score vinculado al modelo de train y al modelo de Test entonces creo el modelo lo entreno y establezco la predicción y luego hago la medición de la precis lo que vamos a hacer a continuación es aplicar una técnica de stemming o de lematización para reducir la cantidad de dimensiones que hemos tenido como resultado de esta primera aproximación de cada uno de los vectores que representa cada una de estas noticias Pero antes de pasar a eso quiero rever un poquito este concepto de la precisión del modelo de train y test esto nosotros lo venimos trabajando desde Machine learning y en un primer análisis podremos pensar de que es muy bueno el score de modelo de TR y el score de modelo de Test eh No es bajo pero tiene esta cuestión de que está un poquito alejado del modelo de entrenamiento y eso consideramos en el mundo de machin que no es bueno aquí en el caso de los textos esto hay que revero muy bien y hay técnicas para poder justamente superar hasta el límite de lo posible esto por qué Porque los textos en realidad muchas veces tienen una diversidad que cuando separa un conjunto de entrenamiento un conjunto de Test puede ser que en algún caso esa separación sea muy diferente y si bien uno pretende que sea un algoritmo que generalice lo mejor posible esto depende mucho en el mundo en nlp de cómo estén distribuidos los datos entre el conjunto de Test y entrenamiento Así que a tomarlo como algo no tan malo Este score Y si vamos por supuesto que se puede mejorar desde ya por supuesto hay otros modelos que son más avanzados Queen scores más altos pero si vamos a centrar nuestra idea en cuanto a este práctico con lo que dije Recién con el propósito de reducir el tamaño de los vectores para eso tenemos el stemming la climatización que vimos en el lab anterior empezamos por el stemming en donde lo primero que vamos hacer es como hicimos en el lab anterior importar las librerías siempre usando nltk en el caso de temin Recuerden que en el caso de la matización usamos space bien cargo también la información de las librerías que sirven para tokenizar y para quitar las Wars y cargo eh o Creo mejor dicho una variable que es una instancia no steaming destacando que voy a usar un lenguaje en castellano y luego voy a definir una función que lo que va a hacer es recibir un texto en este caso el de cada noticia y tokenizar Y estimarlo sí En realidad va a hacer varias cosas en principio lo va a tokenizar lo va a estimar Y además le vamos a quitar toda aquella palabra que no presente parte del vocabulario alfabético sí eh como lo son por ejemplo los signos de puntuación H Entonces vamos a hacer todo esto en esta función que tenemos aquí abajo pero Primero lo primero que es eh crear el steamer y ahora repasamos la función que estamos creando Aquí bien el criterio de esta función que es esta que está aquí sería vamos a la línea siguiente vamos a crear una nueva columna que vamos a poner New stem como diciendo bueno es la news pero habiéndole aplicado el proceso de steaming y vamos a hacerla justamente tomando la news original y aplicándole con el método punto Apple el la función token steam la función token steam va a ser lo que dijimos recién va a hacer en principio una tokenización y luego va a serer el stemming aplicando dos acciones más en principio reducir todo el texto a minúscula y además quitar cualquier elemento que no sea de tipo alfabético todo eso lo tengo en esta función Ahora sí vamos a la función justamente la función se llama token steam tal cual yo lo puse aquí en este appli y luego lo que recibe es un text el text en realidad va a ser cada una de las news es decir yo le voy a ir pasando a través de método Apple justamente cada una de las news cada una de las news va a ser tokenizado como con Word token i pero previamente esa news va a ser transformada toda a minúscula entonces token hizo lo que previamente fue transformado a minúscula Con eso tengo en tokens todos los tokens de esa newo y lo que voy a hacer va a ser recorrer cada uno de esos token estimar losos sí que tal el sufijo se acuerdan de esto que vimos hace un rato lava anterior perdón y lo voy a hacer por cada token de tokens o sea por cada palabra de esa news diciéndole que esa palabra tiene que estar dentro de token Alpha tiene que ser un carácter alfabético finalmente voy a devolver esto Esta combinación que es típica de este tipo de funciones que es un espacio en blanco punj Sims que lo que hace es como voy a recibir un montón de palabras de tokens que están estimados voy a ponerlo uno al lado de otro justamente se hace con esta instrucción con el este espacio en blanco punto join que juntar justamente los steams que son los steam la salida de toda esta lógica que acabo de mostrar recién bien ejecutamos esta función cuando se ejecuta la función Es simplemente para que quede cargada la función dec la función no hace nada sino hasta el momento que es usada y luego la voy a usar una vez definida una vez que es reconocida para este este Notebook en esta línea que está acá que como decía recién va a crear una nueva columna que es la news pero ahora estimada tokenizado y habiéndole quitado justamente las letras en mayúscula y los caracteres que no son alfabéticos bien ahí terminó ejecutamos en Entonces esta línea y para ver cuál es el resultado voy a tomar la primer noticia la primer New habiéndole aplicado todas estas cuatro cosas que dijimos recién Está bien entonces la ejecuto y veo que el resultado es esto para ver bien ya se lee Sí cuando lo estamos leyendo a primera vista vemos que ha recortado muchas palabras justamente con este criterio de quitarle el sufijo si voy hacia arriba y la comparo con la frase original durante el foro la banca articuladora empresarial para el desarrollo sostenible etcétera etcétera se transformó en duran el for la Ban articul empresarial par el bueno Esto insisto no lo tomen como una cuestión de decir bueno Esto destruye el lenguaje transforma en algo ilegible estamos hablando de cómo maneja internamente el np con esta cuestión de los vectores y esta reducción a pesar de que a la vista humana creo que se interpreta bastante claramente per obviamente no forma parte del Castellano puro puede nuestro inteligencia interpretarlo de manera correcta bien a partir de ahora qué vamos a hacer ya no vamos a entrenar un algoritmo con news Sí y con type sino con news steam y con type por eso redefin la x y la i y bueno separo conjunto de Test y conjunto de entrenamiento y hago la vectorización O sea todo lo que hice antes en TR lo hago en una sola y lo que voy a ver ahora lo más importante fíjense ahora tengo 973 observaciones igual que antes pero el vector no tiene la misma dimensión igual que antes por qué Porque justamente esta cuestión de haber quitado el sufijo de las palabras hizo que mi vocabulario ahora sea mucho menor y Por ende la dimensión de mis vectores es mucho menor si me fijo Cómo era la dimensión anterior era 26 6807 Y gracias al stemming se reduce a 11928 menos de la mitad lo importante y lo significativo de esto bien creo el modelo y voy a medir la precisión y veo que la precisión pasó a ser 092 y 078 bueno a primera vista uno puede decir pero estamos peor que antes a ver siempre seamos medidos con esto por qué Porque justamente si es un poquito inferior antes tenía 093 y 080 ahora tengo 092 y 078 Pero tengo una pequeña diferencia de precisión con un conjunto de vectores que tiene menos de la mitad de la dimensionalidad  bien y finalmente para cerrar este lap vamos a la climatización La idea es seguir un proceso parecido al stening para verificar que justamente a través de estas técnicas se reduce considerablemente la dimensión de los vectores pero Pero obviamente tomando el texto original no el último que está estimado Sí entonces lo que tengo que hacer es instalar Eh bueno que es import spacy sí habiéndolo instalado antes y instalando también el el diccionario de lenguaje en castellano bien Digo como lo vimos antes a continuación creo una variable nlp como hicimos hoy space load y cargo justamente ese dicionario en castellano Y a partir de esto lo que tengo que hacer es trabajar como hicimos hoy con una nueva función esa función la idea avanzo Está aquí es lo mismo que hicimos hoy así como teníamos hoy el news steaming creo que lo habamos puesto a ver vamos aquí arriba el news stem Sí ahora vamos a crear un news lema que es la news con Apple aplicándole el método Apple redundancia solamente que en este caso no vamos a usar la función que usamos hoy que le habíamos puesto aquí arriba token steam sino que le vamos a poner aa función token lema esta función es parecida a lo que tuvimos hoy en este caso lo primero que hace es tokenizar no obviamente con el mismo método que antes porque ahora Estamos usando la librería spacy O sea que lo hace con nlp sí nlp acuérdense que es la instancia que hice de spacy load y lo que hace es justamente tokenizar en virtud de un texto que es pasado a lo mismo que hoy con otro método con otra librería pero lo mismo que hoy sí con lo cual ahora tengo tokens y creo un for parecido a lo que estuvimos hoy donde digo justamente token lema O sea aplico la lematización a cada token por cada token de tokens sí tokens es esta variable que insisto tiene todos los el contenido de una determinada news en un determinado momento hecho en tokes dividido en palabras sí bien entonces por cada palabra del conjunto de palabras siendo que el conjunto de palabras y toda la news siendo que ese token esté dentro de Alpha sea un elemento de is Alfa que responde a este método is Alfa que es propio de python no que también lo usamos hace un rato para ver que solamente tenga en cuenta los elementos de orden alfabético entonces una vez que tomo los tokens de la New y selecciono solamente aquellos que son y Alfa hago la ligmatizacion soy aquí arriba recuerdan un sh in the steams decía que de esta manera arma como una cadena donde vuelve a juntar todas las palabras que antes separó en la toqua S como un proceso inverso s no saco o separo las palabras le aplico el stemning o le aplico la lematización y luego las vuelvo a juntar para que me devuelva toda la news completa pero habiéndole pasado antes el stemin y ahora la lematización Bueno eso es lo que aplica esta línea lo que vamos a hacer con esto es cargar sí la la función si no no me la va a reconocer para que ahora allí yo cree esta esta nueva columna bien habiendo terminado este proceso al igual que lo que hicimos hoy vamos a imprimir la primer New en el caso de la lematización y fíjense que justamente aplica la lematización en este resultado y es diferente Pero también diferente al este me tornaba esto en una en un conjunto de palabras mucho más ilegible porque quitaba el sufijo en este caso lo que hace es tratar de reemplazar eh A esa palabra por su palabra raíz con lo cual en muchos casos una palabra puede estar más de una vez sí eh o reiteradas veces en este texto y no tener otra similar a la cual que haya que recurrir para buscar una palabra raí se acuerdan que hoy decíamos correr corriendo y quizás aquí por ejemplo la palabra durante fíjense que está está al 100% Por decirlo deguna manera porque quizás no haya otra palabra en todo este abecedario de todas estas news con la cual pueda confrontarse y poder lograr una una palabra raíz que represente a durante y otra palabra que esté muy cercana a durante sí Entonces esto obviamente transforma este texto en algo más legible pero aún así ha hecho una reducción importante de la dimensionalidad de los vectores Y eso lo vamos a comprobar ahora para lo cual lo que vamos a hacer es en principio como hicimos hoy con el caso del stemin redefinir la x y la i separar conjunto de tes y entrenamiento y vectorizar todo este conjunto ya está y ahora voy a averiguar Cuál es el tamaño del vector fíjense que el tamaño del vector es de 16736 es decir es una reducción inferior Sí en resultado digamos tiene una dimensión mayor Sí pero bueno en en inferior en en el resultado porque esta ha sido la mejor reducción porque redujo a 11,000 y esta redujo a 16,000 pero si la comparo con la dimensión original que era de 26,800 aún así es una muy buena reducción que está un poco por encima del 50% del vocabulario que se obtuvo al principio cuando no había aplicado ni el stemi ni la lematización bien en cuanto a eh crear el modelo lem matizado y eh medir la precisión veo que tengo buenos scores y que es guarda bastante relación con el caso de El el stemin tenía 92 y 78 Aquí tengo 91 y 79 sí es decir que la conclusión de esto es que la reducción de dimensiones que es muy importante no ha afectado el nivel de precisión del modelo tanto en el caso del stemin como en el caso de la climatización con lo cual tengo modelos casi una precisión prácticamente idéntica si una pequeñísima diferencia pero con reducciones de tamaños de El 50% y en el caso de la alción casi un 50% Así que esto es lo más importante tener en cuenta de los propósitos de este práctico y para todo lo que venga ahora más hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda  parte