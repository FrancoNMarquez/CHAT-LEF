 Titulo: Clase13 (parte2) Curso de Inteligencia Artificial 
 URL https://youtu.be/2jAJjJ2WO3Y  
 1095 segundos de duracion 
 Hola bienvenidos Esta es la clase número 13 del curso de ia de ifes en ella Seguiremos con lo experimentado en la parte anterior y daremos las pautas para la evaluación final del módulo  Bueno Hola a todos nuevamente continuamos con la clase número 13 de este módulo de Machine learning última clase de módulos segunda parte donde en la primer parte habíamos visto un análisis de un proyecto en Google y vamos a continuar con ello justamente En esta segunda parte rápidamente vamos al cable y allí teníamos recordemos un poquito lo que habíamos visto en la primer parte donde habíamos hablado de modelo armado con un algoritmo de Random Forest para predecir la posibilidad de que una persona pudiese tener o no cardiopatía esto lo habíamos hecho en base a un conjunto de datos este que está aquí donde bueno teníamos características de salud digamos de algunas características de la salud de cada una de estas personas y en base a ello teníamos la variable que Target donde decíamos si esa persona tenía o no una cardiopatía y con eso encontrar un modelo para poder predecir que cualquier persona de la cual tengamos todos estos datos podamos saber si puede o no tener esa esa patología bien avanzamos con el proyecto justamente con todo esto que ya recordamos bien lo que hemos hecho hasta llegar al final en donde teníamos un modelo que nos daba una curacy de el orden de aquí lo tenemos 99,44% para el conjunto de entrenamiento y 88,49% para el conjunto de Test que propone el autor hacer con esto bien podemos ver que aquí lo que busca es un improvement es mejorar este a través de algunas técnicas que justamente entiende el autor que pueden hacer que obtengamos un mejor score que este que está aquí pero antes de eso vamos a ir a implementar un algoritmo de regresión logística y estas partes que estamos viendo aquí que estamos pasando de largo Bueno ya lo vamos a ver más adelante porque los pasamos de largo no es una cuestión que vamos a dejar por fuera sino que va a tener otro contexto en el cual lo vamos a utilizar por lo tanto Vamos hasta la sección donde el autor aborda el análisis de los institutos regresión model para justamente ver si se puede mejorar el álgebraid implementando este otro modelo el logístico recreation se ve es un algoritmo que bueno como su nombre lo indica se diferencia de el de regresión logística que vimos nosotros anteriormente en que tiene incluido un modelo de validación Cruzada en este caso en autor usa este en lugar del modelo que usamos nosotros igual el algoritmo que usamos nosotros y vamos a incluir en este caso el solver que sí lo utilizamos como hiper parámetro cuando implementamos nuestro modelo de relación lógica luego bueno en la separación de los conjuntos de entrenamiento y de Test con trayentes split y el escaneamiento de los datos a continuación se genera o Se entrena el algoritmo se hacen las predicciones se hacen no sé que Brasil y se imprimen los hay que ir así y los valores que se obtienen son obviamente mucho más bajos y los obtenidos con Random Forest 76.176.1 en los dos casos es bueno que coincidan pero es malo como Score t luego tenemos la matriz de confusión donde vemos bueno obviamente aquí en la diagonal principal los aciertos y en la diagonal opuesta como siempre los valores que fueron predecidos de manera incorrecta Bueno pero no obstante haber utilizado Este modelo de regresión logística aún con Esta técnica que incluye la validación Cruzada no ha dado buen resultado puesto que el score ha sido muy bajo Comparado con lo que habíamos obtenido antes con Random Forest por eso y volviendo a la sección que salteamos previamente vamos a ver cuáles son los intentos del autor por mejorar ese modelo justamente de Random Forest bien Por eso aquí habla de un mejoramiento de ese modelo donde habla justamente de los problemas de las clases desbalanceadas y va a utilizar una técnica que se llama Smooth que sirve justamente para trabajar con eso nosotros algo habíamos hablado de las clases de balanceadas recordemos en esta parte de la primer parte de esta clase justamente donde abordamos el problema de que había características que tenían valores optativos opcionales que tenían una gran diferencia en cantidad de apariciones de un valor o cantidad de apariciones del otro valor sí y no por ejemplo en algunos casos como mostrábamos aquí bueno avanzando con lo que aquí hace buscando abordar este problema con una herramienta que se llama Smooth que se está que está aquí giga a un score muy bueno aquí lo vamos a ver del orden del 99.67% en el conjunto de entrenamiento y un 89.5% en el conjunto de Test luego bueno muestra también esta esta performance a través de la matriz de conducción y usa una herramienta se llama un mapa de calor sí un hitmap de correlación esto es este mapa que vemos aquí que analiza algunas cuestiones y luego también intenta mejorar aún más el Akira y del modelo con la extracción de características a través de la herramienta psa cosa que ya hemos visto nosotros en clases anteriores bien Por lo tanto vemos todo este desarrollo aquí y llega a una quiero decir mejor todavía del 99.67 al 99.69 en el caso del conjunto de entrenamiento y también mejora el anti del conjunto de Test pasando a 91.32% ustedes se preguntarán por qué esta última parte la pasamos tan rápido y porque no le explicamos como hicimos en las etapas anteriores bueno la respuesta a esta pregunta es que esta Va a ser una de las instancias que van a tener que hacer ustedes como parte de la evaluación final de este módulo de Machine learning en particular y de todo el cuatrimestre en general es decir lo primero que van a tener que Investigar qué es esto de las clases de balanceadas que hablamos en la primera parte de esta clase pero profundizar un poquito más este concepto pero fundamentalmente Esta técnica de Smooth La idea es que investiguen de qué se trata si puedan documentar esto puedan defenderlo por supuesto y poder llevar todo este código que está aquí al colap que venimos desarrollando desde el principio de esta clase bien luego vamos a hacer lo propio también con esta herramienta que se llama mapa de creador Para qué sirve un mapa de calor por qué usamos un mapa de calor y cómo se usa como está aquí tratando de tomar conocimiento de este código que no es muy largo ni muy complejo y lo pueden hacer tranquilamente y luego de nuevo llevarlo al cola para poder implementarlo en la práctica que venimos llevando a cabo finalmente también vamos a hacer lo mismo para esta extracción de este características con psa está este conjunto de datos tenía 38 características y se reduce a 27 bueno Esto no va a tener que hacer una una gran explicación desde la fundamentación de la extracción de características porque ya un tema dado en la clase pero sí la idea es poder llevar este código al collage y poder Bueno de nuevo tener todo este coral completo tal cual lo propone aquí el autor y hay un tema más que formaría parte y cerraría esta instancia evaluativo en este caso fíjense que Esta técnica es utiliza un concepto de canaibos negros son vecinos y este concepto de canews implica algo así como vecinos más cercanos Este es un tema que en este caso es como un hiper parámetro para Esta técnica y me trae a colación otra cuestión que no está relacionada directamente con esto sino desde el concepto pero no desde lo que les voy a pedir que investiguen y es justamente un algoritmo que se llama kesinos más cercanos sí en este caso es parecido al concepto esto es solamente aplicado no como un hiper parámetro sino como un modelo Así que la última parte de la investigación va a ser justamente investigar de qué se trata este algoritmo de campesinos más cercanos que si ustedes hacen memoria recordarán que en la primer clase entre la lista de los algoritmos supervisados estaba justamente el último justamente era este cabecino más cercanos bien este último elemento ustedes tienen que investigarlo documentarlo desde la teoría y también con alguna aplicación práctica que ustedes puedan encontrar y presentar con un kolab algo parecido a lo que vemos aquí con este algoritmo de Random Forest bueno pueda llevarlo ustedes a un colap sobre esta temática de cabecinos más cercanos explicado paso a paso para que lo puedan demostrar Bueno ya habiendo terminado todo esta cuestión que tiene que ver con la evaluación de este módulo de Machine learning y como dije antes de todo el cuatrimestre vamos a terminar la clase poniendo otro ejemplo más que lo que vimos justamente en todo el resto de la clase pero en este caso de un modelo no supervisado en cambio de lo que vimos antes que era un modelo supervisado con algoritmo de tipo supervisado justamente volvemos acá y tenemos aquí otra opción le dejo aquí arriba la URL para que puedan entrar a ella y se trata de un kamiz aplicado a un problema de segmentación de clientes La idea es que exploremos juntos este proyecto no tan paso a paso como hicimos en el caso anterior yo creo que ya pueden tomar un poco la la guía que usamos en el caso anterior para aplicarla aquí pero sí por lo menos describir un poquito los las etapas los componentes de este proyecto para que puedan hacerlo mejor en principio tienen aquí una descripción muy importante que es como una guía de todo este proyecto donde bueno habla de la importación de librería de la exploración de datos la visualización de datos bueno la creación del modelo de kamins la selección de los clusters si acuérdense que siempre tenemos que con la técnica del codo ver cuál es la cantidad ideal de cluster para este modelo de camisas y finalmente un ploteo de cada una de las telas de los cluster digamos que hemos logrado y sus respectivos centroides Bueno si vemos esto tenemos justamente aquí la primer parte la importación de las librerías la exploración de datos en este caso los datos Recuerden que en el caso anterior habíamos simulado la situación de que no nos teníamos a mano y vamos a buscarlo a un raw de github en este caso si los tenemos vamos a la sección input y aquí está el dato con lo cual lo puedo bajar Al conjunto de datos con esta opción de Download vuelvo al notebook y bueno una vez que ya lo tenemos en nuestro disco lo levantó como hacemos habitualmente no como está aquí sino como hemos usado en las experiencias anteriores de las prácticas en cola Bueno me voy a encontrar con este conjunto de datos bueno hago un shape el distrive veo los tipos de datos de cada una de las características verificó cenus y Empiezo con la visualización de datos en este caso fíjense que existe aquí siempre una Bueno una herramienta que busca justamente Mostrar o esconder el código si yo escondo el código voy a ver nada más que los gráficos que busco justamente generar sin ver todo el código anterior si lo abro Aquí voy a poder ver cuál es el código que yo escribí para generar estos gráficos en principio lo primero que hace es hacer un gráfico de despliegue para ver cómo están distribuidos los datos de la edad del ingreso anual y del gasto promedio luego hace lo propio con el conteo de observaciones por género la cantidad de masculinos y la cantidad de femeninos a continuación hace un ploteo de la relación entre la Edad el ingreso anual y el gasto promedio Aquí está oculto el código lo va a abrir y veo que hago un gráfico muy interesante una matriz de 3 por 3 de su plots donde obviamente relaciono cada una de las características consigo misma Por lo cual siempre me dibuja aquí aquí aquí esa recta a 45 grados y después hago lo propio con las restantes age con gasto anual con ingreso anual perdón y es con gasto promedio lo mismo con los otros tres casos bien luego hago lo propio pero distribuyendo por género es decir fíjense acá Este es age versus anual inconvent voy hasta aquí arriba ya que tengo age versus es este mismo gráfico solamente que este gráfico tenía todos los puntos pintados del mismo color porque no diferenciaba género y aquí si lo hace otra forma distinta de presentar los datos fíjense que aquí está el Celeste como masculino y el naranja como femenino bien lo propio hace con la relación de las otras variantes sí de variables de entrada Aquí tengo la otra y luego lo que hace es buscar una distribución de los valores pero con otro tipo de gráficos de box blog donde me muestra cómo están distribuidos los géneros en las características de la edad en las características del ingreso anual y en las características del gasto bien Todo esto le permite hacer un preanálisis que ya lo van a poder ver bien en detalle de las características de los datos para luego pasar a crear el modelo Aquí está el cluster Inc y lo hace en principio acuérdense de la técnica del codo de recorrer a través de un Ford distintos valores de posibles cluster aquí va de 1 a 10 y bueno genera entonces 10 modelos diferentes cada uno con una cantidad de cluster diferente para qué para lograr el gráfico del codo que se esté que está aquí que me dice que para este modelo de camisas lo ideal es un cluster es un modelo perdón de 4 clacs aquí el codo acuérdense que me demuestra eso el codo cuando cambia acuérdense abruptamente la inercia sí de la recta bien aquí tenemos entonces con 4 crea el algoritmo sí gráfica la resultante a través de este código que está aquí arriba de bueno cómo se distribuyen estos cuatro sí acá tengo 1 2 3 4 donde están con un color los clusters con un color los puntos de cada uno de esos clusters y con un naranja todos y cada uno de los cuatro centrales bien Esto lo repite también para el caso de la segmentación usando el ingreso anual y el gasto promedio sí lo mismo genera la técnica del code en este caso concluye que 5 es el codo por lo tanto crea un modelo con cinco planos y genera de nuevo el gráfico en este caso va a ver obviamente un cluster más pero con la misma lógica de distribución de colores de los puntos de los cluster de los cluster y de los sectores bien y finalmente Busca poner las tres características edad ingreso anual y gasto promedio y busca generar otro modelo de camines en el cual concluye que el codo está ubicado en este caso en el 6 y crea un modelo justamente de site claster en este caso el gráfico es diferente porque es un gráfico de tres dimensiones y aquí van a poder comprobar justamente los seis Class Sí este este este Y estos dos que son parecidos por el color pero este tiene una tonalidad más clarita y es una tonalidad más oscura bueno Esto también no pueden usar ustedes de una forma de cambiar los colores para que sean a la vista mucho más significativas las diferencias les dejo este proyecto para que ustedes lo puedan implementar llevarlo su cola y tratar de seguir desarrollando esta experiencia de investigación a partir de la cual Espero que lo puedan hacer y en caso que tengan algún problema ya saben cómo pueden recurrir al campo virtual para consultarme lo que necesite Bueno hasta aquí llegamos con esta última clase quedo en contacto con ustedes y a la espera de sus trabajos finales con lo cual Espero que aprenden este módulo con éxito nos vemos hasta aquí llegamos con esta clase número 13 y también finalizamos con el módulo de Machine learning Te espero en el próximo módulo el de Deep learning nos vemos 