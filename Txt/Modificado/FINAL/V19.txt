 Titulo: Clase10 (parte 2) del Curso de Inteligencia Artificial 
 URL https://youtu.be/urDhXUtxGWM  
 1390 segundos de duracion 
 Hola bienvenidos Esta es la segunda parte de la clase número 10 del curso de Inteligencia artificial de ifes te invito a que sigamos repasando y aprendiendo conceptos nuevos en esta segunda parte empezamos   Hola nuevamente vamos a empezar la segunda parte de esta clase número 10 y como dijimos al principio el tema de esta segunda parte es el título que está aquí la reducción de la dimensionalidad de los datos para ello vamos a recurrir a una parte de la teoría de la clase donde abordamos el tema de regresión lineal concretamente cuando hablábamos de la reelección lineal múltiple y este gráfico que está aquí teníamos Bueno un caso donde teníamos tres variables de entrada la inversión en ventas y en tv y en radio La mención publicitaria y lo que aquí hacemos relacionábamos los tres puntos o sea las tres inversiones En Radio TV y ventas y cada una de estas relaciones nos daban un punto lo que intentamos Buscar En aquel momento fue un plano que pasará lo más cerca posible de cada uno de esos puntos para poder hablar o reconocer un modelo bien aquí tenemos algo parecido con este caso que tenemos aquí tenemos tres variables de entrada que hemos graficado en un gráfico de tridimensional Y estos son los puntos que representan las uniones de cada una de esas observaciones y los tres puntos de cada una de ellas encuentro un plano sí que está aquí y este plano que está aquí me permite pensar en una posibilidad que me acerca al objetivo de reducir la dimensionalidad De qué trata esto fíjese que este plano que está aquí justamente en esta suerte de puntos que están muy cerca pero no están tan cerca como poder para poder ser atravesados de pleno con un plano me permite pensar en un plano que por lo menos toca muchos puntos y pasa muy cerca de otros con lo cual Esto me lleva a mí a la posibilidad de transformar esta figura de aquí en esta figura que está acá a la derecha porque supóngase que lo que yo hice aquí es este plano Tomar todos los puntos que están sobre el plano y dejarlos tal cual están y los planos que están por encima del plano o por debajo del plano aplastarlo o sea como si los quisiera agarrar con la mano y llevarlos hacia el plano con lo cual habiendo hecho esto paso de esta forma que está aquí a esta forma que está aquí obviamente esto no representa la realidad al 100% porque Porque evidentemente estos puntos que no están sobre el plano tiene una distancia respecto del plan pero haciendo transformación ya tengo una pérdida de información porque insisto la información no es la real porque está en un lugar donde originalmente no estaba pero está muy cerca del lugar donde originalmente estaba entonces Pongo aquí en la balanza la pérdida de la información versus prescindir de una dimensión es decir pasar de trabajar en lugar de con tres dimensiones con dos dimensiones en que impacta esto que Obviamente el algoritmo puede trabajar de manera más rápida en un paralelismo lo que vimos antes en la primer parte de esta clase donde hablábamos de quitar una una variable de entrada para que el algoritmo fuera más eficiente o más rápido en este caso no estamos quitando lo que estamos haciendo es transformando sigo teniendo tres dimensiones se sigo teniendo tres variables de entrada pero al transformarlas pasadas de un esquema de un plano en un eje de tres dimensiones a esto que estamos viendo aquí a la derecha que es la distribución de puntos en un eje de dos dimensiones y quitado una dimensión con lo cual he reducido la dimensionalidad sin perder o tratando de que la pérdida de información sea la menor posible este caso no solamente puede darse una situación como la que vimos recién sino que los puntos pueden tener otra forma muy curiosa pero es posible que tengan esta forma y es normal que puedan tenerla que es una forma de rollo Ven aquí están los puntos distribuidos en una forma de rollo y aquí lo que intentamos reflejar con los colores es que cada uno de estos puntos que tiene un color diferente está identificado con una clase o con una categoría diferente sí se acuerdan lo que hablamos respecto de la radiación logística en cuanto Por ejemplo si era un tipo de planta un tipo de flor suponga ser que al amarillo son todos los puntos que representan el color los naranjas setosa y los rojos Virginia bueno acá hay más casos que eso pero para tomar un ejemplo de referencia lo que hicimos antes qué pasaría aquí o qué resultados se produciría si intentáramos aplicar la misma lógica que hicimos aquí arriba Bueno si nosotros aplastamos estos puntos que están en este rollo para llevarlos a un plano esto que está en el gráfico del centro o sea obtendría una reducción de la dimensionalidad Pero qué pasa los puntos estarían muy mezclados con lo cual si yo a partir de ahora lo que quiero hacer es poder identificar en estamentos bien claros diferenciar los puntos de una categoría respecto de los otros recuérdense que la categoría la estamos representando con los colores evidentemente gráfico me muestra que eso es imposible porque todos los colores están mezclados y Establecer un algoritmo que diferencia una cosa de la otra va a ser muy complicado para este caso existe otra solución que cuál es es la de desarrollar el rollo Aunque parezca cómico a la expresión es decir en lugar de aplastarlo lo que hago es estirar toda esa sábana de puntos Y lograr una imagen como la que está aquí es decir fíjense que con esta otra estrategia lo que hice fue reducir una dimensión pero que los puntos estén bien separados de modo de que además de reducir una dimensión me sea fácil encontrar un modelo que pueda diferenciar un color una categoría de otra hasta ahora hemos visto casos donde pasé de tres dimensiones a dos dimensiones Eso es todo en realidad la reducción de la dimensionalidad también se puede aplicar para pasar de dos dimensiones a una dimensión vamos a ver un caso tenemos aquí un conjunto de puntos sobre un gráfico de dos ejes recién Cuando tenemos un problema de pasar de dos dimensiones a tres dimensiones dijimos que queríamos Buscar un plano Qué pasará lo más cerca posible del conjunto de puntos ahora que queremos pasar de dos dimensiones a una dimensión El problema va a ser encontrar una recta sobre la cual se recuesta en la mayoría de los puntos Yo propongo aquí en este gráfico tres tipos diferentes de rectas en un caso una recta de tan sólido en otro caso una recta de trazo de puntos y en nuestro caso una recta de trazo de guiones voy a graficar cada una de estas tres casos en estos tres elásticos que están aquí a la derecha en el caso de la línea sólida si yo aplastase todos los puntos sobre esa recta vería un resultado como este si por el contrario hiciera lo propio con la línea de trazos obtendría un resultado como este y si hiciese lo propio sobre esta línea de puntos donde los puntos va a ganar redundancia están muy lejos y yo los aplastase obtendría este resultado que está aquí debajo la pregunta ahora es cuál de las tres rectas Será la mejor para la reducción de la dimensionalidad de dos dimensiones a una dimensión Obviamente el primer caso y por qué Porque es el que conserva el mayor nivel de varianza Qué quiere decir esto el que controla la mayor cantidad de variedades de puntos que se acercan a la realidad nosotros tenemos que tener siempre en cuenta que lo que queremos lograr aquí es reducir la dimensionalidad en este caso de dos dimensiones a una perdiendo la menor cantidad de información real posible fíjense vamos al caso si vemos el caso menos conveniente si quieren el peor el que corresponde a la línea de puntos vamos a suponer que yo quisiera aplastar todos los puntos sobre justamente esta recta de trazo de puntos me pararía aquí para poder mirar los puntos en forma totalmente perpendicular Y entonces Podría tener esta imagen reflejada mirada desde arriba desde aquí arriba para poder aplastar los puntos contra esta recta de puntos pero yo tengo por ejemplo un punto que está aquí al medio que puede ser este puede ser este o puede ser este fíjense el nivel de diferencia que existe entre este punto este punto y este punto mirado desde arriba cuando yo lo voy a aplastar sobre esta línea son lo mismo pero en la realidad Está muy dispersos con lo cual Aquí tengo una enorme pérdida de información Porque estos puntos que mirados de aquí son lo mismo en la realidad están muy dispersos vamos al caso más conveniente que elegimos como más conveniente justamente fíjense que yo si quiero recostar todos los puntos sobre esta línea sólida Me podría parar aquí arriba o aquí arriba y Si miro ahí voy a ver puntos que aquí parecen la misma cosa y que no lo son como por ejemplo si quiero ver un punto por aquí puede ser uno de estos este o este pero fíjense que la diferencia entre este punto y este por tomar los extremos más alejados no tienen la misma diferencia que en el caso anterior que tomamos como ejemplo este y este con lo cual la pérdida de información no es tan grande Por ende en nivel de varianza se conserva y Por ende Este es el mejor modelo para este caso de transformación de un escenario y dos dimensiones en una dimensión bien habiendo pasado toda esta introducción teórica vamos a ir a la práctica concretamente de poder probar este efecto de la reducción de la dimensionalidad vamos a hacerlo con un caso de regresión logística y para ello vamos a utilizar un Data set que no hemos utilizado antes que está dentro de las opciones de Data set que me ofrece la librería esto ya lo hemos utilizado justamente en la clase de regresión logística donde si ustedes recordarán justamente de cycle Data set sacamos un Data set que hablaba sobre casos de cáncer de seno y una vez que habíamos incorporado ese Data set veíamos la esquilas claves en las cuales venía ese dataset preparado digamos de antemano justamente a través de la librería de Shakira en este caso lo mismo yo lo que voy a utilizar es justamente a partir de fetch Open nml un Data set que se llama guión 784 este Data set lo vamos a incorporar luego de vamos a ejecutar las librerías las otras ya las conocen región logística warnings y vamos a proceder a abrir este conjunto de datos va a tardar un buen tiempo ya vamos a ver por qué Porque es un conjunto de datos con muchas observaciones concretamente con 70.000 observaciones bastante superior a todo lo que hemos venido trabajando hasta ahora bueno como ven ha tomado un minuto este y aquí tenemos ya el dataser incorporado el mismo se llama menist y en realidad es considerado el Hello World de la visión artificial esto para quien no está en el mundo la informática Hello World es la típica expresión con la que se busca desarrollar las primeras líneas de código cuando uno aprende cualquier tipo de lenguaje en este caso la visión artificial es una de las cosas que vamos a ver cuando hagamos el módulo de Deep learning y largamente vamos a ver porque los módulos más importantes desarrollados Dentro de este curso este conjunto es un conjunto de 70.000 observaciones como ya les había adelantado que tiene ya preparado un conjunto de entrenamiento de 60.000 y un conjunto de pruebas de 10.000 En qué consiste este conjunto de datos en números manuscritos de 0 a 9 como se logró Esto bueno convocando a personas estudiantes de High schools y de empleados de la oficina de descenso del Estados Unidos para que dibujaran números manualmente y luego los escanearon lo que hicieron fue transformar cada uno de estos este números en una digitalización de 28 dígitos por 28 dígitos por eso justamente se llama 784 por el resultado de la multiplicación de 28 por 28 bien luego que hicieron esto también lo interesante es que convocaron a otras personas totalmente diferentes para generar el Data set de prueba es decir que a diferencia de lo que hemos hecho hasta ahora donde nosotros somos quienes separamos el conjunto de entrenamiento el conjunto de Test pero todo proviene de la misma fuente en este caso como ya viene preparado el conjunto de entrenamiento con 60 mil observaciones fue desarrollado en base a la escritura de un grupo de personas y los restantes Diez mil observaciones fueron incorporadas a partir del registro manual de otras personas diferentes lo cual hace mucho más rico justamente el valor del conjunto de Test porque representa lo que podría ser la realidad nuestra o sea nosotros entrenamos y creamos un modelo con los datos que ya tenemos y luego hay que probarlo con los datos futuros que van a aparecer bueno acá el papel de los datos futuros lo desarrollan estas personas distintas que generaron el conjunto de Test respecto de las que generaron el conjunto de entrenamiento dejando las explicaciones de lado Entonces pasamos por separar los conjuntos y ya no usamos el 30 split porque como dijimos antes ya vienen separados los conjuntos de entrenamiento y test 60.000 por y disminuirse por otro con lo cual lo que pongo aquí es Ni más ni menos que eso generó X3 con 60.000 y perdón ítems y XT con las restantes 10.000 observaciones bien lo que estamos creando o lo que buscamos crear aquí es como dice el título un modelo de relación logística para salida no binaria por qué no binaria porque no son dos opciones de salida Cuántas son 10 porque son números que van de cero al nueve Sí por eso es fundamental como hicimos recordarán y volvemos aquí a la clase de regresión logística cuando usábamos salidas múltiples Aquí está el múltiples multinomial Bueno aquí tengo que volver a hacer lo mismo y como recordarán también de aquella clase utilizábamos un solver para mejorar la u optimizar Esa es la palabra correcta el algoritmo que estábamos desarrollando el modelo que estábamos desarrollando aquí lo mismo y vamos a usar el solar más común que es lbs bien qué más incorporamos aquí porque luego lo único que tenemos que hacer es hacer el feed con el grupo de entrenamiento de X ahí que es lo que venimos haciendo habitualmente vamos a incorporar justamente en la librería aquí incorporamos Time porque gracias a ello lo que vamos a intentar hacer es fijar la hora que tiene la computadora cargada Al momento de empezar a entrenar el algoritmo y la hora que tiene la computadora cargada Al momento de terminar ese proceso para qué Para poder medir el tiempo que duró la generación del modelo Bueno vamos a hacer eso y observamos que el tiempo fue de 43 segundos con 50 esta información la tengo aquí también por supuesto estas otras formas de poder verla poder capturarla y ponerla en una variable o el destino que yo quiera darle bien Ahora medimos la precisión para el conjunto de Test y entrenamientos y veo que arroja estos valores 093 y 092 55 son muy similares uno como otro y son buenos valores ahora lo que vamos a ver es una forma adicional de medir el score Por qué Porque hasta ahora lo que hacíamos era medir el score con el conjunto entero de entrenamiento xtrain e-train o con el conjunto entero de 3x3 también podemos hacerlo justamente con Akira score de sitetrix en donde lo que hacemos es cruzar el itest es decir la salida del conjunto de Test con el hiper es decir con las predicciones que es la comparación más concreta que hemos hecho en muchos de los gráficos que hemos desarrollado hasta ahora entonces para eso lo que voy a hacer es primero generar las predicciones y luego comparar hites e ipred lo ejecuto y tengo un score igual fíjense qué casualidad al del conjunto de esto no siempre es así pero en este caso ha sido escéntrico bien Ahora vamos a pasar a el tema justamente de esta segunda parte implementar la reducción de la dimensionalidad para eso vamos a incorporar pca pca es justamente una de las formas más conocidas y más implementadas de poder reducir la dimensionalidad con lo cual lo primero que tenemos que hacer es crear una instancia de pca pero incorporando este hiper parámetro es fundamental este hiper parámetro porque lo que le estoy diciendo aquí es que quiero que busque una reducción de la dimensionalidad pero con una conservación de la información de El 095 es decir que quiero apenas un 5% y no más que eso de pérdida de información si supieron ese score o ese nivel de pérdida no me interesa una reducción de la dimensionalidad es decir buscaría otra o si no existe no lo aplicaría se entiende porque por lo general Este es el parámetro de referencia típico que se toma para estos casos sí Buscar una reducción con una pérdida mayor no tendría sentido porque lo que gano por un lado lo pierdo por el otro bien luego lo que generó es la transformación del conjunto entrenamiento del conjunto de Test Recuerden que esto es parecido a lo que hacíamos con el escaneamiento de datos hacíamos un escalet fit transform y un escale transform en este caso es pese a fit transform y pese a transform en el caso de x-trait transform en el caso del transporte y tenemos dos variables que le hemos puesto bien con esas dos variables que son versiones reducidas de X3 voy a crear mi modelo como lo creo con multinomial como trabajamos antes con el solver como trabajamos antes y le cambiamos aquí el perdón no le cambiamos el Random deberíamos hacerlo porque si no nos vamos a confundir con el otro sería 43 y el anterior es 42 mido el tiempo antes del entrenamiento mido el tiempo después del entrenamiento y le pasó los valores en este caso no X3 sino XT reducida e itrain bien ejecutamos la transformación toma un tiempo también y ahora creamos el modelo y medimos el tiempo que ya lo vi un poquito por aquí arriba 977 mido la precisión y la Score y fíjense la precisión que tengo en este caso 092 1 y 0 92 01 comparada con el caso anterior es bastante similar no guardan una gran diferencia Cuál es la gran diferencia que tengo aquí Cuál es el tiempo que tardé para generar el primer modelo 43 segundos Cuál es el tiempo que empleé para generar el segundo modelo 9 segundos casi 10 segundos es decir Prácticamente la cuarta parte de lo que en plena otro modelo esto en realidad con tiempos como los que tenemos aquí que se cuentan en segundos no es obviamente significativo la diferencia tampoco significativa con lo cual aquí podría aplicar cualquiera de los dos modelos con reducción o sin reducción Lo importante es que toman este caso sencillo que lo podemos ver rápidamente para que ustedes lo lleven a casos mucho más complejos como los que se nos van a presentar a futuro y vamos a estar hablando de diferencias de horas y quizás porque pensar en más de horas sino en días porque realmente entrenar modelos como los que vamos a ver en Deep learning lleva muchísimo tiempo obviamente de mucho dependiendo del tipo de recursos informáticos que tengamos finalmente les dejo una apéndice con un ejercicio sencillo para que ustedes prueben en su casa donde la idea es probar varios algoritmos para un mismo caso para un mismo conjunto de datos Y luego verificar los scores Aquí tengo estos tres algoritmos y el conjunto de datos y lo que hago es crear un algoritmo otro algoritmo al otro algoritmo y luego medir justamente los score en Los conjuntos de entrenamiento en el conjunto ítems Y compararlos esto es parte de lo que ustedes van a tener que hacer muchas veces esto obviamente es una versión muy sencillo pero la idea de fondo es la que ustedes van a tener que aplicar muchas veces donde ante un mismo problema van a tener que utilizar diferentes algoritmos Probar con diferentes algoritmos y luego dentro de cada uno de esos diferentes algoritmos cambiar su ciper parámetros buscando siempre mejores con también ver todo lo que es previo a el entrenamiento del modelo dependiendo independientemente perdón de El algoritmo que se trate Qué quiere decir esto bueno empezar a escalar datos empezará a transformar datos con One hotcoder como hicimos hoy con el tema de pasar variables categóricas a numéricas y también con lo que vimos hoy como tema importante esta clase la reducción de la dimensionalidad un conjunto de herramientas todo en busca de el mejor score y el mejor modelo nos vemos la clase que viene hasta aquí llegamos con esta clase gracias a ellas podido consolidar y ampliar los conceptos adquiridos anteriormente Te espero en la próxima clase para aprender un nuevo algoritmo nos vemos 