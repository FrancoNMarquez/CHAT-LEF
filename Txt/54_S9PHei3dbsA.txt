 Titulo: Clase 28 (parte 1) Curso de Inteligencia Artificial 
 URL https://youtu.be/S9PHei3dbsA  
 1522 segundos de duracion 
 Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número [Música] [Aplausos] [Música] 28 [Música] Hola a todos Esta es la clase número 28 del curso de Inteligencia artificial de ifes vamos a empezar en esta clase a ver las redes de tipo Deep learning para el mundo del nlp y con ello la red por excelencia del mundo del nlp que son las redes neuronales recurrentes y vamos a ver ver no solamente el modelo básico de este tipo de redes sino una evolución de este tipo de redes que son las redes de tipo lsm normalmente una neurona recurrente se dibuja de este modo donde hay una función que obtiene un valor de entrada y que produce un valor de salida además de la salida también produce otro valor de retroalimentación que pasa a la siguiente de neurona si desplegamos esto que estamos viendo aquí en esta neurona recurrente para poder ver lo que sería una red de tipo neuronal recurrente el resultado sería este leyendo de izquierda a derecha podemos introducir X en la primer neurona y calcular un resultado y así como un valor que pasa a la siguiente neurona en la segunda neurona ingresa un nuevo valor x el cual junto con el que viene de la neurona anterior Calcula a su vez un nuevo valor I en la tercer neurona ingresa un nuevo valor x el cual junto con el valor que viene de la neurona anterior Calcula otro nuevo valor I Y así sucesivamente esa secuencia paso a paso que va a continuar con la misma lógica va a dar lugar a lo que llamamos redes neuronales recurrentes Pero qué son estos valores x que entran en secuencia en cada una de las etapas de una red neuronal recurrente es Ni más ni menos que las palabras que pueden formar parte de una frase como esto que tenemos aquí Hola Cómo te va hoy lo cual describe lo que ya sabemos es un texto Ni más ni menos que una secuencia de palabras la arquitectura y el funcionamiento de tipo secuencial que tienen las redes neuronales recurrentes justamente hacen a que sean tan importantes en la filosofía que tienen los textos justamente de tipo secuencial y por ello la importancia de este tipo de redes en el mundo del nlp como vimos las redes neuronales recurrentes trabajan con dos entradas por cada una de las neuronas primero con cada una de estas palabras que hacemos entrar en secuencia y luego con la inform ación que viene de las neuronas anteriores con lo cual tiene la información de la nueva palabra que entra y del contexto que ofrecen las palabras precedentes pero lamentablemente ese contexto no es indefinido no es infinito Qué pasa si esta frase que vengo trabajando en la primera parte de esta animación Hola Cómo te va hoy yo le agrego más frases como la que vemos aquí en este día de otoño voy a tener un problema que se denomina falta de memoria por qué Porque el contexto de las palabras que están más cerca de las que están entrando últimamente van a tener más peso más prioridad y más relevancia que las palabras que ya quedaron muy atrás en el tiempo porque forman parte del principio del texto para solucionar este problema de falta de memoria es que se crearon unas redes especiales de tipo redes neurales recurrentes que se llaman redes de tipo lst m Pero por qué hablamos tanto de contexto porque es tan importante este concepto en el mundo del nlp bueno porque una de las tareas más importantes del nlp es la generación de texto y la generación de texto se trata de una palabra que aparece a continuación de otras precedentes y tiene que tener sentido con esas palabras precedentes justamente porque sio el algoritmo va a poner una nueva palabra sin tener en cuenta lo anterior y lo que va a ar es una secuencia de palabras que no tengan ningún sentido ni ninguna relevancia uno de los ejemplos más claros de esto es justamente el chat gpt un generador de texto por excelencia Y ustedes tienen seguramente más que presente que si ustedes le ponen un contexto a chat gpt le dicen Bueno quiero que escribas este texto como si fueses un experto en supongamos marketing el el chat gpt va a generar justamente un texto más apropiado y más relevante con lo que ustedes buscan si ustedes no le marcan en el prom Cuál es el contexto obviamente no lo va a hacer tan bien ni tan ajustado a lo que ustedes buan bien Por eso en el mundo de las redes neuronales recurrentes Obviamente el contexto es muy importante y para ello vamos al siguiente ejemplo vamos a suponer que tenemos esta frase hoy tenemos un hermoso cielo evidentemente Esta es una frase muy corta y Por ende la posibilidad de conocer el contexto completo de todas las palabras que llevan hasta este momento esta frase que solamente tiene cinco palabras va a ser muy factible y Por ende es muy probable que elija una palabra acertada y que tenga sentido con el contexto Como por ejemplo la palabra azul y la frase termina siendo hoy tenemos un hermoso cielo azul con lo cual es una frase que tiene un sentido contextual Pero pensemos Cómo sería la situación ante un ejemplo como este yo vivía en Alemania por ello cuando iba a la escuela los maestros me enseñaron cómo hablar en obviamente la palabra sería alemán dado que la cuarta palabra de esta frase es Alemania Pero esto implicaría que a esa altura después del n tiene que estar recordando la cuarta palabra y la cuarta palabra por la característica de una red neuronal recurrente simple sabemos que por el problema de la pérdida de memoria muy probablemente Ya esa palabra Alemania tenga poco peso y pueda llegar a inferir otra palabra que no sea alemán por lo tanto existen las redes de tipo lstm que son justamente como vimos antes las que me van a dar la solución para que yo pueda tener justamente el problema de memoria solucionado y que allí aparezca la palabra que sugiere este contexto que es alemán en concreto las redes neuronales que hemos estado observando van pasando la secuencia de palabras y aprendiendo el contexto pero ese contexto a largo plazo puede ser engañoso y es posible que no podamos ver como los significados de las palabras lej ganas van perdiendo relevancia y puedo confundir el contexto las redes lstm son la solución para este problema puesto que introducen algo que se llama estado de Zelda que es un contexto que se puede mantener durante la secuencia a largas secuencias de tiempo y que puede aportar contexto desde el principio mismo de la oración lo fascinante es que también puede proceder de manera bidireccional y en ese caso podría ser que las palabras posterior de la oración también puedan aportar contexto a las anteriores para que la red pueda aprender la semántica de la oración con mayor precisión bueno para terminar esta primer parte de esta clase vamos a ir al lab pnl 7 donde vamos a usar por primera vez una red neuronal pero aún no de tipo recurrente ni mucho menos de tipo lstm lo vamos a hacer para un problema de detección de frases sarcásticas es decir vamos a tener un dataset que nos va a dar justamente una serie de frases y van a estar rotuladas para que después yo pueda ponerle una frase nueva y ver justamente con esa red entrenada si puedo detectar si esa nueva frase es o no sarcástica lo primero que hago como siempre es importar las librerías primero Jason porque justamente el dataset que voy a usar es o está hecho Perdón en formato Jason tensor Flow que es lo que ya conocemos librería que ya hemos usado antes y que nos va a permitir justamente empezar a a volver a trabajar mejor dicho con redes de tipo de learning pandas maplit wordcloud que es una herramienta que les voy a mostrar justamente para ver cómo hacemos esa famosa nube de palabras y obviamente luego la librería tokenizer y pat sequences para justamente lo que es el armado de la secuencia de tokens de cada una de las palabras de este Jason que voy a trabajar así que voy a importar esas librerías que acabo de mencionar y luego voy a montar como siempre el Drive desde el cual ustedes como siempre les digo van a tomar el archivo que yo les voy a pasar a través del Campus bien Voy a leer el Jason y voy a crear un dataframe justamente en este caso con r Jason y el nombre del archivo que es sarsan Jason bien y luego voy a hacer un Head para mirar rápidamente el contenido de este dataframe bien Aquí tengo el dataframe donde veo justamente que tengo una headline que es como un titular el la URL donde está ese artículo de ese titular y una identificación con uno o cero que es lo que rotula o etiqueta esta headline como de tipo sarcástico o no es decir en este caso y sarcastic 1 es sarcástico y cero es no paso seguido voy a hacer un shap para reconocer la estructura de este dataframe y puedo ver que tengo 28619 observaciones y tres características que son las que ya vimos antes aquí arriba bien lo que vamos a hacer ahora es hacer una nube de palabras en realidad esto no es obligatorio no es exigible para los propósitos de lo que vamos a ver con el algoritmo que queremos hacer y el objetivo de ese algoritmo pero sí es una herramienta muy importante para el análisis de datos con lo cual acá voy a crear primero a través de plt la configuración del tamaño en que quiero Que aparezca ese cuadrante que me va a mostrar nu de palabras y luego voy a crear con wc una instancia de wordcloud que es justamente la librería que me permite eso con lo cual voy a definir algunos parámetros la máxima cantidad de palabras que quiero el ancho el alto y voy a con generate tomar Qué tipo de eh características Pero qué tipo de observaciones son las que quiero para este trabajo que voy a hacer y justamente lo voy a trabajar con esta condición de que headline que es lo que voy a tomar en consideración que es esta columna sí lo voy a circunscribir a solamente aquellas eh aquellas observaciones cuyo is sarcastic sea igual a cero qué quiere decir con esto que justamente voy a elegir los casos como dice el título aquí arriba no sarcásticos y luego con plt y MS lo que hago justamente es imprimir este wc que es esta númera de puntos Así que lo ejecutamos y vamos a ver como siempre sabemos que es esta mecánica de este tipo de gráficos donde muestra en con mayor tamaño Sí este las palabras que más frecuentes salen y lo inverso para las palabras que menos frecuencia tienen en este caso no se trata de frecuencia no frecuencia sino las palabras que tienen o que aparecen más identificadas con titulares no sarcásticos y más chicas las que representan lo contrario sí bien ahí tenemos entonces Trump new se Donald TR nuevamente Woman people way bien Vamos a hacer lo mismo ahora pero para lo contrario para las frases sarcásticas bien entonces en este caso la lógica es exactamente lo mismo lo que voy a cambiar simplemente aquí ese condicionamiento donde voy a poner que el campo o en este caso Perdón la característica y sarcastic le voy a poner que sea igual a uno así como antes puse que fuese igual a cer Bueno ejecutamos este gráfico y y esperamos el resultado a ver que nos muestra ahora para el caso de los titulares de tipo sarcástico y cuáles son las palabras más relevantes entre los titulares de tipo sarcástico bien acá tenemos Man New Como las palabras más importantes y luego nation report make Bueno lo que ustedes Ven aquí y pueden con tranquilidad analizar y ver por qué Por ejemplo las palabras Money New representan algo muy típico en los titulares de sarcástico bueno habiendo podido conocer esta nueva herramienta que les va a permitir incorporar una opción más a la hora del análisis de datos vamos a como hacemos siempre crear la x y la y en este caso la x va a ser el titular el headline y la i justamente la etiqueta de si algo es sarcástico o no bien creamos nuestra x y nuestra I y voy a hacer una visualización rápida del x simplemente para ver que justamente tengo en las x los cantidad de observaciones totales Las 28618 observaciones que son headlines que corresponden o yo he colocado dentro de la x para qué Para justamente empezar por hacer la división como hacemos siempre de los conjuntos de Test y los conjunto de entrenamiento en este caso no voy a usar al TR spr sino que lo voy a hacer numéricamente yo a dedo como quien dice entonces voy a poner las training synthesis y las testing synesis o sea las dos X en el primer caso de la cer a la 20000 y el último caso Perdón hasta la anterior a la 20000 y en el último caso es decir las de testing desde la 20000 hasta la última que como dijimos antes es la 28618 lo mismo voy a hacer para las training labels y las testing labels con los elementos que voy a sacar en este caso de la variable I bien ejecutamos esto y ahora vamos a proceder a la tokenización bien como estamos acostum hacerlo ya vamos a crear un tokenizador justamente utilizando la librería tokenizer en este caso la tomamos no de nltk sino de tensor flow bien Aquí le voy a indicar que el tamaño máximo de vocabulario que quiero va a ser 10,000 Ya vimos en la clase pasada que yo puedo asumir que el vocabulario es la representación total de todas las palabras de todo lo que yo tengo como conjunto de dasos o simplemente puedo decir solamente quiero esta cantidad y obviamente va a cortar a esa ese número la lo que es el vocabulario completo con lo cual puede ser que sea justo o no es un número obviamente elegido con un criterio pero no es Exacto con lo cual puede ser que el vocabulario sea más grande o más chico que eso qué pasa si es más chico el vocabulario Bueno aquí entra en juego justamente este parámetro que estoy poniendo aquí el o token donde le indico que si como el vocabolo es más chico que la cantidad total de palabras que tienen todo este conjunto de datos cuando me toque una palabra que no esté en el vocabulario no voy a saber qué número ponerle porque justamente esa palabra no existe en el vocabulario Entonces en ese caso le voy a poner este indicador que está aquí que es out of vocabulary que justamente Esto me permite hacer en el caso de que la palabra no exista qué oob token voy a elegir se puede poner esta referencia que es la clásica que se pone en estos casos u otra que ustedes pueden elegir bien una vez que creo el tokenizador lo que hago es entrenar mi tokenizador justamente a partir de la Fuente de datos que yo tengo que son los training synthesis que son las primeras 19 20000 porque empieza de cero hasta la anterior a 200000 las primeras 20.000 los primeros 20000 elementos de training sentesis sí bien entonces una vez que entreno al tokenizador lo que hago es empezar a crear un índice para cada una de las palabras Sí entonces lo que hago es con Word Index sobre tokenizer bueno bueno colocar ese resultado en una variable que he dado llamarle al igual que el método Word Index bien Ahora convertimos los titulares los headlines del conjunto de train en secuencias de números con qué con el método Tex to sequences que ya lo habemos usado con la otra librería ahora con tokenizer pero de transfer Flow sobre las oraciones de entrenamiento y esas oraciones de entrenamiento pasadas a secuencias la voy a poner en una variable que le voy a llamar secuencias de entrenamiento sí bien una vez que tengo eso voy a padar esas secuencias con piding post Recuerden que el tema de padar es si el la cantidad de números sí que representa esa oración es inferior al tamaño del vector lo que tengo que hacer es rellenar todos es espacios vacíos con elementos que habitualmente son ceros sí lo que tengo que indicar es si lo que voy a hacer es ponerlos al principio esos ceros o al final en este caso con post lo que hago es que ponga el texto al principio y ese relleno con ceros al final bien Luego hacemos Exactamente lo mismo o sea las secuencias y el padeo así como lo hice antes para el conjunto entrenamiento ahora también para el conjunto de Test bien ejecuto toda esta cadena de instrucciones y voy a pasar ahora a ver cómo poder identificar si un una secuencia de números que representa un texto sí es o no un texto de tipo sarcástico para ello vamos a construir una red de tipo de-learning para poner en en uso a nuestros embedding Sí en principio voy a crear como ya lo hemos hecho otras veces con queras de tesor flow sequential cada una de las capas de mi eh red neuronal y la voy a llamar a este modelo model lo primero que pongo es una capa de de tipo embedding donde le voy a decir con el primer parámetro que el vocabulario que ya como ya vimos antes es de 10,000 palabras la el tamaño de los embedding es el vector los vectores de cada una de estas palabras van a ser de 16 posiciones y el tamaño máximo de input es decir los textos que vayan entrando sí deberían tener una dimensión de hasta 100 caracteres luego voy a crear una capa de pulling de una dimensión sí esto no confundir con lo que usamos en las rales convolucionales más allá que que también había una capa de pulling y luego voy a hacer dos capas densas una de 24 neuronas con la función de activación relu que ya hemos usado antes y finalmente una capa de salida de tipo densa también con eh una una este una función de activación de tipo sigmoide dado que tengo una sola neurona Por qué tengo una sola neurona porque este es un un problema de tipo binario tengo que decir si un texto es o no sarcástico con lo cual tengo dos salidas posibles es sarcástico o no es sarcástico bien y finalmente la compilación que lo voy a comparar lo voy a compilar perdón con la función de pérdida vinary Cross entropy Recuerden que esta la uso cuando es justamente una salida de tipo binario así que tengo una red que me va a dar solamente dos opciones no es de tipo categórica el Cross entropy que lo usaba cuando tenía más de dos opciones de salida voy a usar el optimizador Adam y le voy a decir que la métrica para ir midiendo la precisión de esta red sea la de acur ejecuto esta celda y luego con el sumari vamos a ver como muchas otras veces s Bueno un pantallazo de cómo quedó conformada nuestra red neuronal habiendo hecho esto voy a pasar a entrenar a mi modelo y voy a guardar la la historia digamos de cada uno de estos entrenamientos para después graficarlos dentro de esta variable history voy a crear una variable para poner la cantidad de epoch que voy a definir en TR y Bueno luego lo que tengo que hacer como siempre model fit pongo justamente las los elementos de entrenamiento que en este caso son training pad que son los vectores que representan a las headlines luego las labs que me dicen si esa cada una de esas headlines es o no Eh sarcástico bueno la cantidad de epoch y los conjuntos de validación el det test para justamente las noticias y el D test para las etiquetas bien ejecutamos esto y va a tardar un tiempito porque son 30 eh epoch los que hemos aplicado Aquí bien aquí Terminamos el entrenamiento tenemos un muy buen accuracy 99% eh para el conjunto de entrenamiento y un muy buen accuracy también para el conjunto de validación que está en el orden del 80% bien Ahora vamos a probar esta red que hemos creado esta redal que hemos creado poniendo en este caso eh dentro bueno creando una variable sentences que que le voy a poner dos oraciones donde la primera bueno está escrita en inglés la pueden Traducir si quieren yo busqué funciones este dado que el dataset está puesto en inglés este he buscado oraciones típicas de habla inglesa que representan Sarcasmo Y en este caso la primera representa Sarcasmo y la segunda simplemente anuncia cuándo va a empezar a eh la temporada final de Game of thrones es una un titular no sarcástico en este caso la las dos oraciones que están dentro esta var sentences voy a aplicarles el mismo criterio que utilicé para con los datos que usé para entrenar es decir lo que tengo que hacerlos pasar por el método text to sequences y el resultado de ello lo voy a poner en una variable que le he puesto a llamar sequences luego insisto al igual que lo que hice con los datos que entrené los voy a padar Por lo cual Bueno sigo exactamente la misma lógica que en aquel caso una vez que tengo estas dos oraciones que han sido eh inicializadas y padas voy a hacer la predicción Entonces el resultado final que está en esta variable padded voy a aplicarlo a predict de model y voy a imprimir el resultado bien ejecuto eso y veo que en el primer caso me da 9.06 al elevado a la -10 lo cual me daría en realidad un 90.66 por con lo cual quiere decir que la primer frase tiene un 90 por eh está más cerca de digamos de eh de ser de tipo sarcástica en un casi 100% que no sarcástica por el contrario la otra es 5.9 elevado a la 8 a 10 a la -8 perdón con lo cual obviamente es un 0,0005 de eh posibilidades de ser sarcástica Por ende es una frase no sarcástica y justamente como dijimos antes lo que anuncia aquí obviamente es un titular serio y totalmente alejado de ser algo sarcástico aquí teng tengo en este caso tres oraciones donde las dos primeras son sarcásticas y la última no el resto de de de lo que yo aplico aquí en este caso del código es exactamente el mismo Solamente he cambiado el contenido de la variable sentence y lo ejecuto y veo el resultado donde justamente las dos primeras eh oraciones tienen en el primer caso un 96 y en la segunda un 99% de posibilidad de ser sarcásticas Sí están más cerca del uno Por ende justamente son sarcásticas y eh la otra es 2.1 elevado a la 10 a la-4 con lo cual 0.0002 y justamente está mucho más cerca del cero Por ende es una frase caratulada o rotulada por este modelo que hemos creado de tipo no sarcástico bien hasta acá llegamos entonces con la teoría de esta clase y con la práctica de esta primer parte de la clase en la segunda parte de la clase vamos a hacer una práctica donde Ahí sí vamos a aplicar justamente la creación de Nuestra primer red neuronal recurrente con el stm hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda [Música] parte