 Titulo: Clase 31 (parte 1) Curso de Inteligencia Artificial 
 URL https://youtu.be/dv7dK-Zm89I  
 1372 segundos de duracion 
 Hola bienvenidos al curso de Inteligencia artificial de ifes esta es la primera parte de la clase número [Música] [Aplausos] [Música] 31 [Música] Hola Bienvenidos a todos a la clase número 31 del curso de Inteligencia artificial de ifes Vamos a continuar como venimos haciendo las últimas clases en el mundo de nlp concretamente hoy vamos a ver el tema de cómo resumir textos y lo vamos a hacer tomando varias fuentes en principio resumiendo textos desde un propio texto puede ser hecho en PDF en Word en txt o también la opción como habíamos visto en la clase pasada de utilizar Wikipedia como fuente de ingreso de información pero también vamos a resumir desde un archivo de tipo de audio de tipo mp4 o bien desde un eh video alojado en YouTube así que bueno eh en las distintas partes de esta clase vamos a hacer estos objetivos siempre con este elemento transversal que es poder resumir texto y usar algunas cosas que usamos la clase pasada como poder ese resumen pasarlo a un archivo de audio bien vamos a empezar entonces con la primera parte de esta clase viendo cómo resumir textos desde un texto en PDF bien aquí estamos en el colab la pln 12 donde bueno el título resalta lo que dijimos recién lo primero que vamos a hacer además de conectarnos como siempre es instalar esta librería que venimos usando todos estos estas clases anteriores Así que empezamos por ello y como sabemos que una de las cosas que tenemos que hacernos es conectarnos a la Api de Open vamos a montar el Drive donde sabemos que tenemos allí el archivo pun m de environment donde está entre otras cosas la Api ahora que montamos el Drive bueno justamente lo que hacemos Es configurar el entorno para recoger entre otras cosas la apq Así que procedemos a eso y vamos ahora sí habiendo contextualizado todas estas cosas que necesitamos para operar empezar con el propósito o el primer propósito de esta primera parte de esta clase que es resumir un texto en este caso vamos a empezar con un texto corto nuestro ya conocido chat gpt txt que hemos usado en clases anteriores vamos a volver a usarlo con ese objetivo Entonces lo primero que hacemos Es justamente acceder a ese documento y cargarlo dentro de una variable que le ponemos como nombre texto como ya lo hemos hecho antes una vez que hacemos esto lo imprimimos el contenido ya lo conocemos lo hacemos a título de volver a bueno poner sobre relieve eh o recordar Mejor dicho De qué se trataba este contenido que era una explicación bastante breve de de qué se trata el ch gpt bien y ahora para hacer el resumen vamos a cargar dos librerías primero la de chat Open eii que es justamente lo que vamos a usar para que nos haga ese resumen y luego vamos a cargar de lun Chain esema estas Este variables que usamos para hacer el prom acuérdense que yo acá voy a usar openi Pero además tengo que mandarle un prom y el prom se configura usando estas vares que hemos puesto aquí que ya lo hemos usado la clase pasada y por eso voy a cargar estas librerías y a continuación voy a crear una variable mensajes donde voy a cargar justamente Dos De esas tres variables de esos tres parámetros Perdón que uno es el System m recordemos que el System mees era aquel que nos marcaba el contexto como cuando usamos chat gpt y le decimos que queremos que haga algo Como si fuese un profesional en tal cosa destinado a tales tipos profesionales supongamos que en este caso quiero resumir chat gpt como le estoy poniendo aquí usted es un especialista en resumir textos y el destinatario es un tecnólogo o bien el destinatario no es un tecnólogo entonces obviamente la salida va a ser diferente porque el prom es diferente en este caso no hago referencia así Quién es el destinatario de este documento sino hago referencia acerca de quién Debería ser la persona que lo debería hacer o sea Quién es la persona que es un experto en resumen y que le pido que me haga un resumen y una vez que hago eso que son todas las cuestiones de contexto le doy el objetivo en sí mismo que es que quiero que me haga un resumen del siguiente texto y recordemos que en este caso ese es el human mesage sí la parte del prom que se llama human magis y que le pongo una variable que es justamente el texto sobre el cual va a trabajar Obviamente el objetivo es este y puede aplicar a cualquier texto y justamente en la variable texto es lo que va a cambiar el elemento que va a tomar para resumir en este caso chat gpt txt y en otro caso será cualquier otro texto como lo vamos vamos a usar más adelante con un archivo de tipo PDF a continuación voy a crear una instancia llm que justamente es la estancia de chat Open e que es la red que voy a usar con este propósito de hacer un resumen voy a usar el modelo gpt 4 Recuerden que es más caro que el gpt 3.5 Turbo que venimos usando en clases anteriores con una temperatura de cero por qué Porque aquí no quiero creatividad un resumen no debe ser creativo salvo que yo quiera algo así en principio Debería ser algo por eso la temperatura cer0 que Recuerden que cuando está más cerca de dos chat gpt se pone más creativo y cuando está más cerca de cer0 es más preciso más exacto luego lo que voy a hacer va a ser verificar este texto que voy a usar sí del chat gpt txt cuántos token tiene y justamente me dice que tengo 76 76 tokens Sí luego de ello esto es a título de información nada más no no es una un paso que sí o sí tenemos que hacer pero es importante Por qué Porque Recuerden que dependiendo de la cantidad de tokens lo que me cobra chpt H Así que no es un dato menor Más allá de que no es exigible hacerlo y luego Bueno lo que tengo que hacer es el resumen Cómo hago el resumen bueno Llamando al modelo el lm que crea Aquí esta instancia de chat Open a Y qué le voy a pasar le voy a pasar los mensajes y los mensajes son los prom Sí qué son estos dos que tengo aquí arriba es decir Usted es un especialista resumir textos haga un resumen del siguiente texto Cuál es el texto bueno el que hablamos recién sí Entonces ejecutamos esto y luego vamos a ver justamente en la variable resumen el resultado de aplicar este prom en este modelo y este prom qué hace me hace un resumen en base qué en base a texto y cuál es texto el texto nuestro que venimos trabajando bien así que bueno Esto l un tiempito y luego vamos a poder ver a través de la propiedad content de resumen de est de resumen justamente el contenido resumido ya lo he hecho lo imprimimos y aquí tenemos chat gpt es un hito en la Inteligencia artificial cuyo desarrollo bueno etcétera etcétera etcétera aquí continúo un poquito más y ahí podemos ver completamente todo este resumen que tenemos más o menos unos 15 renglones de algo que originalmente ven era obviamente bastante más largo que eso no O sea que es es un buen resumen H bien pero ahora con la intención de entender la potencia del prom acá es muy importante que lo que tenemos que entender es que cuando trabajo con estas grandes redes eh lo que yo tengo que tratar de hacer es eh focalizar sobre el prom y todo lo que yo vaya configurando sobre el prom va a ser bueno algo que me va a permitir ajustar o hacer lo que se llama un tuneo fino de esa red en virtud de mis propósitos con lo cual voy a cambiar un poquito el prom y le voy a decir que me haga ese resumen pero en otro idioma fíjense que es Exactamente igual solamente que aquí al final Le agregué este texto y traduzca el resumen en idioma inglés sí es decir que lo que cambié fue el prom con lo cual reconfiguró esta var de mensajes y vuelvo a pedirle bueno vuelvo a hacerme un resumen sí en este caso cargar la var resumen Perdón sobre la llm con estos mensajes nuevos que en realidad lo único que varía es esto que le he agregado aquí que dice y traduzca el resumen en idioma inglés bien terminado esto veo que aquí está el resumen donde bueno lo veremos un poquito más tiene más o menos la misma extensión que la anterior Pero obviamente es esto mismo que me envío aquí en castellano en idioma inglés sí esto digamos para que entiendan que también estas cuestiones se pueden agregar y todo lo que ustedes quieran agregarle digamos en el prom insisto en lineamiento al objetivo de ustedes bueno va a ser una forma de ajustar el modelo lo más probable a lo que ustedes tengan como intención Bueno hasta aquí parece que este proceso es muy simple pero no siempre es así hemos tomado un texto muy corto y esa es la razón de esa simplicidad Cuál es la situación cualquier llm cualquiera de estos grandes modelos Por más que vayan abriendo su capacidad de recibir inputs de información no son infinitos o sea yo no puedo darle un texto de cualquier tamaño en la última bueno en los últimos cambios digamos ha pasado chat gpt a abrir irse a mayores tamaños a mayores volúmenes Pero insisto esto no quiere decir que siempre por más grande que sea esa amplitud Yo voy a poder poner cualquier texto con lo cual voy a tener que tener presente que es muy probable que lo tenga que ir haciendo en splits que es lo que hemos hecho antes en clases anteriores es decir voy a tener que subdividir el texto e ir entregando eso en etapas con el objetivo de lograr un resumen el resumen lo voy a lograr solamente que el método va a ser diferente por eso aquí lo que vamos a hacer es tomar un documento más grande que el anterior para poder llevar a cabo ese proceso y que ustedes sepan Cómo proceder en caso de que tengan un documento que exceda la capacidad de un input en un solo acto un input simple para tomar completamente todo el documento en este caso vamos a tomar un archivo de los que ya teníamos de recursos humanos que se llama archivo 7.pdf ustedes lo tienen dentro del campo virtual Así que lo primero que hacemos Es instalar esta librería para poder leer archivo de tipo PDF que ya lo Hi antes así que arrancamos con esto y luego vamos a hacer lo propio con algunas librerías de chat Open e con recursive ch splitter que ya lo usamos justamente para splitear el texto y con lumar change que justamente es lo que nos va a permitir hacer un resumen pero en formato de cadena o sea No todo en un solo acto sino en cada teniendo en cuenta que mi texto ahora está splite está subdividido Así que cargamos también esas librerías y paso seguido voy a en lector sí hacer un PDF reader se cargar El lector en base a este archivo como que estamos tomando como input archivo 7.pdf así que bueno creamos esta nueva instancia lector y lo que hacemos a continuación es poder recorrer se acuerdan cada una de estas partes lo que hacía El lector era dividirme en páginas este documento PDF lo que tengo que hacer es tomar cada una de las páginas tomar el texto y poner una variable la siguiente página la agrego a esa variable la siguiente página la agrego esa variable con lo cual tengo cada una de las páginas convertidas a textos y sumadas en una nueva variable de tipo texto que le hemos dado en Llamar texto justamente donde Ahora sí voy a tener en formato de texto todo el texto completo de este archivo 7.pdf Así que ejecutamos este y recorremos bastante rápido ya tengo en texto todo el documento y para poder verlo hago un print justamente de texto y bueno que aquí tengo todo el libro completo dentro de una variable bien ahí pueden ver de Que obviamente esto recorrerlo bastante largo Un poquito más arriba Vamos un poco más arriba aquí Bueno Este es todo el documento es un libro dentro de todo corto no obviamente he tratado de hacer eso para justamente que el gasto después que tengamos cuando usamos este chpt no sea tan largo no no se tan grande perdón pero dentro de los libros que teníamos de los 10 libros que tenemos de recursos humanos es uno de los más cortos HM bien entonces ya insisto tengo dentro de esta variable texto todo el contenido de este archivo 7 PDF HM bien con lo cual vamos entonces a continuación a crear una instancia como hicimos recién que también llamamos llm de chat Open pero cambiando en este caso el tipo de modelo no usamos gpt 4 sino gpt 3.5 Turbo justamente buscando que lo que va a hacer a continuación con la tokenización tenga un costo mucho menor ustedes saben que es bastante diferente eh casi un 10 veces menos el costo de diferencia entre gpt 4 y gpt 3.5 Sí con una temperatura de cero Porque queremos hacer un resumen y como dijimos recién la temperatura es lo que hace que el resumen sea lo más preciso y no sea creativo bien creamos esta instancia y luego al igual que hoy medimos o vemos la dimensión de la cantidad de tokens en este caso tenemos 19958 versus lo que habíamos hecho con el ejemplo anterior que teníamos 7677 Sí más del doble tengo en este caso bien Por qué este número es importante porque también me permite a mí medir la cantidad de caracteres que tengo 6941 tokens palabras Estos son caracteres no es lo mismo obviamente pero esos dos valores me van a llevar a mí a poder ver cuál es la cantidad promedio o estimada de por token es decir cuál va a ser el largo promedio de cada una de las palabras 3.46 caracteres por cada palabra obviamente habrá palabras que tengan más o menos esto es un cálculo Por qué Porque lo que vamos a hacer a continuación es ver cuál es el límite que tiene justamente los modelos de chat gpt para poder mandar los split porque por más que yo lo split el documento si esos split tienen un tamaño más más grande que lo que es capaz de recibir echas gpt seguimos en problema O sea mis eh Mis divisiones mis split tienen que ser menores a la capacidad de input que tiene mis modelo de gpt para que por más que yo lo vaya llevando uno a uno puedan ser recibidos entonces aquí tenemos 4096 es la cantidad máxima de tokens que toma Este modelo gpt 3.5 y 8192 la cantidad máxima de tokens que toma el modelo gpt 4 obviamente hay una ventaja para gpt 4 Pero hay un costo mayor y como dijimos recién 10 veces mayor H Así que eh Bueno aquí tenemos que ver relación costo beneficio de acuerdo al caso que tengamos cada uno nosotros como presente y Eh bueno en el caso de elegir un modelo u otro En consecuencia vamos a ver bien si entendemos qué es lo que tenemos que ver ahora y todo el cálculo matemático que tenemos que hacer para proceder yo aquí he deducido que tengo en promedio por cada token 3,4 tes 3,4 letras está bien bien Por qué es importante ese dato porque yo ahora digo Bueno si tomo el primer modelo que es el más económico que me es capaz de recibir hasta 4,096 tokens tendré que calcular Cuántos caracteres tendrían que tener mis chuns sí mis subdivisiones de textos para que correspondan a 4,096 por qué Porque los chunks se calculan por cantidad de caracteres no por cantidad de toques y yo acá tengo el dato de la cantidad de tokes Entonces yo tengo en cant una cantidad promedio sí 3,4 letras por tokens de mi texto Entonces lo multiplico por 4096 y lo que tengo es un número que me dice que cada chun Debería ser si la cantidad promedio de letras que tienen mis palabras es de 3,4 cada Chan Debería ser de 14000 189 caracteres se entiende o sea 4096 tokens por la cantidad promedio de letras que tiene cada tokens me da el tamaño del chun obviamente esto es un valor estimado y un valor de máxima porque sabemos que eso es un valor promedio no todos los tokens tienen la misma cantidad de carácter con lo cual corro el riesgo de excederme porque yo tomo como unidad de medida la cantidad de caracteres y no la cantidad de tokens Entonces vamos a tomar una medida un poquito hacia atrás Y en lugar de decir que mis chun sean de 14189 le voy a poner 12000 es un valor menor y recordemos también el chun overlap es decir que esto es imprescindible para que justamente Esta división de splits no pierda contexto de la continuidad del texto es decir que con esto Recuerden que de estos 12000 caracteres que tiene cada chank habrá 100 que se repiten del chun [Música] anterior una vez que está claro esto lo que hago justamente es proceder a splitear mi texto y poner todos esos splits dentro de una Ray que voy a llamar fragmentos y Lu hago un print de l de fragmentos para ver En cuántos chuns me explicó mi texto completo de insisto mi variable texto que corresponde a el archivo 7 PDF no perdamos de vista ese tema así que bueno lo ejecutamos y veo que hizo un split de seis fragmentos O sea me subdividió ese documento en seis fragmentos y podemos ver uno de ellos por ejemplo el primero fragmento c. p content Y también vamos a ver la cantidad de tokens que le puso a ese primer fragmento Bueno aquí veo todo el fragmento la primera parte y veo que lo hizo en 36 53 to sea bastante menos que el máximo 4096 que aceptas g o sea está bastante ajustado s Pero bueno este eso no es algo problemático porque en realidad este esto cobra por tokes con lo cual que est splite en más o menos split no hace Elo simplemente al tiempo obviamente por supuesto Bueno pero es difícil establecerlo de una manera que sea Exacto así que bueno visto esto lo que podemos hacer es decir Bueno Este es el tamaño de el primer split en tokens y Cuál será el tamaño de los otros Bueno vamos a hacer un for aquí y podemos ver que ven tengo la cantidad de tokens que tiene cada uno de los seis spit Obviamente el último es el más chico Porque es lo que quedó como residente de texto pero digamos que está bastante cómodo quizás si quisiésemos hacerlo en menos Bueno nos pasaríamos de esos 4096 aquí lo que hacer como una cuenta es sumar todo esto y esa suma nos debería dar cuánto nos debería dar esto es decir la cantidad total de tokens que tiene el documento bueno y ahora tenemos que hacer el resumen Ni más ni menos con lo cual lo primero que hago es crear una instancia de lowat sumar Chain con esta variable change dónde le mando Obviamente el modelo el lm que es lo que habíamos distanciado de aquí arriba s nuestro modelo de Open e sí Este modelo de aquí y también le voy a pasar una serie de parámetros que son importantes si bueno el verb false ya saben que lo usamos o no dependiendo Si queremos ver un resultado digamos de los pasos que va haciendo por pantalla o no pero aquí es muy importante este parámetro que es el chin type este parámetro en donde yo le indico map redus le indica que lo que yo tengo que hacer aquí es un resumen de un texto que está splite con lo cual tiene que mapear cada uno de los splits de este documento para generar un único resumen que tenga contexto del total es decir no voy a hacer Si en este caso Tengo seis split seis resúmenes el resumen es uno solo lo que tengo que hacer es tratar de mapear todo Ese Conjunto que está subido para verlo justamente como un todo y hacer un resumen único para Ese Conjunto bien una vez que he creado esta instancia uso esa instancia con el método Run para mandarle todos los fragmentos todos los chunks y que me haga el resumen y ese resumen me lo ponga dentro de esta variable resumen bien una vez que ejecuto esto voy a imprimir el resultado de ese resumen y veo aquí este resultado que en este caso me hace un resumen en inglés bueno esto tiene que ver con el modelo antes usamos el gpt 4 ahora el GP 3.5 Perdón gpt 3.5 Turbo lo cual me ha dado resultado en este caso justamente con esta forma de manejarlo si sin el prom como lo hicimos en el caso anterior que lo hicimos de l change en inglés pero podemos usar justamente el mismo Open Ai para pasarlo a Castellano bueno creando un modelo que use text Vinci que lo hemos usado antes para tareas justamente que tengan que ver con tratamiento de texto en donde Obviamente con una temperatura de cero porque quio una traducción literal en donde lo que voy a hacer con este modelo que estoy creando aquí gpt3 pasarle como input tradúzcame al cast Llano el siguiente texto que está escrito en inglés y le paso como variable resumen que es Ni más ni menos que esta variable que vimos Aquí recién eh Como resumen del texto anterior pero expresado en inglés Entonces ejecutamos esto que está aquí y vamos a poner la respuesta dentro de una V respuesta y vamos a imprimir la Val de respuesta y voy a ver que justamente ahora me va a mostrar Eh mi texto que antes obtuve como resumen en inglés pero ahora en castellano bien con esto terminamos esta primera parte de la clase Así que nos vemos en la segunda parte de la clase hasta aquí llegamos con la primera parte de esta clase nos vemos en la segunda [Música] parte