 Titulo: Clase7 (parte2) del Curso de Inteligencia Artificial 
 URL https://youtu.be/oVYt4UtkvPI  
 1969 segundos de duracion 
 Hola bienvenidos Esta es la segunda parte de la clase número 7 del curso de Inteligencia artificial de ifes en ella vamos a poner en práctica el algoritmo de regresión logística que recién aprendimos vamos por ello [Música] Bueno Hola a todos de nuevo estamos en la segunda parte de la clase número 7 ahora con la práctica que le sigue a la introducción conceptual de la regresión logística que es el título que aquí tenemos en nuestro Notebook clase 7 que vamos a tener que abrir para poder seguir justamente la práctica de este concepto vamos a ver dos ejemplos de regresión logística que tienen que ver con justamente una de las últimas cosas que tratábamos en la introducción teórica por un lado una regresión logística con salida binaria es decir con un Target que es 0 o 1 dentro del Data set que vamos a utilizar y luego vamos a ver un ejemplo con salida discreta no binaria es decir con más de una opción de salida bien pero empecemos con lo primero que es justamente en la opción con salida binaria Y empezamos por la descripción como siempre de las librerías que vamos a utilizar bien vamos a empezar por lo que ya venimos usando habitualmente que es pandas y hasta ahí estamos porque en este caso no vamos a usar ni Nampa ni maplogy si vamos a usar muchas librerías nuevas de sideking learn pueden empezar vamos a usar de Side Killer linear model que habíamos usado en la clase anterior pero en este caso con logística integration porque obviamente es el tipo de algoritmo que vamos a ver ahora pero también vamos a usar además de 30 splits que también lo veníamos usando anteriormente Data sets que es de donde vamos a sacar para el primer ejemplo el conjunto de datos estándar que sirve para escalar datos concepto que hablamos también en la parte teórica de la importancia de escalar los datos y finalmente con fusión Matrix que es justamente una de las formas nuevas que vamos a aprender de cómo evaluar bueno el nivel de eficiencia de un algoritmo Por ende Entonces lo primero que hacemos Es ejecutar esta celda para poder empezar a importar las librerías bueno tenemos un tiempo acá como siempre de delay porque no estábamos conectados Ahí estamos conectados por lo tanto ahora sí le va a dar curso a la ejecución de esta parte importando todas las librerías bien Ahí está tardó dos segundos obviamente porque incluye el tiempo de la conexión con el Google luego vamos a trabajar con el Data set fíjense que en este caso no vamos a tomar un Data set de la forma que lo veníamos haciendo si lo vamos a hacer para el segundo ejemplo Pero de esta manera Vamos a aprender Bueno una forma nueva de incorporar un dataset fíjese que aquí en el título yo lo pongo para que le sirva como referencia que justamente lo que vamos a importar no es un archivo de tipo csv es un Data set preparado sobre casos de cáncer de seno en este caso tenemos Bueno un montón de detalles que tienen que ver con ese tema de salud y finalmente un indicador que una característica Obviamente que nos indica si ese paciente tuvo o no cáncer bien cómo se importa un Data set de este tipo bueno se hace justamente es de la librería que hemos importado datasettes la tenemos aquí arriba Esta de Side y dataset punto loadbrest Cáncer ya es un datas incorporado dentro de todos los que tiene esta librería de datos Sí todos secarán con load y la referencia hacia el nombre del obviamente esta información está dentro de la página de cycle Y ustedes pueden consultar bueno por otros datase que no son estos bien eso lo ponemos dentro de la variable Data y una vez que hacemos eso vamos a hacer un print de Data Kiss y Qué es Data Kiss es un diccionario como Ven aquí con un conjunto de objetos que describen distintas características del conjunto de datos por un lado tengo Data y Target que son los más importantes estos son los datos Y este es el dato objetivo y otros que vamos a usar más adelante como este que describe características de detalle de la composición de ese conjunto de datos Pero lo primero que vamos a hacer es tratar de lograr una forma parecida a la que veníamos trabajando cuando importamos la información de un archivo de tipo ccb o txt o xls bien en este caso la información la tenemos separada como dijimos recién dato por un lado o datos por un lado y dato objetivo por el otro por lo tanto para buscar esa forma que referenciamos hace un rato vamos a tener que usar el método Data frame de pandas y por un lado vamos a tener que poner dentro de DF que es nuestro futuro Data frame justamente los datos y el nombre de esas características porque Data punto Data si acuérdense que esto es Data de el objeto Data me traen los datos puros pero sin los nombres de las columnas para ello voy a tener que especificar con columns igual a Data fitur names fíjense Que aquí tiene filtro names de que cada uno de estos datos los tomas de este diccionario de X es decir estoy aquí con ambas cosas yo logro tener ahora los datos de cada una de las características menos el dato objetivo y los nombres de esa característica finalmente voy a tener que agregarle a ese Data frame el target es decir el dato objetivo con lo cual voy a tener que crear un nuevo una nueva columna aparece otra frame justamente de esta forma como se hace habitualmente y luego el dato concretamente es decir que con esta instrucción lo que estoy haciendo es crear una nueva columna y ponerle el dato para crear la nueva columna implícitamente le estoy poniendo el nombre a esa columna justamente con lo que pongo entre los corchetes y entre comillas una vez que logro con estas dos instrucciones la conformación del Data frame auged para ver resultado bueno todo eso lo ejecutamos ahora y veo resultado Ven aquí tengo los nombres de cada una de las columnas los datos y finalmente el dato objetivo el target que me dice con un cero si esa persona no tuvo cáncer o con uno que esa persona tuvo cáncer bien Ahora vamos a ver lo que referenciamos recién como otra de las claves de este diccionario de claves de el conjunto de datos brest cáncer vamos a ver la descripción de características generales esto es una cuestión que ustedes pueden usar o no no es exigible pero les va a permitir conocer mejor justamente las características del Data lo que vamos a hacer es recurrir a Data punto desk fíjense que recurro nuevamente al nombre Cuando cargue el conjunto de datos Y una de las claves del diccionario es en este caso descom bien y hago un print para ver justamente la composición de la información de ese elemento fíjense que me muestra toda la información del conjunto de datos bien el nombre la cantidad de instancias ese tiene 569 observaciones los atributos bien la información de Qué significa cada uno de los atributos los valores mínimos y máximos de cada uno de esos atributos los atributos a las características no obviamente bien y alguna información más general de bueno Quién fue el que hizo este del origen digamos de este conjunto de datos no bien Por eso digo esa información general no siempre es necesaria toda sí es importante Por ejemplo esto que está arriba que tiene que ver con la cantidad de distancias y el deuto que también lo podemos ver con un shape y bueno de Qué significa cada uno de los de las características para poder entender justamente mejor la característica de los datos y poder justamente operar mejor con ellos bien una de las cosas nuevas que vamos a visualizar ahora es Cross el método de pandas Para qué sirve el método bueno sirve para poder reconocer la característica de los datos Por qué Porque yo aquí tengo obviamente un conjunto de datos que dijimos tienen un dato objetivo que es este que está aquí final que me dice si la persona tiene o tuvo cáncer lo que yo voy a hacer ahora es saber cuántas personas de este conjunto de datos tuvieron o no cancelan Qué hago una tabla Cruzada quien maneja Excel este concepto seguramente lo debe tener la tabla Cruzada se alimenta de Data punto Target es decir los datos objetivos el cero y el uno y lo que le digo es que como operación me cuente los datos de cada una de las columnas es decir tengo que contar Cuántos casos targets hay cero y Cuántos casos targets hay uno lo ejecuto y el resultado se muestra de esta manera en esta primera columna me muestra cero y uno que son los dos datos de tipo binario porque lo que estamos viendo justamente es un tipo binario que tiene solamente dos tipos de resultados y me muestra a continuación con el count sí que lo que yo le pedí aquí tiene cuente Cuántos casos hay 0 y Cuántos casos hay uno bueno con esto ya puedo tener una primera aproximación para entender características de los datos luego de esto lo que voy a hacer es empezar por separar nuevamente los datos por un lado y el target por otro parece que fuera contradictorio que recién nos uní y ahora los separo pero recuerden que justamente lo próximo que tengo que hacer empezar a desarrollar el algoritmo es entrenarlo y para entrenarlo Sí o sí tengo que tener separado por un lado los elementos x los elementos de entrada y por otro elemento de salida que es el así que bueno simplemente los datos que ya tengo los pongo en variables X e Y por una cuestión de nomenclatura tradicional puede ser el nombre que quieran Así que pongo en x los datos de entrada y en los datos de salida y luego hago lo que ya sabemos la separación en xtrain y x test los datos de entrada separados en conjunto de entrenamiento y conjunto de Test y lo vimos los datos de salida separado en itras es decir la salida para entrenamiento y la salida para test y Recuerden que siempre le pongo X e Y que son los datos que yo le doy para que se pare y el t6 que es el 20% que ya lo explicamos la clase pasada puede ser otro valor diferente Pero habitualmente se estipula en el 20% Recuerden que esto lo que hace es poner el 80% en el conjunto de entrenamiento y el 20% en el conjunto de Bueno ahora tenemos que hablar del escalamiento de datos pero la primer pregunta responder aquí es Por qué es importante escalar los datos o por qué es necesario escalar datos vamos a ir al Data set para tratar de entenderlo mejor fíjense que aquí tengo cada una de las características del Data frame del Data set Pero estas no son similares más bien que son todas numéricas que es lo que corresponde pero no son similares y en algunos casos la distancia desde lo numérico de cada una de ellas es muy importante fíjense Aquí tengo por ejemplo como perímetro la cifra 122.80 pero luego en el concepto la característica suavidad tengo un valor que está cuatro o cinco dígitos Perdón después de la coma es decir entre este valor o por ejemplo este del área que es mucho más significativo aún hay una distancia muy grande esto incide a la hora de generar el algoritmo Y si bien coincidir las características las características deben incidir en cuanto a la importancia o lo determinantes que son para el dato objetivo es decir concretamente en este caso ver si esta suavidad o esta área o este perímetro son relevantes para determinar si la persona tiene o no cáncer Pero eso no tiene que depender de la magnitud de las cifras sino de la importancia del dato Espero que se entienda a qué me refiero es decir yo necesito saber si es una persona puede tener cáncer o no en base a su textura su perímetro o su suavidad pero como dato y no en virtud de la magnitud de la cifra comparada con otra para que esto pueda ser más justo y tenga el peso cada característica que corresponde y no el peso dependa del nivel de cifra que manejan es que hay que escalar los datos es decir todos los datos tienen que estar encuadrados entre 0 y 1 eso va a ser de que la diferencia entre el valor de estos reduzca considerablemente aún así va a representar dentro de cada dato justamente un valor escalarmente igual es decir este valor que está aquí dentro del perímetro va a ser siempre menor a este y este va a ser siempre mayor a este y este va a ser siempre menor a este Pero dentro de una escala por eso volviendo al punto yo tengo que ahora como próximo paso escalar los datos después de haberlos separado qué datos tengo que escalar tengo que aclarar siempre los datos x los datos de entrada no los datos de salida los datos de salida siempre va a ser cero y uno Eso no lo voy a escalar si son los datos tal cual está son binarios 0 o 1 pero si lo que tengo escalar son el resto de los datos que son cada una de las características que vimos recién por eso lo primero que tengo que hacer es cargar el estándar escáner si creando un objeto escalar que le puedo poner obviamente como varia Cualquier nombre de variable una vez que tengo ese elemento yo lo que voy a hacer es decirle que el xtrain que ya tenía de antes cuando separé los datos entre entrenamiento y test es igual a esa variable punto fit transform del mismo XT es decir transformó el mismo conjunto de datos que traía escalando todos sus datos creando nuevo x-traime Pero ahora con datos escalados y lo mismo hago con el X test fíjense que en el caso de xtrain yo uso fit guión Bajos transform en el caso de XT uso Solamente porque porque justamente este fit tiene implicancia que en que justamente lo que voy a hacer a continuación que es entrenar el modelo lo hago con xtraing no con x test Por eso uso dos métodos de escalamiento diferente fit transform para los datos que van a ser utilizados para entrenar al modelo y transform para aquellos que simplemente me van a servir como ya sabemos para poder verificar deficiencia del modelo una vez entendido esto vamos entonces a crear nuestro modelo Crea una variable logic model el nombre que quieran como siempre lo digo igualando a logística integration que justamente es el tipo de algoritmo que vamos a hacer ahora una vez que le digo que voy a crear un la variable que es justamente para crear un modelo de regresión logística lo que hago es entrenar el modelo con fit como ya lo hicimos antes con la regresión lineal dándole como parámetros el x-3 y el itrain que son justamente los valores de entrenamiento así que me olvidé de ejecutar esto Perdón ejecutó primero el escalamiento y ahora generar modelo ya tengo el modelo con lo cual hacemos lo que sabemos hacer siempre verificar justamente la precisión de el modelo lo hago y tengo una muy buena precisión fíjense que tengo 0 98 en el modelo de tren y 097 el modelo de Test como ya dijimos no es lo ideal que la precisión del modelo de Test se inferior al modelo de tren por una posible posible problema de sobre ajuste que es un concepto que vimos en la parte teórica de esta misma clase la diferencia no es significativa es decir lo grave aquí es que justamente la presión del modelo de tren sea un número y el modelo test sea mucho menor a ese número no es el caso con lo cual si bien no es lo ideal la diferencia puede representar que el modelo que tenemos para este caso es un buen modelo Qué vamos a hacer a continuación vamos a hacer las predicciones para llegar a ver el concepto de matriz de confusión que Recuerda es una librería que importamos al principio de la clase con lo cual lo que voy a hacer es armar predicciones en a todo el conjunto de Test Sí ese 20% de datos lo voy a tomar los datos de ese 20% del conjunto los voy a tomar para ahora que tengo que el modelo ya generado establecer predicciones y una vez que tengan las predicciones hechas qué voy a hacer lo que venimos haciendo siempre comparar las predicciones con el valor y test que es el valor Real del conjunto de datos que determina el dato objetivo es decir yo voy a tener ahora un dato objetivo real que es el y test sí es este valor que está aquí y un dato objetivo que es producto de una predicción que puede ser igual o no al itest Bueno ahí está justamente la eficiencia del modelo por lo tanto lo primero que hago son estas dos líneas que justamente lo que me permiten es hacer la predicción del conjunto de Test y poner una variable ipred y finalmente lo que hago es la matriz de confusión que insisto es la librería que incorporamos al principio de la clase volvemos arriba por si no lo recordamos sí es esto que importe de cycle volvemos y luego voy a mostrar la matriz la matriz de confusión justamente me permite la comparación que hablábamos recién delites e iliped lo ejecutamos y seguimos hablando de ello Acá tengo la matriz de confusión tiene cuatro componentes y aquí yo he puesto una descripción para que traten de entender Qué significa cada uno de esos cuatro números en principio vértice superior izquierdo es decir el 41 que está aquí me de me Define me detalla la cantidad de valores que debían dar 0 y se predijeron como ceros en la cantidad de aciertos de casos de no cáncer es decir hubo 41 casos que en la realidad no fueron cáncer y en la predicción me dijo que eso no debía ser cáncer pero luego tengo a la derecha verde superior derecho es el 2 de aquí cantidad de valores que deben dar uno y se predijeron como 0 es decir cantidad de valores que fueron cáncer y la predicción falló dijo que fueron casos de no cáncer con lo cual tengo dos errores y luego tengo en la parte inferior izquierdo la cantidad de valor que debían dar cero y se predijeron como lo contrario al 2 de recién o sea un dato que demuestra una falla del algoritmo porque predijo uno es decir un caso de cáncer cuando en la realidad era un caso de no cáncer y finalmente el 70 son la cantidad de valores que debían dar uno y se predijeron como uno es decir otro acierto del algoritmo con lo cual la lectura aquí que voy a ser genérica de esto es que hay 41 + 70 es decir 111 casos que debía predecir algo y justamente Esa predicción fue correcta y tres casos el 2 y el 1 donde la predicción falló esto lo que me permite justamente con una visualización muy rápida reconocer la matriz de confusión que se suma a lo que yo veo aquí como presión de modelo veo con la presión es muy alta con lo cual el error es bastante bajo porque ustedes consideran la suma total de valores del conjunto es decir 70 más 41 111 más 3 114 sobre 114 datos sí o casos Mejor dicho hay solamente tres fallas habría que sacarlos justamente del mismo porcentuales y me va a dar un valor cercano a esto es decir me tiene que dar un valor de un error del 0,1 perdón cercano al 97 al 98 porque estamos hablando del conjunto de 0,03% de error sobre el total de datos bien Esto es a partir de ahora y más que nada para los modelos de regresión logística una buena herramienta para poder medir la precisión del modelo Bueno ahora vamos con el segundo ejemplo de la práctica en donde justamente como dice que el título tenemos que ver el algoritmo de regresión logística pero con salida discreta no binaria si ya no un caso de cero o unos de una persona que tiene cáncer o no como el ejemplo recién sino en este caso una situación que nos va a dar tres resultados posibles bien antes de describir los datos que vamos a usar vamos a empezar como siempre con las librerías que vamos a importar en este caso tenemos pandas warnings y yo y además tenemos por un lado logística recreation estándar Scanner 30 splits y confusión Matrix que son las cuatro mismas que usamos en el ejemplo por eso este ejercicio va a ser bastante rápido porque muchos pasos se van a repetir y va a haber otra cuestión que también va a acelerar este ejercicio que es que vamos a volver a trabajar con un set de datos proveniente de un archivo de tipo csv tal cual lo veníamos haciendo en los casos anteriores de las clases anteriores Así que empecemos con eso importamos las librerías vamos ahora a importar el archivo y así tenemos el archivo usuarios Win Mac link ssd bien ahí ya está importado de qué se trata este Data set bueno lo vamos a averiguar cuando lo carguemos como siempre con ricsb de pandas a la variable de datos y hacemos un dato Head aquí lo ejecutamos y vemos un conjunto de datos tal cual lo que yo puse aquí arriba es un archivo csv con datos de los sistemas operativos utilizados por los usuarios donde la variable objetivo o Target es justamente la característica clase clase describe el tipo de sistema operativo que usa el usuario teniendo la codificación puesta de modo de que 0 representa Windows 1 Mac y dos Linux en este caso tenemos otras cuatro características con las cuales justamente vamos a ir buscando el mejor algoritmo posible bien visto esto podemos hacer un shape para reconocer que ese conjunto de datos tiene observaciones y como estamos viendo aquí justamente cinco características dentro de la que describimos recién como objetivo es la clase bien Vamos a hacer una tabla Cruzada como hicimos en el ejemplo de recién también repetimos la experiencia solamente que ahora tenemos tres resultados posibles antes Teníamos dos que era 01 en este caso tenemos 0 1 o 2 bien entonces con la tabla Cruzada y volviendo a hacer la operación de conteo nos dice que este Data se tiene 86 casos de sistema operativo Windows 40 Mac y 44 Linux a continuación voy a separar los datos en x y en y tal cual lo hice en el ejemplo anterior por lo tanto voy a tomar para el caso de X los datos con las características duración páginas y valor y para la y la variable objetivo clase bien una vez que tengo separados los datos lo ejecutamos voy a separar los datos en entrenamiento y test Aquí no hay nada diferente a lo que hicimos antes tampoco hay nada diferente respecto del escalamiento de datos Exactamente lo mismo que en el caso recién y ahora aparece la primera diferencia por un lado Cuando creo el modelo de agresión logística a través de logística generation y esta variable que estoy creando Aquí estos paréntesis antes estaban vacíos ahora va a tener dos parámetros por un lado múltiplas igual a multinomial es para decirle que este modelo no es de tipo binario que es de tipo multiplace es decir que tiene tres o más posibles salidas y con Max iter está escrito aquí arriba es el número máximo de iteraciones necesarias para que los solucionadores converjan una definición muy compleja para ver nuestra altura Vamos a darle una explicación muy sencilla y después si la clase que viene le vamos a dar una explicación más profunda en la introducción teórica de la clase que viene todos los modelos que trabajan buscando el mejor algoritmo lo hacen reiterando a través de procesos de entrenamiento un proceso que hace que justamente vaya mejorando y la solución se logre de la manera más rápidamente posible la solución óptima se logre de la manera más rápidamente posible esto puede darse en una determinada cantidad de ciclos que justamente puede ser que sea un valor muy alto o muy bajo pero que justamente con Max híper le decimos quiero que hagas hasta mil intentos para lograr la mejor solución si al intento número 1000 no hay una mejor solución me tomo Esa sí antes de eso existe una mejor solución No voy a llegar a esa cantidad de intentos sí de iteración de reiteraciones Entonces esto es para qué Para que no vayamos buscando a través de una cantidad ilimitada de iteración y esto nos lleve a un gasto computacional muy alto en Pos de lograr un buen algoritmo entonces este parámetro indica eso por ahora nos vamos a crear con esta aplicación sencilla la clase que viene lo vamos a ver en profundidad y vamos a ver el concepto de optimizador que tiene que ver justamente con este argumento luego justamente hacemos el entrenamiento que sí es igual a lo que hicimos antes así que bueno avancemos creado el modelo medimos su presión y vemos la presión del modelo es bastante pobre 69% en el modelo de entrenamiento 64 en el modelo de Test no es lo ideal ya la diferencia es más grande que en el caso anterior Por lo cual no es un buen modelo vamos a hacer las predicciones como lo hicimos antes las podemos ver en este caso justamente poniendo ipred me muestra la raid fíjense todo lo que predijo tomando como conjunto de entrada el conjunto de Test se hizo todas predicciones No hay ninguna predicción 1 fíjense que son todas es decir Windows o Linux No hay ninguna opción de Mac sí bien hacemos ahora la matriz de confusión que se acuerdan que era aquello que nos permitía también ver la precisión del modelo al igual que score de manera complementaria corporación de alguna manera y vemos la materia de confusión que en este caso no es una matriz dos por dos sino tres por tres obviamente porque tengo tres tipos de salida diferentes con lo cual como Leo yo aquí Cuáles son las precisiones correctas y las incorrectas bueno mirando la diagonal principal en este caso 15 predicciones correctas respecto de en la situación cero o la salida cero cero ese ninguna fue correcta en el caso de el tipo de salida Mac lo cual no me parece lógico porque justamente estamos viendo acá que no hay ninguna que sea Mac son todas Windows o Linux y 7 correctas en el caso de Linux fíjese que en el caso de Linux las 7 son las siete que había es decir no se equivocó en ningún caso en el caso de Windows 15 fueron correctas pero 7 fueron incorrectas y en el caso de Mac evidentemente es muy mala la precisión por eso no es de extrañarse el resultado visto los números de score que vimos aquí arriba son muy bajos con lo cual tengo una posibilidad en el conjunto de test del 36% de equivocarme y justamente se haga un cálculo matemático respecto de la cantidad total de elemento de test y la cantidad de errores me tiene que dar ese indicado vamos a hacer una prueba ahora sin escalar los datos como dice el título aquí y agregando un solver Qué es un solver Bueno un poco es esta cuestión que referenciamos recién cuando hablábamos de los optimizadores que me permiten a mí lograr un mejor algoritmo bien es optimizadores hay varios y se pueden ir cambiando esto Ya es empezar a lo que se conoce habitualmente en la guerra como tuneo de El algoritmo cuando voy justamente buscando el mejor o la mejor versión en cada uno de los entrenamientos en cada uno de los intentos de la mejor algo bien acá tenemos un primer ejemplo en el caso anterior también con el Max iter también es un tuneo digamos de este los algoritmos y aquí en este caso voy a cambiar no solamente el Max Inter pasándolo de mil a 5.000 es ir ampliando la la espera digamos la tolerancia de cantidad de intentos para lograr el mejor sino también le voy a cambiar el solver ya tiene un solver por defecto la regresión logística que es lo que está escrito aquí arriba lbfs pero eso también se puede cambiar buscando que a través de otros solucionador o a través de otro optimizador podamos lograr un mejor algoritmo con un mejor score en este caso hay varios tenemos Newton saga y sag además de este que es el más conocido o el que está por defecto que no hace falta ponerlo por supuesto ya está implícito y en este caso vamos a usar el Newton porque es el que me va a dar el mejor score pero ustedes si quieren pueden escribiendo ese aga o ese ag cambiar este parámetro acá y probar por sus propios medios justamente Cuál es la precisión que le va a dar bien Luego de eso hago el del Fit como siempre y mido la precisión obviamente anterior a eso hice justamente la separación de los valores y la separación de los conjuntos también de entrenamiento y test ejecuto todo tengo todo hecho en una sola en un solo conjunto de instrucciones no hay ningún problema con eso lo puedo hacer separado o todo junto bien en este caso veo que el nivel de score es muy superior al anterior tenemos 76 y 85 y algo muy bueno en score de Test es mejor que el entrenamiento lo cual es justamente una de las cosas que más deseables son a la hora de buscar un mejor un buen algoritmo no bien y luego vamos a hacer las predicciones en la misma expresión el mismo conjunto de instrucciones hacemos también la matriz de confusión fíjense que la matriz de confusión es mejor que la anterior en el caso de Linux sigue siendo muy bueno en el caso de Windows creo si no me equivoco Sí fue superior era 15 y 7 ahora es 16 y 3 y fíjense que en el caso de Mac que no había aparecido nada hoy justamente ahora aparece un 6 cuando Antes había un cero con lo cual la diagonal principal me muestra 16 + 6 son 22 y 729 casos de predicción exitosa mientras aquí tenía 22 obviamente todo responde al nivel de presión si estoy a 64 antes y ahora tengo 85 bueno evidentemente se tiene que ver también reflejado en la matriz de confusión bien hasta aquí esta clase quedó pendiente seguramente en la cabeza de todos ustedes este concepto del solver y del Max iter no se hagan problema en la clase vamos a hacer una introducción teórica donde vamos a hablar de este tema y lo vamos a hacer con gráficos Sí con animaciones que seguramente nos van a permitir entender esto mucho más fácilmente que aquí en el código por eso es que les pedí la licencia de darle una explicación muy breve muy superficial para después profundizar en la clase que viene así que nos vemos en la clase que viene hemos terminado la clase número 7 ahora ya sabemos manejar dos algoritmos la regresión lineal y la regresión logística nos vemos en la próxima clase para aprender un nuevo algoritmo de Machine learning hasta entonces [Música]